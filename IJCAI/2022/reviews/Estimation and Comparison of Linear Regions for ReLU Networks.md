
## Estimation and Comparison of Linear Regions for ReLU Networks
Yuan Wang
Keywords: Machine Learning: Learning Theory, Machine Learning: Optimisation, Machine Learning: Theory of Deep Learning
IJCAI/2022/Proceedings/0492 - Estimation and Comparison of Linear Regions for ReLU Networks.pdf

### Implementation
_Given the documentation given by the authors on the method, how much time investment would it be to re-implement the method from scratch?_

[9]

The authors do not publish their implementation. The authors state they 'follow' tensorflow playground as a framework in 4.2. 

### Data
_Given the data description in the documentation, how much effort take to either: Find the same dataset the authors used, or similar datasets and defend the comparability, or acquire one from scratch?_

[1]

(1/1)

The authors simulate data by randomly initialised NN weights and simulate data by a specified formula.

### Configuration 
_Given the (hyper)parameters, including semantic parameters, of the method: How much effort would it take to acquire the algorithm configurations used for their results, and compare against their budgetary constraints?_

[1]

The authors measure neural network cell activation for ReLU in two synthetic settings. The various architectures are specified.

### Experimental Procedure
_Given the experimental set-up of the work, how difficult is it to set up a new experiment, similar to those presented in the original work, with the same procedure?_

[1]

The authors state how they split the data for exerpiment 2 into training and test data. They measure the effect over various architectures and use accuracy range as a metric with mean and deviation. 

### Expertise
_How much effort would it take to acquire the expertise required to reproduce the work independently relying on the available documentation?_

[6]

Requires experience in NN and optimisation techniques, especially to grasp the experiments and analysis presented.
