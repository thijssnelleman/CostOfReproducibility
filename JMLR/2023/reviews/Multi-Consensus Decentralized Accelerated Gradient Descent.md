
## Multi-Consensus Decentralized Accelerated Gradient Descent
Haishan Ye, Luo Luo, Ziang Zhou, Tong Zhang
Keywords: 
JMLR/2023/Proceedings/221210 - Multi-Consensus Decentralized Accelerated Gradient Descent.pdf
Project URL: nan

### Implementation
_Given the documentation given by the authors on the method, how much time investment would it be to re-implement the method from scratch?_

[10]

The authors do not provide their implementation.

### Data
_Given the data description in the documentation, how much effort take to either: Find the same dataset the authors used, or similar datasets and defend the comparability, or acquire one from scratch?_

[4]

(1/1)

The authors use the real-world dataset 'a9a' and provide a citations but no link. No other details.

### Configuration 
_Given the (hyper)parameters, including semantic parameters, of the method: How much effort would it take to acquire the algorithm configurations used for their results, and compare against their budgetary constraints?_

[1]

The authors define the logistic regression function in equetion 31. The authors conduct experiments with various values of parameter sigma. 

### Experimental Procedure
_Given the experimental set-up of the work, how difficult is it to set up a new experiment, similar to those presented in the original work, with the same procedure?_

[1]

The authors run the experiments with m = 100 agents. The experiments results are presented over the number of gradient computations and a formula as metric on the y-axis. The authors state the settings of their networks in 6.1. Aggregation and variance not applicable, neither data split.

### Expertise
_How much effort would it take to acquire the expertise required to reproduce the work independently relying on the available documentation?_

[8]

Requires expertise on gradient descent.
