## Visual Data-Type Understanding does not emerge from scaling Vision-Language Models
Vishaal Udandarao, Max F. Burg, Samuel Albanie, Matthias Bethge
Keywords: 
ICLR/2024/Proceedings/18460 - Visual Data-Type Understanding does not emerge from scaling Vision-Language Models.pdf
Project URL: nan

### Implementation
_Given the documentation shared by the authors on a new method, how much effort would it be to re-implement the method from scratch?_

[10]

The authors provide a link to their implementation in the reproducibility statement (https://github.com/bethgelab/DataTypeIdentification). Repository only has a readme and a message stating "Stay tuned! Code and datasets will be updated soon!".

### Data
_Given the data description in the documentation, how much effort would it take to either: Find the same data set the authors used, or a similar data set and defend the comparability, or acquire one from scratch?_

[5]

(0/2)

The authors use 27 datasets to construct theirs and list them in appendix A table 3. Descriptions per dataset there and in section 3. Datasets link is empty.

### Configuration 
_Given the (hyper)parameters, including semantic parameters, of the method: How much effort would it take to acquire the algorithm configurations used for obtaining the reported results, and compare them against their computation budget?_

[1]

The authors benchmark pretrained models on their dataset.

### Experimental Procedure
_Given the setup of experiments reported in the work, how difficult is it to set up a new experiment with the same procedure, similar to those presented in the original work?_

[1]

Authors measure informedness, defined in 4.1. Authors present boxplots across models per class. 

### Expertise
_How much effort would it take to acquire the expertise required to reproduce the work independently relying solely on the available documentation?_

[6]

-
