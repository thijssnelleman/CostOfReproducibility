## Mixture of LoRA Experts
xun wu, Shaohan Huang, Furu Wei
Keywords: 
ICLR/2024/Proceedings/17571 - Mixture of LoRA Experts.pdf
Project URL: nan

### Implementation
_Given the documentation shared by the authors on a new method, how much effort would it be to re-implement the method from scratch?_

[9]

Authors provide implementation link in the abstract (https://github.com/yushuiwx/MoLE). Link gives a 404. Overview in figure 1/2.

### Data
_Given the data description in the documentation, how much effort would it take to either: Find the same data set the authors used, or a similar data set and defend the comparability, or acquire one from scratch?_

[5]

(6/6)

ANLI-R1 ANLI-R2 ANLI-R3 QNLI WNLI, Big-Bench Hard, not all cited.

### Configuration 
_Given the (hyper)parameters, including semantic parameters, of the method: How much effort would it take to acquire the algorithm configurations used for obtaining the reported results, and compare them against their computation budget?_

[6]

A few HP values stated in 4.1. but not clear if all are there. No overview or acquisition.

### Experimental Procedure
_Given the setup of experiments reported in the work, how difficult is it to set up a new experiment with the same procedure, similar to those presented in the original work?_

[1]

Metrics stated in 4.1. Results are single run. Authors train on all datasets except BBH and evaluate on this.

### Expertise
_How much effort would it take to acquire the expertise required to reproduce the work independently relying solely on the available documentation?_

[7]

-
