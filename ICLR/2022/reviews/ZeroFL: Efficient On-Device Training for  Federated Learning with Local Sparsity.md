## ZeroFL: Efficient On-Device Training for  Federated Learning with Local Sparsity
Xinchi Qiu, Javier Fernandez-Marques, Pedro Porto Buarque de Gusmao, Yan Gao, Titouan Parcollet, Nicholas Lane
Keywords: 
ICLR/2022/Proceedings/6036 - ZeroFL: Efficient On-Device Training for  Federated Learning with Local Sparsity.pdf
Project URL: nan

### Implementation
_Given the documentation shared by the authors on a new method, how much effort would it be to re-implement the method from scratch?_

[10]

The authors do not provide their implementation. 

### Data
_Given the data description in the documentation, how much effort would it take to either: Find the same data set the authors used, or a similar data set and defend the comparability, or acquire one from scratch?_

[4]

(3/3)

The authors use CIFAR-10, FEMNIST and SpeechCommands. Little information is given on the datasets but citations are provided.

### Configuration 
_Given the (hyper)parameters, including semantic parameters, of the method: How much effort would it take to acquire the algorithm configurations used for obtaining the reported results, and compare them against their computation budget?_

[2]

The authors state the HP values in text in appendix A.6. No structured overview. As the authors use these methods as input for their own, rather than configuration of their own method.

### Experimental Procedure
_Given the setup of experiments reported in the work, how difficult is it to set up a new experiment with the same procedure, similar to those presented in the original work?_

[3]

The authors measure IID, non-IID and test accuracy as wel as file size. Aggregation/variation not clear. Data split is static from the datasets as specified in appendix A.5. 

### Expertise
_How much effort would it take to acquire the expertise required to reproduce the work independently relying solely on the available documentation?_

[5]

-
