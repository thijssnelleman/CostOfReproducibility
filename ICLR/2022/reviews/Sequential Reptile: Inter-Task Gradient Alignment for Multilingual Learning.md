## Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning
Seanie Lee, Hae Beom Lee, Juho Lee, Sung Ju Hwang
Keywords: 
ICLR/2022/Proceedings/6199 - Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning.pdf
Project URL: nan

### Implementation
_Given the documentation shared by the authors on a new method, how much effort would it be to re-implement the method from scratch?_

[10]

The authors provide pseudo code in algorithm 1. No other details given.

### Data
_Given the data description in the documentation, how much effort would it take to either: Find the same data set the authors used, or a similar data set and defend the comparability, or acquire one from scratch?_

[2]

(4/4)

The authors conduct a synthetic experiment, three local optima in a 2d space. They also conduct experiments on TYDI-QA, WikiAnn, MLQA, provide descriptions and citations. A link to TYDI-QA is given. More details would be welcome.

### Configuration 
_Given the (hyper)parameters, including semantic parameters, of the method: How much effort would it take to acquire the algorithm configurations used for obtaining the reported results, and compare them against their computation budget?_

[4]

The authors state hyperparameter values in 4.2 under implementation details in text. Structured overview an acquisition missing.

### Experimental Procedure
_Given the setup of experiments reported in the work, how difficult is it to set up a new experiment with the same procedure, similar to those presented in the original work?_

[1]

The authors measure F1 and EM score and data splits are given in table 6 and 7. Results are single run.

### Expertise
_How much effort would it take to acquire the expertise required to reproduce the work independently relying solely on the available documentation?_

[6]

-
