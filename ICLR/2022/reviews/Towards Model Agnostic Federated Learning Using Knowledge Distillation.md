## Towards Model Agnostic Federated Learning Using Knowledge Distillation
Andrei Afonin, Sai Karimireddy
Keywords: 
ICLR/2022/Proceedings/6644 - Towards Model Agnostic Federated Learning Using Knowledge Distillation.pdf
Project URL: nan

### Implementation
_Given the documentation shared by the authors on a new method, how much effort would it be to re-implement the method from scratch?_

[10]

The authors do not provide their implementation. 

### Data
_Given the data description in the documentation, how much effort would it take to either: Find the same data set the authors used, or a similar data set and defend the comparability, or acquire one from scratch?_

[5]

(1/1)

The authors use MNIST but provide no information on the dataset.

### Configuration 
_Given the (hyper)parameters, including semantic parameters, of the method: How much effort would it take to acquire the algorithm configurations used for obtaining the reported results, and compare them against their computation budget?_

[4]

The authors state the hyperparameter values in text in appendix A. No acquisition or structured overview.

### Experimental Procedure
_Given the setup of experiments reported in the work, how difficult is it to set up a new experiment with the same procedure, similar to those presented in the original work?_

[2]

The authors present the results on the test set of MNIST, although it is implied rather than stated that they use the static test set of MNIST. Metrics are accuracy of single runs over multiple rounds. 

### Expertise
_How much effort would it take to acquire the expertise required to reproduce the work independently relying solely on the available documentation?_

[5]

-
