eid,doi,pii,pubmed_id,title,subtype,subtypeDescription,creator,afid,affilname,affiliation_city,affiliation_country,author_count,author_names,author_ids,author_afids,coverDate,coverDisplayDate,publicationName,issn,source_id,eIssn,aggregationType,volume,issueIdentifier,article_number,pageRange,description,authkeywords,citedby_count,openaccess,freetoread,freetoreadLabel,fund_acr,fund_no,fund_sponsor
2-s2.0-85141235454,10.1609/aaai.v36i11.21470,,,'Beach' to 'Bitch': Inadvertent Unsafe Transcription of Kids' Content on YouTube,cp,Conference Paper,Ramesh K.,60001777;60028079;60104532,Rochester Institute of Technology;Manipal Institute of Technology;Indian School of Business,Rochester;Manipal;Hyderabad,United States;India;India,3.0,"Ramesh, Krithika;Khudabukhsh, Ashiqur R.;Kumar, Sumeet",57213197420;36835705200;57213855653,60028079;60001777;60104532,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,12108-12118,"Over the last few years, YouTube Kids has emerged as one of the highly competitive alternatives to television for children's entertainment. Consequently, YouTube Kids' content should receive an additional level of scrutiny to ensure children's safety. While research on detecting offensive or inappropriate content for kids is gaining momentum, little or no current work exists that investigates to what extent AI applications can (accidentally) introduce content that is inappropriate for kids. In this paper, we present a novel (and troubling) finding that well-known automatic speech recognition (ASR) systems may produce text content highly inappropriate for kids while transcribing YouTube Kids' videos. We dub this phenomenon as inappropriate content hallucination. Our analyses suggest that such hallucinations are far from occasional, and the ASR systems often produce them with high confidence. We release a first-of-its-kind data set of audios for which the existing state-of-the-art ASR systems hallucinate inappropriate content for kids. In addition, we demonstrate that some of these errors can be fixed using language models.",,10,1.0,all publisherfullgold,All Open Access Gold,ISB,,International Society of Biomechanics
2-s2.0-85163187687,,,,3DB: A Framework for Debugging Computer Vision Models,cp,Conference Paper,Leclerc G.,60022195;60021726,Massachusetts Institute of Technology;Microsoft Research,Cambridge;Redmond,United States;United States,12.0,"Leclerc, Guillaume;Salman, Hadi;Ilyas, Andrew;Vemprala, Sai;Engstrom, Logan;Vineet, Vibhav;Xiao, Kai;Zhang, Pengchuan;Santurkar, Shibani;Yang, Greg;Kapoor, Ashish;Mądry, Aleksander",56853237600;57218718431;57204781168;56732900600;57204801909;24825459900;57210636103;57193832213;56841541500;57200558199;36856640700;24171757000,60022195;60022195;60022195;60021726;60022195;60021726;60022195;60021726;60022195;60021726;60021726;60022195,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"We introduce 3DB: an extendable, unified framework for testing and debugging vision models using photorealistic simulation. We demonstrate, through a wide range of use cases, that 3DB allows users to discover vulnerabilities in computer vision systems and gain insights into how models make decisions. 3DB captures and generalizes many robustness analyses from prior work, and enables one to study their interplay. Finally, we find that the insights generated by the system transfer to the physical world. We are releasing 3DB as a library alongside a set of examples, guides, and documentation.",,9,0.0,,,NSF,FA8750-19-2-1000,National Science Foundation
2-s2.0-85195298105,,,,A Bayesian Bradley-Terry model to compare multiple ML algorithms on multiple data sets,ar,Article,Wainer J.,60029570,Universidade Estadual de Campinas,Campinas,Brazil,1.0,"Wainer, Jacques",7004361186,60029570,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"This paper presents a Bayesian model, called the Bayesian Bradley Terry (BBT) model, for comparing multiple algorithms on multiple data sets based on any metric. The model is an extension of the Bradley Terry model, which tracks the number of wins each algorithm has on different data sets. Unlike frequentist methods such as Demsar tests on mean rank or multiple pairwise Wilcoxon tests, the Bayesian approach provides a more nuanced understanding of the algorithms’ performance and allows for the definition of the “region of practical equivalence” (ROPE) for two algorithms. Additionally, the paper introduces the concept of “local ROPE,” which assesses the significance of the difference in mean measure between two algorithms using effect sizes, and can be applied in frequentist approaches as well. Both an R package and a Python program implementing the BBT are available for use.",Bayesian | Bradley-Terry model | Comparison of classifiers | Comparison of regressors | Multiple algorithms | Multiple data sets,4,0.0,,,,,
2-s2.0-85140356073,,,,A CONDITIONAL POINT DIFFUSION-REFINEMENT PARADIGM FOR 3D POINT CLOUD COMPLETION,cp,Conference Paper,Lyu Z.,60030612;60005510;60002798;60280914,"University of California, San Diego;Nanyang Technological University;Chinese University of Hong Kong;Shanghai Artificial Intelligence Laboratory",La Jolla;Singapore City;Hong Kong;Shanghai,United States;Singapore;Hong Kong;China,5.0,"Lyu, Zhaoyang;Kong, Zhifeng;Xu, Xudong;Pan, Liang;Lin, Dahua",57215304257;57219524596;57215771538;57217199511;8662530900,60002798-60280914;60030612;60002798;60005510;60002798-60280914,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"3D point cloud is an important 3D representation for capturing real world 3D objects. However, real-scanned 3D point clouds are often incomplete, and it is important to recover complete point clouds for downstream applications. Most existing point cloud completion methods use Chamfer Distance (CD) loss for training. The CD loss estimates correspondences between two point clouds by searching nearest neighbors, which does not capture the overall point density distribution on the generated shape, and therefore likely leads to non-uniform point cloud generation. To tackle this problem, we propose a novel Point Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The CGNet uses a conditional generative model called the denoising diffusion probabilistic model (DDPM) to generate a coarse completion conditioned on the partial observation. DDPM establishes a one-to-one pointwise mapping between the generated point cloud and the uniform ground truth, and then optimizes the mean squared error loss to realize uniform generation. The RFNet refines the coarse output of the CGNet and further improves quality of the completed point cloud. Furthermore, we develop a novel dual-path architecture for both networks. The architecture can (1) effectively and efficiently extract multi-level features from partially observed point clouds to guide completion, and (2) accurately manipulate spatial locations of 3D points to obtain smooth surfaces and sharp details. Extensive experimental results on various benchmark datasets show that our PDR paradigm outperforms previous state-of-the-art methods for point cloud completion. Remarkably, with the help of the RFNet, we can accelerate the iterative generation process of the DDPM by up to 50 times without much performance drop.",,46,0.0,,,CUHK,14205719,Chinese University of Hong Kong
2-s2.0-105018672015,,,,A Comparison of Continuous-Time Approximations to Stochastic Gradient Descent,ar,Article,Ankirchner S.,60029507,Friedrich-Schiller-Universität Jena,Jena,Germany,2.0,"Ankirchner, Stefan;Perko, Stefan",8330011500;58779230000,60029507;60029507,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Applying a stochastic gradient descent (SGD) method for minimizing an objective gives rise to a discrete-time process of estimated parameter values. In order to better understand the dynamics of the estimated values, many authors have considered continuous-time approximations of SGD. We refine existing results on the weak error of first-order ODE and SDE approximations to SGD for non-infinitesimal learning rates. In particular, we explicitly compute the linear term in the error expansion of gradient flow and two of its stochastic counterparts, with respect to a discretization parameter h. In the example of linear regression, we demonstrate the general inferiority of the deterministic gradient flow approximation in comparison to the stochastic ones, for batch sizes which are not too large. Further, we demonstrate that for Gaussian features an SDE approximation with state-independent noise (CC) is preferred over using a state-dependent coefficient (NCC). The same comparison holds true for features of low kurtosis or large batch sizes. However, the relationship reverses for highly leptokurtic features or small batch sizes.",gradient flow | stochastic differential equation | stochastic gradient descent | Talay-Tubaro expansion | weak approximation,4,0.0,,,,,
2-s2.0-85189355818,10.1609/aaai.v38i9.28926,,,A Compiler for Weak Decomposable Negation Normal Form,cp,Conference Paper,Illner P.,60016605,Charles University,Prague,Czech Republic,2.0,"Illner, Petr;Kučera, Petr",58968477600;56398611900,60016605;60016605,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,9.0,,10562-10570,"This paper integrates weak decomposable negation normal form (wDNNF) circuits, introduced by Akshay et al. in 2018, into the knowledge compilation map. This circuit type generalises decomposable negation normal form (DNNF) circuits in such a way that they allow a restricted form of sharing variables among the inputs of a conjunction node. We show that wDNNF circuits have the same properties as DNNF circuits regarding the queries and transformations presented in the knowledge compilation map, whilst being strictly more succinct than DNNF circuits (that is, they can represent Boolean functions compactly). We also present and evaluate a knowledge compiler, called Bella, for converting CNF formulae into wDNNF circuits. Our experiments demonstrate that wDNNF circuits are suitable for configuration instances.",,1,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85204296808,,,,A Conflict-Embedded Narrative Generation Using Commonsense Reasoning,cp,Conference Paper,Song Y.,60007511;60017776;129472283;131699639,Sungkyunkwan University;Hongik University;NAVER Cloud;NCSoft,Seoul;Seoul;St Cloud;,South Korea;South Korea;United States;,6.0,"Song, Youngrok;Cho, Gunhee;Kim, Hyun Ju;Kim, Youngjune;Bae, Byung Chull;Cheong, Yun Gyung",57203138971;59332875000;57216710793;57388649400;25960703000;15055378700,60007511;60007511;129472283;131699639;60017776;60007511,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,7744-7752,"Conflict is a critical element in the narrative, inciting dramatic tension. This paper introduces CNGCI (Conflict-driven Narrative Generation through Commonsense Inference), a neuro-symbolic framework designed to generate coherent stories embedded with conflict using commonsense inference. Our framework defines narrative conflict by leveraging the concept of a soft causal threat, where conflict serves as an obstacle that reduces the likelihood of achieving the protagonist's goal by weakening the causal link between context and goal through defeasible inference. Comparative studies against multiple story generation baselines utilizing commonsense reasoning show that our framework outperforms the baselines in creating narratives that distinctly embody conflict while maintaining coherency.",,1,0.0,,,NRF,,National Research Foundation of Korea
2-s2.0-85168235896,10.1609/aaai.v37i7.26017,,,A Data Source for Reasoning Embodied Agents,cp,Conference Paper,Lanchantin J.,60355330,Meta Ai,Menlo Park,United States,6.0,"Lanchantin, Jack;Sukhbaatar, Sainbayar;Synnaeve, Gabriel;Sun, Yuxuan;Srinet, Kavya;Szlam, Arthur",57191491253;55669357500;36987111200;57219469776;57192113924;11539079600,60355330;60355330;60355330;60355330;60355330;60355330,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,8438-8446,"Recent progress in using machine learning models for reasoning tasks has been driven by novel model architectures, large-scale pre-training protocols, and dedicated reasoning datasets for fine-tuning. In this work, to further pursue these advances, we introduce a new data generator for machine reasoning that integrates with an embodied agent. The generated data consists of templated text queries and answers, matched with world-states encoded into a database. The world-states are a result of both world dynamics and the actions of the agent. We show the results of several baseline models on instantiations of train sets. These include pre-trained language models fine-tuned on a text-formatted representation of the database, and graph-structured Transformers operating on a knowledge-graph representation of the database. We find that these models can answer some questions about the world-state, but struggle with others. These results hint at new research directions in designing neural reasoning models and database representations. Code to generate the data and train the models will be released at github.com/facebookresearch/neuralmemory.",,7,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85180933541,10.1613/JAIR.1.14388,,,"A General Model for Aggregating Annotations Across Simple, Complex, and Multi-Object Annotation Tasks",ar,Article,Braylan A.,60013372;60076757;60150459;60116471,"The University of Texas at Austin;Amazon.com, Inc.;Department of Computer Science;McCombs School of Business",Austin;Seattle;Austin;Austin,United States;United States;United States;United States,4.0,"Braylan, Alexander;Marabella, Madalyn;Alonso, Omar;Lease, Matthew",57188996898;58786951500;23011639800;13005498000,60150459;60116471;60076757;60013372,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,901-973,"Human annotations are vital to supervised learning, yet annotators often disagree on the correct label, especially as annotation tasks increase in complexity. A common strategy to improve label quality is to ask multiple annotators to label the same item and then aggregate their labels. To date, many aggregation models have been proposed for simple categorical or numerical annotation tasks, but far less work has considered more complex annotation tasks, such as those involving open-ended, multivariate, or structured responses. Similarly, while a variety of bespoke models have been proposed for specific tasks, our work is the first we are aware of to introduce aggregation methods that generalize across many, diverse complex tasks, including sequence labeling, translation, syntactic parsing, ranking, bounding boxes, and keypoints. This generality is achieved by applying readily available task-specific distance functions, then devising a task-agnostic method to model these distances between labels, rather than the labels themselves. This article presents a unified treatment of our prior work on complex annotation modeling and extends that work with investigation of three new research questions. First, how do complex annotation task and dataset properties impact aggregation accuracy? Second, how should a task owner navigate the many modeling choices in order to maximize aggregation accuracy? Finally, what tests and diagnoses can verify that aggregation models are specified correctly for the given data? To understand how various factors impact accuracy and to inform model selection, we conduct large-scale simulation studies and broad experiments on real, complex datasets. Regarding testing, we introduce the concept of unit tests for aggregation models and present a suite of such tests to ensure that a given model is not mis-specified and exhibits expected behavior. Beyond investigating these research questions above, we discuss the foundational concept and nature of annotation complexity, present a new aggregation model as a conceptual bridge between traditional models and our own, and contribute a new general semi-supervised learning method for complex label aggregation that outperforms prior work.",,3,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,NSF,1253413,National Science Foundation
2-s2.0-85189298576,10.1609/aaai.v38i9.28937,,,A General Theoretical Framework for Learning Smallest Interpretable Models,cp,Conference Paper,Ordyniak S.,60012070;60018163,University of Leeds;TU Wien,Leeds;Vienna,United Kingdom;Austria,4.0,"Ordyniak, Sebastian;Paesani, Giacomo;Rychlicki, Mateusz;Szeider, Stefan",25960321900;57203852339;57701660300;6602786002,60012070;60012070;60012070;60018163,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,9.0,,10662-10669,"We develop a general algorithmic framework that allows us to obtain fixed-parameter tractability for computing smallest symbolic models that represent given data. Our framework applies to all ML model types that admit a certain extension property. By establishing this extension property for decision trees, decision sets, decision lists, and binary decision diagrams, we obtain that minimizing these fundamental model types is fixed-parameter tractable. Our framework even applies to ensembles, which combine individual models by majority decision.",,4,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85147540801,10.1609/aaai.v36i10.21405,,,A Graph Convolutional Network with Adaptive Graph Generation and Channel Selection for Event Detection,cp,Conference Paper,Xie Z.,60009860,Fudan University,Shanghai,China,2.0,"Xie, Zhipeng;Tu, Yumin",57198628049;58093767500,60009860;60009860,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,11522-11529,"Graph convolutional networks have been successfully applied to the task of event detection. However, existing works rely heavily on a fixed syntactic parse tree structure from an external parser. In addition, the information content extracted for aggregation is determined simply by the (syntactic) edge direction or type but irrespective of what semantics the vertices have, which is somewhat rigid. With this work, we propose a novel graph convolutional method that combines an adaptive graph generation technique and a multi-channel selection strategy. The adaptive graph generation technique enables the gradients to pass through the graph sampling layer by using the ST-Gumbel-Softmax trick. The multi-channel selection strategy allows two adjacent vertices to automatically determine which information channels to get through for information extraction and aggregation. The proposed method achieves the state-of-the-art performance on ACE2005 dataset.",,15,1.0,all publisherfullgold,All Open Access Gold,NSFC,62076072,National Natural Science Foundation of China
2-s2.0-85163090663,,,,A Hierarchical Transitive-Aligned Graph Kernel for Un-attributed Graphs,cp,Conference Paper,Bai L.,60023237;60016418;60013131,Beijing Normal University;University of York;Central University of Finance and Economics,Beijing;York;Beijing,China;United Kingdom;China,3.0,"Bai, Lu;Cui, Lixin;Hancock, Edwin R.",50361034800;57191873900;7202876234,60023237-60013131;60013131;60016418,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,1327-1336,"In this paper, we develop a new graph kernel, namely the Hierarchical Transitive-Aligned Kernel, by transitively aligning the vertices between graphs through a family of hierarchical prototype graphs. Comparing to most existing state-of-the-art graph kernels, the proposed kernel has three theoretical advantages. First, it incorporates the locational correspondence information between graphs into the kernel computation, and thus overcomes the shortcoming of ignoring structural correspondences arising in most R-convolution kernels. Second, it guarantees the transitivity between the correspondence information that is not available for most existing matching kernels. Third, it incorporates the information of all graphs under comparisons into the kernel computation process, and thus encapsulates richer characteristics. Experimental evaluations demonstrate the effectiveness of the new transitive-aligned kernel.",,15,0.0,,,NSFC,61602535,National Natural Science Foundation of China
2-s2.0-85180301846,,,,A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture,cp,Conference Paper,Shen Q.,60020547;60007278;60022317;124531910;131186154,"Texas A&M University;University of California, Irvine;University of Macau;Zhejiang Lab;Institute of Collaborative Innovation",College Station;Irvine;Taipa;Shanghai;,United States;United States;Macao;China;,6.0,"Shen, Qianqian;Zhao, Yunhan;Kwon, Nahyun;Kim, Jeeeun;Li, Yanan;Kong, Shu",57870041000;57216947249;57222380296;56159870300;57192556687;57216136329,124531910;60007278;60020547;60020547;124531910;60020547-131186154-60022317,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Instance detection (InsDet) is a long-lasting problem in robotics and computer vision, aiming to detect object instances (predefined by some visual examples) in a cluttered scene. Despite its practical significance, its advancement is overshadowed by Object Detection, which aims to detect objects belonging to some predefined classes. One major reason is that current InsDet datasets are too small in scale by today's standards. For example, the popular InsDet dataset GMU (published in 2016) has only 23 instances, far less than COCO (80 classes), a well-known object detection dataset published in 2014. We are motivated to introduce a new InsDet dataset and protocol. First, we define a realistic setup for InsDet: training data consists of multi-view instance captures, along with diverse scene images allowing synthesizing training images by pasting instance images on them with free box annotations. Second, we release a real-world database, which contains multi-view capture of 100 object instances, and high-resolution (6k×8k) testing images. Third, we extensively study baseline methods for InsDet on our dataset, analyze their performance and suggest future work. Somewhat surprisingly, using the off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving >10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).",,1,0.0,,,NSFC,62206256,National Natural Science Foundation of China
2-s2.0-85167964610,10.1609/aaai.v37i11.26593,,,A Latent-Variable Model for Intrinsic Probing,cp,Conference Paper,Stańczak K.,60022195;60025858;60030840;60355330,Massachusetts Institute of Technology;ETH Zürich;Københavns Universitet;Meta Ai,Cambridge;Zurich;Copenhagen;Menlo Park,United States;Switzerland;Denmark;United States,5.0,"Stańczak, Karolina;Hennigen, Lucas Torroba;Williams, Adina;Cotterell, Ryan;Augenstein, Isabelle",57223802604;57219757293;56387940900;56350054200;55236320300,60030840;60022195;60355330;60025858;60030840,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,13591-13599,"The success of pre-trained contextualized representations has prompted researchers to analyze them for the presence of linguistic information. Indeed, it is natural to assume that these pre-trained representations do encode some level of linguistic knowledge as they have brought about large empirical improvements on a wide variety of NLP tasks, which suggests they are learning true linguistic generalization. In this work, we focus on intrinsic probing, an analysis technique where the goal is not only to identify whether a representation encodes a linguistic attribute but also to pinpoint where this attribute is encoded. We propose a novel latent-variable formulation for constructing intrinsic probes and derive a tractable variational approximation to the log-likelihood. Our results show that our model is versatile and yields tighter mutual information estimates than two intrinsic probes previously proposed in the literature. Finally, we find empirical evidence that pre-trained representations develop a cross-lingually entangled notion of morphosyntax.",,2,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85152230236,10.1613/JAIR.1.14113,,,A Logic of East and West,ar,Article,Du H.,60026851;60031031;60073652;60007989;60012070;60015875;60104720;60111768,University of Oxford;Shandong University;Tongji University;Universiteit Utrecht;University of Leeds;University of Aberdeen;University of Nottingham Ningbo China;The Alan Turing Institute,Oxford;Jinan;Shanghai;Utrecht;Leeds;Aberdeen;Ningbo;London,United Kingdom;China;China;Netherlands;United Kingdom;United Kingdom;China;United Kingdom,6.0,"Du, Heshan;Alechina, Natasha;Farjudian, Amin;Logan, Brian;Zhou, Can;Cohn, Anthony G.",55327433000;6603450996;8403010200;56092567900;57216587487;57225810681,60104720;60007989;60104720;60007989-60015875;60026851;60012070-60111768-60073652-60031031,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,527-565,"We propose a logic of east and west (LEW) for points in 1D Euclidean space. It formalises primitive direction relations: east (E), west (W) and indeterminate east/west (I<inf>ew</inf>). It has a parameter τ ∈ N<inf>></inf><inf>1</inf>, which is referred to as the level of indeterminacy in directions. For every τ ∈ N<inf>></inf><inf>1</inf>, we provide a sound and complete axiomatisation of LEW, and prove that its satisfiability problem is NP-complete. In addition, we show that the finite axiomatisability of LEW depends on τ: if τ = 2 or τ = 3, then there exists a finite sound and complete axiomatisation; if τ > 3, then the logic is not finitely axiomatisable. LEW can be easily extended to higher-dimensional Euclidean spaces. Extending LEW to 2D Euclidean space makes it suitable for reasoning about not perfectly aligned representations of the same spatial objects in different datasets, for example, in crowd-sourced digital maps.",,2,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,ATI,825619,Alan Turing Institute
2-s2.0-85190405106,10.1613/jair.1.15213,,,A Map of Diverse Synthetic Stable Matching Instances,ar,Article,Boehmer N.,60011604;60017351,Technische Universität Berlin;AGH University of Krakow,Berlin;Krakow,Germany;Poland,3.0,"Boehmer, Niclas;Heeger, Klaus;Szufa, Stanisław",57219118917;57212274933;57204360148,60011604;60011604;60017351,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,1113-1166,"Focusing on Stable Roommates (SR), we contribute to the toolbox for conducting experiments for stable matching problems. We introduce the polynomial-time computable mutual attraction distance to measure the similarity of SR instances, analyze its properties, and use it to create a map of SR instances. This map visualizes 460 synthetic SR instances (each sampled from one of ten different statistical cultures) as follows: Each instance is a point in the plane, and two points are close on the map if the corresponding SR instances are similar with respect to our mutual attraction distance to each other. Subsequently, we conduct several illustrative experiments and depict their results on the map, illustrating the map’s usefulness as a non-aggregate visualization tool, the diversity of our generated dataset, and the need to use instances sampled from different statistical cultures. Lastly, we extend our approach to the bipartite Stable Marriage problem.",,2,1.0,all publisherfullgold,All Open Access Gold,ERC,NI 369/22,European Research Council
2-s2.0-85165168530,10.1613/jair.1.13934,,,A Markov Framework for Learning and Reasoning About Strategies in Professional Soccer,ar,Article,Van Roy M.,60008141;60121221,Örebro Universitet;Departement Computerwetenschappen,Orebro;Leuven,Sweden;Belgium,5.0,"Van Roy, Maaike;Robberechts, Pieter;Yang, Wen Chi;De Raedt, Luc;Davis, Jesse",57222576578;57205377775;57223722730;55760010700;57222752756,60121221;60121221;60121221;60121221-60008141;60121221,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,517-562,"Strategy-optimization is a fundamental element of dynamic and complex team sports such as soccer, American football, and basketball. As the amount of data that is collected from matches in these sports has increased, so has the demand for data-driven decision-making support. If alternative strategies need to be balanced, a data-driven approach can uncover insights that are not available from qualitative analysis. This could tremendously aid teams in their match preparations. In this work, we propose a novel Markov model-based framework for soccer that allows reasoning about the specific strategies teams use in order to gain insights into the efficiency of each strategy. The framework consists of two components: (1) a learning component, which entails modeling a team’s offensive behavior by learning a Markov decision process (MDP) from event data that is collected from the team’s matches, and (2) a reasoning component, which involves a novel application of probabilistic model checking to reason about the efficacy of the learned strategies of each team. In this paper, we provide an overview of this framework and illustrate it on several use cases using real-world event data from three leagues. Our results show that the framework can be used to reason about the shot decision-making of teams and to optimise the defensive strategies used when playing against a particular team. The general ideas presented in this framework can easily be extended to other sports.",,13,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,EC,101070149,Vlaamse regering
2-s2.0-85129605896,10.1613/jair.1.13610,,,A Metric Space for Point Process Excitations,ar,Article,Marmarelis M.G.,60015400,Information Sciences Institute,Marina del Rey,United States,3.0,"Marmarelis, Myrl G.;Steeg, Greg Ver;Galstyan, Aram",57216494220;26326519600;7003679856,60015400;60015400;60015400,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,1323-1353,"A multivariate Hawkes process enables self- and cross-excitations through a triggering matrix that behaves like an asymmetrical covariance structure, characterizing pairwise interactions between the event types. Full-rank estimation of all interactions is often infeasible in empirical settings. Models that specialize on a spatiotemporal application alleviate this obstacle by exploiting spatial locality, allowing the dyadic relationships between events to depend only on separation in time and relative distances in real Euclidean space. Here we generalize this framework to any multivariate Hawkes process, and harness it as a vessel for embedding arbitrary event types in a hidden metric space. Specifically, we propose a Hidden Hawkes Geometry (HHG) model to uncover the hidden geometry between event excitations in a multivariate point process. The low dimensionality of the embedding regularizes the structure of the inferred interactions. We develop a number of estimators and validate the model by conducting several experiments. In particular, we investigate regional infectivity dynamics of COVID-19 in an early South Korean record and recent Los Angeles confirmed cases. By additionally performing synthetic experiments on short records as well as explorations into options markets and the Ebola epidemic, we demonstrate that learning the embedding alongside a point process uncovers salient interactions in a broad range of applications.",,1,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,
2-s2.0-105018455136,,,,A Multilabel Classification Framework for Approximate Nearest Neighbor Search,ar,Article,Hyvönen V.,60002952;60103653;60136640,Helsingin Yliopisto;Aalto University;School of Computer Science,Helsinki;Espoo;Pittsburgh,Finland;Finland;United States,3.0,"Hyvönen, Ville;Jääsaari, Elias;Roos, Teemu",56185173100;57193614267;7006941073,60103653;60136640;60002952,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"To learn partition-based index structures for approximate nearest neighbor (ANN) search, both supervised and unsupervised machine learning algorithms have been used. Existing supervised algorithms select all the points that belong to the same partition element as the query point as nearest neighbor candidates. Consequently, they formulate the learning task as finding a partition in which the nearest neighbors of a query point belong to the same partition element with it as often as possible. In contrast, we formulate the candidate set selection in ANN search directly as a multilabel classification problem where the labels correspond to the nearest neighbors of the query point. In the proposed framework, partition-based index structures are interpreted as partitioning classifiers for solving this classification problem. Empirical results suggest that, when combined with any partitioning strategy, the natural classifier based on the proposed framework leads to a strictly improved performance compared to the earlier candidate set selection methods. We also prove a sufficient condition for the consistency of a partitioning classifier for ANN search, and illustrate the result by verifying this condition for chronological k-d trees and (both dense and sparse) random projection trees.",approximate nearest neighbor search | multilabel classification | partitioning models | statistical learning theory,1,0.0,,,FCAI,345635,Finnish Center for Artificial Intelligence
2-s2.0-85148762026,,,,A NON-PARAMETRIC REGRESSION VIEWPOINT: GENERALIZATION OF OVERPARAMETRIZED DEEP RELU NETWORK UNDER NOISY OBSERVATIONS,cp,Conference Paper,Suh N.,60136858,College of Engineering,Atlanta,United States,3.0,"Suh, Namjoon;Ko, Hyunouk;Huo, Xiaoming",57213419843;58147670000;7005818991,60136858;60136858;60136858,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"We study the generalization properties of the overparameterized deep neural network (DNN) with Rectified Linear Unit (ReLU) activations. Under the nonparametric regression framework, it is assumed that the ground-truth function is from a reproducing kernel Hilbert space (RKHS) induced by a neural tangent kernel (NTK) of ReLU DNN, and a dataset is given with the noises. Without a delicate adoption of early stopping, we prove that the overparametrized DNN trained by vanilla gradient descent does not recover the ground-truth function. It turns out that the estimated DNN's L<inf>2</inf> prediction error is bounded away from 0. As a complement of the above result, we show that the ℓ<inf>2</inf>-regularized gradient descent enables the overparametrized DNN to achieve the minimax optimal convergence rate of the L<inf>2</inf> prediction error, without early stopping. Notably, the rate we obtained is faster than O(n<sup>-1/2</sup>) known in the literature.",,2,0.0,,,NSF,CCF-1740776,National Science Foundation
2-s2.0-85161491987,,,,A Neural Corpus Indexer for Document Retrieval,cp,Conference Paper,Wang Y.,60025278;60000745;60014966;60026532,Tsinghua University;University of Illinois Urbana-Champaign;Peking University;Microsoft Corporation,Beijing;Urbana;Beijing;Redmond,China;United States;China;United States,16.0,"Wang, Yujing;Hou, Yingyan;Wang, Haonan;Miao, Ziming;Wu, Shibin;Sun, Hao;Chen, Qi;Xia, Yuqing;Chi, Chengmin;Zhao, Guoshuai;Liu, Zheng;Xie, Xing;Sun, Hao Allen;Deng, Weiwei;Zhang, Qi;Yang, Mao",56347999900;57747964600;57214938083;57747328200;57748939200;57216325974;57207582229;57746999500;57747328300;56026739900;57211759701;57221820833;57973532800;57203399957;54931372200;55703321000,60026532;60026532-60025278;60026532-60000745;60026532;60026532-60025278;60026532-60014966;60026532;60026532;60026532;60026532;60026532;60026532;60026532;60026532;60026532;60026532,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Current state-of-the-art document retrieval solutions mainly follow an index-retrieve paradigm, where the index is hard to be directly optimized for the final retrieval target. In this paper, we aim to show that an end-to-end deep neural network unifying training and indexing stages can significantly improve the recall performance of traditional methods. To this end, we propose Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates relevant document identifiers directly for a designated query. To optimize the recall performance of NCI, we invent a prefix-aware weight-adaptive decoder architecture, and leverage tailored techniques including query generation, semantic document identifiers, and consistency-based regularization. Empirical studies demonstrated the superiority of NCI on two commonly used academic benchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on NQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to the best baseline method.",,106,0.0,,,,,Microsoft
2-s2.0-85163193712,,,,A Neural Pre-Conditioning Active Learning Algorithm to Reduce Label Complexity,cp,Conference Paper,Kong S.T.,60000745;60007511;60016209;60159628,"University of Illinois Urbana-Champaign;Sungkyunkwan University;Dong-A University;Vuno, Inc.",Urbana;Seoul;Busan;Seoul,United States;South Korea;South Korea;South Korea,6.0,"Kong, Seo Taek;Jeon, Soomin;Na, Dongbin;Lee, Jaewon;Lee, Hong Seok;Jung, Kyu Hwan",57202249248;57204811007;59806005600;57219455266;57211989898;14037142200,60000745;60016209;60159628;60159628;60159628;60007511,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Deep learning (DL) algorithms rely on massive amounts of labeled data. Semi-supervised learning (SSL) and active learning (AL) aim to reduce this label complexity by leveraging unlabeled data or carefully acquiring labels, respectively. In this work, we primarily focus on designing an AL algorithm but first argue for a change in how AL algorithms should be evaluated. Although unlabeled data is readily available in pool-based AL, AL algorithms are usually evaluated by measuring the increase in supervised learning (SL) performance at consecutive acquisition steps. Because this measures performance gains from both newly acquired instances and newly acquired labels, we propose to instead evaluate the label efficiency of AL algorithms by measuring the increase in SSL performance at consecutive acquisition steps. After surveying tools that can be used to this end, we propose our neural pre-conditioning (NPC) algorithm inspired by a Neural Tangent Kernel (NTK) analysis. Our algorithm incorporates the classifier's uncertainty on unlabeled data and penalizes redundant samples within candidate batches to efficiently acquire a diverse set of informative labels. Furthermore, we prove that NPC improves downstream training in the large-width regime in a manner previously observed to correlate with generalization. Comparisons with other AL algorithms show that a state-of-the-art SSL algorithm coupled with NPC can achieve high performance using very few labeled data.",,4,0.0,,,,,
2-s2.0-85147941157,,,,A Nonconvex Framework for Structured Dynamic Covariance Recovery,ar,Article,Tsai K.,60000745;60158506;60112748,University of Illinois Urbana-Champaign;The Grainger College of Engineering;The University of Chicago Booth School of Business,Urbana;Urbana;Chicago,United States;United States;United States,3.0,"Tsai, Katherine;Kolar, Mladen;Koyejo, Oluwasanmi",57220950483;24448496700;36156033700,60158506;60112748;60000745,2022-06-01,1 June 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,200,,"We propose a flexible, yet interpretable model for high-dimensional data with time-varying second-order statistics, motivated and applied to functional neuroimaging data. Our approach implements the neuroscientific hypothesis of discrete cognitive processes by factorizing covariances into sparse spatial and smooth temporal components. Although this factorization results in parsimony and domain interpretability, the resulting estimation problem is nonconvex. We design a two-stage optimization scheme with a tailored spectral initialization, combined with iteratively refined alternating projected gradient descent. We prove a linear convergence rate up to a nontrivial statistical error for the proposed descent scheme and establish sample complexity guarantees for the estimator. Empirical results using simulated data and brain imaging data illustrate that our approach outperforms existing baselines.",alternating projected gradient descent | dynamic covariance | functional connectivity | structured factor model | time series data,0,0.0,,,NSF,IIS 2046795,National Science Foundation
2-s2.0-85200536471,,,,A RESTORATION NETWORK AS AN IMPLICIT PRIOR,cp,Conference Paper,Hu Y.,60010261;60006191,Washington University in St. Louis;Google LLC,St. Louis;Mountain View,United States;United States,4.0,"Hu, Yuyang;Delbracio, Mauricio;Milanfar, Peyman;Kamilov, Ulugbek S.",57417323600;22034081300;7004264375;53866608400,60010261;60006191;60006191;60010261-60006191,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Image denoisers have been shown to be powerful priors for solving inverse problems in imaging. In this work, we introduce a generalization of these methods that allows any image restoration network to be used as an implicit prior. The proposed method uses priors specified by deep neural networks pre-trained as general restoration operators. The method provides a principled approach for adapting state-of-the-art restoration models for other inverse problems. Our theoretical result analyzes its convergence to a stationary point of a global functional associated with the restoration operator. Numerical results show that the method using a super-resolution prior achieves state-of-the-art performance both quantitatively and qualitatively. Overall, this work offers a step forward for solving inverse problems by enabling the use of powerful pre-trained restoration models as priors.",,3,0.0,,,,,
2-s2.0-85169653144,10.24963/ijcai.2023/623,,,A Refined Upper Bound and Inprocessing for the Maximum K-plex Problem,cp,Conference Paper,Jiang H.,60028009,Yunnan University,Kunming,China,5.0,"Jiang, Hua;Xu, Fusheng;Zheng, Zhifei;Wang, Bowen;Zhou, Wei",56988107800;58569392000;58569489400;58738191300;56857006600,60028009;60028009;60028009;60028009;60028009,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,5613-5621,"A k-plex of a graph G is an induced subgraph in which every vertex has at most k − 1 nonadjacent vertices. The Maximum k-plex Problem (MKP) consists in finding a k-plex of the largest size, which is NP-hard and finds many applications. Existing exact algorithms mainly implement a branch- and-bound approach and improve performance by integrating effective upper bounds and graph reduction rules. In this paper, we propose a refined upper bound, which can derive a tighter upper bound than existing methods, and an inprocessing strategy, which performs graph reduction incrementally. We implement a new BnB algorithm for MKP that employs the two components to reduce the search space. Extensive experiments show that both the refined upper bound and the inprocessing strategy are very efficient in the reduction of search space. The new algorithm outperforms the state-of-the-art algorithms on the tested benchmarks significantly.",,11,1.0,all publisherfullgold,All Open Access Gold,NSFC,202302AD080006,National Natural Science Foundation of China
2-s2.0-85163055828,,,,A Resilient Distributed Boosting Algorithm,cp,Conference Paper,Filmus Y.,60022403;60006191,Technion - Israel Institute of Technology;Google LLC,Haifa;Mountain View,Israel;United States,3.0,"Filmus, Yuval;Mehalel, Idan;Moran, Shay",36720386900;57224938736;57210472466,60022403;60022403;60022403-60006191,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,6465-6473,"Given a learning task where the data is distributed among several parties, communication is one of the fundamental resources which the parties would like to minimize. We present a distributed boosting algorithm which is resilient to a limited amount of noise. Our algorithm is similar to classical boosting algorithms, although it is equipped with a new component, inspired by Impagliazzo's hard-core lemma (Impagliazzo, 1995), adding a robustness quality to the algorithm. We also complement this result by showing that resilience to any asymptotically larger noise is not achievable by a communication-efficient algorithm.",,0,0.0,,,,,
2-s2.0-85170404365,10.24963/ijcai.2023/608,,,A Rigorous Risk-aware Linear Approach to Extended Markov Ratio Decision Processes with Embedded Learning,cp,Conference Paper,Zadorojniy A.,60004066;130143861,IBM Research - Tokyo;IBM Research - Israel,Tokyo;,Japan;Israel,3.0,"Zadorojniy, Alexander;Osogami, Takayuki;Davidovich, Orit",12808566700;10042517800;57871679900,130143861;60004066;130143861,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,5475-5483,"We consider the problem of risk-aware Markov Decision Processes (MDPs) for Safe AI. We introduce a theoretical framework, Extended Markov Ratio Decision Processes (EMRDP), that incorporates risk into MDPs and embeds environment learning into this framework. We propose an algorithm to find the optimal policy for EMRDP with theoretical guarantees. Under a certain monotonicity assumption, this algorithm runs in strongly-polynomial time both in the discounted and expected average reward models. We validate our algorithm empirically on a Grid World benchmark, evaluating its solution quality, required number of steps, and numerical stability. We find its solution quality to be stable under data noising, while its required number of steps grows with added noise. We observe its numerical stability compared to global methods.",,1,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85199920394,,,,A SELF-ATTENTION ANSATZ FOR AB-INITIO QUANTUM CHEMISTRY,cp,Conference Paper,von Glehn I.,,,,,3.0,"von Glehn, Ingrid;Spencer, James S.;Pfau, David",56102050200;36538142100;57210571928,,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"We present a novel neural network architecture using self-attention, the Wavefunction Transformer (Psiformer), which can be used as an approximation (or Ansatz) for solving the many-electron Schrödinger equation, the fundamental equation for quantum chemistry and material science. This equation can be solved from first principles, requiring no external training data. In recent years, deep neural networks like the FermiNet and PauliNet have been used to significantly improve the accuracy of these first-principle calculations, but they lack an attention-like mechanism for gating interactions between electrons. Here we show that the Psiformer can be used as a drop-in replacement for these other neural networks, often dramatically improving the accuracy of the calculations. On larger molecules especially, the ground state energy can be improved by dozens of kcal/mol, a qualitative leap over previous methods. This demonstrates that self-attention networks can learn complex quantum mechanical correlations between electrons, and are a promising route to reaching unprecedented accuracy in chemical calculations on larger systems.",,17,0.0,,,,,
2-s2.0-85175947509,,,,A SIMPLE YET POWERFUL DEEP ACTIVE LEARNING WITH SNAPSHOTS ENSEMBLES,cp,Conference Paper,Jung S.,60032144,Korea Advanced Institute of Science and Technology,Daejeon,South Korea,3.0,"Jung, Seohyeon;Kim, Sanghyun;Lee, Juho",58507153700;58607861500;57132050100,60032144;60032144;60032144,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Given an unlabeled pool of data and the experts who can label them, active learning aims to build an agent that can effectively acquire data to be queried to the experts, maximizing the gain in performance when trained with them. While there are several principles for active learning, a prevailing approach is to estimate uncertainties of predictions for unlabeled samples and use them to define acquisition functions. Active learning with the uncertainty principle works well for deep learning, especially for large-scale image classification tasks with deep neural networks. Still, it is often overlooked how the uncertainty of predictions is estimated, despite the common findings on the difficulty of accurately estimating uncertainties of deep neural networks. In this paper, we highlight the effectiveness of snapshot ensembles for deep active learning. Compared to the previous approaches based on Monte-Carlo dropout or deep ensembles, we show that a simple acquisition strategy based on uncertainties estimated from parameter snapshots gathered from a single optimization path significantly improves the quality of the acquired samples. Based on this observation, we further propose an efficient active learning algorithm that maintains a single learning trajectory throughout the entire active learning episodes, unlike the existing algorithms training models from scratch for every active learning episode. Through the extensive empirical comparison, we demonstrate the effectiveness of snapshot ensembles for deep active learning. Our code is available at: https://github.com/nannullna/snapshot-al.",,7,0.0,,,NRF,NRF-2021M3E5D9025030,National Research Foundation of Korea
2-s2.0-85212253763,10.1613/jair.1.16882,,,"A Scoping Study of AI Affordances in Early Childhood Education: Mapping the Global Landscape, Identifying Research Gaps, and Charting Future Research Directions",ar,Article,Chen J.J.,60012615,Kean University,Union,United States,1.0,"Chen, Jennifer J.",14035236400,60012615,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,701-740,"Artificial intelligence (AI), manifested in the forms of technologies, systems, tools, and applications, has advanced rapidly, especially in recent years. It has permeated many aspects of human behavior and nearly all sectors of society, such as healthcare and education. In the context of early childhood education (ECE), AI has afforded valuable opportunities that directly and indirectly enhance children’s learning and development. While there are already two existing reviews of the literature on AI in ECE, they show either a lack of descriptive information concerning selected studies or inconsistencies between inclusion/exclusion criteria and selected studies, thereby raising concerns about their rigor. Representing a more methodologically rigorous effort and a significant contribution to the field of AI in ECE, this scoping study aimed to achieve three main goals: (1) “mapping” the global landscape of the current extent, range, and nature of relevant studies on the affordances of AI for use in ECE, (2) identifying potential research gaps, and (3) charting future research directions. Specifically, it addressed this overarching research question: What is the global landscape of the current state of knowledge concerning the affordances of AI for use in ECE? Specifically, the state of knowledge here refers to three aspects: (1) extent, (2) range, and (3) nature. First, regarding the extent aspect, the empirical knowledge was derived from 18 research articles in 11 countries and 16 peer-reviewed academic journals between 2005 and 2023, with 14 of these articles published in the past four years (2020–2023). Second, with respect to the range of study populations, it covered 15,081 children in early childhood (ages 2 to 8 years) across these 11 countries. Third, thematic analysis of these studies revealed four areas of AI affordances: (1) AI as tangible and intangible tools for interactive learning and information retrieval, (2) AI as technology for predicting/classifying children’s conditions, (3) AI as the object for learning by adapting to and personalizing children’s learning, and (4) AI as the subject for children's learning about it. Based on these findings, this scoping review identified three research gaps for future studies: (1) interviewing and/or surveying education stakeholders (parents, educators, policymakers) to explore the affordances of appropriate AI for use with, by, and for children bearing ethical considerations; (2) conducting group comparisons to investigate contextual factors contributing to the “AI divide” among children from different socioeconomic backgrounds; and (3) comparing sociocultural influences on AI use in ECE across cultures.",,10,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85146310446,,,,A Single-Loop Gradient Descent and Perturbed Ascent Algorithm for Nonconvex Functional Constrained Optimization,cp,Conference Paper,Lu S.,60017366,IBM Thomas J. Watson Research Center,Yorktown Heights,United States,1.0,"Lu, Songtao",57201103164,60017366,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,14315-14357,"Nonconvex constrained optimization problems can be used to model a number of machine learning problems, such as multi-class Neyman-Pearson classification and constrained Markov decision processes. However, such kinds of problems are challenging because both the objective and constraints are possibly nonconvex, so it is difficult to balance the reduction of the loss value and reduction of constraint violation. Although there are a few methods that solve this class of problems, all of them are double-loop or triple-loop algorithms, and they require oracles to solve some subproblems up to certain accuracy by tuning multiple hyperparameters at each iteration. In this paper, we propose a novel gradient descent and perturbed ascent (GDPA) algorithm to solve a class of smooth nonconvex inequality constrained problems. The GDPA is a primal-dual algorithm, which only exploits the first-order information of both the objective and constraint functions to update the primal and dual variables in an alternating way. The key feature of the proposed algorithm is that it is a single-loop algorithm, where only two step-sizes need to be tuned. We show that under a mild regularity condition GDPA is able to find Karush-Kuhn-Tucker (KKT) points of nonconvex functional constrained problems with convergence rate guarantees. To the best of our knowledge, it is the first single-loop algorithm that can solve the general nonconvex smooth problems with nonconvex inequality constraints. Numerical results also showcase the superiority of GDPA compared with the best-known algorithms (in terms of both stationarity measure and feasibility of the obtained solutions).",,11,0.0,,,,,
2-s2.0-85203259145,,,,A Top-Down Tree Model Counter for Quantified Boolean Formulas,cp,Conference Paper,Capelli F.,60018178;128615068,Université d'Artois;Institute for Symbolic Artificial Intelligence,Arras;Linz,France;Austria,4.0,"Capelli, Florent;Lagniez, Jean Marie;Plank, Andreas;Seidl, Martina",56006109600;35748976300;58571752100;15021478700,60018178;60018178;128615068;128615068,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1853-1861,"This paper addresses the challenge of solution counting for Quantified Boolean Formulas (QBFs), a task distinct from the well-established model counting problem for SAT (#SAT).Unlike SAT, where models are straightforward assignments to Boolean variables, QBF solution counting involves tree models that capture dependencies among variables within different quantifier blocks.We present a comprehensive top-down tree model counter capable of handling diverse satisfiable QBFs.Highlighting the pivotal role of the branching heuristic, which needs to consider variables in accordance with quantification blocks, we further underscore the significance of dealing with connected components, free variables, and caching.Experimental results indicate that our proposed approach for counting tree models of QBF formulas is highly efficient in practice, surpassing existing state-of-the-art methods designed for this specific purpose.",,5,0.0,,,ANR,ANR-22EXES-0009,Agence Nationale de la Recherche
2-s2.0-85194008878,,,,A Unified Approach to Controlling Implicit Regularization via Mirror Descent,ar,Article,Sun H.,60022195,Massachusetts Institute of Technology,Cambridge,United States,4.0,"Sun, Haoyuan;Gatmiry, Khashayar;Ahn, Kwangjun;Azizan, Navid",58731627500;57219542684;57219115138;57044772000,60022195;60022195;60022195;60022195,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,393,,"Inspired by the remarkable success of large neural networks, there has been significant interest in understanding the generalization performance of over-parameterized models. Substantial efforts have been invested in characterizing how optimization algorithms impact generalization through their “preferred” solutions, a phenomenon commonly referred to as implicit regularization. In particular, it has been argued that gradient descent (GD) induces an implicit `<inf>2</inf>-norm regularization in regression and classification problems. However, the implicit regularization of different algorithms are confined to either a specific geometry or a particular class of learning problems, indicating a gap in a general approach for controlling the implicit regularization. To address this, we present a unified approach using mirror descent (MD), a notable generalization of GD, to control implicit regularization in both regression and classification settings. More specifically, we show that MD with the general class of homogeneous potential functions converges in direction to a generalized maximum-margin solution for linear classification problems, thereby answering a long-standing question in the classification setting. Further, we show that MD can be implemented efficiently and enjoys fast convergence under suitable conditions. Through comprehensive experiments, we demonstrate that MD is a versatile method to produce learned models with different regularizers, which in turn have different generalization performances.",gradient descent | implicit regularization | maximum-margin classification | mirror descent | over-parameterization,9,0.0,,,,,MathWorks
2-s2.0-85204678288,,,,A Unified Framework for Factorizing Distributional Value Functions for Multi-Agent Reinforcement Learning,ar,Article,Sun W.F.,60018029;60076695,National Tsing Hua University;NVIDIA,Hsinchu;Santa Clara,Taiwan;United States,4.0,"Sun, Wei Fang;Lee, Cheng Kuang;See, Simon;Lee, Chun Yi",57221661021;35194264300;7004029208;55700504000,60018029;60076695;60076695;60018029,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"In fully cooperative multi-agent reinforcement learning (MARL) settings, environments are highly stochastic due to the partial observability of each agent and the continuously changing policies of other agents. To address the above issues, we proposed a unified framework, called DFAC, for integrating distributional RL with value function factorization methods. This framework generalizes expected value function factorization methods to enable the factorization of return distributions. To validate DFAC, we first demonstrate its ability to factorize the value functions of a simple matrix game with stochastic rewards. Then, we perform experiments on all Super Hard maps of the StarCraft Multi-Agent Challenge and six self-designed Ultra Hard maps, showing that DFAC is able to outperform a number of baselines.",Distributional RL | Multi-Agent RL | Reinforcement Learning | Value Function Factorization,6,0.0,,,NSTC,111-2223-E-007-004-MY3,National Science and Technology Council
2-s2.0-85174394125,,,,A Unified Framework for Optimization-Based Graph Coarsening,ar,Article,Kumar M.,60032730,Indian Institute of Technology Delhi,New Delhi,India,3.0,"Kumar, Manoj;Sharma, Anurag;Kumar, Sandeep",57203913320;58263545900;59668868100,60032730;60032730;60032730,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"Graph coarsening is a widely used dimensionality reduction technique for approaching large-scale graph machine-learning problems. Given a large graph, graph coarsening aims to learn a smaller-tractable graph while preserving the properties of the originally given graph. Graph data consist of node features and graph matrix (e.g., adjacency and Laplacian). The existing graph coarsening methods ignore the node features and rely solely on a graph matrix to simplify graphs. In this paper, we introduce a novel optimization-based framework for graph dimensionality reduction. The proposed framework lies in the unification of graph learning and dimensionality reduction. It takes both the graph matrix and the node features as the input and learns the coarsen graph matrix and the coarsen feature matrix jointly while ensuring desired properties. The proposed optimization formulation is a multi-block non-convex optimization problem, which is solved efficiently by leveraging block majorization-minimization, log determinant, Dirichlet energy, and regularization frameworks. The proposed algorithms are provably convergent and practically amenable to numerous tasks. It is also established that the learned coarsened graph is ε ∈ (0, 1) similar to the original graph. Extensive experiments elucidate the efficacy of the proposed framework for real-world applications. The code for all the experimental results is available at CODE.",adjacency matrix | clustering | graph classification | graph coarsening | graph learning | Laplacian matrix | optimization | spectral properties | spectral similarity,17,0.0,,,DST,MI02322G,Defence Science and Technology Group
2-s2.0-85211096196,10.1613/jair.1.16019,,,A Unified Perspective on Value Backup and Exploration in Monte-Carlo Tree Search,ar,Article,Dam T.,60012689;60011226;60103653;60071354;129969662,Julius-Maximilians-Universität Würzburg;Technische Universität Darmstadt;Aalto University;Hanoi University of Science and Technology;Hessian.AI,Wurzburg;Darmstadt;Espoo;Hanoi;Kassel,Germany;Germany;Finland;Viet Nam;Germany,4.0,"Dam, Tuan;D’Eramo, Carlo;Peters, Jan;Pajarinen, Joni",57219623876;57192164496;35248912800;25929554800,60071354;60012689-129969662-60011226;129969662-60011226;60103653,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,511-577,"Monte-Carlo Tree Search (MCTS) is a class of methods for solving complex decision-making problems through the synergy of Monte-Carlo planning and Reinforcement Learning (RL). The highly combinatorial nature of the problems commonly addressed by MCTS requires the use of efficient exploration strategies for navigating the planning tree and quickly convergent value backup methods. These crucial problems are particularly evident in recent advances that combine MCTS with deep neural networks for function approximation. In this work, we propose two methods for improving the convergence rate and exploration based on a newly introduced backup operator and entropy regularization. We provide strong theoretical guarantees to bound convergence rate, approximation error, and regret of our methods. Moreover, we introduce a mathematical framework based on the use of the α-divergence for backup and exploration in MCTS. We show that this theoretical formulation unifies different approaches, including our newly introduced ones, under the same mathematical framework, allowing to obtain different methods by simply changing the value of α. In practice, our unified perspective offers a flexible way to balance between exploration and exploitation by tuning the single α parameter according to the problem at hand. We validate our methods through a rigorous empirical study from basic toy problems to the complex Atari games, and including both MDP and POMDP problems.",,4,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,DFG,PA 3179/1-1,Deutsche Forschungsgemeinschaft
2-s2.0-85137852464,10.24963/ijcai.2022/606,,,A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model,cp,Conference Paper,Sun X.,60002798;60098464;60124576,"Chinese University of Hong Kong;Microsoft Research Asia;Key Laboratory of Computational Linguistics, Ministry of Education, Peking University",Hong Kong;Beijing;Beijing,Hong Kong;China;China,6.0,"Sun, Xin;Ge, Tao;Ma, Shuming;Li, Jingjing;Wei, Furu;Wang, Houfeng",57204688248;55953962900;57200277125;57210214753;23995914700;57193227507,60124576;60098464;60098464;60002798;60098464;60124576,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4367-4374,"Synthetic data construction of Grammatical Error Correction (GEC) for non-English languages relies heavily on human-designed and language-specific rules, which produce limited error-corrected patterns. In this paper, we propose a generic and language-independent strategy for multilingual GEC, which can train a GEC system effectively for a new non-English language with only two easy-to-access resources: 1) a pre-trained cross-lingual language model (PXLM) and 2) parallel translation data between English and the language. Our approach creates diverse parallel GEC data without any language-specific operations by taking the non-autoregressive translation generated by PXLM and the gold translation as error-corrected sentence pairs. Then, we reuse PXLM to initialize the GEC model and pre-train it with the synthetic data generated by itself, which yields further improvement. We evaluate our approach on three public benchmarks of GEC in different languages. It achieves the state-of-the-art results on the NLPCC 2018 Task 2 dataset (Chinese) and obtains competitive performance on Falko-Merlin (German) and RULEC-GEC (Russian). Further analysis demonstrates that our data construction method is complementary to rule-based approaches.",,17,1.0,all publisherfree2read,All Open Access Bronze,NSFC,2020BD021,National Natural Science Foundation of China
2-s2.0-85184515330,,,,A Unified Theory of Diversity in Ensemble Learning,ar,Article,Wood D.,60003771;60020650,The University of Manchester;University of Bristol,Manchester;Bristol,United Kingdom;United Kingdom,6.0,"Wood, Danny;Mu, Tingting;Webb, Andrew M.;Reeve, Henry W.J.;Luján, Mikel;Brown, Gavin",57685914200;36793369900;58587847500;43061497800;57205867906;7406467765,60003771;60003771;60003771;60020650;60003771;60003771,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,359,,"We present a theory of ensemble diversity, explaining the nature of diversity for a wide range of supervised learning scenarios. This challenge has been referred to as the “holy grail” of ensemble learning, an open research issue for over 30 years. Our framework reveals that diversity is in fact a hidden dimension in the bias-variance decomposition of the ensemble loss. We prove a family of exact bias-variance-diversity decompositions, for a wide range of losses in both regression and classification, e.g., squared, cross-entropy, and Poisson losses. For losses where an additive bias-variance decomposition is not available (e.g., 0/1 loss) we present an alternative approach: quantifying the effects of diversity, which turn out to be dependent on the label distribution. Overall, we argue that diversity is a measure of model fit, in precisely the same sense as bias and variance, but accounting for statistical dependencies between ensemble members. Thus, we should not be ‘maximising diversity’ as so many works aim to do—instead, we have a bias/variance/diversity trade-off to manage.",bias | diversity | ensembles | variance,65,0.0,,,EPSRC,EP/N035127/1,Royal Society
2-s2.0-85195870437,,,,A VARIATIONAL PERSPECTIVE ON SOLVING INVERSE PROBLEMS WITH DIFFUSION MODELS,cp,Conference Paper,Mardani M.,60076695,NVIDIA,Santa Clara,United States,4.0,"Mardani, Morteza;Song, Jiaming;Kautz, Jan;Vahdat, Arash",26325501300;57202059115;7006458237;36015722100,60076695;60076695;60076695;60076695,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-to-noise-ratio (SNR). Our approach provides a new variational perspective for solving inverse problems with diffusion models, allowing us to formulate sampling as stochastic optimization, where one can simply apply off-the-shelf solvers with lightweight iterates. Our experiments for various linear and nonlinear image restoration tasks demonstrate the strengths of our method compared with state-of-the-art sampling-based diffusion models. The code is available online.",,36,0.0,,,,,
2-s2.0-85174397433,,,,A Watermark for Large Language Models,cp,Conference Paper,Kirchenbauer J.,60020304,"University of Maryland, College Park",College Park,United States,6.0,"Kirchenbauer, John;Geiping, Jonas;Wen, Yuxin;Katz, Jonathan;Miers, Ian;Goldstein, Tom",57712873800;57201420593;57215596635;55613231842;55815678900;14055829100,60020304;60020304;60020304;60020304;60020304;60020304,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,17061-17084,"Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of “green” tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.",,297,0.0,,,NSF,HR00112020007,National Science Foundation
2-s2.0-85147733396,,,,A Worst Case Analysis of Calibrated Label Ranking Multi-label Classification Method,ar,Article,Mello L.H.S.,60028426,Universidade Federal do Espírito Santo,Vitoria,Brazil,3.0,"Mello, Lucas H.S.;Varejão, Flávio M.;Rodrigues, Alexandre L.",56494480300;6506091200;24172397800,60028426;60028426;60028426,2022-07-01,1 July 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Most multi-label classification methods are evaluated on real datasets, which is a good practice for comparing the performance among methods on the average scenario. Due to the large amount of factors to consider, this empirical approach does not explain, nor does show the factors impacting the performance. A reasonable way to understand some of the performance’s factors of multi-label methods independently of the context is to find a mathematical proof about them. In this paper, mathematical proofs are given for the multilabel method ranking by pairwise comparison and its extension for classification named by calibrated label ranking, showing their performance on a worst case scenario for five multilabel metrics. The pairwise approach adopted by ranking by pairwise comparison enables the algorithm to achieve the optimal performance on Spearman rank correlation. However, the findings presented in this paper clearly show that the same pairwise approach adopted by the algorithm is also a crucial factor contributing to a very poor performance on other multi-label metrics.",Loss minimization | Multi-label learning | Pairwise preference,0,0.0,,,,,
2-s2.0-85173664500,,,,ACHIEVE THE MINIMUM WIDTH OF NEURAL NETWORKS FOR UNIVERSAL APPROXIMATION,cp,Conference Paper,Cai Y.,60023237,Beijing Normal University,Beijing,China,1.0,"Cai, Yongqiang",57194778353,60023237,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"The universal approximation property (UAP) of neural networks is fundamental for deep learning, and it is well known that wide neural networks are universal approximators of continuous functions within both the L<sup>p</sup> norm and the continuous/uniform norm.However, the exact minimum width, w<inf>min</inf>, for the UAP has not been studied thoroughly.Recently, using a decoder-memorizer-encoder scheme, Park et al.(2021) found that w<inf>min</inf> = max(d<inf>x</inf> + 1, d<inf>y</inf>) for both the L<sup>p</sup>-UAP of ReLU networks and the C-UAP of ReLU+STEP networks, where d<inf>x</inf>, d<inf>y</inf> are the input and output dimensions, respectively.In this paper, we consider neural networks with an arbitrary set of activation functions.We prove that both C-UAP and L<sup>p</sup>-UAP for functions on compact domains share a universal lower bound of the minimal width; that is, w<inf>min</inf><sup>\ast</sup> = max(d<inf>x</inf>, d<inf>y</inf>).In particular, the critical width, w<inf>min</inf><sup>\ast</sup>, for L<sup>p</sup>-UAP can be achieved by leaky-ReLU networks, provided that the input or output dimension is larger than one.Our construction is based on the approximation power of neural ordinary differential equations and the ability to approximate flow maps by neural networks.The nonmonotone or discontinuous activation functions case and the one-dimensional case are also discussed.",,8,0.0,,,NSFC,12201053,National Natural Science Foundation of China
2-s2.0-85199856840,,,,ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING,cp,Conference Paper,Zhang Q.,60019647;60003269;127828259,Georgia Institute of Technology;Princeton University;Microsoft Azure AI,Atlanta;Princeton;Bellevue,United States;United States;United States,7.0,"Zhang, Qingru;Chen, Minshuo;Bukharin, Alexander;He, Pengcheng;Cheng, Yu;Chen, Weizhu;Zhao, Tuo",57222422161;57208438306;57219742576;57205508983;55421399200;23007589000;55178265500,60019647;60003269;60019647;127828259;127828259;127828259;60019647,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/QingruZhang/AdaLoRA.",,270,0.0,,,,,
2-s2.0-85132792005,,,,ADAVI: AUTOMATIC DUAL AMORTIZED VARIATIONAL INFERENCE APPLIED TO PYRAMIDAL BAYESIAN MODELS,cp,Conference Paper,Rouillard L.,60106017,Université Paris-Saclay,Gif-sur-Yvette,France,2.0,"Rouillard, Louis;Wassermann, Demian",57211431785;8265430900,60106017;60106017,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Frequently, population studies feature pyramidally-organized data represented using Hierarchical Bayesian Models (HBM) enriched with plates. These models can become prohibitively large in settings such as neuroimaging, where a sample is composed of a functional MRI signal measured on 300 brain locations, across 4 measurement sessions, and 30 subjects, resulting in around 1 million latent parameters. Such high dimensionality hampers the usage of modern, expressive flow-based techniques. To infer parameter posterior distributions in this challenging class of problems, we designed a novel methodology that automatically produces a variational family dual to a target HBM. This variational family, represented as a neural network, consists in the combination of an attention-based hierarchical encoder feeding summary statistics to a set of normalizing flows. Our automatically-derived neural network exploits exchangeability in the plate-enriched HBM and factorizes its parameter space. The resulting architecture reduces by orders of magnitude its parameterization with respect to that of a typical flow-based representation, while maintaining expressivity. Our method performs inference on the specified HBM in an amortized setup: once trained, it can readily be applied to a new data sample to compute the parameters' full posterior. We demonstrate the capability and scalability of our method on simulated data, as well as a challenging high-dimensional brain parcellation experiment. We also open up several questions that lie at the intersection between normalizing flows, SBI, structured Variational Inference, and inference amortization.",,2,0.0,,,,,
2-s2.0-85141978959,,,,ADBench: Anomaly Detection Benchmark,cp,Conference Paper,Han S.,60027950;60032744,Carnegie Mellon University;Shanghai University of Finance and Economics,Pittsburgh;Shanghai,United States;China,5.0,"Han, Songqiao;Hu, Xiyang;Huang, Hailiang;Jiang, Minqi;Zhao, Yue",14833985300;57218718420;7405613383;57688177800;57211496895,60032744;60027950;60032744;60032744;60027950,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Given a long list of anomaly detection algorithms developed in the last few decades, how do they perform with regard to (i) varying levels of supervision, (ii) different types of anomalies, and (iii) noisy and corrupted data? In this work, we answer these key questions by conducting (to our best knowledge) the most comprehensive anomaly detection benchmark with 30 algorithms on 57 benchmark datasets, named ADBench. Our extensive experiments (98,436 in total) identify meaningful insights into the role of supervision and anomaly types, and unlock future directions for researchers in algorithm selection and design. With ADBench, researchers can efficiently conduct comprehensive and fair evaluations for newly proposed methods on the datasets (including our contributed ones from natural language and computer vision domains) against the existing baselines. To foster accessibility and reproducibility, we fully open-source ADBench and the corresponding results.",,279,0.0,,,NSFC,72271151,National Natural Science Foundation of China
2-s2.0-85148431115,,,,ALIFE: Adaptive Logit Regularizer and Feature Replay for Incremental Semantic Segmentation,cp,Conference Paper,Oh Y.,60016912,Yonsei University,Seoul,South Korea,3.0,"Oh, Youngmin;Baek, Donghyeon;Ham, Bumsub",57223757653;57238864400;36061611900,60016912;60016912;60016912,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"We address the problem of incremental semantic segmentation (ISS) recognizing novel object/stuff categories continually without forgetting previous ones that have been learned. The catastrophic forgetting problem is particularly severe in ISS, since pixel-level ground-truth labels are available only for the novel categories at training time. To address the problem, regularization-based methods exploit probability calibration techniques to learn semantic information from unlabeled pixels. While such techniques are effective, there is still a lack of theoretical understanding of them. Replay-based methods propose to memorize a small set of images for previous categories. They achieve state-of-the-art performance at the cost of large memory footprint. We propose in this paper a novel ISS method, dubbed ALIFE, that provides a better compromise between accuracy and efficiency. To this end, we first show an in-depth analysis on the calibration techniques to better understand the effects on ISS. Based on this, we then introduce an adaptive logit regularizer (ALI) that enables our model to better learn new categories, while retaining knowledge for previous ones. We also present a feature replay scheme that memorizes features, instead of images directly, in order to reduce memory requirements significantly. Since a feature extractor is changed continually, memorized features should also be updated at every incremental stage. To handle this, we introduce category-specific rotation matrices updating the features for each category separately. We demonstrate the effectiveness of our approach with extensive experiments on standard ISS benchmarks, and show that our method achieves a better trade-off in terms of accuracy and efficiency.",,30,0.0,,,MSIP,2022-22-0002,"Ministry of Science, ICT and Future Planning"
2-s2.0-85148052998,,,,ALMA: Alternating Minimization Algorithm for Clustering Mixture Multilayer Network,ar,Article,Fan X.,60022144,University of Central Florida,Orlando,United States,4.0,"Fan, Xing;Pensky, Marianna;Yu, Feng;Zhang, Teng",57222312460;6601953547;57191997341;35747692400,60022144;60022144;60022144;60022144,2022-10-01,1 October 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,330,,"The paper considers a Mixture Multilayer Stochastic Block Model (MMLSBM), where layers can be partitioned into groups of similar networks, and networks in each group are equipped with a distinct Stochastic Block Model. The goal is to partition the multilayer network into clusters of similar layers, and to identify communities in those layers. Jing et al. (2020) introduced the MMLSBM and developed a clustering methodology, TWIST, based on regularized tensor decomposition. The present paper proposes a different technique, an alternating minimization algorithm (ALMA), that aims at simultaneous recovery of the layer partition, together with estimation of the matrices of connection probabilities of the distinct layers. Compared to TWIST, ALMA achieves higher accuracy, both theoretically and numerically.",Alternating Minimization | Clustering | Multilayer Network | Stochastic Block Model,9,0.0,,,NSF,CNS-1818500,National Science Foundation
2-s2.0-85132278697,,,,ANALYTIC-DPM: AN ANALYTIC ESTIMATE OF THE OPTIMAL REVERSE VARIANCE IN DIFFUSION PROBABILISTIC MODELS,cp,Conference Paper,Bao F.,60014402;60104026;116578043,Renmin University of China;Beijing National Research Center for Information Science and Technology;Beijing Key Laboratory of Big Data Management and Analysis Methods,Beijing;Beijing;Beijing,China;China;China,4.0,"Bao, Fan;Li, Chongxuan;Zhu, Jun;Zhang, Bo",57219510048;57189095239;56734692500;57190839301,60104026;60014402-116578043;60104026;60104026,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose Analytic-DPM, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20× to 80× speed up.",,210,0.0,,,RUC,BJJWZYJH012019100020098,Renmin University of China
2-s2.0-85149845549,,,,ANY-SCALE BALANCED SAMPLERS FOR DISCRETE SPACES,cp,Conference Paper,Sun H.,60030835;60019647;60006191,University of Alberta;Georgia Institute of Technology;Google LLC,Edmonton;Atlanta;Mountain View,Canada;United States;United States,5.0,"Sun, Haoran;Dai, Bo;Sutton, Charles;Schuurmans, Dale;Dai, Hanjun",57808336300;58918912100;57204256039;57204335408;57189099007,60006191-60019647;60019647-60006191;60006191;60006191-60030835;60006191,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"The locally balanced informed proposal has proved to be highly effective for sampling from discrete spaces. However, its success relies on the “local” factor, which ensures that whenever the proposal distribution is restricted to be near the current state, the locally balanced weight functions are asymptotically optimal and the gradient approximations are accurate. In seeking a more efficient sampling algorithm, many recent works have considered increasing the scale of the proposal distributions, but this causes the “local” factor to no longer hold. Instead, we propose any-scale balanced samplers to repair the gap in non-local proposals. In particular, we substitute the locally balanced function with an any-scale balanced function that can self-adjust to achieve better efficiency for proposal distributions at any scale. We also use quadratic approximations to capture curvature of the target distribution and reduce the error in the gradient approximation, while employing a Gaussian integral trick with a special estimated diagonal to efficiently sample from the quadratic proposal distribution. On various synthetic and real distributions, the proposed sampler substantially outperforms existing approaches.",,6,0.0,,,,,
2-s2.0-85188742137,,,,AR-DIFFUSION: Auto-Regressive Diffusion Model for Text Generation,cp,Conference Paper,Wu T.,60025278;60009860;60010432;60026532;60098464;126193811;128819501;131185500,Tsinghua University;Fudan University;Soochow University;Microsoft Corporation;Microsoft Research Asia;Pengcheng Laboratory;IDEA Research;Microsoft Azure AI,Beijing;Shanghai;Suzhou;Redmond;Beijing;Shenzhen;Wuhan;,China;China;China;United States;China;China;China;,12.0,"Wu, Tong;Fan, Zhihao;Liu, Xiao;Zheng, Hai Tao;Gong, Yeyun;Shen, Yelong;Jiao, Jian;Li, Juntao;Wei, Zhongyu;Guo, Jian;Duan, Nan;Chen, Weizhu",57815187200;57204472851;57206739254;57203433967;55953770000;56729408300;36470257500;57195358513;51666060600;57345851000;52163366000;23007589000,60025278;60009860;60098464;60025278-126193811;60098464;131185500;60026532;60010432;60009860;128819501;60098464;131185500,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained with a left-to-right auto-regressive approach. To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-DIFFUSION). AR-DIFFUSION ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right. In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, AR-DIFFUSION clearly demonstrated its superiority over existing diffusion language models and that it can be 100× ∼ 600× faster when achieving comparable results. Our code is available at this https URL.",,47,0.0,,,MOE,62276154,Ministry of Education of the People's Republic of China
2-s2.0-85147730093,10.1609/aaai.v36i6.20584,,,ASM2TV: An Adaptive Semi-supervised Multi-Task Multi-View Learning Framework for Human Activity Recognition,cp,Conference Paper,Chen Z.,60031031;60003088,Shandong University;The George Washington University,"Jinan;Washington, D.C.",China;United States,3.0,"Chen, Zekai;Zhang, Xiao;Cheng, Xiuzhen",57222097875;58872540500;7401754621,60003088;60031031;60031031,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,6342-6349,"Many real-world scenarios, such as human activity recognition (HAR) in IoT, can be formalized as a multi-task multi-view learning problem. Each specific task consists of multiple shared feature views collected from multiple sources, either homogeneous or heterogeneous. Common among recent approaches is to employ a typical hard/soft sharing strategy at the initial phase separately for each view across tasks to uncover common knowledge, underlying the assumption that all views are conditionally independent. On the one hand, multiple views across tasks possibly relate to each other under practical situations. On the other hand, supervised methods might be insufficient when labeled data is scarce. To tackle these challenges, we introduce a novel framework ASM2TV for semi-supervised multi-task multi-view learning. We present a new perspective named gating control policy, a learnable task-view-interacted sharing policy that adaptively selects the most desirable candidate shared block for any view across any task, which uncovers more fine-grained task-view-interacted relatedness and improves inference efficiency. Significantly, our proposed gathering consistency adaption procedure takes full advantage of large amounts of unlabeled fragmented time-series, making it a general framework that accommodates a wide range of applications. Experiments on two diverse real-world HAR benchmark datasets collected from various subjects and sources demonstrate our framework’s superiority over other state-of-the-arts. The detailed codes are available at https://github.com/zachstarkk/ASM2TV.",,8,1.0,all publisherfullgold,All Open Access Gold,NSFC,62072279,National Natural Science Foundation of China
2-s2.0-85199908202,,,,AUGMENTATION WITH PROJECTION: TOWARDS AN EFFECTIVE AND EFFICIENT DATA AUGMENTATION PARADIGM FOR DISTILLATION,cp,Conference Paper,Wang Z.,60000745;60006191;117213308,University of Illinois Urbana-Champaign;Google LLC;University of Washington,Urbana;Mountain View;WASHINGTON,United States;United States;United States,8.0,"Wang, Ziqi;Wu, Yuexin;Liu, Frederick;Liu, Daogao;Hou, Le;Yu, Hongkun;Li, Jing;Ji, Heng",57216695714;57195629718;57216612502;57222869202;57219758842;57217169447;57221574076;35240121900,60000745;60006191;60006191;117213308;60006191;60006191;60006191;60000745,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Knowledge distillation is one of the primary methods of transferring knowledge from large to small models. However, it requires massive task-specific data, which may not be plausible in many real-world applications. Data augmentation methods such as representation interpolation, token replacement, or augmentation with models are applied to tackle this problem. However, these data augmentation methods either potentially cause shifts in decision boundaries (representation interpolation), are not expressive enough (token replacement), or introduce too much computational overhead (augmentation with models). To this end, we propose AugPro (Augmentation with Projection), an effective and efficient data augmentation method for distillation. Our method builds on top of representation interpolation augmentation methods to maintain the diversity of expressions and converts the augmented data to tokens to avoid shifting decision boundaries. It uses simple operations that come with little computational overhead. The results on multiple GLUE tasks show that our methods can improve distillation performance by a large margin at a low time cost. Codes are available at https://github.com/google-research/google-research/tree/master/augpro.",,3,0.0,,,,,
2-s2.0-85147900804,,,,AUTO-TRANSFER: LEARNING TO ROUTE TRANSFERABLE REPRESENTATIONS,cp,Conference Paper,Murugesan K.,60025534;60011048,Rensselaer Polytechnic Institute;IBM Research,Troy;Yorktown Heights,United States;United States,6.0,"Murugesan, Keerthiram;Sadashivaiah, Vijay;Luss, Ronny;Shanmugam, Karthikeyan;Chen, Pin Yu;Dhurandhar, Amit",57188881575;56943483400;25630866500;59174685500;36930105800;13610030000,60011048;60025534;60011048;60011048;60011048;60011048,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Knowledge transfer between heterogeneous source and target networks and tasks has received a lot of attention in recent times as large amounts of quality labelled data can be difficult to obtain in many applications. Existing approaches typically constrain the target deep neural network (DNN) feature representations to be close to the source DNNs feature representations, which can be limiting. We, in this paper, propose a novel adversarial multi-armed bandit approach which automatically learns to route source representations to appropriate target representations following which they are combined in meaningful ways to produce accurate target models. We see upwards of 5% accuracy improvements compared with the state-of-the-art knowledge transfer methods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67 and Stanford40 where the source dataset is ImageNet. We qualitatively analyze the goodness of our transfer scheme by showing individual examples of the important features our target network focuses on in different layers compared with the (closest) competitors. We also observe that our improvement over other methods is higher for smaller target datasets making it an effective tool for small data applications that may benefit from transfer learning.",,4,0.0,,,,,
2-s2.0-85161923913,,,,AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS,cp,Conference Paper,Zhang Z.,60025084;60076757,"Shanghai Jiao Tong University;Amazon.com, Inc.",Shanghai;Seattle,China;United States,4.0,"Zhang, Zhuosheng;Zhang, Aston;Li, Mu;Smola, Alex",57203781650;56720676200;35110581300;6701849799,60025084;60076757;60076757;60076757,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Large Language Models (LLMs) can carry out complex reasoning tasks by generating intermediate reasoning steps. These steps are triggered by what is called chain-of-thought (CoT) prompting, which comes in two flavors: one leverages a simple prompt like “Let's think step by step” to facilitate step-by-step reasoning before answering a question (Zero-Shot-CoT). The other uses manual demonstrations, each composed of a question and a reasoning chain that leads to an answer (Manual-CoT). Unfortunately, the superior performance of the latter strategy crucially hinges on manually generating task-specific demonstrations. This makes it far less scalable and more dependent on the talent of the CoT engineer. We show that such manual efforts may be eliminated by leveraging LLMs to generate the reasoning chains on its own. Since these generated chains often come with mistakes we propose a number of mitigation strategies. Our proposed Auto-CoT method automaticaly samples diverse questions and we perform post-processing quality control to generate usable reasoning chains from Zero-Shot-CoT. On ten public benchmark reasoning tasks, Auto-CoT performs on par with Manual-CoT without the need for human intervention. Code is available at https://github.com/amazon-research/auto-cot.",,273,0.0,,,,,
2-s2.0-85150381129,,,,AUTOMATIC LOSS FUNCTION SEARCH FOR PREDICT-THEN-OPTIMIZE PROBLEMS WITH STRONG RANKING PROPERTY,cp,Conference Paper,Wang B.,60003500;60003059;60021726,The Ohio State University;London School of Economics and Political Science;Microsoft Research,Columbus;London;Redmond,United States;United Kingdom;United States,6.0,"Wang, Boshi;Yi, Jialin;Dong, Hang;Qiao, Bo;Luo, Chuan;Lin, Qingwei",57550765800;57233876300;57225873162;57203399691;55750203700;55370882300,60003500;60003059;60021726;60021726;60021726;60021726,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Combinatorial optimization problems with parameters to be predicted from side information are commonly seen in a variety of problems during the paradigm shifts from reactive decision making to proactive decision making. Due to the misalignment between the continuous prediction results and the discrete decisions in optimization problems, it is hard to achieve a satisfactory prediction result with the ordinary l<inf>2</inf> loss in the prediction phase. To properly connect the prediction loss with the optimization goal, in this paper we propose a total group preorder (TGP) loss and its differential version called approximate total group preorder (ATGP) loss for predict-then-optimize (PTO) problems with strong ranking property. These new losses are provably more robust than the usual l<inf>2</inf> loss in a linear regression setting and have great potential to extend to other settings. We also propose an automatic searching algorithm that adapts the ATGP loss to PTO problems with different combinatorial structures. Extensive experiments on the ranking problem, the knapsack problem, and the shortest path problem have demonstrated that our proposed method can achieve a significantly better performance compared to the other methods designed for PTO problems.",,0,0.0,,,,,
2-s2.0-85174886249,,,,AUTOTRANSFER: AUTOML WITH KNOWLEDGE TRANSFER - AN APPLICATION TO GRAPH NEURAL NETWORKS,cp,Conference Paper,Cao K.,60141508,Stanford Engineering,Stanford,United States,4.0,"Cao, Kaidi;You, Jiaxuan;Liu, Jiaju;Leskovec, Jure",57205541670;57195955464;58158464400;12241436100,60141508;60141508;60141508;60141508,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"AutoML has demonstrated remarkable success in finding an effective neural architecture for a given machine learning task defined by a specific dataset and an evaluation metric. However, most present AutoML techniques consider each task independently from scratch, which requires exploring many architectures, leading to high computational costs. Here we propose AUTOTRANSFER, an AutoML solution that improves search efficiency by transferring the prior architectural design knowledge to the novel task of interest. Our key innovation includes a task-model bank that captures the model performance over a diverse set of GNN architectures and tasks, and a computationally efficient task embedding that can accurately measure the similarity among different tasks. Based on the task-model bank and the task embeddings, we estimate the design priors of desirable models of the novel task, by aggregating a similarity-weighted sum of the top-K design distributions on tasks that are similar to the task of interest. The computed design priors can be used with any AutoML search algorithm. We evaluate AUTOTRANSFER on six datasets in the graph machine learning domain. Experiments demonstrate that (i) our proposed task embedding can be computed efficiently, and that tasks with similar embeddings have similar best-performing architectures; (ii) AUTOTRANSFER significantly improves search efficiency with the transferred design priors, reducing the number of explored architectures by an order of magnitude. Finally, we release GNN-BANK-101, a large-scale dataset of detailed GNN training information of 120,000 task-model combinations to facilitate and inspire future research.",,5,0.0,,,,,
2-s2.0-85191179278,,,,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,cp,Conference Paper,Fu Y.,60025084,Shanghai Jiao Tong University,Shanghai,China,4.0,"Fu, Yangqing;Sun, Ming;Nie, Buqing;Gao, Yue",57794891500;58265567000;57223908305;57210341899,60025084;60025084;60025084;60025084,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10% - 45% search space reduction.",,2,0.0,,,NSFC,92248303,Fundamental Research Funds for the Central Universities
2-s2.0-85174390365,,,,Accounting For Informative Sampling When Learning to Forecast Treatment Outcomes Over Time,cp,Conference Paper,Vanderschueren T.,60031101;60025063;60012937;130354078,University of Cambridge;KU Leuven;Universiteit Antwerpen;The Alan Turing Institute,Cambridge;Leuven;Antwerpen;,United Kingdom;Belgium;Belgium;Germany,4.0,"Vanderschueren, Toon;Curth, Alicia;Verbeke, Wouter;van der Schaar, Mihaela",57453674700;57216499761;16065279300;35605361700,60025063-60012937;60031101;60025063;60031101-130354078,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,34855-34874,"Machine learning (ML) holds great potential for accurately forecasting treatment outcomes over time, which could ultimately enable the adoption of more individualized treatment strategies in many practical applications. However, a significant challenge that has been largely overlooked by the ML literature on this topic is the presence of informative sampling in observational data. When instances are observed irregularly over time, sampling times are typically not random, but rather informative-depending on the instance's characteristics, past outcomes, and administered treatments. In this work, we formalize informative sampling as a covariate shift problem and show that it can prohibit accurate estimation of treatment outcomes if not properly accounted for. To overcome this challenge, we present a general framework for learning treatment outcomes in the presence of informative sampling using inverse intensity-weighting, and propose a novel method, TESAR-CDE, that instantiates this framework using Neural CDEs. Using a simulation environment based on a clinical use case, we demonstrate the effectiveness of our approach in learning under informative sampling.",,4,0.0,,,FWO,11I7322N,AstraZeneca
2-s2.0-85163222232,,,,Action-modulated midbrain dopamine activity arises from distributed control policies,cp,Conference Paper,Lindsey J.,60030162,Columbia University,New York,United States,2.0,"Lindsey, Jack;Litwin-Kumar, Ashok",57225828792;54881314900,60030162;60030162,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Animal behavior is driven by multiple brain regions working in parallel with distinct control policies. We present a biologically plausible model of off-policy reinforcement learning in the basal ganglia, which enables learning in such an architecture. The model accounts for action-related modulation of dopamine activity that is not captured by previous models that implement on-policy algorithms. In particular, the model predicts that dopamine activity signals a combination of reward prediction error (as in classic models) and “action surprise,"" a measure of how unexpected an action is relative to the basal ganglia's current policy. In the presence of the action surprise term, the model implements an approximate form of Q-learning. On benchmark navigation and reaching tasks, we show empirically that this model is capable of learning from data driven completely or in part by other policies (e.g. from other brain regions). By contrast, models without the action surprise term suffer in the presence of additional policies, and are incapable of learning at all from behavior that is completely externally driven. The model provides a computational account for numerous experimental findings about dopamine activity that cannot be explained by classic models of reinforcement learning in the basal ganglia. These include differing levels of action surprise signals in dorsal and ventral striatum, decreasing amounts of movement-modulated dopamine activity with practice, and representations of action initiation and kinematics in dopamine activity. It also provides further predictions that can be tested with recordings of striatal dopamine activity.",,6,0.0,,,NSF,DBI–1707398,National Science Foundation
2-s2.0-85178999418,10.1613/JAIR.1.14819,,,Actor Prioritized Experience Replay,ar,Article,Saglam B.,60157832;60014808,Yale School of Engineering & Applied Science;Bilkent Üniversitesi,New Haven;Ankara,United States;Turkey,4.0,"Saglam, Baturay;Mutlu, Furkan B.;Cicek, Dogan C.;Kozat, Suleyman S.",57283902400;57283678500;57284558900;6603217246,60157832;60014808;60014808;60014808,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,639-672,"A widely-studied deep reinforcement learning (RL) technique known as Prioritized Experience Replay (PER) allows agents to learn from transitions sampled with non-uniform probability proportional to their temporal-difference (TD) error. Although it has been shown that PER is one of the most crucial components for the overall performance of deep RL methods in discrete action domains, many empirical studies indicate that it considerably underperforms off-policy actor-critic algorithms. We theoretically show that actor networks cannot be effectively trained with transitions that have large TD errors. As a result, the approximate policy gradient computed under the Q-network diverges from the actual gradient computed under the optimal Q-function. Motivated by this, we introduce a novel experience replay sampling framework for actor-critic methods, which also regards issues with stability and recent findings behind the poor empirical performance of PER. The introduced algorithm suggests a new branch of improvements to PER and schedules effective and efficient training for both actor and critic networks. An extensive set of experiments verifies our theoretical findings, showing that our method outperforms competing approaches and achieves state-of-the-art results over the standard off-policy actor-critic algorithms.",,34,1.0,all publisherfullgold,All Open Access Gold,,Incentivo/SAU/LA0003/2013,Bilkent Üniversitesi
2-s2.0-85163051148,,,,AdAUC: End-to-end Adversarial AUC Optimization Against Long-tail Problems,cp,Conference Paper,Hou W.,60019499;60027363;60030904;60271961;60273040;60118460,Chinese Academy of Sciences;University of Chinese Academy of Sciences;Institute of Computing Technology Chinese Academy of Sciences;Peng Cheng Laboratory;Institute of Information Engineering;Alibaba Group Holding Limited,Beijing;Beijing;Beijing;Shenzhen;Beijing;Hangzhou,China;China;China;China;China;China,6.0,"Hou, Wenzheng;Xu, Qianqian;Yang, Zhiyong;Bao, Shilong;He, Yuan;Huang, Qingming",57793768400;35410751100;55605770884;57211692440;57215363339;8435766200,60030904-60027363;60030904;60027363;60273040-60027363;60118460;60030904-60027363-60019499-60271961,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,8903-8925,"It is well-known that deep learning models are vulnerable to adversarial examples. Existing studies of adversarial training have made great progress against this challenge. As a typical trait, they often assume that the class distribution is overall balanced. However, long-tail datasets are ubiquitous in a wide spectrum of applications, where the amount of head class instances is larger than the tail classes. Under such a scenario, AUC is a much more reasonable metric than accuracy since it is insensitive toward class distribution. Motivated by this, we present an early trial to explore adversarial training methods to optimize AUC. The main challenge lies in that the positive and negative examples are tightly coupled in the objective function. As a direct result, one cannot generate adversarial examples without a full scan of the dataset. To address this issue, based on a concavity regularization scheme, we reformulate the AUC optimization problem as a saddle point problem, where the objective becomes an instance-wise function. This leads to an end-to-end training protocol. Furthermore, we provide a convergence guarantee of the proposed algorithm. Our analysis differs from the existing studies since the algorithm is asked to generate adversarial examples by calculating the gradient of a min-max problem. Finally, the extensive experimental results show the performance and robustness of our algorithm in three long-tail datasets.",,8,0.0,,,NSFC,61931008,National Natural Science Foundation of China
2-s2.0-105018463394,,,,Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees,ar,Article,Xiao N.,60017161;60003707;60098513,National University of Singapore;Academy of Mathematics and System Sciences Chinese Academy of Sciences;Hangzhou City University,Singapore City;Beijing;Hangzhou,Singapore;China;China,4.0,"Xiao, Nachuan;Hu, Xiaoyin;Liu, Xin;Toh, Kim Chuan",57220030182;57214244173;60137513000;24294588700,60017161;60098513;60003707;60017161,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.",Adam | gradient clipping | nonconvex optimization | nonsmooth optimization | stochastic subgradient methods,18,0.0,,,ZJNSF,LQ23A010002,Natural Science Foundation of Zhejiang Province
2-s2.0-85191184342,,,,AdaptSSR: Pre-training User Model with Augmentation-Adaptive Self-Supervised Ranking,cp,Conference Paper,Yu Y.,60019118;60002836;128411864;126182441,University of Science and Technology of China;Hefei University of Technology;State Key Laboratory of Cognitive Intelligence;OPPO Research Institute,Hefei;Hefei;Hefei;Shenzhen,China;China;China;China,10.0,"Yu, Yang;Liu, Qi;Zhang, Kai;Zhang, Yuren;Song, Chao;Hou, Min;Yuan, Yuqing;Ye, Zhihao;Zhang, Zaixi;Yu, Sanshi Lei",57195493658;56382635200;58376409600;57218706175;58731248100;57211746749;58669010400;58731858300;57224945168;58030746700,60019118-128411864;60019118-128411864;60019118-128411864;60019118-128411864;126182441;60002836;126182441;126182441;60019118-128411864;60019118-128411864,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"User modeling, which aims to capture users' characteristics or interests, heavily relies on task-specific labeled data and suffers from the data sparsity issue. Several recent studies tackled this problem by pre-training the user model on massive user behavior sequences with a contrastive learning task. Generally, these methods assume different views of the same behavior sequence constructed via data augmentation are semantically consistent, i.e., reflecting similar characteristics or interests of the user, and thus maximizing their agreement in the feature space. However, due to the diverse interests and heavy noise in user behaviors, existing augmentation methods tend to lose certain characteristics of the user or introduce noisy behaviors. Thus, forcing the user model to directly maximize the similarity between the augmented views may result in a negative transfer. To this end, we propose to replace the contrastive learning task with a new pretext task: Augmentation-Adaptive Self-Supervised Ranking (AdaptSSR), which alleviates the requirement of semantic consistency between the augmented views while pre-training a discriminative user model. Specifically, we adopt a multiple pairwise ranking loss which trains the user model to capture the similarity orders between the implicitly augmented view, the explicitly augmented view, and views from other users. We further employ an in-batch hard negative sampling strategy to facilitate model training. Moreover, considering the distinct impacts of data augmentation on different behavior sequences, we design an augmentation-adaptive fusion mechanism to automatically adjust the similarity order constraint applied to each sample based on the estimated similarity between the augmented views. Extensive experiments on both public and industrial datasets with six downstream tasks verify the effectiveness of AdaptSSR.",,1,0.0,,,NKRDPC,2021YFF0901003,National Key Research and Development Program of China
2-s2.0-85174403232,,,,Adapting to game trees in zero-sum imperfect information games,cp,Conference Paper,Fiegel C.,60005667;60111161;60022532;60272370;114886459,École Normale Supérieure de Lyon;DeepMind Technologies Limited;ENSAE Paris;OMRON SINIC X Corporation;Criteo AI Lab,Lyon;London;Palaiseau;Tokyo;Paris,France;United Kingdom;France;Japan;France,6.0,"Fiegel, Côme;Ménard, Pierre;Kozuno, Tadashi;Munos, Rémi;Perchet, Vianney;Valko, Michal",58040835300;57209736581;42961642900;6603222926;36020384800;25642222500,60022532;60005667;60272370;60111161;60022532-114886459;60111161,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,10093-10135,"Imperfect information games (IIG) are games in which each player only partially observes the current game state. We study how to learn ε-optimal strategies in a zero-sum IIG through self-play with trajectory feedback. We give a problem-independent lower bound Õ(H(A<inf>X</inf> + B<inf>Y</inf>)/ε<sup>2</sup>) on the required number of realizations to learn these strategies with high probability, where H is the length of the game, A<inf>X</inf> and B<inf>Y</inf> are the total number of actions for the two players. We also propose two Follow the Regularized leader (FTRL) algorithms for this setting: Balanced FTRL which matches this lower bound, but requires the knowledge of the information set structure beforehand to define the regularization; and Adaptive FTRL which needs Õ(H<sup>2</sup>(A<inf>X</inf> + B<inf>Y</inf>)/ε<sup>2</sup>) realizations without this requirement by progressively adapting the regularization to the observations.",,9,0.0,,,ANR,ANR-19-CE23-0026,Agence Nationale de la Recherche
2-s2.0-85148747287,,,,Adaptive Accelerated (Extra-)Gradient Methods with Variance Reduction,cp,Conference Paper,Liu Z.,60019674;60141178,Boston University;Khoury College of Computer Sciences,Boston;Boston,United States;United States,4.0,"Liu, Zijian;Nguyen, Ta Duy;Ene, Alina;Nguyen, Huy L.",57440607600;57207161045;35111058000;58314632100,60019674;60019674;60019674;60141178,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,13947-13994,"In this paper, we study the finite-sum convex optimization problem focusing on the general convex case. Recently, the study of variance reduced (VR) methods and their accelerated variants has made exciting progress. However, the step size used in the existing VR algorithms typically depends on the smoothness parameter, which is often unknown and requires tuning in practice. To address this problem, we propose two novel adaptive VR algorithms: Adaptive Variance Reduced Accelerated Extra-Gradient (AdaVRAE) and Adaptive Variance Reduced Accelerated Gradient (AdaVRAG). Our algorithms do not require knowledge of the smoothness parameter. AdaVRAE uses O (equation presented) and AdaVRAG uses O (equation presented) gradient evaluations to attain an O(ε)-suboptimal solution, where n is the number of functions in the finite sum and β is the smoothness parameter. This result matches the best-known convergence rate of non-adaptive VR methods and it improves upon the convergence of the state of the art adaptive VR method, AdaSVRG. We demonstrate the superior performance of our algorithms compared with previous methods in experiments on real-world datasets.",,3,0.0,,,NSF,CCF-1750716,National Science Foundation
2-s2.0-85176324811,,,,Adaptive False Discovery Rate Control with Privacy Guarantee,ar,Article,Xia X.,60145787;60183197,College of Liberal Arts and Sciences;HKU Business School,Ames;Hong Kong,United States;Hong Kong,2.0,"Xia, Xintao;Cai, Zhanrui",58411874500;57217221464,60145787;60183197,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,252,,"Differentially private multiple testing procedures can protect the information of individuals used in hypothesis tests while guaranteeing a small fraction of false discoveries. In this paper, we propose a differentially private adaptive FDR control method that can control the classic FDR metric exactly at a user-specified level α with a privacy guarantee, which is a non-trivial improvement compared to the differentially private Benjamini-Hochberg method proposed in Dwork et al. (2021). Our analysis is based on two key insights: 1) a novel p-value transformation that preserves both privacy and the mirror conservative property, and 2) a mirror peeling algorithm that allows the construction of the filtration and application of the optimal stopping technique. Numerical studies demonstrate that the proposed DP-AdaPT performs better compared to the existing differentially private FDR control methods. Compared to the non-private AdaPT, it incurs a small accuracy loss but significantly reduces the computation cost.",differential privacy | false discovery rate | model-free | Selective inference,2,0.0,,,,,
2-s2.0-85147994660,,,,Adaptive Greedy Algorithm for Moderately Large Dimensions in Kernel Conditional Density Estimation,ar,Article,Nguyen M.L.J.,60019816;60138422;60105735,Universiteit Leiden;Université Gustave Eiffel;Centre de Recherche en Mathematiques de la Decision,Leiden;Marne-la-Vallee;Paris,Netherlands;France;France,3.0,"Nguyen, Minh Lien Jeanne;Lacour, Claire;Rivoirard, Vincent",57219494299;23091730400;15763776400,60019816;60138422;60105735,2022-08-01,1 August 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"This paper studies the estimation of the conditional density f(x, ·) of Y<inf>i</inf> given X<inf>i</inf> = x, from the observation of an i.i.d. sample (X<inf>i</inf>, Y<inf>i</inf>) ∈ R<sup>d</sup>, i ∈ {1, . . ., n}. We assume that f depends only on r unknown components with typically r d. We provide an adaptive fully-nonparametric strategy based on kernel rules to estimate f. To select the bandwidth of our kernel rule, we propose a new fast iterative algorithm inspired by the Rodeo algorithm (Wasserman and Lafferty, 2006) to detect the sparsity structure of f. More precisely, in the minimax setting, our pointwise estimator, which is adaptive to both the regularity and the sparsity, achieves the quasi-optimal rate of convergence. Our results also hold for (unconditional) density estimation. The computational complexity of our method is only O(dn log n). A deep numerical study shows nice performances of our approach.",Conditional density | Greedy algorithm | Kernel density estimators | Minimax rates | Sparsity,0,0.0,,,,,
2-s2.0-85132021168,10.1613/jair.1.12997,,,Adaptive Greedy versus Non-Adaptive Greedy for Influence Maximization,ar,Article,Chen W.,60025778;60025084;60031321;60021726,"University of Michigan, Ann Arbor;Shanghai Jiao Tong University;The Fu Foundation School of Engineering and Applied Science;Microsoft Research",Ann Arbor;Shanghai;New York;Redmond,United States;China;United States;United States,4.0,"Chen, Wei;Peng, Binghui;Schoenebeck, Grant;Tao, Biaoshuai",57087470200;57209236857;22938696100;55445473500,60021726;60031321;60025778;60025084,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,303-351,"We consider the adaptive influence maximization problem: given a network and a budget k, iteratively select k seeds in the network to maximize the expected number of adopters. In the full-adoption feedback model, after selecting each seed, the seed-picker observes all the resulting adoptions. In the myopic feedback model, the seed-picker only observes whether each neighbor of the chosen seed adopts. Motivated by the extreme success of greedy-based algorithms/heuristics for influence maximization, we propose the concept of greedy adaptivity gap, which compares the performance of the adaptive greedy algorithm to its non-adaptive counterpart. Our first result shows that, for submodular influence maximization, the adaptive greedy algorithm can perform up to a (1 − 1/e)-fraction worse than the non-adaptive greedy algorithm, and that this ratio is tight. More specifically, on one side we provide examples where the performance of the adaptive greedy algorithm is only a (1−1/e) fraction of the performance of the non-adaptive greedy algorithm in four settings: for both feedback models and both the independent cascade model and the linear threshold model. On the other side, we prove that in any submodular cascade, the adaptive greedy algorithm always outputs a (1 − 1/e)-approximation to the expected number of adoptions in the optimal non-adaptive seed choice. Our second result shows that, for the general submodular diffusion model with full-adoption feedback, the adaptive greedy algorithm can outperform the non-adaptive greedy algorithm by an unbounded factor. Finally, we propose a risk-free variant of the adaptive greedy algorithm that always performs no worse than the non-adaptive greedy algorithm.",,12,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,NSF,1535912,National Science Foundation
2-s2.0-85147603366,10.1609/aaai.v36i3.20201,,,Adaptive Hypergraph Neural Network for Multi-Person Pose Estimation,cp,Conference Paper,Xu X.,60022381,Beijing Jiaotong University,Beijing,China,3.0,"Xu, Xixia;Zou, Qi;Lin, Xue",57216038977;7103094214;59645531800,60022381;60022381;60022381,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,2955-2963,"This paper proposes a novel two-stage hypergraph-based framework, dubbed ADaptive Hypergraph Neural Network (AD-HNN) to estimate multiple human poses from a single image, with a keypoint localization network and an Adaptive-Pose Hypergraph Neural Network (AP-HNN) added onto the former network. For providing better guided representations of AP-HNN, we employ a Semantic Interaction Convolution (SIC) module within the initial localization network to acquire more explicit predictions. Build upon this, we design a novel adaptive hypergraph to represent a human body for capturing high-order semantic relations among different joints. Notably, it can adaptively adjust the relations between joints and seek the most reasonable structure for the variable poses to benefit the keypoint localization. These two stages are combined to be trained in an end-to-end fashion. Unlike traditional Graph Convolutional Networks (GCNs) that are based on a fixed tree structure, AP-HNN can deal with ambiguity in human pose estimation. Experimental results demonstrate that the AD-HNN achieves state-of-the-art performance both on the MS-COCO, MPII and CrowdPose datasets.",,17,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-105018577974,,,,Additive smoothing error in backward variational inference for general state-space models,ar,Article,Chagneux M.,60029116;60004130;60111003;60106218,"Télécom Paris;Université de Bretagne-Sud;Laboratoire de Probabilités, Statistique et Modélisation;Laboratoire de Mathématiques d'Orsay",Palaiseau;Lorient;Paris;Orsay,France;France;France;France,4.0,"Chagneux, Mathis;Gassiat, Élisabeth;Gloaguen, Pierre;Le Corff, Sylvain",57748685200;6603584305;56481565700;50361404200,60029116;60106218;60004130;60111003,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"We consider the problem of state estimation in general state-space models using variational inference. For a generic variational family defined using the same backward decomposition as the actual joint smoothing distribution, we establish under mixing assumptions that the variational approximation of expectations of additive state functionals induces an error which grows at most linearly in the number of observations. This guarantee is consistent with the known upper bounds for the approximation of smoothing distributions using standard Monte Carlo methods. We illustrate our theoretical result with state-of-the art variational solutions based both on the backward parameterization and on alternatives using forward decompositions. This numerical study proposes guidelines for variational inference based on neural networks in state-space models.",Backward decomposition | Smoothing | State inference | State-space models | Variational inference,3,0.0,,,,,
2-s2.0-85170363146,10.24963/ijcai.2023/289,,,Adversarial Contention Resolution Games,cp,Conference Paper,Chionas G.,60020661;60105348,University of Liverpool;Augusta University,Liverpool;Augusta,United Kingdom;United States,4.0,"Chionas, Giorgos;Chlebus, Bogdan S.;Kowalski, Dariusz R.;Krysta, Piotr",57221094196;7003965940;7005277945;6603178483,60020661;60105348;60105348;60020661,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,2598-2606,"We study contention resolution (CR) on a shared channel modelled as a game with selfish players. There are n agents and the adversary chooses some k ≤ n of them as players. Each participating player in a CR game has a packet to transmit. A transmission is successful if it is performed as the only one at a round. Each player aims to minimize its packet latency. We introduce the notion of adversarial equilibrium (AE), which incorporates adversarial selection of players. We develop efficient deterministic communication algorithms that are also AE. We characterize the price of anarchy in the CR games with respect to AE.",,1,1.0,all publisherfullgold,All Open Access Gold,UoL,2131538,University of Liverpool
2-s2.0-85163101102,,,,Adversarial Robustness against Multiple and Single lp-Threat Models via Quick Fine-Tuning of Robust Classifiers,cp,Conference Paper,Croce F.,60017246,Eberhard Karls Universität Tübingen,Tubingen,Germany,2.0,"Croce, Francesco;Hein, Matthias",57208002969;55171596600,60017246;60017246,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,4436-4454,"A major drawback of adversarially robust models, in particular for large scale datasets like ImageNet, is the extremely long training time compared to standard ones. Moreover, models should be robust not only to one l<inf>p</inf>-threat model but ideally to all of them. In this paper we propose Extreme norm Adversarial Training (E-AT) for multiple-norm robustness which is based on geometric properties of l<inf>p</inf>-balls. E-AT costs up to three times less than other adversarial training methods for multiple-norm robustness. Using E-AT we show that for ImageNet a single epoch and for CIFAR-10 three epochs are sufficient to turn any l<inf>p</inf>-robust model into a multiple-norm robust model. In this way we get the first multiple-norm robust model for ImageNet and boost the state-of-the-art for multiple-norm robustness to more than 51% on CIFAR-10. Finally, we study the general transfer via finetuning of adversarial robustness between different individual l<inf>p</inf>-threat models and improve the previous SOTA l<inf>1</inf>-robustness on both CIFAR-10 and ImageNet. Extensive experiments show that our scheme works across datasets and architectures including vision transformers.",,15,0.0,,,DFG,389792660,Deutsche Forschungsgemeinschaft
2-s2.0-85177454504,,,,Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach,cp,Conference Paper,Zhao K.,60005510;129304167,Nanyang Technological University;C3 AI,Singapore City;Singapore City,Singapore;Singapore,6.0,"Zhao, Kai;Kang, Qiyu;Song, Yang;She, Rui;Wang, Sijie;Tay, Wee Peng",57909978400;57196022385;56149485400;59796780200;57723148500;18233764400,60005510;60005510;129304167;60005510;60005510;60005510,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experiments is available at https://github.com/zknus/NeurIPS-2023-HANG-Robustness.",,25,0.0,,,MOE,MOE-T2EP20220-0002,Ministry of Education - Singapore
,,,,"Align, Perturb and Decouple: Toward Better Leverage of Difference Information for RSI Change Detection",,,,,,,,,,,,,,,,,,,,,,,,,8,,,,,,
2-s2.0-105000518776,,,,AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models,cp,Conference Paper,Lu H.,60026851;60025038;60013372;60007174;60018038;60010756;60033286,"University of Oxford;University of California, Berkeley;The University of Texas at Austin;Lawrence Berkeley National Laboratory;Nankai University;Dartmouth College;International Computer Science Institute",Oxford;Berkeley;Austin;Berkeley;Tianjin;Hanover;Berkeley,United Kingdom;United States;United States;United States;China;United States;United States,6.0,"Lu, Haiquan;Zhou, Yefan;Liu, Shiwei;Wang, Zhangyang;Mahoney, Michael W.;Yang, Yaoqing",59253445800;57311154000;57217824780;56288839400;7202006961;57201950952,60018038;60010756;60026851;60013372;60033286-60007174-60025038;60010756,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Recent work on pruning large language models (LLMs) has shown that one can eliminate a large number of parameters without compromising performance, making pruning a promising strategy to reduce LLM model size. Existing LLM pruning strategies typically assign uniform pruning ratios across layers, limiting overall pruning ability; and recent work on layerwise pruning of LLMs is often based on heuristics that can easily lead to suboptimal performance. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory, in particular the shape of empirical spectral densities (ESDs) of weight matrices, to design improved layerwise pruning ratios for LLMs. Our analysis reveals a wide variability in how well-trained, and thus relatedly how prunable, different layers of an LLM are. Based on this, we propose AlphaPruning, which uses shape metrics to allocate layerwise sparsity ratios in a more theoretically-principled manner. AlphaPruning can be used in conjunction with multiple existing LLM pruning methods. Our empirical results show that AlphaPruning prunes LLaMA-7B to 80% sparsity while maintaining reasonable perplexity, marking a first in the literature on LLMs. We have open-sourced our code.",,9,0.0,,,NSF,W911NF20C0035,National Science Foundation
2-s2.0-85142029797,10.1613/jair.1.13706,,,Altruistic Hedonic Games,ar,Article,Kerkmann A.M.,60025310,Heinrich-Heine-Universität Düsseldorf,Dusseldorf,Germany,7.0,"Kerkmann, Anna Maria;Nguyen, Nhan Tam;Rey, Anja;Rey, Lisa;Rothe, Jörg;Schend, Lena;Wiechers, Alessandra",57212537658;36625818500;36447871100;56938200800;7006973162;55322329500;57218957971,60025310;60025310;60025310;60025310;60025310;60025310;60025310,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,129-169,"Hedonic games are coalition formation games in which players have preferences over the coalitions they can join. For a long time, all models of representing hedonic games were based upon selfish players only. Among the known ways of representing hedonic games compactly, we focus on friend-oriented hedonic games and propose a novel model for them that takes into account not only the players’ own preferences but also their friends’ preferences. Depending on the order in which players look at their own or their friends’ preferences, we distinguish three degrees of altruism: selfish-first, equal-treatment, and altruistic-treatment preferences. We study both the axiomatic properties of these games and the computational complexity of problems related to various common stability concepts.",,13,1.0,all publisherfullgold,All Open Access Gold,DFG,RO-1202/14-2,Deutsche Forschungsgemeinschaft
2-s2.0-85137871297,10.24963/ijcai.2022/704,,,Am I No Good? Towards Detecting Perceived Burdensomeness and Thwarted Belongingness from Suicide Notes,cp,Conference Paper,Ghosh S.,60014153;60104342,Indian Institute of Technology Bombay;Indian Institute of Technology Patna,Mumbai;Patna,India;India,3.0,"Ghosh, Soumitra;Ekbal, Asif;Bhattacharyya, Pushpak",57191619547;23093674100;7101803108,60104342;60104342;60014153,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,5073-5079,"The World Health Organization (WHO) has emphasized the importance of significantly accelerating suicide prevention efforts to fulfill the United Nations' Sustainable Development Goal (SDG) objective of 2030. In this paper, we present an end-to-end multitask system to address a novel task of detection of two interpersonal risk factors of suicide, Perceived Burdensomeness (PB) and Thwarted Belongingness (TB) from suicide notes. We also introduce a manually translated code-mixed suicide notes corpus, CoMCEASE-v2.0, based on the benchmark CEASE-v2.0 dataset, annotated with temporal orientation, PB and TB labels. We exploit the temporal orientation and emotion information in the suicide notes to boost overall performance. For comprehensive evaluation of our proposed method, we compare it to several state-of-the-art approaches on the existing CEASE-v2.0 dataset and the newly announced CoMCEASEv2.0 dataset. Empirical evaluation suggests that temporal and emotional information can substantially improve the detection of PB and TB.",,2,1.0,all publisherfree2read,All Open Access Bronze,Meity,,Ministry of Electronics and Information technology
2-s2.0-85185200429,10.1613/jair.1.15249,,,An Algorithm with Improved Complexity for Pebble Motion/Multi-Agent Path Finding on Trees,ar,Article,Ardizzoni S.,60025641;60004969,Universität Freiburg;Università di Parma,Freiburg im Breisgau;Parma,Germany;Italy,5.0,"Ardizzoni, Stefano;Saccani, Irene;Consolini, Luca;Locatelli, Marco;Nebel, Bernhard",57222421245;57900999600;6602431976;55335875600;7003717286,60004969;60004969;60004969;60004969;60025641,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,483-514,"The pebble motion on trees (PMT) problem consists in finding a feasible sequence of moves that repositions a set of pebbles to assigned target vertices. This problem has been widely studied because, in many cases, the more general Multi-Agent path finding (MAPF) problem on graphs can be reduced to PMT. We propose a simple and easy to implement procedure, which finds solutions of length O(|P|nc + n<sup>2</sup>), where n is the number of nodes, P is the set of pebbles, and c the maximum length of corridors in the tree. This complexity result is more detailed than the current best known result O(n<sup>3</sup>), which is equal to our result in the worst case, but does not capture the dependency on c and |P|.",,2,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85137861056,10.24963/ijcai.2022/3,,,An EF2X Allocation Protocol for Restricted Additive Valuations,cp,Conference Paper,Akrami H.,60033241;60017853;60000256;60150459,Universität des Saarlandes;Iranian Research Institute for Fundamental Sciences;Max Planck Institute for Informatics;Department of Computer Science,Saarbrucken;Tehran;Saarbrucken;Austin,Germany;Iran;Germany;United States,3.0,"Akrami, Hannaneh;Rezvan, Rojin;Seddighin, Masoud",57209890006;57218605454;56530663600,60000256-60033241;60150459;60017853,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,17-23,"We study the problem of fairly allocating a set of indivisible goods to a set of n agents. Envy-freeness up to any good (EFX) criterion (which requires that no agent prefers the bundle of another agent after the removal of any single good) is known to be a remarkable analogue of envy-freeness when the resource is a set of indivisible goods. In this paper, we investigate EFX for restricted additive valuations, that is, every good has a non-negative value, and every agent is interested in only some of the goods. We introduce a natural relaxation of EFX called EFkX which requires that no agent envies another agent after the removal of any k goods. Our main contribution is an algorithm that finds a complete (i.e., no good is discarded) EF2X allocation for restricted additive valuations. In our algorithm we devise new concepts, namely configuration and envy-elimination that might be of independent interest. We also use our new tools to find an EFX allocation for restricted additive valuations that discards at most [n/2] - 1 goods.",,18,1.0,all publisherfree2read,All Open Access Bronze,,,
2-s2.0-85204286376,,,,An NCDE-based Framework for Universal Representation Learning of Time Series,cp,Conference Paper,Liu Z.,60013789;60280914,Beihang University;Shanghai Artificial Intelligence Laboratory,Beijing;Shanghai,China;China,5.0,"Liu, Zihan;Du, Bowen;Ye, Junchen;Wen, Xianqing;Sun, Leilei",57789607700;25930830100;57210644346;59332574600;55493029500,60013789-60280914;60013789;60013789;60013789;60013789,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4623-4633,"Exploiting self-supervised learning (SSL) to extract the universal representations of time series could not only capture the natural properties of time series but also offer huge help to the downstream tasks. Nevertheless, existing time series representation learning (TSRL) methods face challenges in attaining universality. Indeed, existing methods relying solely on one SSL strategy (either contrastive learning (CL) or generative) often fall short in capturing rich semantic information for various downstream tasks. Moreover, time series exhibit diverse distributions and inherent characteristics, particularly with the common occurrence of missing values, posing a notable challenge for existing backbones in effectively handling such diverse time series data. To bridge these gaps, we propose CTRL, a framework for universal TSRL. For the first time, we employ Neural Controlled Differential Equation (NCDE) as the backbone for TSRL, which captures the continuous processes and exhibits robustness to missing data. Additionally, a dual-task SSL strategy, integrating both reconstruction and contrasting tasks, is proposed to enrich the semantic information of the learned representations. Furthermore, novel hard negative construction and false negative elimination mechanisms are proposed to improve sampling efficiency and reduce sampling bias in CL. Finally, extensive experiments demonstrate the superiority of CTRL in forecasting, classification, and imputation tasks, particularly its outstanding robustness to missing data.",,2,0.0,,,NKRDPC,2021ZD0111201,National Key Research and Development Program of China
,,,,An empirical analysis of compute-optimal large language model training,,,,,,,,,,,,,,,,,,,,,,,,,5,,,,,,
2-s2.0-85130506796,10.1609/aaai.v36i11.21500,,,Anatomizing Bias in Facial Analysis,cp,Conference Paper,Singh R.,60104343;60105479,"Indian Institute of Technology Jodhpur;Indraprastha Institute of Information Technology, Delhi",Jodhpur;New Delhi,India;India,4.0,"Singh, Richa;Majumdar, Puspita;Mittal, Surbhi;Vatsa, Mayank",15061841400;57205627176;57221140944;55908650100,60104343;60104343-60105479;60104343;60104343,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,12351-12358,"Existing facial analysis systems have been shown to yield biased results against certain demographic subgroups. Due to its impact on society, it has become imperative to ensure that these systems do not discriminate based on gender, identity, or skin tone of individuals. This has led to research in the identification and mitigation of bias in AI systems. In this paper, we encapsulate bias detection/estimation and mitigation algorithms for facial analysis. Our main contributions include a systematic review of algorithms proposed for understanding bias, along with a taxonomy and extensive overview of existing bias mitigation algorithms. We also discuss open challenges in the field of biased facial analysis.",,21,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,डीएसटी,,"Department of Science and Technology, Ministry of Science and Technology, India"
2-s2.0-85191166099,,,,Anytime-Competitive Reinforcement Learning with Policy Prior,cp,Conference Paper,Yang J.,60031581;60029526;60108865,"California Institute of Technology;University of California, Riverside;The Chinese University of Hong Kong, Shenzhen",Pasadena;Riverside;Shenzhen,United States;United States;China,5.0,"Yang, Jianyi;Li, Pengfei;Li, Tongxin;Wierman, Adam;Ren, Shaolei",57215586013;57279218300;57203617780;8148116600;24773942300,60029526;60029526;60108865;60031581;60029526,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"This paper studies the problem of Anytime-Competitive Markov Decision Process (A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim to optimize the expected reward while constraining the expected cost over random dynamics, but the cost in a specific episode can still be unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the expected reward while guaranteeing a bounded cost in each round of any episode against a policy prior. We propose a new algorithm, called Anytime-Competitive Reinforcement Learning (ACRL), which provably guarantees the anytime cost constraints. The regret analysis shows the policy asymptotically matches the optimal reward achievable under the anytime competitive constraints. Experiments on the application of carbon-intelligent computing verify the reward performance and cost constraint guarantee of ACRL.",,4,0.0,,,NSFC,UDF01002773,National Natural Science Foundation of China
2-s2.0-85212786660,10.1613/jair.1.16374,,,Approximate Counting of Linear Extensions in Practice,ar,Article,Talvitie T.,60002952,Helsingin Yliopisto,Helsinki,Finland,2.0,"Talvitie, Topi;Koivisto, Mikko",56275714600;8540307200,60002952;60002952,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,643-681,"We investigate the problem of computing the number of linear extensions of a given partial order on n elements. The problem has applications in numerous areas, such as sorting, planning, and learning graphical models. The problem is #P-hard but admits fully polynomial-time approximation schemes. However, the polynomial complexity bounds of the known schemes involve high degrees and large constant factors, rendering the schemes only feasible when n is some dozens. We present novel schemes, which stem from the idea of not requiring provable polynomial worst-case running time bounds. Using various new algorithmic techniques and implementation optimizations, we discover schemes that yield speedups by several orders of magnitude, enabling accurate approximations even when n is in several hundreds.",,0,1.0,all publisherfullgold,All Open Access Gold,AKA,316771,Research Council of Finland
2-s2.0-85204308684,,,,Approximate Dec-POMDP Solving Using Multi-Agent A,cp,Conference Paper,Koops W.,60005322;60016529,Ruhr-Universitat Bochum;Radboud Universiteit,Bochum;Nijmegen,Germany;Netherlands,3.0,"Koops, Wietze;Junges, Sebastian;Jansen, Nils",58568962400;55321319200;36646082100,60016529;60016529;60016529-60005322,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,6743-6751,"We present an A<sup>∗</sup>-based algorithm to compute policies for finite-horizon Dec-POMDPs. Our goal is to sacrifice optimality in favor of scalability for larger horizons. The main ingredients of our approach are (1) using clustered sliding window memory, (2) pruning the A<sup>∗</sup> search tree, and (3) using novel A<sup>∗</sup> heuristics. Our experiments show competitive performance to the state-of-the-art. Moreover, for multiple benchmarks, we achieve superior performance. In addition, we provide an A<sup>∗</sup> algorithm that finds upper bounds for the optimum, tailored towards problems with long horizons. The main ingredient is a new heuristic that periodically reveals the state, thereby limiting the number of reachable beliefs. Our experiments demonstrate the efficacy and scalability of the approach.",,1,0.0,,,ERC,101077178,European Research Council
2-s2.0-85124176414,,,,Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks,ar,Article,Li Z.,60014966;60017161;60003269,Peking University;National University of Singapore;Princeton University,Beijing;Singapore City;Princeton,China;Singapore;United States,4.0,"Li, Zhong;Han, Jiequn;Weinan, E.;Li, Qianxiao",57221150938;57221974701;57194935723;57200083741,60014966;60003269;60003269;60017161,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"We perform a systematic study of the approximation properties and optimization dynamics of recurrent neural networks (RNNs) when applied to learn input-output relationships in temporal data. We consider the simple but representative setting of using continuous-time linear RNNs to learn from data generated by linear relationships. On the approximation side, we prove a direct and an inverse approximation theorem of linear functionals using RNNs, which reveal the intricate connections between memory structures in the target and the corresponding approximation efficiency. In particular, we show that temporal relationships can be effectively approximated by RNNs if and only if the former possesses sufficient memory decay. On the optimization front, we perform detailed analysis of the optimization dynamics, including a precise understanding of the difficulty that may arise in learning relationships with long-term memory. The term “curse of memory” is coined to describe the uncovered phenomena, akin to the “curse of dimension” that plagues high-dimensional function approximation. These results form a relatively complete picture of the interaction of memory and recurrent structures in the linear dynamical setting.",Approximation | Curse of memory | Dynamical systems | Optimization | Recurrent neural networks,35,0.0,,,NRF,NRF-NRFF13-2021-0005,National Research Foundation Singapore
2-s2.0-85205583567,,,,Are Emergent Abilities of Large Language Models a Mirage?,cp,Conference Paper,Schaeffer R.,60141508,Stanford Engineering,Stanford,United States,3.0,"Schaeffer, Rylan;Miranda, Brando;Koyejo, Sanmi",57221350848;57202641041;57846623900,60141508;60141508;60141508,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Recent work claims that large language models display emergent abilities: abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in models with scale. Specifically, nonlinear or discontinuous metrics produce seemingly emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on the Beyond the Imitation Game Benchmark (BIG-Bench); and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep network architectures. Via all three analyses, we provide evidence that emergent abilities disappear with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.",,198,0.0,,,APSF,2205329,Google
2-s2.0-85202017089,,,,Are Watermarks Bugs for Deepfake Detectors? Rethinking Proactive Forensics,cp,Conference Paper,Wu X.,60032356,Hunan University,Changsha,China,5.0,"Wu, Xiaoshuai;Liao, Xin;Ou, Bo;Liu, Yuling;Qin, Zheng",57224922739;26665606800;36696723900;55742299800;57198995944,60032356;60032356;60032356;60032356;60032356,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,6089-6097,"AI-generated content has accelerated the topic of media synthesis, particularly Deepfake, which can manipulate our portraits for positive or malicious purposes. Before releasing these threatening face images, one promising forensics solution is the injection of robust watermarks to track their own provenance. However, we argue that current watermarking models, originally devised for genuine images, may harm the deployed Deepfake detectors when directly applied to forged images, since the watermarks are prone to overlap with the forgery signals used for detection. To bridge this gap, we thus propose AdvMark, on behalf of proactive forensics, to exploit the adversarial vulnerability of passive detectors for good. Specifically, AdvMark serves as a plug-and-play procedure for fine-tuning any robust watermarking into adversarial watermarking, to enhance the forensic detectability of watermarked images; meanwhile, the watermarks can still be extracted for provenance tracking. Extensive experiments demonstrate the effectiveness of the proposed AdvMark, leveraging robust watermarking to fool Deepfake detectors, which can help improve the accuracy of downstream Deepfake detection without tuning the in-the-wild detectors. We believe this work will shed some light on the harmless proactive forensics against Deepfake.",,10,0.0,,,NSFC,U22A2030,National Natural Science Foundation of China
2-s2.0-85137869539,10.24963/ijcai.2022/684,,,Art Creation with Multi-Conditional StyleGANs,cp,Conference Paper,Dobler K.,60012345;60106550,Christian-Albrechts-Universität zu Kiel;Hasso-Plattner-Institut für Softwaresystemtechnik GmbH,Kiel;Potsdam,Germany;Germany,6.0,"Dobler, Konstantin;Hübscher, Florian;Westphal, Jan;Sierra-Múnera, Alejandro;de Melo, Gerard;Krestel, Ralf",57481374400;57481392600;57222723816;57204428149;23088528100;23008868200,60106550;60106550;60106550;60106550;60106550;60012345,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4936-4942,"Creating art is often viewed as a uniquely human endeavor. In this paper, we introduce a multi-conditional Generative Adversarial Network (GAN) approach trained on large amounts of human paintings to synthesize realistic-looking paintings that emulate human art. Our approach is based on the StyleGAN neural network architecture, but incorporates a custom multi-conditional control mechanism that provides fine-granular control over characteristics of the generated paintings, e.g., with regard to the perceived emotion evoked in a spectator. We also investigate several evaluation techniques tailored to multi-conditional generation.",,5,1.0,all publisherfree2read,All Open Access Bronze,,,
2-s2.0-85189652191,10.1609/aaai.v38i17.29849,,,Aspect-Based Sentiment Analysis with Explicit Sentiment Augmentations,cp,Conference Paper,Ouyang J.,60007711,Jilin University,Changchun,China,6.0,"Ouyang, Jihong;Yang, Zhiyao;Liang, Silong;Wang, Bing;Wang, Yimeng;Li, Ximing",8242998500;57963970900;58783763400;57375838200;57362373300;56327328700,60007711;60007711;60007711;60007711;60007711;60007711,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,17.0,,18842-18850,"Aspect-based sentiment analysis (ABSA), a fine-grained sentiment classification task, has received much attention recently. Many works investigate sentiment information through opinion words, such as “good” and “bad”. However, implicit sentiment data widely exists in the ABSA dataset, whose sentiment polarity is hard to determine due to the lack of distinct opinion words. To deal with implicit sentiment, this paper proposes an ABSA method that integrates explicit sentiment augmentations (ABSA-ESA) to add more sentiment clues. We propose an ABSA-specific explicit sentiment generation method to create such augmentations. Specifically, we post-train T5 by rule-based data and employ three strategies to constrain the sentiment polarity and aspect term of the generated augmentations. We employ Syntax Distance Weighting and Unlikelihood Contrastive Regularization in the training procedure to guide the model to generate the explicit opinion words with the same polarity as the input sentence. Meanwhile, we utilize the Constrained Beam Search to ensure the augmentations are aspect-related. We test ABSA-ESA on two ABSA benchmarks. The results show that ABSA-ESA outperforms the SOTA baselines on implicit and explicit sentiment accuracy.",,18,1.0,all publisherfullgold,All Open Access Gold,JLU,2021ZD0112500,Jilin University
2-s2.0-85147694056,10.1609/aaai.v36i1.19956,,,Assessing a Single Image in Reference-Guided Image Synthesis,cp,Conference Paper,Guo J.,60016930;60104026;60272618;126241012,Beijing University of Posts and Telecommunications;Beijing National Research Center for Information Science and Technology;Kuaishou;Beijing Academy of Artificial Intelligence,Beijing;Beijing;Beijing;Beijing,China;China;China;China,6.0,"Guo, Jiayi;Du, Chaoqun;Wang, Jiangshan;Huang, Huijuan;Wan, Pengfei;Huang, Gao",57211200155;57375696700;59866478200;57221470943;57221702931;7403425368,60104026;60104026;60016930;60272618;60272618;60104026-126241012,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,1078-1086,"Assessing the performance of Generative Adversarial Networks (GANs) has been an important topic due to its practical significance. Although several evaluation metrics have been proposed, they generally assess the quality of the whole generated image distribution. For Reference-guided Image Synthesis (RIS) tasks, i.e., rendering a source image in the style of another reference image, where assessing the quality of a single generated image is crucial, these metrics are not applicable. In this paper, we propose a general learning-based framework, Reference-guided Image Synthesis Assessment (RISA) to quantitatively evaluate the quality of a single generated image. Notably, the training of RISA does not require human annotations. In specific, the training data for RISA are acquired by the intermediate models from the training procedure in RIS, and weakly annotated by the number of models' iterations, based on the positive correlation between image quality and iterations. As this annotation is too coarse as a supervision signal, we introduce two techniques: 1) a pixel-wise interpolation scheme to refine the coarse labels, and 2) multiple binary classifiers to replace a naïve regressor. In addition, an unsupervised contrastive loss is introduced to effectively capture the style similarity between a generated image and its reference image. Empirical results on various datasets demonstrate that RISA is highly consistent with human preference and transfers well across models.",,14,1.0,all publisherfullgold,All Open Access Gold,NSFC,61906106,National Natural Science Foundation of China
2-s2.0-85147247091,10.1613/jair.1.13769,,,Asymmetric Action Abstractions for Planning in Real-Time Strategy Games,ar,Article,Moraes R.O.,60030835;60013918,University of Alberta;Universidade Federal de Vicosa,Edmonton;Vicosa,Canada;Brazil,3.0,"Moraes, Rubens O.;Nascimento, Mario A.;Lelis, Levi H.S.",57203248193;35808804000;35812244500,60013918;60030835;60030835,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,1103-1137,"Action abstractions restrict the number of legal actions available for real-time planning in zero-sum extensive-form games, thus allowing algorithms to focus their search on a set of promising actions. Even though unabstracted game trees can lead to optimal policies, due to real-time constraints and the tree size, they are not a practical choice. In this context, we introduce an action abstraction scheme which we call asymmetric action abstraction. Asymmetric abstractions allow search algorithms to “pay more attention” to some aspects of the game by unevenly dividing the algorithm’s search effort amongst different aspects of the game. We also introduce four algorithms that search in asymmetrically abstracted game trees to evaluate the effectiveness of our abstraction schemes. Two of our algorithms are adaptations of algorithms developed for searching in action-abstracted spaces, Portfolio Greedy Search and Stratified Strategy Selection, and the other two are adaptations of an algorithm developed for searching in unabstracted spaces, NaïveMCTS. An extensive set of experiments in a real-time strategy game shows that search algorithms using asymmetric abstractions are able to outperform all other search algorithms tested.",,1,1.0,all publisherfullgold,All Open Access Gold,UFV,,Universidad Francisco de Vitoria
2-s2.0-85148040857,,,,Asymptotic Study of Stochastic Adaptive Algorithms in Non-convex Landscape,ar,Article,Gadat S.,60030553;60111239,Université de Rennes;Toulouse School of Economics - Recherche - (TSE-R),Rennes;Toulouse,France;France,2.0,"Gadat, Sébastien;Gavra, Ioana",25637165000;57205418548,60111239;60030553,2022-08-01,1 August 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,228,,"This paper studies some asymptotic properties of adaptive algorithms widely used in optimization and machine learning, and among them Adagrad and Rmsprop, which are involved in most of the blackbox deep learning algorithms. Our setup is the non-convex landscape optimization point of view, we consider a one time scale parametrization and the situation where these algorithms may or may not be used with mini-batches. We adopt the point of view of stochastic algorithms and establish the almost sure convergence of these methods when using a decreasing step-size towards the set of critical points of the target function. With a mild extra assumption on the noise, we also obtain the convergence towards the set of minimizers of the function. Along our study, we also obtain a “convergence rate” of the methods, in the vein of the works of Ghadimi and Lan (2013).",Convergence of random variables | Stochastic adaptive algorithm | Stochastic optimization,12,0.0,,,ANR,ANR-17-EURE-0010,Fondation Simone et Cino Del Duca
2-s2.0-85167990941,10.1609/aaai.v37i3.25391,,,Attention-Based Depth Distillation with 3D-Aware Positional Encoding for Monocular 3D Object Detection,cp,Conference Paper,Wu Z.,60025761;60009860;125063529,Huazhong University of Science and Technology;Fudan University;Zongmu Technology,Wuhan;Shanghai;Shanghai,China;China;China,5.0,"Wu, Zizhang;Wu, Yunzhe;Pu, Jian;Li, Xianzhi;Wang, Xiaoquan",57218707356;57958504900;37001303000;57192496554;57219736491,125063529;125063529;60009860;60025761;125063529,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,2892-2900,"Monocular 3D object detection is a low-cost but challenging task, as it requires generating accurate 3D localization solely from a single image input. Recent developed depthassisted methods show promising results by using explicit depth maps as intermediate features, which are either precomputed by monocular depth estimation networks or jointly evaluated with 3D object detection. However, inevitable errors from estimated depth priors may lead to misaligned semantic information and 3D localization, hence resulting in feature smearing and suboptimal predictions. To mitigate this issue, we propose ADD, an Attention-based Depth knowledge Distillation framework with 3D-aware positional encoding. Unlike previous knowledge distillation frameworks that adopt stereo- or LiDAR-based teachers, we build up our teacher with identical architecture as the student but with extra ground-truth depth as input. Credit to our teacher design, our framework is seamless, domain-gap free, easily implementable, and is compatible with object-wise ground-truth depth. Specifically, we leverage intermediate features and responses for knowledge distillation. Considering long-range 3D dependencies, we propose 3D-aware self-attention and target-aware cross-attention modules for student adaptation. Extensive experiments are performed to verify the effectiveness of our framework on the challenging KITTI 3D object detection benchmark. We implement our framework on three representative monocular detectors, and we achieve state-ofthe-art performance with no additional inference computational cost relative to baseline models. Our code is available at https://github.com/rockywind/ADD.",,25,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85130311083,,,,Attraction-Repulsion Spectrum in Neighbor Embeddings,ar,Article,Böhm J.N.,60017246,Eberhard Karls Universität Tübingen,Tubingen,Germany,3.0,"Böhm, Jan Niklas;Berens, Philipp;Kobak, Dmitry",57219764233;27267498000;55316161300,60017246;60017246;60017246,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Neighbor embeddings are a family of methods for visualizing complex high-dimensional data sets using kNN graphs. To find the low-dimensional embedding, these algorithms combine an attractive force between neighboring pairs of points with a repulsive force between all points. One of the most popular examples of such algorithms is t-SNE. Here we empirically show that changing the balance between the attractive and the repulsive forces in t-SNE using the exaggeration parameter yields a spectrum of embeddings, which is characterized by a simple trade-off: stronger attraction can better represent continuous manifold structures, while stronger repulsion can better represent discrete cluster structures and yields higher kNN recall. We find that UMAP embeddings correspond to t-SNE with increased attraction; mathematical analysis shows that this is because the negative sampling optimization strategy employed by UMAP strongly lowers the effective repulsion. Likewise, ForceAtlas2, commonly used for visualizing developmental single-cell transcriptomic data, yields embeddings corresponding to t-SNE with the attraction increased even more. At the extreme of this spectrum lie Laplacian eigenmaps. Our results demonstrate that many prominent neighbor embedding algorithms can be placed onto the attraction-repulsion spectrum, and highlight the inherent trade-offs between them.",dimensionality reduction | neighbor embedding | visualization,42,0.0,,,NIH,U19MH114830,National Institutes of Health
2-s2.0-85174422840,,,,Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold Functions with Nasty Noise,cp,Conference Paper,Zeng S.,60148800,"Charles V. Schaefer, Jr. School of Engineering and Science",Hoboken,United States,2.0,"Zeng, Shiwei;Shen, Jie",57224919209;56734869500,60148800;60148800,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,40719-40748,"The concept class of low-degree polynomial threshold functions (PTFs) plays a fundamental role in machine learning. In this paper, we study PAC learning of K-sparse degree-d PTFs on R<sup>n</sup>, where any such concept depends only on K out of n attributes of the input. Our main contribution is a new algorithm that runs in time (nd/∊)<sup>O</sup>(d<sup>)</sup> and under the Gaussian marginal distribution, PAC learns the class up to error rate ∊ with O(<sup>K</sup><inf>∊2</inf><sup>4</sup><inf>d</inf><sup>d</sup> · log<sup>5d</sup> n) samples even when an η ≤ O(∊<sup>d</sup>) fraction of them are corrupted by the nasty noise of Bshouty et al. (2002), possibly the strongest corruption model. Prior to this work, attribute-efficient robust algorithms are established only for the special case of sparse homogeneous halfspaces. Our key ingredients are: 1) a structural result that translates the attribute sparsity to a sparsity pattern of the Chow vector under the basis of Hermite polynomials, and 2) a novel attribute-efficient robust Chow vector estimation algorithm which uses exclusively a restricted Frobenius norm to either certify a good approximation or to validate a sparsity-induced degree-2d polynomial as a filter to detect corrupted samples.",,1,0.0,,,SIT,,Stevens Institute of Technology
2-s2.0-85170376793,10.24963/ijcai.2023/350,,,Augmenting Automated Spectrum Based Fault Localization for Multiple Faults,cp,Conference Paper,Chatterjee P.,60007249;60021988;60002769;60002115,Universidade do Porto;Indian Institute of Technology Kanpur;Faculdade de Ciências da Universidade de Lisboa;Instituto de Engenharia de Sistemas e Computadores: Investigação e Desenvolvimento em Lisboa,Porto;Kanpur;Lisbon;Lisbon,Portugal;India;Portugal;Portugal,4.0,"Chatterjee, Prantik;Campos, José;Abreu, Rui;Roy, Subhajit",57551635800;35306564600;57209112913;55860935300,60021988;60007249-60002769;60007249-60002115;60021988,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,3140-3148,"Spectrum-based Fault Localization (SBFL) uses the coverage of test cases and their outcome (pass/fail) to predict the “suspiciousness” of program components, e.g., lines of code. SBFL is, perhaps, the most successful fault localization technique due to its simplicity and scalability. However, SBFL heuristics do not perform well in scenarios where a program may have multiple faulty components. In this work, we propose a new algorithm that “augments” previously proposed SBFL heuristics to produce a ranked list where faulty components ranked low by base SBFL metrics are ranked significantly higher. We implement our ideas in a tool, ARTEMIS, that attempts to “bubble up” faulty components which are ranked lower by base SBFL metrics. We compare our technique to the most popular SBFL metrics and demonstrate statistically significant improvement in the developer effort for fault localization with respect to the basic strategies.",,2,1.0,all publisherfullgold,All Open Access Gold,FCT,UIDB/00408/2020,Fundação para a Ciência e a Tecnologia
2-s2.0-85160853870,,,,AutoML Two-Sample Test,cp,Conference Paper,Kübler J.M.,60030569;60117087,Max Planck Institute for Intelligent Systems;CISPA - Helmholtz Center for Information Security,Tubingen;Saarbrucken,Germany;Germany,5.0,"Kübler, Jonas M.;Stimper, Vincent;Buchholz, Simon;Muandet, Krikamol;Schölkopf, Bernhard",57219494313;57213269722;55948418900;25031812100;7004460308,60030569;60030569;60030569;60117087;60030569,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Two-sample tests are important in statistics and machine learning, both as tools for scientific discovery as well as to detect distribution shifts. This led to the development of many sophisticated test procedures going beyond the standard supervised learning frameworks, whose usage can require specialized knowledge about two-sample testing. We use a simple test that takes the mean discrepancy of a witness function as the test statistic and prove that minimizing a squared loss leads to a witness with optimal testing power. This allows us to leverage recent advancements in AutoML. Without any user input about the problems at hand, and using the same method for all our experiments, our AutoML two-sample test achieves competitive performance on a diverse distribution shift benchmark as well as on challenging two-sample testing problems. We provide an implementation of the AutoML two-sample test in the Python package autotst.",,12,0.0,,,BMBF,2064/1,Bundesministerium für Bildung und Forschung
2-s2.0-85203789601,,,,Automated Loss function Search for Class-imbalanced Node Classification,cp,Conference Paper,Guo X.,60025578,Xidian University,Xi'an,China,4.0,"Guo, Xinyu;Wu, Kai;Zhang, Xiaoyu;Liu, Jing",59174552200;56693397800;57194220065;57196292840,60025578;60025578;60025578;60025578,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,16958-16973,"Class-imbalanced node classification tasks are prevalent in real-world scenarios. Due to the uneven distribution of nodes across different classes, learning high-quality node representations remains a challenging endeavor. The engineering of loss functions has shown promising potential in addressing this issue. It involves the meticulous design of loss functions, utilizing information about the quantities of nodes in different categories and the network's topology to learn unbiased node representations. However, the design of these loss functions heavily relies on human expert knowledge and exhibits limited adaptability to specific target tasks. In this paper, we introduce a high-performance, flexible, and generalizable automated loss function search framework to tackle this challenge. Across 15 combinations of graph neural networks and datasets, our framework achieves a significant improvement in performance compared to state-of-the-art methods. Additionally, we observe that homophily in graph-structured data significantly contributes to the transferability of the proposed framework.",,0,0.0,,,NSFC,62206205,National Natural Science Foundation of China
2-s2.0-85137915214,10.24963/ijcai.2022/61,,,Automated Synthesis of Mechanisms,cp,Conference Paper,Mittelmann M.,60017293;60020551,Università degli Studi di Napoli Federico II;Université Toulouse 1 Capitole,Naples;Toulouse,Italy;France,4.0,"Mittelmann, Munyque;Maubert, Bastien;Murano, Aniello;Perrussel, Laurent",57209180196;35092930800;14061017200;55892807300,60020551;60017293;60017293;60020551,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,426-432,"Mechanism Design aims to design a game so that a desirable outcome is reached regardless of agents' self-interests. In this paper, we show how this problem can be rephrased as a synthesis problem, where mechanisms are automatically synthesized from a partial or complete specification in a high-level logical language. We show that Quantitative Strategy Logic is a perfect candidate for specifying mechanisms as it can express complex strategic and quantitative properties. We solve automated mechanism design in two cases: when the number of actions is bounded, and when agents play in turn.",,19,1.0,all publisherfree2read repository repositoryam,All Open Access Bronze Green,H2020,952215,Horizon 2020 Framework Programme
2-s2.0-85124643264,10.1613/JAIR.1.13280,,,Automatic Recognition of the General-Purpose Communicative Functions defined by the ISO 24617-2 Standard for Dialog Act Annotation,ar,Article,Ribeiro E.,60028300;60002115,Iscte – Instituto Universitário de Lisboa;Instituto de Engenharia de Sistemas e Computadores: Investigação e Desenvolvimento em Lisboa,Lisbon;Lisbon,Portugal;Portugal,3.0,"Ribeiro, Eugénio;Ribeiro, Ricardo;de Matos, David Martins",57139895500;13609216700;54959008500,60002115;60028300;60002115,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,397-436,"From the perspective of a dialog system, it is important to identify the intention behind the segments in a dialog, since it provides an important cue regarding the information that is present in the segments and how they should be interpreted. ISO 24617-2, the standard for dialog act annotation, defines a hierarchically organized set of general-purpose communicative functions which correspond to different intentions that are relevant in the context of a dialog. We explore the automatic recognition of these communicative functions in the DialogBank, which is a reference set of dialogs annotated according to this standard. To do so, we propose adaptations of existing approaches to flat dialog act recognition that allow them to deal with the hierarchical classification problem. More specifically, we propose the use of an end-to-end hierarchical network with cascading outputs and maximum a posteriori path estimation to predict the communicative function at each level of the hierarchy, preserve the dependencies between the functions in the path, and decide at which level to stop. Furthermore, since the amount of dialogs in the DialogBank is small, we rely on transfer learning processes to reduce overfitting and improve performance. The results of our experiments show that our approach outperforms both a flat one and hierarchical approaches based on multiple classifiers and that each of its components plays an important role towards the recognition of general-purpose communicative functions.",,4,1.0,all publisherfullgold,All Open Access Gold,FCT,SFRH/BD/148142/2019,Fundação para a Ciência e a Tecnologia
2-s2.0-85199684274,10.1613/jair.1.15786,,,Axiomatization of Non-Recursive Aggregates in First-Order Answer Set Programming,ar,Article,Fandinno J.,60018988,University of Nebraska Omaha,Omaha,United States,3.0,"Fandinno, Jorge;Hansen, Zachary;Lierler, Yuliya",56288378500;57427178000;6505874007,60018988;60018988;60018988,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,977-1031,"This paper contributes to the development of theoretical foundations of answer set programming. Groundbreaking work on the SM operator by Ferraris, Lee, and Lifschitz proposed a definition/semantics for logic (answer set) programs based on a syntactic transformation similar to parallel circumscription. That definition radically differed from its predecessors by using classical (second-order) logic and avoiding reference to either grounding or fixpoints. Yet, the work lacked the formalization of crucial and commonly used answer set programming language constructs called aggregates. In this paper, we present a characterization of logic programs with aggregates based on a many-sorted generalization of the SM operator. This characterization introduces new function symbols for aggregate operations and aggregate elements, whose meaning can be fixed by adding appropriate axioms to the result of the SM transformation. We prove that our characterization coincides with the ASP-Core-2 semantics for logic programs and, if we allow non-positive recursion through aggregates, it coincides with the semantics of the answer set solver CLINGO.",,4,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85150393201,,,,BADPRE: TASK-AGNOSTIC BACKDOOR ATTACKS TO PRE-TRAINED NLP FOUNDATION MODELS,cp,Conference Paper,Chen K.,60003970;60014966;60005510;60023380;60271961;125575937,Zhejiang University;Peking University;Nanyang Technological University;Chongqing University;Peng Cheng Laboratory;Shannon.AI,Hangzhou;Beijing;Singapore City;Chongqing;Shenzhen;Shannon,China;China;Singapore;China;China;Ireland,7.0,"Chen, Kangjie;Meng, Yuxian;Sun, Xiaofei;Guo, Shangwei;Zhang, Tianwei;Li, Jiwei;Fan, Chun",57219735841;57216617236;57216611212;56834904300;55635885400;56350059800;57221317075,60005510;125575937;125575937;60023380;60005510;125575937-60003970;60014966-60271961,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose BadPre, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.",,31,0.0,,,NRF,AISG2-PhD-2021-08-023[T],National Research Foundation Singapore
2-s2.0-85189499114,10.1609/aaai.v38i5.28255,,,BARET: Balanced Attention Based Real Image Editing Driven by Target-Text Inversion,cp,Conference Paper,Qiao Y.,60025278;60003970;60117660;131121067,Tsinghua University;Zhejiang University;Westlake University;OPPO Research Institute,Beijing;Hangzhou;Hangzhou;,China;China;China;,7.0,"Qiao, Yuming;Wang, Fanyi;Su, Jingwen;Zhang, Yanhao;Yu, Yunjie;Wu, Siyu;Qi, Guo Jun",59875173300;57203321298;58161255600;57840560500;58775350800;58775513500;15521333300,131121067-60025278;131121067;131121067;131121067;131121067;60003970;131121067-60117660,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,5.0,,4560-4568,"Image editing approaches with diffusion models have been rapidly developed, yet their applicability are subject to requirements such as specific editing types (e.g., foreground or background object editing, style transfer), multiple conditions (e.g., mask, sketch, caption), and time consuming fine-tuning of diffusion models. For alleviating these limitations and realizing efficient real image editing, we propose a novel editing technique that only requires an input image and target text for various editing types including non-rigid edits without finetuning diffusion model. Our method contains three novelties: (I) Target-text Inversion Schedule (TTIS) is designed to finetune the input target text embedding to achieve fast image reconstruction without image caption and acceleration of convergence. (II) Progressive Transition Scheme applies progressive linear interpolation between target text embedding and its fine-tuned version to generate transition embedding for maintaining non-rigid editing capability. (III) Balanced Attention Module (BAM) balances the tradeoff between textual description and image semantics. By the means of combining self-attention map from reconstruction process and crossattention map from transition process, the guidance of target text embeddings in diffusion process is optimized. In order to demonstrate editing capability, effectiveness and efficiency of the proposed BARET, we have conducted extensive qualitative and quantitative experiments. Moreover, results derived from user study and ablation study further prove the superiority over other methods.",,2,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85189289446,10.1609/aaai.v38i9.28900,,,BAT: Behavior-Aware Human-Like Trajectory Prediction for Autonomous Driving,cp,Conference Paper,Liao H.,60025278;60014966;60005465;60023380;60022317,Tsinghua University;Peking University;University of Electronic Science and Technology of China;Chongqing University;University of Macau,Beijing;Beijing;Chengdu;Chongqing;Taipa,China;China;China;China;Macao,8.0,"Liao, Haicheng;Li, Zhenning;Shen, Huanming;Zeng, Wenxuan;Liao, Dongping;Li, Guofa;Li, Shengbo Eben;Xu, Chengzhong",58138235300;56915035200;58779442900;58590738000;58036187700;56029730000;15065803700;55600419500,60022317;60022317;60005465;60014966;60022317;60023380;60025278;60022317,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,9.0,,10332-10340,"The ability to accurately predict the trajectory of surrounding vehicles is a critical hurdle to overcome on the journey to fully autonomous vehicles. To address this challenge, we pioneer a novel behavior-aware trajectory prediction model (BAT) that incorporates insights and findings from traffic psychology, human behavior, and decision-making. Our model consists of behavior-aware, interaction-aware, priority-aware, and position-aware modules that perceive and understand the underlying interactions and account for uncertainty and variability in prediction, enabling higher-level learning and flexibility without rigid categorization of driving behavior. Importantly, this approach eliminates the need for manual labeling in the training process and addresses the challenges of non-continuous behavior labeling and the selection of appropriate time windows. We evaluate BAT's performance across the Next Generation Simulation (NGSIM), Highway Drone (HighD), Roundabout Drone (RounD), and Macao Connected Autonomous Driving (MoCAD) datasets, showcasing its superiority over prevailing state-of-the-art (SOTA) benchmarks in terms of prediction accuracy and efficiency. Remarkably, even when trained on reduced portions of the training data (25%), our model outperforms most of the baselines, demonstrating its robustness and efficiency in predicting vehicle trajectories and the potential to reduce the amount of data required to train autonomous vehicles, especially in corner cases. In conclusion, the behavior-aware model represents a significant advancement in the development of autonomous vehicles capable of predicting trajectories with the same level of proficiency as human drivers. The project page is available on our GitHub.",,55,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85150341533,,,,BDDM: BILATERAL DENOISING DIFFUSION MODELS FOR FAST AND HIGH-QUALITY SPEECH SYNTHESIS,cp,Conference Paper,Lam M.W.Y.,60114181,Tencent,Shenzhen,China,4.0,"Lam, Max W.Y.;Wang, Jun;Su, Dan;Yu, Dong",57204213288;55902731900;57204214455;35764924800,60114181;60114181;60114181;60114181,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We propose a new bilateral denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel bilateral modeling objective. We show that the new surrogate objective can achieve a lower bound of the log marginal likelihood tighter than a conventional surrogate. We also find that BDDM allows inheriting pre-trained score network parameters from any DPMs and consequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling. Our experiments demonstrate that BDDMs can generate high-fidelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave). We release our code at https://github.com/tencent-ailab/bddm.",,48,0.0,,,,,
2-s2.0-85177479561,,,,BEEF: BI-COMPATIBLE CLASS-INCREMENTAL LEARNING VIA ENERGY-BASED EXPANSION AND FUSION,cp,Conference Paper,Wang F.Y.,60033100;60114181,Nanjing University;Tencent,Nanjing;Shenzhen,China;China,7.0,"Wang, Fu Yun;Zhou, Da Wei;Liu, Liu;Ye, Han Jia;Bian, Yatao;Zhan, De Chuan;Zhao, Peilin",57387307100;57225801190;57453677000;57002054200;57219636616;56186431200;36445134800,60033100-60114181;60033100;60114181;60033100;60114181;60033100;60114181,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Neural networks suffer from catastrophic forgetting when sequentially learning tasks phase-by-phase, making them inapplicable in dynamically updated systems. Class-incremental learning (CIL) aims to enable neural networks to learn different categories at multi-stages. Recently, dynamic-structure-based CIL methods achieve remarkable performance. However, these methods train all modules in a coupled manner and do not consider possible conflicts among modules, resulting in spoilage of eventual predictions. In this work, we propose a unifying energy-based theory and framework called Bi-Compatible Energy-Based Expansion and Fusion (BEEF) to analyze and achieve the goal of CIL. We demonstrate the possibility of training independent modules in a decoupled manner while achieving bi-directional compatibility among modules through two additionally allocated prototypes, and then integrating them into a unifying classifier with minimal cost. Furthermore, BEEF extends the exemplar-set to a more challenging setting, where exemplars are randomly selected and imbalanced, and maintains its performance when prior methods fail dramatically. Extensive experiments on three widely used benchmarks: CIFAR-100, ImageNet-100, and ImageNet-1000 demonstrate that BEEF achieves state-of-the-art performance in both the ordinary and challenging CIL settings. The Code is available at https://github.com/G-U-N/ICLR23-BEEF.",,25,0.0,,,,61921006,
2-s2.0-105000492893,,,,BIFRÖST: 3D-Aware Image compositing with Language Instructions,cp,Conference Paper,Li L.,60009860;60005465;60002798;60008592,Fudan University;University of Electronic Science and Technology of China;Chinese University of Hong Kong;Hong Kong University of Science and Technology,Shanghai;Chengdu;Hong Kong;Hong Kong,China;China;Hong Kong;Hong Kong,7.0,"Li, Lingxiao;Gong, Kaixiong;Li, Weihong;Dai, Xili;Chen, Tao;Yuan, Xiaojun;Yue, Xiangyu",59800339600;57222719364;59451020500;56581030600;57192810199;59702060600;57203987027,60002798;60002798;60002798;60008592;60009860;60005465;60002798,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"This paper introduces Bifröst, a novel 3D-aware framework that is built upon diffusion models to perform instruction-based image composition. Previous methods concentrate on image compositing at the 2D level, which fall short in handling complex spatial relationships (e.g., occlusion). Bifröst addresses these issues by training MLLM as a 2.5D location predictor and integrating depth maps as an extra condition during the generation process to bridge the gap between 2D and 3D, which enhances spatial comprehension and supports sophisticated spatial interactions. Our method begins by fine-tuning MLLM with a custom counterfactual dataset to predict 2.5D object locations in complex backgrounds from language instructions. Then, the image-compositing model is uniquely designed to process multiple types of input features, enabling it to perform high-fidelity image compositions that consider occlusion, depth blur, and image harmonization. Extensive qualitative and quantitative evaluations demonstrate that Bifröst significantly outperforms existing methods, providing a robust solution for generating realistically composited images in scenarios demanding intricate spatial understanding. This work not only pushes the boundaries of generative image compositing but also reduces reliance on expensive annotated datasets by effectively utilizing existing resources in innovative ways.",,0,0.0,,,UoL,62306261,Google
2-s2.0-85135124772,,,,BOOTSTRAPPED META-LEARNING,cp,Conference Paper,Flennerhag S.,129320562,DeepMind,,,6.0,"Flennerhag, Sebastian;van Hasselt, Hado;Schroecker, Yannick;Zahavy, Tom;Silver, David;Singh, Satinder",57208442136;21744168500;57193494502;56303988700;7202151417;55548164600,129320562;129320562;129320562;129320562;129320562;129320562,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an ε-greedy Q-learning agent-without backpropagating through the update rule.",,20,0.0,,,,,
2-s2.0-85128375767,10.1609/aaai.v36i1.19876,,,Backprop-Free Reinforcement Learning with Active Neural Generative Coding,cp,Conference Paper,Ororbia A.G.,60001439;60001777,Pennsylvania State University;Rochester Institute of Technology,University Park;Rochester,United States;United States,2.0,"Ororbia, Alexander G.;Mali, Ankur",56394594800;57209027566,60001777;60001439,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,29-37,"In humans, perceptual awareness facilitates the fast recognition and extraction of information from sensory input. This awareness largely depends on how the human agent interacts with the environment. In this work, we propose active neural generative coding, a computational framework for learning action-driven generative models without backpropagation of errors (backprop) in dynamic environments. Specifically, we develop an intelligent agent that operates even with sparse rewards, drawing inspiration from the cognitive theory of planning as inference. We demonstrate on several simple control problems that our framework performs competitively with deep Q-learning. The robust performance of our agent offers promising evidence that a backprop-free approach for neural inference and learning can drive goal-directed behavior.",,11,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85181245424,,,,Bagging in overparameterized learning: Risk characterization and risk monotonization,ar,Article,Patil P.,60025038;60027950;60136640,"University of California, Berkeley;Carnegie Mellon University;School of Computer Science",Berkeley;Pittsburgh;Pittsburgh,United States;United States;United States,3.0,"Patil, Pratik;Du, Jin Hong;Kuchibhotla, Arun Kumar",55946311400;57869118800;56381472000,60025038;60136640;60027950,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,319,,"Bagging is a commonly used ensemble technique in statistics and machine learning to improve the performance of prediction procedures. In this paper, we study the prediction risk of variants of bagged predictors under the proportional asymptotics regime, in which the ratio of the number of features to the number of observations converges to a constant. Specifically, we propose a general strategy to analyze the prediction risk under squared error loss of bagged predictors using classical results on simple random sampling. Specializing the strategy, we derive the exact asymptotic risk of the bagged ridge and ridgeless predictors with an arbitrary number of bags under a well-specified linear model with arbitrary feature covariance matrices and signal vectors. Furthermore, we prescribe a generic cross-validation procedure to select the optimal subsample size for bagging and discuss its utility to eliminate the non-monotonic behavior of the limiting risk in the sample size (i.e., double or multiple descents). In demonstrating the proposed procedure for bagged ridge and ridgeless predictors, we thoroughly investigate the oracle properties of the optimal subsample size and provide an in-depth comparison between different bagging variants.",divide-and-conquer | proportional asymptotics | ridge regression | subagging,10,0.0,,,ONR,N00014-20-1-2787,Office of Naval Research
2-s2.0-85214086147,,,,Bandit problems with fidelity rewards,ar,Article,Lugosi G.,60015150;60032942;60032907;60030003;60105207,Imperial College London;Universitat Pompeu Fabra Barcelona;Institució Catalana de Recerca i Estudis Avançats;Cisco Systems;Barcelona School of Economics,London;Barcelona;Barcelona;San Jose;Barcelona,United Kingdom;Spain;Spain;United States;Spain,3.0,"Lugosi, Gábor;Pike-Burke, Ciara;Savalle, Pierre André",7004291134;57204810174;55376973000,60032907-60032942-60105207;60015150;60030003,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,328,,"The fidelity bandits problem is a variant of the K-armed bandit problem in which the reward of each arm is augmented by a fidelity reward that provides the player with an additional payoff depending on how ‘loyal’ the player has been to that arm in the past. We propose two models for fidelity. In the loyalty-points model the amount of extra reward depends on the number of times the arm has previously been played. In the subscription model the additional reward depends on the current number of consecutive draws of the arm. We consider both stochastic and adversarial problems. Since single-arm strategies are not always optimal in stochastic problems, the notion of regret in the adversarial setting needs careful adjustment. We introduce three possible notions of regret and investigate which can be bounded sublinearly. We study in detail the special cases of increasing, decreasing and coupon (where the player gets an additional reward after every m plays of an arm) fidelity rewards. For the models which do not necessarily enjoy sublinear regret, we provide a worst case lower bound. For those models which exhibit sublinear regret, we provide algorithms and bound their regret.",fidelity reward | multi-armed bandit problem | regret minimization,0,0.0,,,UPF,,Universitat Pompeu Fabra
2-s2.0-85203843031,,,,Batch Singular Value Polarization and Weighted Semantic Augmentation for Universal Domain Adaptation,cp,Conference Paper,Wang Z.,60021182;60130479,"Sun Yat-Sen University;School of Computer Science and Technology, Harbin Institute of Technology",Guangzhou;Harbin,China;China,5.0,"Wang, Ziqi;Wang, Wei;Huang, Chao;Wen, Jie;Wang, Cong",57216695714;57208538575;57361720500;56346214000;57209930195,60021182;60021182;60021182;60130479;60021182,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,52361-52371,"As a more challenging domain adaptation setting, universal domain adaptation (UniDA) introduces category shift on top of domain shift, which needs to identify unknown category in the target domain and avoid misclassifying target samples into source private categories.To this end, we propose a novel UniDA approach named Batch Singular value Polarization and Weighted Semantic Augmentation (BSP-WSA).Specifically, we adopt an adversarial classifier to identify the target unknown category and align feature distributions between the two domains.Then, we propose to perform SVD on the classifier's outputs to maximize larger singular values while minimizing those smaller ones, which could prevent target samples from being wrongly assigned to source private classes.To better bridge the domain gap, we propose a weighted semantic augmentation approach for UniDA to generate data on common categories between the two domains.Extensive experiments on three benchmarks demonstrate that BSP-WSA could outperform existing state-of-the-art UniDA approaches.",,0,0.0,,,NSFC,62306343,National Natural Science Foundation of China
2-s2.0-85168250126,10.1609/aaai.v37i9.26355,,,Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution Generalization,cp,Conference Paper,Zhu L.,60025084,Shanghai Jiao Tong University,Shanghai,China,4.0,"Zhu, Lin;Wang, Xinbing;Zhou, Chenghu;Ye, Nanyang",57198444787;22136880500;7403347013;57202057254,60025084;60025084;60025084;60025084,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,11461-11469,"Recent advances in large pre-trained models showed promising results in few-shot learning. However, their generalization ability on two-dimensional Out-of-Distribution (OoD) data, i.e., correlation shift and diversity shift, has not been thoroughly investigated. Researches have shown that even with a significant amount of training data, few methods can achieve better performance than the standard empirical risk minimization method (ERM) in OoD generalization. This few-shot OoD generalization dilemma emerges as a challenging direction in deep neural network generalization research, where the performance suffers from overfitting on few-shot examples and OoD generalization errors. In this paper, leveraging a broader supervision source, we explore a novel Bayesian cross-modal image-text alignment learning method (Bayes-CAL) to address this issue. Specifically, the model is designed as only text representations are fine-tuned via a Bayesian modelling approach with gradient orthogonalization loss and invariant risk minimization (IRM) loss. The Bayesian approach is essentially introduced to avoid overfitting the base classes observed during training and improve generalization to broader unseen classes. The dedicated loss is introduced to achieve better image-text alignment by disentangling the causal and non-casual parts of image features. Numerical experiments demonstrate that Bayes-CAL achieved state-of-the-art OoD generalization performances on two-dimensional distribution shifts. Moreover, compared with CLIP-like models, Bayes-CAL yields more stable generalization performances on unseen classes. Our code is available at https://github.com/LinLLLL/BayesCAL.",,8,1.0,all publisherfullgold,All Open Access Gold,NSFC,62106139,National Natural Science Foundation of China
2-s2.0-85167780518,,,,Bayesian Design Principles for Frequentist Sequential Learning,cp,Conference Paper,Xu Y.,60027552,Columbia Business School,New York,United States,2.0,"Xu, Yunbei;Zeevi, Assaf",57219757811;7102794829,60027552;60027552,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,38768-38800,"We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to create “algorithmic beliefs” at each round, and use Bayesian posteriors to make decisions. This is the first approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the “best-of-all-worlds” empirical performance in the stochastic, adversarial, and non-stationary environments. And we illustrate how these principles can be used in linear bandits, convex bandits, and reinforcement learning.",,9,0.0,,,,,
2-s2.0-85145017210,,,,"Bayesian Model Selection, the Marginal Likelihood, and Generalization",cp,Conference Paper,Lotfi S.,60021784,New York University,New York,United States,5.0,"Lotfi, Sanae;Izmailov, Pavel;Benton, Gregory;Goldblum, Micah;Wilson, Andrew Gordon",57222380460;57205290654;57218715386;57211069935;57203334715,60021784;60021784;60021784;60021784;60021784,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,14223-14247,"How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.",,41,0.0,,,NSF,193471,National Science Foundation
2-s2.0-85124193196,,,,Bayesian Multinomial Logistic Normal Models through Marginally Latent Matrix-T Processes,ar,Article,Silverman J.D.,60001439;60008724;60005200,Pennsylvania State University;Duke University;Duke University School of Medicine,University Park;Durham;Durham,United States;United States;United States,5.0,"Silverman, Justin D.;Roche, Kimberly;Holmes, Zachary C.;David, Lawrence A.;Mukherjee, Sayan",57193351541;57194501171;57218572048;24334858500;59662708500,60001439;60008724;60005200;60005200;60005200,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Bayesian multinomial logistic-normal (MLN) models are popular for the analysis of sequence count data (e.g., microbiome or gene expression data) due to their ability to model multivariate count data with complex covariance structure. However, existing implementations of MLN models are limited to small datasets due to the non-conjugacy of the multinomial and logistic-normal distributions. Motivated by the need to develop efficient inference for Bayesian MLN models, we develop two key ideas. First, we develop the class of Marginally Latent Matrix-T Process (Marginally LTP) models. We demonstrate that many popular MLN models, including those with latent linear, non-linear, and dynamic linear structure are special cases of this class. Second, we develop an efficient inference scheme for Marginally LTP models with specific accelerations for the MLN subclass. Through application to MLN models, we demonstrate that our inference scheme are both highly accurate and often 4-5 orders of magnitude faster than MCMC.",Bayesian Statistics | Count Data | Gene Expression | Microbiome | Multivariate Analysis,12,0.0,,,NSF,DEB-1840223,National Science Foundation
2-s2.0-85163147719,,,,Bayesian Persuasion for Algorithmic Recourse,cp,Conference Paper,Harris K.,60027950,Carnegie Mellon University,Pittsburgh,United States,6.0,"Harris, Keegan;Chen, Valerie;Kim, Joon Sik;Talwalkar, Ameet;Heidari, Hoda;Wu, Zhiwei Steven",57224849987;57210958265;57208438670;24829809300;56394228400;56272530400,60027950;60027950;60027950;60027950;60027950;60027950,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"When subjected to automated decision-making, decision subjects may strategically modify their observable features in ways they believe will maximize their chances of receiving a favorable decision. In many practical situations, the underlying assessment rule is deliberately kept secret to avoid gaming and maintain competitive advantage. The resulting opacity forces the decision subjects to rely on incomplete information when making strategic feature modifications. We capture such settings as a game of Bayesian persuasion, in which the decision maker offers a form of recourse to the decision subject by providing them with an action recommendation (or signal) to incentivize them to modify their features in desirable ways. We show that when using persuasion, the decision maker and decision subject are never worse off in expectation, while the decision maker can be significantly better off. While the decision maker's problem of finding the optimal Bayesian incentive-compatible (BIC) signaling policy takes the form of optimization over infinitely-many variables, we show that this optimization can be cast as a linear program over finitely-many regions of the space of possible assessment rules. While this reformulation simplifies the problem dramatically, solving the linear program requires reasoning about exponentially-many variables, even in relatively simple cases. Motivated by this observation, we provide a polynomial-time approximation scheme that recovers a near-optimal signaling policy. Finally, our numerical simulations on semi-synthetic data empirically demonstrate the benefits of using persuasion in the algorithmic recourse setting.",,9,0.0,,,NSF,1939606,National Science Foundation
2-s2.0-85213814720,,,,Bayesian Spiked Laplacian Graphs,ar,Article,Duan L.L.,60154244;60154289,Herbert Wertheim College of Engineering;College of Liberal Arts and Sciences,Gainesville;Gainesville,United States;United States,3.0,"Duan, Leo L.;Michailidis, George;Ding, Mingzhou",55935166400;6701747952;57199665712,60154289;60154289;60154244,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"In network analysis, it is common to work with a collection of graphs that exhibit heterogeneity. For example, neuroimaging data from patient cohorts are increasingly available. A critical analytical task is to identify communities, and graph Laplacian-based methods are routinely used. However, these methods are currently limited to a single network and also do not provide measures of uncertainty on the community assignment. In this work, we first propose a probabilistic network model called the “Spiked Laplacian Graph” that considers an observed network as a transform of the Laplacian and degree matrices of the network generating process, with the Laplacian eigenvalues modeled by a modified spiked structure. This effectively reduces the number of parameters in the eigenvectors, and their sign patterns allow efficient estimation of the underlying community structure. Further, the posterior distribution of the eigenvectors provides uncertainty quantification for the community estimates. Second, we introduce a Bayesian non-parametric approach to address the issue of heterogeneity in a collection of graphs. Theoretical results are established on the posterior consistency of the procedure and provide insights on the trade-off between model resolution and accuracy. We illustrate the performance of the methodology on synthetic data sets, as well as a neuroscience study related to brain activity in working memory.",Isoperimetric Constant | Mixed-Effect Eigendecomposition | Normalized Graph Cut | Stiefel Manifold,2,0.0,,,,,
2-s2.0-85164378792,,,,Bayes–Newton Methods for Approximate Bayesian Inference with PSD Guarantees,ar,Article,Wilkinson W.J.,60103653,Aalto University,Espoo,Finland,3.0,"Wilkinson, William J.;Särkkä, Simo;Solin, Arno",57193826536;6508313785;57077059600,60103653;60103653;60103653,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"We formulate natural gradient variational inference (VI), expectation propagation (EP), and posterior linearisation (PL) as generalisations of Newton’s method for optimising the parameters of a Bayesian posterior distribution. This viewpoint explicitly casts inference algorithms under the framework of numerical optimisation. We show that common approximations to Newton’s method from the optimisation literature, namely Gauss–Newton and quasi-Newton methods (e.g., the BFGS algorithm), are still valid under this ‘Bayes–Newton’ framework. This leads to a suite of novel algorithms which are guaranteed to result in positive semi-definite (PSD) covariance matrices, unlike standard VI and EP. Our unifying viewpoint provides new insights into the connections between various inference schemes. All the presented methods apply to any model with a Gaussian prior and non-conjugate likelihood, which we demonstrate with (sparse) Gaussian processes and state space models.",Approximate Bayesian inference | expectation propagation | Gaussian processes | optimisation | variational inference,7,0.0,,,AKA,324345,Research Council of Finland
2-s2.0-85139456042,10.1613/JAIR.1.13666,,,Better Decision Heuristics in CDCL through Local Search and Target Phases,ar,Article,Cai S.,60027363;60025641;60021931;60025256,University of Chinese Academy of Sciences;Universität Freiburg;Johannes Kepler University Linz;Institute of Software Chinese Academy of Sciences,Beijing;Freiburg im Breisgau;Linz;Beijing,China;Germany;Austria;China,4.0,"Cai, Shaowei;Zhang, Xindi;Fleury, Mathias;Biere, Armin",36600115200;57219114373;57102825200;8556731200,60025256-60027363;60025256-60027363;60025641-60021931;60025641-60021931,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,1515-1563,"On practical applications, state-of-the-art SAT solvers dominantly use the conictdriven clause learning (CDCL) paradigm. An alternative for satisfiable instances is local search solvers, which is more successful on random and hard combinatorial instances. Although there have been attempts to combine these methods in one framework, a tight integration which improves the state of the art on a broad set of application instances has been missing. We present a combination of techniques that achieves such an improvement. Our first contribution is to maximize in a local search fashion the assignment trail in CDCL, by sticking to and extending promising assignments via a technique called target phases. Second, we relax the CDCL framework by again extending promising branches to complete assignments while ignoring conicts. These assignments are then used as starting point of local search which tries to find improved assignments with fewer unsatisfied clauses. Third, these improved assignments are imported back to the CDCL loop where they are used to determine the value assigned to decision variables. Finally, the conict frequency of variables in local search can be exploited during variable selection in branching heuristics of CDCL. We implemented these techniques to improve three representative CDCL solvers (Glucose, MapleLcm DistChronoBT, and Kissat). Experiments on benchmarks from the main tracks of the last three SAT Competitions from 2019 to 2021 and an additional benchmark set from spectrum allocation show that the techniques bring significant improvements, particularly and not surprisingly, on satisfiable real-world application instances. We claim that these techniques were essential to the large increase in performance witnessed in the SAT Competition 2020 where Kissat and Relaxed LcmdCbDl NewTech were leading the field followed by CryptoMiniSAT-Ccnr, which also incorporated similar ideas.",,24,1.0,all publisherfullgold,All Open Access Gold,NSFC,62122078,National Natural Science Foundation of China
2-s2.0-85137886976,10.24963/ijcai.2022/398,,,Better Embedding and More Shots for Few-shot Learning,cp,Conference Paper,Chi Z.,60011069,East China University of Science and Technology,Shanghai,China,5.0,"Chi, Ziqiu;Wang, Zhe;Yang, Mengping;Guo, Wei;Xu, Xinlei",57223049034;59682916600;57326810800;57207730292;57282795200,60011069;60011069;60011069;60011069;60011069,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2874-2880,"In few-shot learning, methods are enslaved to the scarce labeled data, resulting in suboptimal embedding. Recent studies learn the embedding network by other large-scale labeled data. However, the trained network may give rise to the distorted embedding of target data. We argue two respects are required for an unprecedented and promising solution. We call them Better Embedding and More Shots (BEMS). Suppose we propose to extract embedding from the embedding network. BE maximizes the extraction of general representation and prevents over-fitting information. For this purpose, we introduce the topological relation for global reconstruction, avoiding excessive memorizing. MS maximizes the relevance between the reconstructed embedding and the target class space. In this respect, increasing the number of shots is a pivotal but intractable strategy. As a creative method, we derive the bound of information-theory-based loss function and implicitly achieve infinite shots with negligible cost. A substantial experimental analysis is carried out to demonstrate the state-of-the-art performance. Compared to the baseline, our method improves by up to 10%+. We also prove that BEMS is suitable for both standard pre-trained and meta-learning embedded networks.",,6,1.0,all publisherfree2read,All Open Access Bronze,NSFC,20511100600,National Natural Science Foundation of China
2-s2.0-85147407067,10.1609/aaai.v36i6.20573,,,Better Parameter-Free Stochastic Optimization with ODE Updates for Coin-Betting,cp,Conference Paper,Chen K.,60019674;60021726,Boston University;Microsoft Research,Boston;Redmond,United States;United States,3.0,"Chen, Keyi;Langford, John;Orabona, Francesco",57737174800;57206411433;9733480600,60019674;60021726;60019674,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,6239-6247,"Parameter-free stochastic gradient descent (PFSGD) algorithms do not require setting learning rates while achieving optimal theoretical performance. In practical applications, however, there remains an empirical gap between tuned stochastic gradient descent (SGD) and PFSGD. In this paper, we close the empirical gap with a new parameter-free algorithm based on continuous-time Coin-Betting on truncated models. The new update is derived through the solution of an Ordinary Differential Equation (ODE) and solved in a closed form. We show empirically that this new parameter-free algorithm outperforms algorithms with the “best default” learning rates and almost matches the performance of finely tuned baselines without anything to tune.",,15,1.0,all publisherfullgold,All Open Access Gold,NSF,1925930,National Science Foundation
2-s2.0-85163175374,,,,Beyond Time-Average Convergence: Near-Optimal Uncoupled Online Learning via Clairvoyant Multiplicative Weights Update,cp,Conference Paper,Piliouras G.,121763255;116604213,EPFL;SUTD,Ecublens;Singapore City,Switzerland;Singapore,3.0,"Piliouras, Georgios;Sim, Ryann;Skoulakis, Stratis",35113645700;57221872415;57192389216,116604213;116604213;121763255,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"In this paper we provide a novel and simple algorithm, Clairvoyant Multiplicative Weights Updates (CMWU), for convergence to Coarse Correlated Equilibria (CCE) in general games. CMWU effectively corresponds to the standard MWU algorithm but where all agents, when updating their mixed strategies, use the payoff profiles based on tomorrow's behavior, i.e. the agents are clairvoyant. CMWU achieves constant regret of ln(m)/η in all normal-form games with m actions and fixed step-sizes η. Although CMWU encodes in its definition a fixed point computation, which in principle could result in dynamics that are neither computationally efficient nor uncoupled, we show that both of these issues can be largely circumvented. Specifically, as long as the step-size η is upper bounded by 1/(n-1)V, where n is the number of agents and [0, V] is the payoff range, then the CMWU updates can be computed linearly fast via a contraction map. This implementation results in an uncoupled online learning dynamic that admits a O(log T)-sparse sub-sequence where each agent experiences at most O(nV log m) regret. This implies that the CMWU dynamics converge with rate O(nV log m log T/T) to a CCE and improves on the current state-of-the-art convergence rate of uncoupled online learning dynamics [13, 1].",,15,0.0,,,EPSRC,A20H6b0151,Engineering and Physical Sciences Research Council
2-s2.0-85143062043,,,,Beyond neural scaling laws: beating power law scaling via data pruning,cp,Conference Paper,Sorscher B.,60012708;60017246;127294950,Stanford University;Eberhard Karls Universität Tübingen;Meta AI Research,Stanford;Tubingen;Meta,United States;Germany;United States,5.0,"Sorscher, Ben;Geirhos, Robert;Shekhar, Shashank;Ganguli, Surya;Morcos, Ari S.",57218716119;57200525208;57423136900;8422780800;56946640900,60012708;60017246;127294950;60012708-127294950;127294950,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.",,260,0.0,,,,,
2-s2.0-85203813187,,,,BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization,cp,Conference Paper,Zou L.,60003970;60002798,Zhejiang University;Chinese University of Hong Kong,Hangzhou;Hong Kong,China;Hong Kong,6.0,"Zou, Lancheng;Zhao, Wenqian;Yin, Shuo;Bai, Chen;Sun, Qi;Yu, Bei",57677947600;57442478300;57193129687;57226285742;57216141937;54939654000,60002798;60002798;60002798;60002798;60003970;60002798,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,62978-62992,"Nowadays, Large Language Models (LLMs) mostly possess billions of parameters, bringing significant challenges to hardware platforms.Although quantization is an efficient approach to reduce computation and memory overhead for inference optimization, we stress the challenge that mainstream low-bit quantization approaches still suffer from either various data distribution outliers or a lack of hardware efficiency.We also find that low-bit data format has further potential expressiveness to cover the atypical language data distribution.In this paper, we propose a novel numerical representation, Bi-Exponent Block Floating Point (BiE), and a new quantization flow.BiE quantization shows accuracy superiority and hardware friendliness on various models and benchmarks.",,3,0.0,,,,,
2-s2.0-85214326886,10.1613/jair.1.16759,,,"Bias Mitigation Methods: Applicability, Legality, and Recommendations for Development",ar,Article,Waller M.,60011520;60119999,King's College London;Department of Computer Science and Technology,London;Cambridge,United Kingdom;United Kingdom,4.0,"Waller, Madeleine;Rodrigues, Odinaldo;Ah Lee, Michelle Seng;Cocarascu, Oana",57226147547;22433790300;57215871652;57191373347,60011520;60011520;60119999;60011520,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,1043-1078,"As algorithmic decision-making systems (ADMS) are increasingly deployed across various sectors, the importance of research on fairness in Artificial Intelligence (AI) continues to grow. In this paper we highlight a number of significant practical limitations and regulatory compliance issues associated with the application of existing bias mitigation methods to ADMS. We present an example of an algorithmic system used in recruitment to illustrate these limitations. Our analysis of existing methods indicates a pressing need for a change in the approach to the development of new methods. In order to address the limitations, we provide recommendations for key factors to consider in the development of new bias mitigation methods that aim to be effective in real-world scenarios and comply with legal requirements in the European Union, United Kingdom and United States, such as non-discrimination, data protection and sector-specific regulations. Further, we suggest a checklist relating to these recommendations that should be included with the development of new bias mitigation methods.",,3,1.0,all publisherfullgold,All Open Access Gold,ATI,EP/S023356/1,Alan Turing Institute
2-s2.0-105018669660,,,,"Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box",ar,Article,Giordano R.,60025038;60026553;60141072,"University of California, Berkeley;University of Melbourne;MIT Department of Electrical Engineering and Computer Science",Berkeley;Melbourne;Cambridge,United States;Australia;United States,3.0,"Giordano, Ryan;Ingram, Martin;Broderick, Tamara",57189099716;57201057403;35172635600,60025038;60026553;60141072,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce “deterministic ADVI” (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the “sample average approximation” (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior covariances via linear response (LR). In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform well with relatively few samples even in very high dimensions, though we also show that such favorable results cannot extend to variational approximations that are too expressive relative to mean-field ADVI. We show on a variety of real-world problems that DADVI reliably finds good solutions with default settings (unlike ADVI) and, together with LR covariances, is typically faster and more accurate than standard ADVI.",Automatic differentiation variational inference | Black box variational inference | Linear response covariances | Mean field approximation | Sample average approximation | Stochastic gradient,5,0.0,,,NSF,,National Science Foundation
2-s2.0-85183922044,10.1613/jair.1.15248,,,Boolean Observation Games,ar,Article,van Ditmarsch H.,60008134;60021988,CNRS Centre National de la Recherche Scientifique;Indian Institute of Technology Kanpur,Paris;Kanpur,France;India,2.0,"van Ditmarsch, Hans;Simon, Sunil",57215882990;35113903900,60008134;60021988,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,307-357,"We introduce Boolean Observation Games, a subclass of multi-player finite strategic games with incomplete information and qualitative objectives. In Boolean observation games, each player is associated with a finite set of propositional variables of which only it can observe the value, and it controls whether and to whom it can reveal that value. It does not control the given, fixed, value of variables. Boolean observation games are a generalization of Boolean games, a well-studied subclass of strategic games but with complete information, and wherein each player controls the value of its variables. In Boolean observation games, player goals describe multi-agent knowledge of variables. As in classical strategic games, players choose their strategies simultaneously and therefore observation games capture aspects of both imperfect and incomplete information. They require reasoning about sets of outcomes given sets of indistinguishable valuations of variables. An outcome relation between such sets determines what the Nash equilibria are. We present various outcome relations, including a qualitative variant of ex-post equilibrium. We identify conditions under which, given an outcome relation, Nash equilibria are guaranteed to exist. We also study the complexity of checking for the existence of Nash equilibria and of verifying if a strategy profile is a Nash equilibrium. We further study the subclass of Boolean observation games with ‘knowing whether’ goal formulas, for which the satisfaction does not depend on the value of variables. We show that each such Boolean observation game corresponds to a Boolean game and vice versa, by a different correspondence, and that both correspondences are precise in terms of existence of Nash equilibria.",,1,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,
2-s2.0-85168236529,10.1609/aaai.v37i9.26302,,,Boosted Dynamic Neural Networks,cp,Conference Paper,Yu H.,60025278;60000745;60012317;129497991,Tsinghua University;University of Illinois Urbana-Champaign;University of Oregon;Wormpex AI Research,Beijing;Urbana;Eugene;Beijing,China;United States;United States;China,5.0,"Yu, Haichao;Li, Haoxiang;Hua, Gang;Huang, Gao;Shi, Humphrey",57195937772;55923799900;57215375265;7403425368;58530591700,60000745;129497991;129497991;60025278;60000745-60012317,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,10989-10997,"Early-exiting dynamic neural networks (EDNN), as one type of dynamic neural networks, has been widely studied recently. A typical EDNN has multiple prediction heads at different layers of the network backbone. During inference, the model will exit at either the last prediction head or an intermediate prediction head where the prediction confidence is higher than a predefined threshold. To optimize the model, these prediction heads together with the network backbone are trained on every batch of training data. This brings a train-test mismatch problem that all the prediction heads are optimized on all types of data in training phase while the deeper heads will only see difficult inputs in testing phase. Treating training and testing inputs differently at the two phases will cause the mismatch between training and testing data distributions. To mitigate this problem, we formulate an EDNN as an additive model inspired by gradient boosting, and propose multiple training techniques to optimize the model effectively. We name our method BoostNet. Our experiments show it achieves the state-of-the-art performance on CIFAR100 and ImageNet datasets in both anytime and budgeted-batch prediction modes. Our code is released at https://github.com/SHI-Labs/Boosted-Dynamic-Networks.",,16,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85137934080,10.24963/ijcai.2022/416,,,Bootstrapping Informative Graph Augmentation via A Meta Learning Approach,cp,Conference Paper,Gao H.,60027363;60025278;60025256,University of Chinese Academy of Sciences;Tsinghua University;Institute of Software Chinese Academy of Sciences,Beijing;Beijing;Beijing,China;China;China,6.0,"Gao, Hang;Li, Jiangmeng;Qiang, Wenwen;Si, Lingyu;Sun, Fuchun;Zheng, Changwen",57275373300;57223355548;57202680422;57211123420;55555423500;7401934931,60027363-60025256;60027363-60025256;60027363-60025256;60027363-60025256;60025278;60025256,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3001-3007,"Recent works explore learning graph representations in a self-supervised manner. In graph contrastive learning, benchmark methods apply various graph augmentation approaches. However, most of the augmentation methods are non-learnable, which causes the issue of generating unbeneficial augmented graphs. Such augmentation may degenerate the representation ability of graph contrastive learning methods. Therefore, we motivate our method to generate augmented graph with a learnable graph augmenter, called MEta Graph Augmentation (MEGA). We then clarify that a”good” graph augmentation must have uniformity at the instance-level and informativeness at the feature-level. To this end, we propose a novel approach to learning a graph augmenter that can generate an augmentation with uniformity and informativeness. The objective of the graph augmenter is to promote our feature extraction network to learn a more discriminative feature representation, which motivates us to propose a meta-learning paradigm. Empirically, the experiments across multiple benchmark datasets demonstrate that MEGA outperforms the state-of-the-art methods in graph self-supervised learning tasks. Further experimental studies prove the effectiveness of different terms of MEGA. Our codes are available at https://github.com/hang53/MEGA.",,8,1.0,all publisherfree2read,All Open Access Bronze,CAS,XDA19020500,Chinese Academy of Sciences
2-s2.0-85138218329,,,,Boulevard: Regularized Stochastic Gradient Boosted Trees and Their Limiting Distribution,ar,Article,Zhou Y.,60025038;60278093,"University of California, Berkeley;Cornell Ann S. Bowers College of Computing and Information Science",Berkeley;Ithaca,United States;United States,2.0,"Zhou, Yichen;Hooker, Giles",57189056963;16555012700,60278093;60025038,2022-06-01,1 June 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"This paper examines a novel gradient boosting framework for regression. We regularize gradient boosted trees by introducing subsampling and employ a modified shrinkage algorithm so that at every boosting stage the estimate is given by an average of trees. The resulting algorithm, titled “Boulevard”, is shown to converge as the number of trees grows. This construction allows us to demonstrate a central limit theorem for this limit, providing a characterization of uncertainty for predictions. A simulation study and real world examples provide support for both the predictive accuracy of the model and its limiting behavior.",gradient boosting | limiting distribution | regression tree | regularization,3,0.0,,,NSF,DMS-1712554,National Science Foundation
2-s2.0-85137864514,10.24963/ijcai.2022/486,,,Bounded Memory Adversarial Bandits with Composite Anonymous Delayed Feedback,cp,Conference Paper,Wan Z.,60027363;60030904,University of Chinese Academy of Sciences;Institute of Computing Technology Chinese Academy of Sciences,Beijing;Beijing,China;China,3.0,"Wan, Zongqi;Sun, Xiaoming;Zhang, Jialin",57679794800;57202064811;57211379124,60030904-60027363;60030904-60027363;60030904-60027363,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3501-3507,"We study the adversarial bandit problem with composite anonymous delayed feedback. In this setting, losses of an action are split into d components, spreading over consecutive rounds after the action is chosen. And in each round, the algorithm observes the aggregation of losses that come from the latest d rounds. Previous works focus on oblivious adversarial setting, while we investigate the harder non-oblivious setting. We show non-oblivious setting incurs Ω(T) pseudo regret even when the loss sequence is bounded memory. However, we propose a wrapper algorithm which enjoys o(T) policy regret on many adversarial bandit problems with the assumption that the loss sequence is bounded memory. Especially, for K-armed bandit and bandit convex optimization, we have Õ(T<sup>2</sup>/<sup>3</sup>) policy regret bound. We also prove a matching lower bound for K-armed bandit. Our lower bound works even when the loss sequence is oblivious but the delay is non-oblivious. It answers the open problem proposed in [Wang et al., 2021], showing that non-oblivious delay is enough to incur Ω(~ T<sup>2</sup>/<sup>3</sup>) regret.",,2,1.0,all publisherfree2read,All Open Access Bronze,NSFC,61832003,National Natural Science Foundation of China
2-s2.0-85141802804,,,,Bounding the Error of Discretized Langevin Algorithms for Non-Strongly Log-Concave Targets,ar,Article,Dalalyan A.S.,60022020;60092945;60022532,University of Warwick;King Abdullah University of Science and Technology;ENSAE Paris,Coventry;Thuwal;Palaiseau,United Kingdom;Saudi Arabia;France,3.0,"Dalalyan, Arnak S.;Karagulyan, Avetik;Riou-Durand, Lionel",9743621700;57207817943;57207962685,60022532;60092945;60022020,2022-09-01,1 September 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"In this paper, we provide non-asymptotic upper bounds on the error of sampling from a target density over R<sup>p</sup> using three schemes of discretized Langevin diffusions. The first scheme is the Langevin Monte Carlo (LMC) algorithm, the Euler discretization of the Langevin diffusion. The second and the third schemes are, respectively, the kinetic Langevin Monte Carlo (KLMC) for differentiable potentials and the kinetic Langevin Monte Carlo for twice-differentiable potentials (KLMC2). The main focus is on the target densities that are smooth and log-concave on R<sup>p</sup>, but not necessarily strongly log-concave. Bounds on the computational complexity are obtained under two types of smoothness assumption: the potential has a Lipschitz-continuous gradient and the potential has a Lipschitz-continuous Hessian matrix. The error of sampling is measured by Wasserstein-q distances. We advocate for the use of a new dimension-adapted scaling in the definition of the computational complexity, when Wasserstein-q distances are considered. The obtained results show that the number of iterations to achieve a scaled-error smaller than a prescribed value depends only polynomially in the dimension.",Approximate sampling | Langevin Monte Carlo | log-concave distributions,22,0.0,,,EPSRC,ANR-11-IDEX-0003/Labex Ecodec/ANR-11-LABX-0047,Engineering and Physical Sciences Research Council
2-s2.0-85192839303,10.1613/jair.1.15317,,,Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks,ar,Article,Ramachandranpillai R.,60009358,Linköpings Universitet,Linkoping,Sweden,4.0,"Ramachandranpillai, Resmi;Sikder, Md Fahim;Bergström, David;Heintz, Fredrik",57211461510;57192666396;57219734349;6603720152,60009358;60009358;60009358;60009358,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,1313-1341,"Synthetic data generation offers a promising solution to enhance the usefulness of Electronic Healthcare Records (EHR) by generating realistic de-identified data. However, the existing literature primarily focuses on the quality of synthetic health data, neglecting the crucial aspect of fairness in downstream predictions. Consequently, models trained on synthetic EHR have faced criticism for producing biased outcomes in target tasks. These biases can arise from either spurious correlations between features or the failure of models to accurately represent sub-groups. To address these concerns, we present Bias-transforming Generative Adversarial Networks (Bt-GAN), a GAN-based synthetic data generator specifically designed for the healthcare domain. In order to tackle spurious correlations (i), we propose an information-constrained Data Generation Process (DGP) that enables the generator to learn a fair deterministic transformation based on a well-defined notion of algorithmic fairness. To overcome the challenge of capturing exact sub-group representations (ii), we incentivize the generator to preserve sub-group densities through score-based weighted sampling. This approach compels the generator to learn from underrepresented regions of the data manifold. To evaluate the effectiveness of our proposed method, we conduct extensive experiments using the Medical Information Mart for Intensive Care (MIMIC-III) database. Our results demonstrate that Bt-GAN achieves state-of-the-art accuracy while significantly improving fairness and minimizing bias amplification. Furthermore, we perform an in-depth explainability analysis to provide additional evidence supporting the validity of our study. In conclusion, our research introduces a novel and professional approach to addressing the limitations of synthetic data generation in the healthcare domain. By incorporating fairness considerations and leveraging advanced techniques such as GANs, we pave the way for more reliable and unbiased predictions in healthcare applications.",,15,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,EC,,Knut och Alice Wallenbergs Stiftelse
2-s2.0-85138792790,10.1613/JAIR.1.13816,,,C-Face: Using Compare Face on Face Hallucination for Low-Resolution Face Recognition,ar,Article,Han F.,60033100,Nanjing University,Nanjing,China,4.0,"Han, Feng;Wang, Xudong;Shen, Furao;Zhao, Jian",58108298700;57907499700;24482362400;55441427100,60033100;60033100;60033100;60033100,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,1715-1737,"Face hallucination is a task of generating high-resolution (HR) face images from low-resolution (LR) inputs, which is a subfield of the general image super-resolution. However, most of the previous methods only consider the visual effect, ignoring how to maintain the identity of the face. In this work, we propose a novel face hallucination model, called C-Face network, which can generate HR images with high visual quality while preserving the identity information. A face recognition network is used to extract the identity features in the training process. In order to make the reconstructed face images keep the identity information to a great extent, a novel metric, i.e., C-Face loss, is proposed. We also propose a new training algorithm to deal with the convergence problem. Moreover, since our work mainly focuses on the recognition accuracy of the output, we integrate face recognition into the face hallucination process which ensures that the model can be used in real scenarios. Extensive experiments on two large scale face datasets demonstrate that our C-Face network has the best performance compared with other state-of-the-art methods.",,3,1.0,all publisherfullgold,All Open Access Gold,NSFC,61876076,National Natural Science Foundation of China
2-s2.0-85199863223,,,,CAN NEURAL NETWORKS LEARN IMPLICIT LOGIC FROM PHYSICAL REASONING?,cp,Conference Paper,Traylor A.,60011460,Brown University,Providence,United States,3.0,"Traylor, Aaron;Feiman, Roman;Pavlick, Ellie",57216612842;24436646500;56122514800,;60011460;,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Despite the success of neural network models in a range of domains, it remains an open question whether they can learn to represent abstract logical operators such as negation and disjunction. We test the hypothesis that neural networks without inherent inductive biases for logical reasoning can acquire an implicit representation of negation and disjunction. Here, implicit refers to limited, domain-specific forms of these operators, which work in psychology suggests may be a precursor (developmentally and evolutionarily) to the type of abstract, domain-general logic that is characteristic of adult humans. To test neural networks, we adapt a test designed to diagnose the presence of negation and disjunction in animals and pre-verbal children, which requires inferring the location of a hidden object using constraints of the physical environment as well as implicit logic: if a ball is hidden in A or B, and shown not to be in A, can the subject infer that it is in B? Our results show that, despite the neural networks learning to track objects behind occlusion, they are unable to generalize to a task that requires implicit logic. We further show that models are unable to generalize to the test task even when they are trained directly on a logically identical (though visually dissimilar) task. However, experiments using transfer learning reveal that the models do recognize structural similarity between tasks which invoke the same logical reasoning pattern, suggesting that some desirable abstractions are learned, even if they are not yet sufficient to pass established tests of logical reasoning.",,1,0.0,,,,HR00111990064,
2-s2.0-85162730218,,,,CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains,cp,Conference Paper,Gebele J.,60023020;60011604;60019957,"Universitat Autònoma de Barcelona;Technische Universität Berlin;Fachhochschule Kempten, Hochschule für Technik und Wirtschaft",Cerdanyola del Valles;Berlin;Kempten,Spain;Germany;Germany,3.0,"Gebele, Julian;Stuhr, Bonifaz;Haselberger, Johann",57760451500;57215416352;57223433413,60019957;60019957-60023020;60019957-60011604,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Unsupervised Domain Adaptation demonstrates great potential to mitigate domain shifts by transferring models from labeled source domains to unlabeled target domains. While Unsupervised Domain Adaptation has been applied to a wide variety of complex vision tasks, only few works focus on lane detection for autonomous driving. This can be attributed to the lack of publicly available datasets. To facilitate research in these directions, we propose CARLANE, a 3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE encompasses the single-target datasets MoLane and TuLane and the multi-target dataset MuLane. These datasets are built from three different domains, which cover diverse scenes and contain a total of 163K unique images, 118K of which are annotated. In addition we evaluate and report systematic baselines, including our own method, which builds upon Prototypical Cross-domain Self-supervised Learning. We find that false positive and false negative rates of the evaluated domain adaptation methods are high compared to those of fully supervised baselines. This affirms the need for benchmarks such as CARLANE to further strengthen research in Unsupervised Domain Adaptation for lane detection. CARLANE, all evaluated models and the corresponding implementations are publicly available at https://carlanebenchmark.github.io.",,5,0.0,,,,,
2-s2.0-85175786081,,,,CAUSAL REPRESENTATION LEARNING FOR INSTANTANEOUS AND TEMPORAL EFFECTS IN INTERACTIVE SYSTEMS,cp,Conference Paper,Lippe P.,60002483;60357266,Universiteit van Amsterdam;Qualcomm AI Research,Amsterdam;San Diego,Netherlands;United States,6.0,"Lippe, Phillip;Magliacane, Sara;Löwe, Sindy;Asano, Yuki M.;Cohen, Taco;Gavves, Efstratios",57219767629;55453849700;57209616182;57219629051;7202415785;36731111500,60002483;60002483;60002483;60002483;60357266;60002483,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Causal representation learning is the task of identifying the underlying causal variables and their relations from high-dimensional observations, such as images.Recent work has shown that one can reconstruct the causal variables from temporal sequences of observations under the assumption that there are no instantaneous causal relations between them.In practical applications, however, our measurement or frame rate might be slower than many of the causal effects.This effectively creates “instantaneous” effects and invalidates previous identifiability results.To address this issue, we propose iCITRIS, a causal representation learning method that allows for instantaneous effects in intervened temporal sequences when intervention targets can be observed, e.g., as actions of an agent.iCITRIS identifies the potentially multidimensional causal variables from temporal observations, while simultaneously using a differentiable causal discovery method to learn their causal graph.In experiments on three datasets of interactive systems, iCITRIS accurately identifies the causal variables and their causal graph.",,23,0.0,,,UvA,,Universiteit van Amsterdam
2-s2.0-85200550231,,,,CAUSALLM IS NOT OPTIMAL FOR IN-CONTEXT LEARNING,cp,Conference Paper,Ding N.,60006191,Google LLC,Mountain View,United States,5.0,"Ding, Nan;Levinboim, Tomer;Wu, Jialin;Goodman, Sebastian;Soricut, Radu",59070400200;14010614700;58342390200;57207854851;56618480500,60006191;60006191;60006191;60006191;60006191,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.",,8,0.0,,,,,
2-s2.0-85200572765,,,,CAUSALTIME: REALISTICALLY GENERATED TIME-SERIES FOR BENCHMARKING OF CAUSAL DISCOVERY,cp,Conference Paper,Cheng Y.,60025278;60088468,Tsinghua University;China PLA General Hospital,Beijing;Beijing,China;China,6.0,"Cheng, Yuxiao;Wang, Ziqian;Xiao, Tingxiong;Zhong, Qin;Suo, Jinli;He, Kunlun",57670331500;58134124200;57937821000;57214077038;23398627200;7202010630,60025278;60025278;60025278;60088468;60025278;60088468,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Time-series causal discovery (TSCD) is a fundamental problem of machine learning. However, existing synthetic datasets cannot properly evaluate or predict the algorithms' performance on real data. This study introduces the CausalTime pipeline to generate time-series that highly resemble the real data and with ground truth causal graphs for quantitative performance evaluation. The pipeline starts from real observations in a specific scenario and produces a matching benchmark dataset. Firstly, we harness deep neural networks along with normalizing flow to accurately capture realistic dynamics. Secondly, we extract hypothesized causal graphs by performing importance analysis on the neural network or leveraging prior knowledge. Thirdly, we derive the ground truth causal graphs by splitting the causal model into causal term, residual term, and noise term. Lastly, using the fitted network and the derived causal graph, we generate corresponding versatile time-series proper for algorithm assessment. In the experiments, we validate the fidelity of the generated data through qualitative and quantitative experiments, followed by a benchmarking of existing TSCD algorithms using these generated datasets. CausalTime offers a feasible solution to evaluating TSCD algorithms in real applications and can be generalized to a wide range of fields. For easy use of the proposed approach, we also provide a user-friendly website, hosted on www.causaltime.cc.",,7,0.0,,,NSFC,Z200021,Natural Science Foundation of Beijing Municipality
2-s2.0-85130311518,,,,CD-split and HPD-split: Efficient Conformal Regions in High Dimensions,ar,Article,Izbicki R.,60013792,Universidade Federal de São Carlos,Sao Carlos,Brazil,3.0,"Izbicki, Rafael;Shimizu, Gilson;Stern, Rafael B.",36026756300;59778083900;55538881800,60013792;60013792;60013792,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Conformal methods create prediction bands that control average coverage assuming solely i.i.d. data. Although the literature has mostly focused on prediction intervals, more general regions can often better represent uncertainty. For instance, a bimodal target is better represented by the union of two intervals. Such prediction regions are obtained by CD-split, which combines the split method and a data-driven partition of the feature space which scales to high dimensions. CD-split however contains many tuning parameters, and their role is not clear. In this paper, we provide new insights on CD-split by exploring its theoretical properties. In particular, we show that CD-split converges asymptotically to the oracle highest predictive density set and satisfies local and asymptotic conditional validity. We also present simulations that show how to tune CD-split. Finally, we introduce HPD-split, a variation of CD-split that requires less tuning, and show that it shares the same theoretical guarantees as CD-split. In a wide variety of our simulations, CD-split and HPD-split have better conditional coverage and yield smaller prediction regions than other methods.",conditional validity | Conformal prediction | FlexCode | local validity,34,0.0,,,FAPESP,2019/11321-9,Fundação de Amparo à Pesquisa do Estado de São Paulo
2-s2.0-85185446014,,,,CHAIN-OF-EXPERTS: WHEN LLMS MEET COMPLEX OPERATIONS RESEARCH PROBLEMS,cp,Conference Paper,Xiao Z.,60003970;60119391,Zhejiang University;Huawei Noah's Ark Lab,Hangzhou;Hong Kong,China;Hong Kong,11.0,"Xiao, Ziyang;Zhang, Dongxiang;Wu, Yangjun;Xu, Lilin;Wang, Yuan;Han, Xiongwei;Fu, Xiaojin;Zhong, Tao;Zeng, Jia;Song, Mingli;Chen, Gang",58020505000;34267886900;57567859100;59638720900;57079310600;57233385300;58634515600;59249408900;56011678600;9333722700;57114035800,60003970;60003970;60003970;60003970;60119391;60119391;60119391;60119391;60119391;60003970;60003970,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Large language models (LLMs) have emerged as powerful techniques for various NLP tasks, such as mathematical reasoning and plan generation. In this paper, we study automatic modeling and programming for complex operations research (OR) problems, so as to alleviate the heavy dependence on domain experts and benefit a spectrum of industry sectors. We present the first LLM-based solution, namely Chain-of-Experts (CoE), a novel multi-agent cooperative framework to enhance reasoning capabilities. Specifically, each agent is assigned a specific role and endowed with domain knowledge related to OR. We also introduce a conductor to orchestrate these agents via forward thought construction and backward reflection mechanism. Furthermore, we build a benchmark dataset (ComplexOR) of complex OR problems to facilitate OR research and community development. Experimental results show that CoE significantly outperforms the state-of-the-art LLM-based approaches both on LPWP and ComplexOR.",,20,0.0,,,NKRDPC,2022YFF0902000,National Key Research and Development Program of China
2-s2.0-85170401069,10.24963/ijcai.2023/504,,,CLE-ViT: Contrastive Learning Encoded Transformer for Ultra-Fine-Grained Visual Categorization,cp,Conference Paper,Yu X.,60022020;60032987,University of Warwick;Griffith University,Coventry;Brisbane,United Kingdom;Australia,3.0,"Yu, Xiaohan;Wang, Jun;Gao, Yongsheng",56510536400;57221358111;57208234538,60032987;60022020;60032987,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,4531-4539,"Ultra-fine-grained visual classification (ultra-FGVC) targets at classifying sub-grained categories of fine-grained objects. This inevitably requires discriminative representation learning within a limited training set. Exploring intrinsic features from the object itself, e.g., predicting the rotation of a given image, has demonstrated great progress towards learning discriminative representation. Yet none of these works consider explicit supervision for learning mutual information at instance level. To this end, this paper introduces CLE-ViT, a novel contrastive learning encoded transformer, to address the fundamental problem in ultra-FGVC. The core design is a self-supervised module that performs self-shuffling and masking and then distinguishes these altered images from other images. This drives the model to learn an optimized feature space that has a large inter-class distance while remaining tolerant to intra-class variations. By incorporating this self-supervised module, the network acquires more knowledge from the intrinsic structure of the input data, which improves the generalization ability without requiring extra manual annotations. CLE-ViT demonstrates strong performance on 7 publicly available datasets, demonstrating its effectiveness in the ultra-FGVC task. The code is available at https://github.com/Markin-Wang/CLEViT.",,13,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85199872305,,,,CLEAN-IMAGE BACKDOOR: ATTACKING MULTILABEL MODELS WITH POISONED LABELS ONLY,cp,Conference Paper,Chen K.,60003970;60005510;125575937,Zhejiang University;Nanyang Technological University;Shannon.AI,Hangzhou;Singapore City;Shannon,China;Singapore;Ireland,5.0,"Chen, Kangjie;Lou, Xiaoxuan;Xu, Guowen;Li, Jiwei;Zhang, Tianwei",57219735841;57195468198;57189258305;57216664608;55635885400,60005510;60005510;60005510;60003970-125575937;60005510,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Multi-label models have been widely used in various applications including image annotation and object detection. The fly in the ointment is its inherent vulnerability to backdoor attacks due to the adoption of deep learning techniques. However, all existing backdoor attacks exclusively require to modify training inputs (e.g., images), which may be impractical in real-world applications. In this paper, we aim to break this wall and propose the first clean-image backdoor attack, which only poisons the training labels without touching the training samples. Our key insight is that in a multi-label learning task, the adversary can just manipulate the annotations of training samples consisting of a specific set of classes to activate the backdoor. We design a novel trigger exploration method to find convert and effective triggers to enhance the attack performance. We also propose three target label selection strategies to achieve different goals. Experimental results indicate that our clean-image backdoor can achieve a 98% attack success rate while preserving the model's functionality on the benign inputs. Besides, the proposed clean-image backdoor can evade existing state-of-the-art defenses.",,15,0.0,,,MOE,AISG2-PhD-2021-08-023[T],Ministry of Education - Singapore
2-s2.0-85163100813,,,,COLA: Consistent Learning with Opponent-Learning Awareness,cp,Conference Paper,Willi T.,60016849;60026851;60278837,University of Toronto;University of Oxford;Vector Institute,Toronto;Oxford;Toronto,Canada;United Kingdom;Canada,4.0,"Willi, Timon;Letcher, Alistair;Treutlein, Johannes;Foerster, Jakob",57537593800;57204047655;57225095178;57194215126,60026851;;60016849-60278837;60026851,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,23804-23831,"Learning in general-sum games is unstable and frequently leads to socially undesirable (Pareto-dominated) outcomes. To mitigate this, Learning with Opponent-Learning Awareness (LOLA) introduced opponent shaping to this setting, by accounting for each agent's influence on their opponents' anticipated learning steps. However, the original LOLA formulation (and follow-up work) is inconsistent because LOLA models other agents as naive learners rather than LOLA agents. In previous work, this inconsistency was suggested as a cause of LOLA's failure to preserve stable fixed points (SFPs). First, we formalize consistency and show that higher-order LOLA (HOLA) solves LOLA's inconsistency problem if it converges. Second, we correct a claim made in the literature by Schäfer and Anandkumar (2019), proving that Competitive Gradient Descent (CGD) does not recover HOLA as a series expansion (and fails to solve the consistency problem). Third, we propose a new method called Consistent LOLA (COLA), which learns update functions that are consistent under mutual opponent shaping. It requires no more than second-order derivatives and learns consistent update functions even when HOLA fails to converge. However, we also prove that even consistent update functions do not preserve SFPs, contradicting the hypothesis that this shortcoming is caused by LOLA's inconsistency. Finally, in an empirical evaluation on a set of general-sum games, we find that COLA finds prosocial solutions and that it converges under a wider range of learning rates than HOLA and LOLA. We support the latter finding with a theoretical result for a simple game.",,29,0.0,,,U of T,,University of Toronto
2-s2.0-85150380618,,,,COMPARING DISTRIBUTIONS BY MEASURING DIFFERENCES THAT AFFECT DECISION MAKING,cp,Conference Paper,Zhao S.,60141508,Stanford Engineering,Stanford,United States,6.0,"Zhao, Shengjia;Sinha, Abhishek;He, Yutong;Perreault, Aidan;Song, Jiaming;Ermon, Stefano",57202058101;57202730088;57222041507;57214230231;57202059115;35791579200,60141508;60141508;60141508;60141508;60141508;60141508,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task - two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the decision task, this generalizes the Jensen-Shannon divergence and the maximum mean discrepancy family. We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods. In addition, a modeler can directly specify their preferences when comparing distributions through the decision loss. We apply this property to understanding the effects of climate change on different economic activities and selecting features targeting different decision tasks.",,9,0.0,,,NSF,1522054,National Science Foundation
2-s2.0-85197377831,,,,COMPOSITIONAL PREFERENCE MODELS FOR ALIGNING LMS,cp,Conference Paper,Go D.,60016912;60017317;60121135,Yonsei University;University of Sussex;Naver Labs Corporation,Seoul;Brighton;Seongnam,South Korea;United Kingdom;South Korea,5.0,"Go, Dongyoung;Korbak, Tomasz;Kruszewski, Germán;Rozen, Jos;Dymetman, Marc",57712202900;56979374800;56349717600;57205157402;8446543000,60016912;60017317;60121135;60121135;,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. Through these simple steps, CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgement. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using CPMs tend to be preferred over samples obtained using conventional PMs. Overall, our approach demonstrates the benefits of endowing PMs with priors about which features determine human preferences while relying on LM capabilities to extract those features in a scalable and robust way.",,5,0.0,,,,,
2-s2.0-85150390073,,,,CONNECTOME-CONSTRAINED LATENT VARIABLE MODELS OF WHOLE-BRAIN NEURAL ACTIVITY,cp,Conference Paper,Mi L.,60009982;129320563;101060277,Harvard University;MIT;HHMI,Cambridge;;New York,United States;;United States,7.0,"Mi, Lu;Xu, Richard;Prakhya, Sridhama;Lin, Albert;Shavit, Nir;Samuel, Aravinthan D.T.;Turaga, Srinivas C.",57214473517;58147006700;57224433584;57219440435;57206129122;7102135876;24823119500,101060277-129320563;101060277;101060277;60009982;129320563;60009982;101060277,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"The availability of both anatomical connectivity and brain-wide neural activity measurements in C. elegans make the worm a promising system for learning detailed, mechanistic models of an entire nervous system in a data-driven way. However, one faces several challenges when constructing such a model. We often do not have direct experimental access to important modeling details such as single-neuron dynamics and the signs and strengths of the synaptic connectivity. Further, neural activity can only be measured in a subset of neurons, often indirectly via calcium imaging, and significant trial-to-trial variability has been observed. To address these challenges, we introduce a connectome-constrained latent variable model (CC-LVM) of the unobserved voltage dynamics of the entire C. elegans nervous system and the observed calcium signals. We used the framework of variational autoencoders to fit parameters of the mechanistic simulation constituting the generative model of the LVM to calcium imaging observations. A variational approximate posterior distribution over latent voltage traces for all neurons is efficiently inferred using an inference network, and constrained by a prior distribution given by the biophysical simulation of neural dynamics. We applied this model to an experimental whole-brain dataset, and found that connectomic constraints enable our LVM to predict the activity of neurons whose activity were withheld significantly better than models unconstrained by a connectome. We explored models with different degrees of biophysical detail, and found that models with realistic conductance-based synapses provide markedly better predictions than current-based synapses for this system.",,4,0.0,,,NIH,1 U01 NS111697-01,National Institutes of Health
2-s2.0-85133020033,,,,CONSTRUCTING A GOOD BEHAVIOR BASIS FOR TRANSFER USING GENERALIZED POLICY UPDATES,cp,Conference Paper,Alver S.,60002494;129320562,Université McGill;DeepMind,Montreal;,Canada;,2.0,"Alver, Safa;Precup, Doina",57219499757;6603288659,60002494;60002494-129320562,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"We study the problem of learning a good set of policies, so that when combined together, they can solve a wide variety of unseen reinforcement learning tasks with no or very little new data. Specifically, we consider the framework of generalized policy evaluation and improvement, in which the rewards for all tasks of interest are assumed to be expressible as a linear combination of a fixed set of features. We show theoretically that, under certain assumptions, having access to a specific set of diverse policies, which we call a set of independent policies, can allow for instantaneously achieving high-level performance on all possible downstream tasks which are typically more complex than the ones on which the agent was trained. Based on this theoretical analysis, we propose a simple algorithm that iteratively constructs this set of policies. In addition to empirically validating our theoretical results, we compare our approach with recently proposed diverse policy set construction methods and show that, while others fail, our approach is able to build a behavior basis that enables instantaneous transfer to all possible downstream tasks. We also show empirically that having access to a set of independent policies can better bootstrap the learning process on downstream tasks where the new reward function cannot be described as a linear combination of the features. Finally, we demonstrate how this policy set can be useful in a lifelong reinforcement learning setting.",,12,0.0,,,NSERC,,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85187768599,,,,CONTRANORM: A CONTRASTIVE LEARNING PERSPECTIVE ON OVERSMOOTHING AND BEYOND,cp,Conference Paper,Guo X.,60014966,Peking University,Beijing,China,4.0,"Guo, Xiaojun;Wang, Yifei;Du, Tianqi;Wang, Yisen",56703875500;57225158815;58141355500;57188869413,60014966;60014966;60014966;60014966,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks (GNNs) and Transformers, where performance worsens as the number of layers increases. Instead of characterizing oversmoothing from the view of complete collapse in which representations converge to a single point, we dive into a more general perspective of dimensional collapse in which representations lie in a narrow cone. Accordingly, inspired by the effectiveness of contrastive learning in preventing dimensional collapse, we propose a novel normalization layer called ContraNorm. Intuitively, ContraNorm implicitly shatters representations in the embedding space, leading to a more uniform distribution and a slighter dimensional collapse. On the theoretical analysis, we prove that ContraNorm can alleviate both complete collapse and dimensional collapse under certain conditions. Our proposed normalization layer can be easily integrated into GNNs and Transformers with negligible parameter overhead. Experiments on various real-world datasets demonstrate the effectiveness of our proposed ContraNorm. Our implementation is available at https://github.com/PKU-ML/ContraNorm.",,58,0.0,,,NKRDPC,2022ZD0160304,Huawei Technologies
2-s2.0-85191198068,,,,CORL: Research-oriented Deep Offline Reinforcement Learning Library,cp,Conference Paper,Tarasov D.,131185519,Tinkoff,,,5.0,"Tarasov, Denis;Nikulin, Alexander;Akimov, Dmitry;Kurenkov, Vladislav;Kolesnikov, Sergey",57939136100;57478843800;57212027695;57517069400;57219546015,131185519;131185519;131185519;131185519;131185519,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"CORL is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.",,24,0.0,,,,,
2-s2.0-85153606680,,,,CORRUPTED IMAGE MODELING FOR SELF-SUPERVISED VISUAL PRE-TRAINING,cp,Conference Paper,Fang Y.,60025761;60021726,Huazhong University of Science and Technology;Microsoft Research,Wuhan;Redmond,China;United States,5.0,"Fang, Yuxin;Dong, Li;Bao, Hangbo;Wang, Xinggang;Wei, Furu",57219633105;57217793334;57200410326;36100811100;23995914700,60025761-60021726;60021726;60021726;60025761;60021726,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"We introduce Corrupted Image Modeling (CIM) for self-supervised visual pretraining. CIM uses an auxiliary generator with a small trainable BEiT (Bao et al., 2021) to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our approach achieves compelling results in vision benchmarks, such as ImageNet classification and ADE20K semantic segmentation.",,20,0.0,,,NKRDPC,2022YFB4500602,National Key Research and Development Program of China
2-s2.0-85174386796,,,,CRAM: A COMPRESSION-AWARE MINIMIZER,cp,Conference Paper,Peste A.,60008134;60103776,CNRS Centre National de la Recherche Scientifique;Institute of Science and Technology Austria (ISTA),Paris;Klosterneuburg,France;Austria,5.0,"Peste, Alexandra;Vladu, Adrian;Kurtic, Eldar;Lampert, Christoph H.;Alistarh, Dan",57022228800;36919560800;57226256707;8420891100;25651727200,60103776;60008134;60103776;60103776;60103776,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Deep neural networks (DNNs) often have to be compressed, via pruning and/or quantization, before they can be deployed in practical settings. In this work we propose a new compression-aware minimizer dubbed CrAM that modifies the optimization step in a principled way, in order to produce models whose local loss behavior is stable under compression operations such as pruning. Thus, dense models trained via CrAM should be compressible post-training, in a single step, without significant accuracy loss. Experimental results on standard benchmarks, such as residual networks for ImageNet classification and BERT models for language modelling, show that CrAM produces dense models that can be more accurate than the standard SGD/Adam-based baselines, but which are stable under weight pruning: specifically, we can prune models in one-shot to 70-80% sparsity with almost no accuracy loss, and to 90% with reasonable (∼ 1%) accuracy loss, which is competitive with gradual compression methods. Additionally, CrAM can produce sparse models which perform well for transfer learning, and it also works for semi-structured 2:4 pruning patterns supported by GPU hardware. The code for reproducing the results is available at: https://github.com/IST-DASLab/CrAM.",,2,0.0,,,ANR,805223 ScaleML,Agence Nationale de la Recherche
2-s2.0-85203824218,,,,CaM: Cache Merging for Memory-efficient LLMs Inference,cp,Conference Paper,Zhang Y.,60026851;60013372;60018205;60032882;60271961,University of Oxford;The University of Texas at Austin;Xiamen University;Technische Universiteit Eindhoven;Peng Cheng Laboratory,Oxford;Austin;Xiamen;Eindhoven;Shenzhen,United Kingdom;United States;China;Netherlands;China,7.0,"Zhang, Yuxin;Du, Yuxuan;Luo, Gen;Zhong, Yunshan;Zhang, Zhenyu;Liu, Shiwei;Ji, Rongrong",57203830412;57980271800;57219521834;57271177600;57223916339;57217824780;23134935200,60018205-60271961;60018205;60018205;60018205;60013372;60026851-60032882;60018205,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,58840-58850,"Despite the exceptional performance of Large Language Models (LLMs), the substantial volume of key-value (KV) pairs cached during inference presents a barrier to their efficient deployment.To ameliorate this, recent works have aimed to selectively eliminate these caches, informed by the attention scores of associated tokens.However, such cache eviction invariably leads to output perturbation, regardless of the token choice.This perturbation escalates with the compression ratio, which can precipitate a marked deterioration in LLM inference performance.This paper introduces Cache Merging (CaM) as a solution to mitigate this challenge.CaM adaptively merges to-be-evicted caches into the remaining ones, employing a novel sampling strategy governed by the prominence of attention scores within discarded locations.In this manner, CaM enables memory-efficient LLMs to preserve critical token information, even obviating the need to maintain their corresponding caches.Extensive experiments utilizing LLaMA, OPT, and GPT-NeoX across various benchmarks corroborate CaM's proficiency in bolstering the performance of memory-efficient LLMs.Code is released at https://github.com/zyxxmu/cam.",,1,0.0,,,NSFC,62272401,National Natural Science Foundation of China
2-s2.0-85146912259,,,,Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation,cp,Conference Paper,Kuleshov V.,60007776,Cornell University,Ithaca,United States,2.0,"Kuleshov, Volodymyr;Deshpande, Shachi",36651233400;57374731000,60007776;60007776,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,11683-11693,"Accurate probabilistic predictions can be characterized by two properties-calibration and sharpness. However, standard maximum likelihood training yields models that are poorly calibrated and thus inaccurate-a 90% confidence interval typically does not contain the true outcome 90% of the time. This paper argues that calibration is important in practice and is easy to maintain by performing low-dimensional density estimation. We introduce a simple training procedure based on recalibration that yields calibrated models without sacrificing overall performance; unlike previous approaches, ours ensures the most general property of distribution calibration and applies to any model, including neural networks. We formally prove the correctness of our procedure assuming that we can estimate densities in low dimensions and we establish uniform convergence bounds. Our results yield empirical performance improvements on linear and deep Bayesian models and suggest that calibration should be increasingly leveraged across machine learning.",,26,0.0,,,TCS,,Tata Consultancy Services
2-s2.0-85174395004,,,,Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?,cp,Conference Paper,Knyazev B.,60010484;60113142;129147347,Samsung Group;Montreal Institute for Learning Algorithms;Canada CIFAR AI,Suwon;Montreal;,South Korea;Canada;Canada,3.0,"Knyazev, Boris;Hwang, Doha;Lacoste-Julien, Simon",55876470300;59623362300;8426902900,60010484;60010484;60010484-60113142-129147347,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,17243-17259,"Pretraining a neural network on a large dataset is becoming a cornerstone in machine learning that is within the reach of only a few communities with large-resources. We aim at an ambitious goal of democratizing pretraining. Towards that goal, we train and release a single neural network that can predict high quality ImageNet parameters of other neural networks. By using predicted parameters for initialization we are able to boost training of diverse ImageNet models available in PyTorch. When transferred to other datasets, models initialized with predicted parameters also converge faster and reach competitive final performance.",,7,0.0,,,,,
2-s2.0-85204284925,,,,Capturing Knowledge Graphs and Rules with Octagon Embeddings,cp,Conference Paper,Charpenay V.,60023998;60000960,Cardiff University;École des Mines de Saint-Étienne,Cardiff;Saint-Etienne,United Kingdom;France,2.0,"Charpenay, Victor;Schockaert, Steven",56405613300;8886625200,60000960;60023998,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3289-3297,"Region based knowledge graph embeddings represent relations as geometric regions. This has the advantage that the rules which are captured by the model are made explicit, making it straightforward to incorporate prior knowledge and to inspect learned models. Unfortunately, existing approaches are severely restricted in their ability to model relational composition, and hence also their ability to model rules, thus failing to deliver on the main promise of region based models. With the aim of addressing these limitations, we investigate regions which are composed of axis-aligned octagons. Such octagons are particularly easy to work with, as intersections and compositions can be straightforwardly computed, while they are still sufficiently expressive to model arbitrary knowledge graphs. Among others, we also show that our octagon embeddings can properly capture a nontrivial class of rule bases. Finally, we show that our model achieves competitive experimental results.",,1,0.0,,,UCA,EP/W003309/1,Université Clermont-Auvergne
2-s2.0-85170388172,10.24963/ijcai.2023/409,,,Cardinality-Minimal Explanations for Monotonic Neural Networks,cp,Conference Paper,El Harzli O.,60026851,University of Oxford,Oxford,United Kingdom,3.0,"El Harzli, Ouns;Grau, Bernardo Cuenca;Horrocks, Ian",57222486324;22834310900;20734105100,60026851;60026851;60026851,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,3677-3685,"In recent years, there has been increasing interest in explanation methods for neural model predictions that offer precise formal guarantees. These include abductive (respectively, contrastive) methods, which aim to compute minimal subsets of input features that are sufficient for a given prediction to hold (respectively, to change a given prediction). The corresponding decision problems are, however, known to be intractable. In this paper, we investigate whether tractability can be regained by focusing on neural models implementing a monotonic function. Although the relevant decision problems remain intractable, we can show that they become solvable in polynomial time by means of greedy algorithms if we additionally assume that the activation functions are continuous everywhere and differentiable almost everywhere. Our experiments suggest favourable performance of our algorithms.",,4,1.0,all publisherfullgold,All Open Access Gold,EPSRC,EP/S019111/1,Engineering and Physical Sciences Research Council
2-s2.0-85170378424,10.24963/ijcai.2023/576,,,Case-Based Reasoning with Language Models for Classification of Logical Fallacies,cp,Conference Paper,Sourati Z.,60143535;60015400;127575517,USC Viterbi School of Engineering;Information Sciences Institute;Armasuisse Science and Technology,Los Angeles;Marina del Rey;Lausanne,United States;United States;Switzerland,4.0,"Sourati, Zhivar;Ilievski, Filip;Sandlin, Hông Ân;Mermoud, Alain",57485706900;57188757237;58031377600;57198500076,60015400-60143535;60015400-60143535;127575517;127575517,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,5188-5196,"The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewer retrieved cases, and that the size of the case database has a negligible effect on the performance. Finally, we dive deeper into the relationship between the properties of the retrieved cases and the model performance.",,3,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,ONR,IIS-2153546,Office of Naval Research
2-s2.0-85163090534,,,,Causal Conceptions of Fairness and their Consequences,cp,Conference Paper,Nilforoshan H.,60012708;60009982;60021784,Stanford University;Harvard University;New York University,Stanford;Cambridge;New York,United States;United States;United States,4.0,"Nilforoshan, Hamed;Gaebler, Johann;Shroff, Ravi;Goel, Sharad",57203125753;57206209917;57117515400;35785827100,60012708;60012708;60021784;60009982,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,16848-16887,"Recent work highlights the role of causality in designing equitable decision-making algorithms. It is not immediately clear, however, how existing causal conceptions of fairness relate to one another, or what the consequences are of using these definitions as design principles. Here, we first assemble and categorize popular causal definitions of algorithmic fairness into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions almost always-in a measure theoretic sense-result in strongly Pareto dominated decision policies, meaning there is an alternative, unconstrained policy favored by every stakeholder with preferences drawn from a large, natural class. For example, in the case of college admissions decisions, policies constrained to satisfy causal fairness definitions would be disfavored by every stakeholder with neutral or positive preferences for both academic preparedness and diversity. Indeed, under a prominent definition of causal fairness, we prove the resulting policies require admitting all students with the same probability, regardless of academic qualifications or group membership. Our results highlight formal limitations and potential adverse consequences of common mathematical notions of causal fairness.",,35,0.0,,,NSF,IIS-2040898,National Science Foundation
2-s2.0-105000511649,,,,Causal Temporal Representation Learning with Nonstationary Sparse Transition,cp,Conference Paper,Song X.,60027950;60195969,Carnegie Mellon University;Mohamed Bin Zayed University of Artificial Intelligence,Pittsburgh;Abu Dhabi,United States;United Arab Emirates,7.0,"Song, Xiangchen;Li, Zijian;Chen, Guangyi;Zheng, Yujia;Fan, Yewen;Dong, Xinshuai;Zhang, Kun",57219766212;57204474898;57201580848;57738979600;58317313400;57220785243;55377020400,60027950;60195969;60027950-60195969;60027950;60027950;60027950;60027950-60195969,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Causal Temporal Representation Learning (Ctrl) methods aim to identify the temporal causal dynamics of complex nonstationary temporal sequences. Despite the success of existing Ctrl methods, they require either directly observing the domain variables or assuming a Markov prior on them. Such requirements limit the application of these methods in real-world scenarios when we do not have such prior knowledge of the domain variables. To address this problem, this work adopts a sparse transition assumption, aligned with intuitive human understanding, and presents identifiability results from a theoretical perspective. In particular, we explore under what conditions on the significance of the variability of the transitions we can build a model to identify the distribution shifts. Based on the theoretical result, we introduce a novel framework, Causal Temporal Representation Learning with Nonstationary Sparse Transition (CtrlNS), designed to leverage the constraints on transition sparsity and conditional independence to reliably identify both distribution shifts and latent factors. Our experimental evaluations on synthetic and real-world datasets demonstrate significant improvements over existing baselines, highlighting the effectiveness of our approach.",,3,0.0,,,NSF,2229881,Salesforce
2-s2.0-85171627621,10.1613/jair.1.14296,,,Certified Dominance and Symmetry Breaking for Combinatorial Optimisation,ar,Article,Bogaerts B.,60030840;60029170;60001490;60026810,Københavns Universitet;Lunds Universitet;University of Glasgow;Vrije Universiteit Brussel,Copenhagen;Lund;Glasgow;Brussels,Denmark;Sweden;United Kingdom;Belgium,4.0,"Bogaerts, Bart;Gocht, Stephan;McCreesh, Ciaran;Nordström, Jakob",55668474300;57195930415;55992148600;14323452400,60026810;60029170-60030840;60001490;60029170-60030840,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,1539-1589,"Symmetry and dominance breaking can be crucial for solving hard combinatorial search and optimisation problems, but the correctness of these techniques sometimes relies on subtle arguments. For this reason, it is desirable to produce efficient, machine-verifiable certificates that solutions have been computed correctly. Building on the cutting planes proof system, we develop a certification method for optimisation problems in which symmetry and dominance breaking is easily expressible. Our experimental evaluation demonstrates that we can efficiently verify fully general symmetry breaking in Boolean satisfiability (SAT) solving, thus providing, for the first time, a unified method to certify a range of advanced SAT techniques that also includes cardinality and parity (XOR) reasoning. In addition, we apply our method to maximum clique solving and constraint programming as a proof of concept that the approach applies to a wider range of combinatorial problems.",,27,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,RAENG,952215,Royal Academy of Engineering
2-s2.0-105000519087,,,,Certified Machine Unlearning via Noisy Stochastic Gradient Descent,cp,Conference Paper,Chien E.,60022195;60021032,Massachusetts Institute of Technology;School of Electrical and Computer Engineering,Cambridge;Atlanta,United States;United States,4.0,"Chien, Eli;Wang, Haoyu;Chen, Ziang;Li, Pan",57218450781;59857254300;57219507346;55495089200,60021032;60022195;60021032;60021032,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"“The right to be forgotten” ensured by laws for user data privacy becomes increasingly important. Machine unlearning aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch. We propose to leverage projected noisy stochastic gradient descent for unlearning and establish its first approximate unlearning guarantee under the convexity assumption. Our approach exhibits several benefits, including provable complexity saving compared to retraining, and supporting sequential and batch unlearning. Both of these benefits are closely related to our new results on the infinite Wasserstein distance tracking of the adjacent (un)learning processes. Extensive experiments show that our approach achieves a similar utility under the same privacy constraint while using 2% and 10% of the gradient computations compared with the state-of-the-art gradient-based approximate unlearning methods for mini-batch and full-batch settings, respectively.",,1,0.0,,,NSF,OAC-2117997,National Science Foundation
2-s2.0-85191156955,,,,Characterizing Out-of-Distribution Error via Optimal Transport,cp,Conference Paper,Lu Y.,60027950;60003915,Carnegie Mellon University;Vanderbilt University,Pittsburgh;Nashville,United States;United States,10.0,"Lu, Yuzhe;Qin, Yilong;Zhai, Runtian;Shen, Andrew;Chen, Ketong;Wang, Zhenlin;Kolouri, Soheil;Stepputtis, Simon;Campbell, Joseph;Sycara, Katia",57219452992;58335924600;57219620280;58340557000;58731358000;58119665100;55497793500;57193027734;57033311200;7006431929,60027950;60027950;60027950;60027950;60027950;60027950;60003915;60027950;60027950;60027950,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Out-of-distribution (OOD) data poses serious challenges in deployed machine learning models, so methods of predicting a model's performance on OOD data without labels are important for machine learning safety. While a number of methods have been proposed by prior work, they often underestimate the actual error, sometimes by a large margin, which greatly impacts their applicability to real tasks. In this work, we identify pseudo-label shift, or the difference between the predicted and true OOD label distributions, as a key indicator of this under-estimation. Based on this observation, we introduce a novel method for estimating model performance by leveraging optimal transport theory, Confidence Optimal Transport (COT), and show that it provably provides more robust error estimates in the presence of pseudo label shift. Additionally, we introduce an empirically-motivated variant of COT, Confidence Optimal Transport with Thresholding (COTT), which applies thresholding to the individual transport costs and further improves the accuracy of COT's error estimates. We evaluate COT and COTT on a variety of standard benchmarks that induce various types of distribution shift - synthetic, novel subpopulation, and natural - and show that our approaches significantly outperform existing state-of-the-art methods with up to 3x lower prediction errors. Our code can be found at https://github.com/luyuzhe111/COT.",,8,0.0,,,,,
2-s2.0-85147248772,10.1613/jair.1.13521,,,Characterizing Tseitin-Formulas with Short Regular Resolution Refutations,ar,Article,de Colnet A.,60018163;60018178,TU Wien;Université d'Artois,Vienna;Arras,Austria;France,2.0,"de Colnet, Alexis;Mengel, Stefan",57212028053;42262314600,60018163;60018178,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,265-286,"Tseitin-formulas are systems of parity constraints whose structure is described by a graph. These formulas have been studied extensively in proof complexity as hard instances in many proof systems. In this paper, we prove that a class of unsatisfiable Tseitin-formulas of bounded degree has regular resolution refutations of polynomial length if and only if the treewidth of all underlying graphs G for that class is in O(log |V (G)|). It follows that unsatisfiable Tseitin-formulas with polynomial length of regular resolution refutations are completely determined by the treewidth of the underlying graphs when these graphs have bounded degree. To prove this, we show that any regular resolution refutation of an unsatisfiable Tseitin-formula with graph G of bounded degree has length 2<sup>Ω</sup>(tw(G)/|V (G)|, thus essentially matching the known 2<sup>O</sup>(tw(G<sup>))</sup>poly(|V (G)|) upper bound. Our proof first connects the length of regular resolution refutations of unsatisfiable Tseitin-formulas to the size of representations of satisfiable Tseitin-formulas in decomposable negation normal form (DNNF). Then we prove that for every graph G of bounded degree, every DNNF-representation of every satisfiable Tseitin-formula with graph G must have size 2<sup>Ω</sup>(tw(G)) which yields our lower bound for regular resolution.",,2,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,ANR,ANR-18-CE40-0011,Agence Nationale de la Recherche
2-s2.0-85174408072,,,,CircuitNet: A Generic Neural Network to Realize Universal Circuit Motif Modeling,cp,Conference Paper,Wang Y.,60098464,Microsoft Research Asia,Beijing,China,9.0,"Wang, Yansen;Jiang, Xinyang;Ren, Kan;Shan, Caihua;Luo, Xufang;Han, Dongqi;Song, Kaitao;Shen, Yifei;Li, Dongsheng",57218224832;57203768541;57188557037;57238510100;57680111500;57194043367;57203343121;57203618753;56194412600,60098464;60098464;60098464;60098464;60098464;60098464;60098464;60098464;60098464,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,35817-35835,"The successes of artificial neural networks (ANNs) are largely attributed to mimicking the human brain structures. Recent advances in neuroscience revealed that neurons interact with each other through various kinds of connectivity patterns to process information, in which the common connectivity patterns are also called circuit motifs. However, many existing ANNs can only model one or two circuit motifs in their architectures, so that their performance may drastically vary among different types of machine learning tasks. In this paper, we propose a new type of neural network inspired by the architectures of neuronal circuits, namely Circuit Neural Network (CircuitNet). In CircuitNet, a group of densely connected neurons, namely circuit motif unit (CMU), form the basic unit of the network, which is capable of modeling universal circuit motifs by adjusting the weights within the CMUs. Compared with traditional feed-forward networks, CircuitNet has the ability to model more types of neuron connections such as feed-back and lateral motifs. Inspired by the locally dense and globally sparse structure of the human brain, several iterations of signal transmission among different CMUs are achieved by sparse connections through the input ports and output ports of different CMUs. Experiments have demonstrated that CircuitNet can outperform popular neural network architectures in function approximation, reinforcement learning, image classification, and time series forecasting tasks.",,1,0.0,,,,,
2-s2.0-85171615648,10.1613/jair.1.14710,,,Classes of Hard Formulas for QBF Resolution,ar,Article,Schleitzer A.,60029507,Friedrich-Schiller-Universität Jena,Jena,Germany,2.0,"Schleitzer, Agnes;Beyersdorff, Olaf",57848921800;14035089700,60029507;60029507,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,1455-1487,"To date, we know only a few handcrafted quantified Boolean formulas (QBFs) that are hard for central QBF resolution systems such as Q-Res and QU-Res, and only one specific QBF family to separate Q-Res and QU-Res. Here we provide a general method to construct hard formulas for Q-Res and QU-Res. The construction uses simple propositional formulas (e.g. minimally unsatisfiable formulas) in combination with easy QBF gadgets (Σb2 formulas without constant winning strategies). This leads to a host of new hard formulas, including new classes of hard random QBFs. We further present generic constructions for formulas separating Q-Res and QU-Res, and for separating Q-Res and LD-Q-Res.",,1,1.0,all publisherfullgold,All Open Access Gold,CZS,BE 4209/3-1,Carl-Zeiss-Stiftung
2-s2.0-85189471382,,,,ClimSim: A large multi-scale dataset for hybrid physics-ML climate emulation,cp,Conference Paper,Yu S.,60027550;60022195;60030612;60021784;60026175;60023471;60002337;60076695;127974031;100519128;116270450;109887167;100456970;101461349;125127506;131186142;131186383;131186286;131185695;131185917,"University of California, Los Angeles;Massachusetts Institute of Technology;University of California, San Diego;New York University;Lawrence Livermore National Laboratory;Pacific Northwest National Laboratory;State University of New York System;NVIDIA;Allen Institute for AI;UCB;Princeton;Tsinghua;COLUMBIA;CSU;UCI;UNIL;DLR;SNL;OSU;BNL",Los Angeles;Cambridge;La Jolla;New York;Livermore;Richland;Albany;Santa Clara;Allen;Braine-l'Alleud;Princeton;;Columbia;;Irvine;;;;;,United States;United States;United States;United States;United States;United States;United States;United States;Israel;Belgium;United States;China;United States;United States;United States;;;;;,56.0,"Yu, Sungduk;Hannah, Walter M.;Peng, Liran;Lin, Jerry;Bhouri, Mohamed Aziz;Gupta, Ritwik;Lütjens, Björn;Will, Justus C.;Behrens, Gunnar;Busecke, Julius J.M.;Loose, Nora;Stern, Charles;Beucler, Tom;Harrop, Bryce E.;Hillman, Benjamin R.;Jenney, Andrea M.;Ferretti, Savannah L.;Liu, Nana;Anandkumar, Anima;Brenowitz, Noah D.;Eyring, Veronika;Geneva, Nicholas;Gentine, Pierre;Mandt, Stephan;Pathak, Jaideep;Subramaniam, Akshay;Vondrick, Carl;Yu, Rose;Zanna, Laure;Zheng, Tian;Abernathey, Ryan P.;Ahmed, Fiaz;Bader, David C.;Baldi, Pierre;Barnes, Elizabeth A.;Bretherton, Christopher S.;Caldwell, Peter M.;Chuang, Wayne;Han, Yilun;Huang, Yu;Iglesias-Suarez, Fernando;Jantre, Sanket;Kashinath, Karthik;Khairoutdinov, Marat;Kurth, Thorsten;Lutsko, Nicholas J.;Ma, Po Lun;Mooers, Griffin;David Neelin, J.;Randall, David A.;Shamekh, Sara;Taylor, Mark A.;Urban, Nathan M.;Yuval, Janni;Zhang, Guang J.;Pritchard, Michael S.",57015826100;40761500800;57384627000;58536679700;57190835913;57207570089;57210800420;58650760300;57652350700;56328702800;57215882354;59293506500;57188866963;55418728800;55338676800;57204825025;58633997900;58650760400;16068201100;55176818100;13402933200;57190393795;19639722300;35272748600;57193864753;57216805557;36618122700;56355283500;8844177800;58651212900;36102344800;57220828212;7102450474;7101759672;57203084853;7004479957;36856321600;58651901200;59787775900;58740422400;57097521700;57221147789;36677426900;6701346974;16301531500;56893786200;15755995900;57216270359;7006417494;7202208382;57220008887;57202522440;9635016300;55671317600;55738957800;23991212200,125127506;60026175;125127506;125127506;100456970;100519128;60022195;125127506;131186383;100456970;116270450;100456970;131186142;60023471;131186286;125127506-131185695;125127506;125127506;60076695;60076695;131186383;60076695;100456970;125127506;60076695;60076695;100456970;60030612;60021784;100456970;100456970;60027550;60026175;125127506;101461349;127974031;60026175;100456970;109887167;100456970;131186383;131185917;60076695;60002337;60076695;60030612;60023471;125127506;60027550;101461349;100456970;131186286;131185917;60022195;60030612;125127506-60076695,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise predictions of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics on a host climate simulator's macro-scale physical state. The dataset is global in coverage, spans multiple years at high sampling frequency, and is designed such that resulting emulators are compatible with downstream coupling into operational climate simulators. We implement a range of deterministic and stochastic regression baselines to highlight the ML challenges and their scoring. The data (https://huggingface.co/datasets/LEAP/ClimSim_high-res<sup>2</sup>) and code (https://leap-stc.github.io/ClimSim) are released openly to support the development of hybrid ML-physics and high-fidelity climate simulations for the benefit of science and society.",,25,0.0,,,NSF,DE-AC52-07NA27344,National Science Foundation
2-s2.0-85147920105,,,,Clustering with Semidefinite Programming and Fixed Point Iteration,ar,Article,Felzenszwalb P.,60011460;60019880,Brown University;School of Engineering,Providence;Providence,United States;United States,3.0,"Felzenszwalb, Pedro;Klivans, Caroline;Paul, Alice",6603384460;21934571600;57188577249,60019880;60011460;60011460,2022-06-01,1 June 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,190,,We introduce a novel method for clustering using a semidefinite programming (SDP) relaxation of the Max k-Cut problem. The approach is based on a new methodology for rounding the solution of an SDP relaxation using iterated linear optimization. We show the vertices of the Max k-Cut relaxation correspond to partitions of the data into at most k sets. We also show the vertices are attractive fixed points of iterated linear optimization. Each step of this iterative process solves a relaxation of the closest vertex problem and leads to a new clustering problem where the underlying clusters are more clearly defined. Our experiments show that using fixed point iteration for rounding the Max k-Cut SDP relaxation leads to significantly better results when compared to randomized rounding.,Clustering | Optimization | Semidefinite programming,2,0.0,,,,,
2-s2.0-85136141290,10.1613/JAIR.1.13689,,,CoLLIE: Continual Learning of Language Grounding from Language-Image Embeddings,ar,Article,Skantze G.,60002014,The Royal Institute of Technology (KTH),Stockholm,Sweden,2.0,"Skantze, Gabriel;Willemsen, Bram",6505943187;57201504524,60002014;60002014,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,1201-1223,"This paper presents CoLLIE: a simple, yet effective model for continual learning of how language is grounded in vision. Given a pre-trained multimodal embedding model, where language and images are projected in the same semantic space (in this case CLIP by OpenAI), CoLLIE learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. This is done by predicting the difference vector that needs to be applied, as well as a scaling factor for this vector, so that the adjustment is only applied when needed. Unlike traditional few-shot learning, the model does not just learn new classes and labels, but can also generalize to similar language use and leverage semantic compositionality. We verify the model's performance on two different tasks of identifying the targets of referring expressions, where it has to learn new language use. The results show that the model can efficiently learn and generalize from only a few examples, with little interference with the model's original zero-shot performance.",,12,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,Knut och Alice Wallenbergs Stiftelse
2-s2.0-85163075028,,,,Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs,cp,Conference Paper,Zhang Y.,60092530,"Huawei Technologies Co., Ltd.",Shenzhen,China,3.0,"Zhang, Yikang;Chen, Zhuo;Zhong, Zhao",57219743013;57269816500;57207222375,60092530;60092530;60092530,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,26068-26084,"In this paper, we propose a Collaboration of Experts (CoE) framework to assemble the expertise of multiple networks towards a common goal. Each expert is an individual network with expertise on a unique portion of the dataset, contributing to the collective capacity. Given a sample, delegator selects an expert and simultaneously outputs a rough prediction to trigger potential early termination. For each model in CoE, we propose a novel training algorithm with two major components: weight generation module (WGM) and label generation module (LGM). It fulfills the co-adaptation of experts and delegator. WGM partitions the training data into portions based on delegator via solving a balanced transportation problem, then impels each expert to focus on one portion by reweighting the losses. LGM generates the label to constitute the loss of delegator for expert selection. CoE achieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy with 194M FLOPs. Combined with PWLU and CondConv, CoE further boosts the accuracy to 80.0% with only 100M FLOPs for the first time. Furthermore, experiment results on the translation task also demonstrate the strong generalizability of CoE. CoE is hardware-friendly, yielding a 3∼6x acceleration compared with existing conditional computation approaches.",,1,0.0,,,,,
2-s2.0-85182979066,10.1613/JAIR.1.15745,,,Collective Belief Revision,ar,Article,Aravanis T.I.,60003684,University of the Peloponnese,Tripolis,Greece,1.0,"Aravanis, Theofanis I.",57193060625,60003684,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,1221-1247,"In this article, we study the dynamics of collective beliefs. As a first step, we formulate David Westlund’s Principle of Collective Change (PCC) —a criterion that characterizes the evolution of collective knowledge— in the realm of belief revision. Thereafter, we establish a number of unsatisfiability results pointing out that the widely-accepted revision operators of Alchourrón, Gärdenfors and Makinson, combined with fundamental types of merging operations —including the ones proposed by Konieczny and Pino Pérez as well as Baral et al.— collide with the PCC. These impossibility results essentially extend in the context of belief revision the negative results established by Westlund for the operations of contraction and expansion. At the opposite of the impossibility results, we also establish a number of satisfiability results, proving that, under certain (rather strict) requirements, the PCC is indeed respected for specific merging operators. Overall, it is argued that the PCC is a rather unsuitable property for characterizing the process of collective change. Last but not least, mainly in response to the unsatisfactory situation related to the PCC, we explore some alternative criteria of collective change, and evaluate their compliance with belief revision and belief merging.",,3,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85192795569,10.1613/jair.1.15748,,,Collision Avoiding Max-Sum for Mobile Sensor Teams,ar,Article,Pertzovskiy A.,60027161;60002765,Ben-Gurion University of the Negev;Bar-Ilan University,Beer-Sheva;Ramat Gan,Israel;Israel,3.0,"Pertzovskiy, Arseni;Zivan, Roie;Agmon, Noa",58576940000;6508323679;23007834000,60027161;60027161;60002765,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,1281-1311,"Recent advances in technology have large teams of robots with limited computation skills working together to achieve a common goal. Their personal actions need to contribute to the joint effort, however, they also must ensure that they do not harm the efforts of the other members of the team, e.g., as a result of collisions. We focus on the distributed target coverage problem, in which the team must cooperate to maximize utility from sensed targets while avoiding collisions with other agents. State-of-the-art solutions focus on the distributed optimization of the coverage task at the team level while neglecting to consider collision avoidance, which could have far-reaching consequences on the overall performance. Therefore, we propose CAMS: a collision-avoiding version of the Max-sum algorithm, for solving problems including mobile sensors. In CAMS, a factor-graph that includes two types of constraints (represented by function-nodes) is iteratively generated and solved. The first type represents the task-related requirements, and the second represents collision avoidance constraints. We prove that consistent beliefs are sent by target representing function-nodes during the run of the algorithm, and identify factor-graph structures on which CAMS is guaranteed to converge to an optimal (collision-free) solution. We present an experimental evaluation in extensive simulations, showing that CAMS produces high-quality collision-free coverage also in large and complex scenarios. We further present evidence from experiments in a real multi-robot system that CAMS outperforms the state of the art in terms of convergence time.",,4,1.0,all publisherfullgold,All Open Access Gold,ISF,1070/20,Leona M. and Harry B. Helmsley Charitable Trust
2-s2.0-85147606081,10.1609/aaai.v36i11.21528,,,Combating Sampling Bias: A Self-Training Method in Credit Risk Models,cp,Conference Paper,Liao J.,60153736;60109039,College of Engineering;Intuit Inc,Davis;Mountain View,United States;United States,6.0,"Liao, Jingxian;Wang, Wei;Xue, Jason;Lei, Anthony;Han, Xue;Lu, Kun",57209347274;57221428316;57222292858;58094899400;57368962500;58094829700,60109039-60153736;60109039;60109039;60109039;60109039;60109039,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,12566-12572,"A significant challenge in credit risk models for underwriting is the presence of bias in model training data. When most credit risk models are built using only applicants who had been funded for credit, such non-random sampling predominantly influenced by credit policymakers and previous loan performances may introduce sampling bias to the models, and thus alter their prediction of default on loan repayment when screening applications from prospective borrowers. In this paper, we propose a novel data augmentation method that aims to identify and pseudo-label parts of the historically declined loan applications to mitigate sampling bias in the training data. We also introduce a new measure to assess the performance from the business perspective, loan application approval rates at various loan default rate levels. Our proposed methods were compared to the original supervised learning model and the traditional sampling issue remedy techniques in the industry. The experiment and early production results from deployed model show that self-training method with calibrated probability as data augmentation selection criteria improved the ability of credit scoring to differentiate default loan applications and, more importantly, can increase loan approval rate up to 8.8%, while keeping similar default rate comparing to baselines. The results demonstrate practical implications on how future underwriting model development processes should follow.",,3,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85159718338,10.1609/aaai.v37i9.26352,,,Combining Adversaries with Anti-adversaries in Training,cp,Conference Paper,Zhou X.,60019533,Tianjin University,Tianjin,China,3.0,"Zhou, Xiaoling;Yang, Nan;Wu, Ou",57219746593;58330095800;24081688000,60019533;60019533;60019533,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,11435-11442,"Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied bounds on each training sample is presented. Meta learning is utilized to optimize the combination weights. Experiments on benchmark datasets under different learning scenarios verify our theoretical findings and the effectiveness of the proposed methodology.",,7,1.0,all publisherfullgold,All Open Access Gold,NSFC,19ZXAZNGX00050,National Natural Science Foundation of China
2-s2.0-85146918377,,,,Community detection in sparse latent space models,ar,Article,Gao F.,60006297;60009860;60032744,University of Pennsylvania;Fudan University;Shanghai University of Finance and Economics,Philadelphia;Shanghai;Shanghai,United States;China;China,3.0,"Gao, Fengnan;Ma, Zongming;Yuan, Hongsong",57188874565;54952343700;57194974800,60009860;60006297;60032744,2022-10-01,1 October 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,322,,"We show that a simple community detection algorithm originated from stochastic blockmodel literature achieves consistency, and even optimality, for a broad and flexible class of sparse latent space models. The class of models includes latent eigenmodels (Hoff, 2008). The community detection algorithm is based on spectral clustering followed by local refinement via normalized edge counting. It is easy to implement and attains high accuracy with a low computational budget. The proof of its optimality depends on a neat equivalence between likelihood ratio test and edge counting in a simple vs. simple hypothesis testing problem that underpins the refinement step, which could be of independent interest.",blockmodel | eigenmodel | minimax rates | social network | spectral clustering,2,0.0,,,NSFC,11690013,National Natural Science Foundation of China
2-s2.0-85137901328,10.24963/ijcai.2022/658,,,Completeness and Diversity in Depth-First Proof-Number Search with Applications to Retrosynthesis,cp,Conference Paper,Franz C.,60019160;60006680,Syddansk Universitet;Bayer AG,Odense;Leverkusen,Denmark;Germany,4.0,"Franz, Christopher;Mogk, Georg;Mrziglod, Thomas;Schewior, Kevin",57887388900;6504274746;6506563088;39362469200,;60006680;60006680;60019160,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4747-4753,"We revisit Depth-First Proof-Number Search (DFPN), a well-known algorithm for solving two-player games. First, we consider the completeness property of the algorithm and its variants, i.e., whether they always find a winning strategy when there exists one. While it is known that the standard version is not complete, we show that the combination with the simple Threshold Controlling Algorithm is complete, solving an open problem from the area. Second, we modify DFPN to compute a diverse set of solutions rather than just a single one. Finally, we apply this new variant in Chemistry to the synthesis planning of new target molecules (Retrosynthesis). In this domain a diverse set of many solutions is desirable. We apply additional modifications from the literature to the algorithm and show that it outperforms Monte-Carlo Tree-Search, another well-known algorithm for the same problem, according to a natural diversity measure.",,4,1.0,all publisherfree2read,All Open Access Bronze,,,
2-s2.0-85183426718,,,,Complex Query Answering on Eventuality Knowledge Graph with Implicit Logical Constraints,cp,Conference Paper,Bai J.,60008592;60076757,"Hong Kong University of Science and Technology;Amazon.com, Inc.",Hong Kong;Seattle,Hong Kong;United States,5.0,"Bai, Jiaxin;Liu, Xin;Wang, Weiqi;Luo, Chen;Song, Yangqiu",57211989884;57206739249;57222025650;55899100500;14039604300,60008592;60008592;60008592;60076757;60008592,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Querying knowledge graphs (KGs) using deep learning approaches can naturally leverage the reasoning and generalization ability to learn to infer better answers. Traditional neural complex query answering (CQA) approaches mostly work on entity-centric KGs. However, in the real world, we also need to make logical inferences about events, states, and activities (i.e., eventualities or situations) to push learning systems from System I to System II, as proposed by Yoshua Bengio. Querying logically from an EVentuality-centric KG (EVKG) can naturally provide references to such kind of intuitive and logical inference. Thus, in this paper, we propose a new framework to leverage neural methods to answer complex logical queries based on an EVKG, which can satisfy not only traditional first-order logic constraints but also implicit logical constraints over eventualities concerning their occurrences and orders. For instance, if we know that Food is bad happens before PersonX adds soy sauce, then PersonX adds soy sauce is unlikely to be the cause of Food is bad due to implicit temporal constraint. To facilitate consistent reasoning on EVKGs, we propose Complex Eventuality Query Answering (CEQA), a more rigorous definition of CQA that considers the implicit logical constraints governing the temporal order and occurrence of eventualities. In this manner, we propose to leverage theorem provers for constructing benchmark datasets to ensure the answers satisfy implicit logical constraints. We also propose a Memory-Enhanced Query Encoding (MEQE) approach to significantly improve the performance of state-of-the-art neural query encoders on the CEQA task.",,18,0.0,,,UGC,RMGS20EG21,University Grants Committee
2-s2.0-85169686742,10.1613/JAIR.1.14648,,,Complexity of Computing the Shapley Value in Partition Function Form Games,ar,Article,Skibski O.,60112534,"University of Warsaw, Institute of Informatics",Warsaw,Poland,1.0,"Skibski, Oskar",53264913500,60112534,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,1237-1274,"We study the complexity of computing the Shapley value in partition function form games. We focus on two representations based on marginal contribution nets (embedded MC-nets and weighted MC-nets) and five extensions of the Shapley value. Our results show that while weighted MC-nets are more concise than embedded MC-nets, they have slightly worse computational properties when it comes to computing the Shapley value: two out of five extensions can be computed in polynomial time for embedded MC-nets and only one for weighted MC-nets.",,0,1.0,all publisherfullgold,All Open Access Gold,NCN,2015/19/D/ST6/03113,Narodowe Centrum Nauki
2-s2.0-85180394508,10.1609/aaai.v38i8.28641,,,Composing Biases by Using CP to Decompose Minimal Functional Dependencies for Acquiring Complex Formulae,cp,Conference Paper,Gindullin R.,60032619;60013373;60110235;60110511,Université Laval;INRIA Institut National de Recherche en Informatique et en Automatique;IMT Atlantique;Laboratoire des Sciences du Numérique de Nantes,Quebec;Le Chesnay;Nantes;Nantes,Canada;France;France;France,5.0,"Gindullin, Ramiz;Beldiceanu, Nicolas;Cheukam-Ngouonou, Jovial;Douence, Rémi;Quimper, Claude Guy",56576292900;8864151600;57836910200;8365644100;6508328101,60110235;60110235-60110511;60110235-60110511;60110235-60110511-60013373;60032619,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,8.0,,8030-8037,"Given a table with a minimal set of input columns that functionally determines an output column, we introduce a method that tries to gradually decompose the corresponding minimal functional dependency (mfd) to acquire a formula expressing the output column in terms of the input columns. A first key element of the method is to create sub-problems that are easier to solve than the original formula acquisition problem, either because it learns formulae with fewer inputs parameters, or as it focuses on formulae of a particular class, such as Boolean formulae; as a result, the acquired formulae can mix different learning biases such as polynomials, conditionals or Boolean expressions. A second key feature of the method is that it can be applied recursively to find formulae that combine polynomial, conditional or Boolean sub-terms in a nested manner. The method was tested on data for eight families of combinatorial objects; new conjectures were found that were previously unattainable. The method often creates conjectures that combine several formulae into one with a limited number of automatically found Boolean terms.",,0,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,EC,101000165,European Commission
2-s2.0-85137908963,10.24963/ijcai.2022/692,,,Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs,cp,Conference Paper,Zhou M.,60003970;60118460,Zhejiang University;Alibaba Group Holding Limited,Hangzhou;Hangzhou,China;China,6.0,"Zhou, Min;Xu, Chenchen;Ma, Ye;Ge, Tiezheng;Jiang, Yuning;Xu, Weiwei",57219492291;57701655900;57318424800;55924154700;57224896539;52964871000,60118460;60118460-60003970;60118460;60118460;60118460;60003970,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4995-5001,"In this paper, we study the graphic layout generation problem of producing high-quality visual-textual presentation designs for given images. We note that image compositions, which contain not only global semantics but also spatial information, would largely affect layout results. Hence, we propose a deep generative model, dubbed as composition-aware graphic layout GAN (CGL-GAN), to synthesize layouts based on the global and spatial visual contents of input images. To obtain training images from images that already contain manually designed graphic layout data, previous work suggests masking design elements (e.g., texts and embellishments) as model inputs, which inevitably leaves hint of the ground truth. We study the misalignment between the training inputs (with hint masks) and test inputs (without masks), and design a novel domain alignment module (DAM) to narrow this gap. For training, we built a large-scale layout dataset which consists of 60,548 advertising posters with annotated layout information. To evaluate the generated layouts, we propose three novel metrics according to aesthetic intuitions. Through both quantitative and qualitative evaluations, we demonstrate that the proposed model can synthesize high-quality graphic layouts according to image compositions. The data and code will be available at https://github.com/minzhouGithub/CGL-GAN.",,36,1.0,all publisherfree2read,All Open Access Bronze,,,
2-s2.0-85162142088,10.1613/jair.1.13899,,,"Computational Modelling of Quantifier Use: Corpus, Models, and Evaluation",ar,Article,Chen G.,60007989,Universiteit Utrecht,Utrecht,Netherlands,2.0,"Chen, Guanyi;Van Deemter, Kees",57216703480;6602523321,60007989;60007989,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,167-206,"A prominent strand of work in formal semantics investigates the ways in which human languages quantify the elements of a set, as when we say All A are B, Few A are B, and so on. Building on a growing body of empirical studies that shed light on the meaning and the use of quantifiers, we extend this line of work by computationally modelling how human speakers textually describe complex scenes in which quantitative relations play an important role. To this end, we conduct a series of elicitation experiments in which human speakers were asked to perform a linguistic task that invites the use of quantified expressions. The experiments result in a corpus, called qtuna, made up of short texts that contain a large variety of quantified expressions. We analyse qtuna, summarise our findings, and explain how we design computational models of human quantifier use accordingly. Finally, we evaluate these models in accordance with qtuna.",,0,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85196109127,10.1613/jair.1.15313,,,Computing Unsatisfiable Cores for LTLf Specifications,ar,Article,Roveri M.,60007989;60015986;60009914;60083112,Universiteit Utrecht;Università di Trento;Free University of Bozen-Bolzano;Bruno Kessler Foundation,Utrecht;Trento;Bolzano;Trento,Netherlands;Italy;Italy;Italy,4.0,"Roveri, Marco;Di Ciccio, Claudio;Di Francescomarino, Chiara;Ghidini, Chiara",56880472700;35193494400;25928227700;6701842843,60015986;60007989;60015986;60009914-60083112,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,517-558,"Linear-time temporal logic on finite traces (LTL<inf>f</inf>) is rapidly becoming a de-facto standard to produce specifications in many application domains (including planning, business process management, run-time monitoring, and reactive synthesis). Several studies have challenged the satisfiability problem thus far. In this paper, we focus instead on unsatisfiable LTL<inf>f</inf> specifications, with the objective of extracting the subset of formulae that cause inconsistencies within them, i.e., the unsatisfiable cores. We provide four algorithms to this end, which leverage the adaptation of a range of state-of-the-art algorithms to LTL<inf>f</inf> satisfiability checking. We implement those algorithms extending the respective implementations and carry out an experimental evaluation on a set of reference benchmarks, restricting to the unsatisfiable specifications. The results put in evidence that the different algorithms and tools exhibit complementary features determining their efficiency and efficacy. Indeed, our findings suggest exploring different strategies and algorithmic solutions for the extraction of unsatisfiable cores from LTL<inf>f</inf> specifications, thus confirming the challenging and multi-faceted nature of this problem.",,7,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,EC,PE00000013,European Commission
2-s2.0-105000672918,,,,Conditional Controllable Image Fusion,cp,Conference Paper,Cao B.,60019533;60025578,Tianjin University;Xidian University,Tianjin;Xi'an,China;China,5.0,"Cao, Bing;Xu, Xingxin;Zhu, Pengfei;Wang, Qilong;Hu, Qinghua",57204475892;59451888900;35779388800;59138495300;7403214664,60019533-60025578;60019533;60019533;60019533;60019533,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Image fusion aims to integrate complementary information from multiple input images acquired through various sources to synthesize a new fused image. Existing methods usually employ distinct constraint designs tailored to specific scenes, forming fixed fusion paradigms. However, this data-driven fusion approach is challenging to deploy in varying scenarios, especially in rapidly changing environments. To address this issue, we propose a conditional controllable fusion (CCF) framework for general image fusion tasks without specific training. Due to the dynamic differences of different samples, our CCF employs specific fusion constraints for each individual in practice. Given the powerful generative capabilities of the denoising diffusion model, we first inject the specific constraints into the pre-trained DDPM as adaptive fusion conditions. The appropriate conditions are dynamically selected to ensure the fusion process remains responsive to the specific requirements in each reverse diffusion stage. Thus, CCF enables conditionally calibrating the fused images step by step. Extensive experiments validate our effectiveness in general fusion tasks across diverse scenarios against the competing methods without additional training. The code is publicly available.",,4,0.0,,,NSFC,62222608,National Natural Science Foundation of China
2-s2.0-85160301566,,,,Conformal Prediction Sets with Limited False Positives,cp,Conference Paper,Fisch A.,60006191;60006320,Google LLC;MIT Computer Science & Artificial Intelligence Laboratory,Mountain View;Cambridge,United States;United States,4.0,"Fisch, Adam;Schuster, Tal;Jaakkola, Tommi;Barzilay, Regina",57200331317;57205163573;7003419235;23007765100,60006320;60006191;60006320;60006320,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,6514-6532,"We develop a new approach to multi-label conformal prediction in which we aim to output a precise set of promising prediction candidates with a bounded number of incorrect answers. Standard conformal prediction provides the ability to adapt to model uncertainty by constructing a calibrated candidate set in place of a single prediction, with guarantees that the set contains the correct answer with high probability. In order to obey this coverage property, however, conformal sets can become inundated with noisy candidates-which can render them unhelpful in practice. This is particularly relevant to practical applications where there is a limited budget, and the cost (monetary or otherwise) associated with false positives is non-negligible. We propose to trade coverage for a notion of precision by enforcing that the presence of incorrect candidates in the predicted conformal sets (i.e., the total number of false positives) is bounded according to a user-specified tolerance. Subject to this constraint, our algorithm then optimizes for a generalized notion of set coverage (i.e., the true positive rate) that allows for any number of true answers for a given query (including zero). We demonstrate the effectiveness of this approach across a number of classification tasks in natural language processing, computer vision, and computational chemistry.",,18,0.0,,,NSF,,National Science Foundation
2-s2.0-105000516791,,,,Connecting Joint-Embedding Predictive Architecture with Contrastive Self-supervised Learning,cp,Conference Paper,Mo S.,60021784;123683234,New York University;CMU,New York;Philadelphia,United States;United States,2.0,"Mo, Shentong;Tong, Shengbang",57221840673;57350142700,123683234;60021784,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"In recent advancements in unsupervised visual representation learning, the Joint-Embedding Predictive Architecture (JEPA) has emerged as a significant method for extracting visual features from unlabeled imagery through an innovative masking strategy. Despite its success, two primary limitations have been identified: the inefficacy of Exponential Moving Average (EMA) from I-JEPA in preventing entire collapse and the inadequacy of I-JEPA prediction in accurately learning the mean of patch representations. Addressing these challenges, this study introduces a novel framework, namely C-JEPA (Contrastive-JEPA), which integrates the Image-based Joint-Embedding Predictive Architecture with the Variance-Invariance-Covariance Regularization (VICReg) strategy. This integration is designed to effectively learn the variance/covariance for preventing entire collapse and ensuring invariance in the mean of augmented views, thereby overcoming the identified limitations. Through empirical and theoretical evaluations, our work demonstrates that C-JEPA significantly enhances the stability and quality of visual representation learning. When pre-trained on the ImageNet-1K dataset, C-JEPA exhibits rapid and improved convergence in both linear probing and fine-tuning performance metrics.",,2,0.0,,,,,
2-s2.0-85203787616,,,,Consistent Adversarially Robust Linear Classification: Non-Parametric Setting,cp,Conference Paper,Dohmatob E.,130369866,FAIR at Meta,Meta,United States,1.0,"Dohmatob, Elvis",55891124400,130369866,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,11149-11164,"For binary classification in d dimensions, it is known that with a sample size of n, an excess adversarial risk of O(d/n) is achievable under strong parametric assumptions about the underlying data distribution (e.g., assuming a Gaussian mixture model). In the case of well-separated distributions, this rate can be further refined to O(1/n). Our work studies the non-parametric setting, where very little is known. With only mild regularity conditions on the conditional distribution of the features, we examine adversarial attacks with respect to arbitrary norms and introduce a straightforward yet effective estimator with provable consistency w.r.t adversarial risk. Our estimator is given by minimizing a series of smoothed versions of the robust 0/1 loss, with a smoothing bandwidth that adapts to both n and d. Furthermore, we demonstrate that our estimator can achieve the minimax excess adversarial risk of Õ(√d/n) for linear classifiers, at the cost of solving possibly rougher optimization problems.",,0,0.0,,,,,
2-s2.0-85204311576,10.24963/ijcai.2024/386,,,Constructive Interpolation and Concept-Based Beth Definability for Description Logics via Sequents,cp,Conference Paper,Lyon T.S.,60018353,Technische Universität Dresden,Dresden,Germany,2.0,"Lyon, Tim S.;Karge, Jonas",57199194257;57226134642,60018353;60018353,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3484-3492,"We introduce a constructive method applicable to a large number of description logics (DLs) for establishing the concept-based Beth definability property (CBP) based on sequent systems. Using the highly expressive DL RIQ as a case study, we introduce novel sequent calculi for RIQ-ontologies and show how certain interpolants can be computed from sequent calculus proofs, which permit the extraction of explicit definitions of implicitly definable concepts. To the best of our knowledge, this is the first sequent-based approach to computing interpolants and definitions within the context of DLs, as well as the first proof that RIQ enjoys the CBP. Moreover, due to the modularity of our sequent systems, our results hold for any restriction of RIQ, and are applicable to other DLs by suitable modifications.",,1,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,BMBF,771779,Bundesministerium für Bildung und Forschung
2-s2.0-85140786522,,,,Contextual Bandits with Large Action Spaces: Made Practical,cp,Conference Paper,Zhu Y.,60032179;60021726,University of Wisconsin-Madison;Microsoft Research,Madison;Redmond,United States;United States,4.0,"Zhu, Yinglun;Foster, Dylan;Langford, John;Mineiro, Paul",57219481635;57189099160;57206411433;6507070804,60032179;60021726;60021726;60021726,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,27428-27453,"A central problem in sequential decision making is to develop algorithms that are practical and computationally efficient, yet support the use of flexible, general-purpose models. Focusing on the contextual bandit problem, recent progress provides provably efficient algorithms with strong empirical performance when the number of possible alternatives (“actions”) is small, but guarantees for decision making in large, continuous action spaces have remained elusive, leading to a significant gap between theory and practice. We present the first efficient, general-purpose algorithm for contextual bandits with continuous, linearly structured action spaces. Our algorithm makes use of computational oracles for (i) supervised learning, and (ii) optimization over the action space, and achieves sample complexity, runtime, and memory independent of the size of the action space. In addition, it is simple and practical. We perform a large-scale empirical evaluation, and show that our approach typically enjoys superior performance and efficiency compared to standard baselines.",,23,0.0,,,,,
2-s2.0-85147404536,,,,Contextual Bandits with Smooth Regret: Efficient Learning in Continuous Action Spaces,cp,Conference Paper,Zhu Y.,60032179;60021726,University of Wisconsin-Madison;Microsoft Research,Madison;Redmond,United States;United States,2.0,"Zhu, Yinglun;Mineiro, Paul",57219481635;6507070804,60032179;60021726,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,27574-27590,"Designing efficient general-purpose contextual bandit algorithms that work with large-or even infinite-action spaces would facilitate application to important scenarios such as information retrieval, recommendation systems, and continuous control. While obtaining standard regret guarantees can be hopeless, alternative regret notions have been proposed to tackle the large action setting. We propose a smooth regret notion for contextual bandits, which dominates previously proposed alternatives. We design a statistically and computationally efficient algorithm-for the proposed smooth regret-that works with general function approximation under standard supervised oracles. We also present an adaptive algorithm that automatically adapts to any smoothness level. Our algorithms can be used to recover the previous minimax/Pareto optimal guarantees under the standard regret, e.g., in bandit problems with multiple best arms and Lipschitz/Hölder bandits. We conduct large-scale empirical evaluations demonstrating the efficacy of our proposed algorithms.",,11,0.0,,,,,
2-s2.0-85182339662,,,,ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling,cp,Conference Paper,Chen Y.,60025084;60009860;60098464,Shanghai Jiao Tong University;Fudan University;Microsoft Research Asia,Shanghai;Shanghai;Beijing,China;China;China,6.0,"Chen, Yuqi;Ren, Kan;Wang, Yansen;Fang, Yuchen;Sun, Weiwei;Li, Dongsheng",58429926500;57188557037;57218224832;57204944818;55726565900;56194412600,60009860-60098464;60098464;60098464;60098464-60025084;60009860;60098464,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously. Traditional methods including recurrent neural networks or Transformer models leverage inductive bias via powerful neural architectures to capture complex patterns. However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms. Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences. It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla Transformer to the continuous-time domain, which explicitly incorporates the modeling abilities of continuous dynamics of Neural ODEs with the attention mechanism of Transformers. We mathematically characterize the expressive power of ContiFormer and illustrate that, by curated designs of function hypothesis, many Transformer variants specialized in irregular time series modeling can be covered as a special case of ContiFormer. A wide range of experiments on both synthetic and real-world datasets have illustrated the superior modeling capacities and prediction performance of ContiFormer on irregular time series data.",,68,0.0,,,NSFC,62172107,National Natural Science Foundation of China
2-s2.0-85189648464,10.1609/aaai.v38i13.29353,,,Continuous Treatment Effect Estimation Using Gradient Interpolation and Kernel Smoothing,cp,Conference Paper,Nagalapatti L.,60014153,Indian Institute of Technology Bombay,Mumbai,India,4.0,"Nagalapatti, Lokesh;Iyer, Akshay;De, Abir;Sarawagi, Sunita",57218846357;58858281600;57200822285;55968663300,60014153;60014153;60014153;60014153,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,13.0,,14397-14404,"We address the Individualized continuous treatment effect (ICTE) estimation problem where we predict the effect of any continuous-valued treatment on an individual using observational data. The main challenge in this estimation task is the potential confounding of treatment assignment with an individual’s covariates in the training data, whereas during inference ICTE requires prediction on independently sampled treatments. In contrast to prior work that relied on regularizers or unstable GAN training, we advocate the direct approach of augmenting training individuals with independently sampled treatments and inferred counterfactual outcomes. We infer counterfactual outcomes using a two-pronged strategy: a Gradient Interpolation for close-to-observed treatments, and a Gaussian Process based Kernel Smoothing which allows us to downweigh high variance inferences. We evaluate our method on five benchmarks and show that our method outperforms six state-of-the-art methods on the counterfactual estimation error. We analyze the superior performance of our method by showing that (1) our inferred counterfactual responses are more accurate, and (2) adding them to the training data reduces the distributional distance between the confounded training distribution and test distribution where treatment is independent of covariates. Our proposed method is model-agnostic and we show that it improves ICTE accuracy of several existing models. We release the code at: https://github.com/nlokeshiisc/GIKS release.",,5,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85162170805,10.1613/jair.1.14117,,,Contract Scheduling with Predictions,ar,Article,Angelopoulos S.,60001422;60033420,Sorbonne Université;York University,Paris;Toronto,France;Canada,2.0,"Angelopoulos, Spyros;Kamali, Shahin",57193202746;15845705100,60001422;60033420,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,395-426,"Contract scheduling is a general technique that allows the design of systems with interruptible capabilities, given an algorithm that is not necessarily interruptible. Previous work on this topic has assumed that the interruption is a worst-case deadline that is unknown to the scheduler. In this work, we study new settings in which the scheduler has access to some imperfect prediction in regards to the interruption. In the first setting, which is inspired by recent advances in learning-enhanced algorithms, the prediction describes the time that the interruption occurs. The second setting introduces a new model in which predictions are elicited as responses to a number of binary queries. For both settings, we investigate tradeoffs between the robustness (i.e., the worst-case performance of the schedule if the prediction is generated adversarially) and the consistency (i.e., the performance assuming that the prediction is error-free). We also establish results on the performance of the schedules as a function of the prediction error.",,11,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,ANR,ANR-19-CE48-0016,Agence Nationale de la Recherche
2-s2.0-85140760362,,,,Contraction rates for sparse variational approximations in Gaussian process regression,ar,Article,Nieman D.,60008734;60021796,Vrije Universiteit Amsterdam;Università Bocconi,Amsterdam;Milan,Netherlands;Italy,3.0,"Nieman, Dennis;Szabo, Botond;van Zanten, Harry",57285008000;44761456100;6602924237,60008734;60021796;60008734,2022-06-01,1 June 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,205,,"We study the theoretical properties of a variational Bayes method in the Gaussian Process regression model. We consider the inducing variables method introduced by Titsias (2009b) and derive sufficient conditions for obtaining contraction rates for the corresponding variational Bayes (VB) posterior. As examples we show that for three particular covariance kernels (Matérn, squared exponential, random series prior) the VB approach can achieve optimal, minimax contraction rates for a sufficiently large number of appropriately chosen inducing variables. The theoretical findings are demonstrated by numerical experiments.",contraction rates | Gaussian Process regression | inducing variables | Variational Bayes,8,0.0,,,H2020,101041064,Horizon 2020 Framework Programme
2-s2.0-85174404262,,,,Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning,cp,Conference Paper,Lu C.,60025278;60014402;60272289,Tsinghua University;Renmin University of China;Guangdong Artificial Intelligence and Digital Economy Laboratory,Beijing;Beijing;Guangzhou,China;China;China,6.0,"Lu, Cheng;Chen, Huayu;Chen, Jianfei;Su, Hang;Li, Chongxuan;Zhu, Jun",57716674100;57226597619;56123394400;37017428500;57189095239;56734692500,60025278;60025278;60025278;60025278;60014402;60025278-60272289,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,22825-22855,"Guided sampling is a vital approach for applying diffusion models in real-world tasks that embeds human-defined guidance during the sampling procedure. This paper considers a general setting where the guidance is defined by an (unnormalized) energy function. The main challenge for this setting is that the intermediate guidance during the diffusion sampling procedure, which is jointly defined by the sampling distribution and the energy function, is unknown and is hard to estimate. To address this challenge, we propose an exact formulation of the intermediate guidance as well as a novel training objective named contrastive energy prediction (CEP) to learn the exact guidance. Our method is guaranteed to converge to the exact guidance under unlimited model capacity and data samples, while previous methods can not. We demonstrate the effectiveness of our method by applying it to offline reinforcement learning (RL). Extensive experiments on D4RL benchmarks demonstrate that our method outperforms existing state-of-the-art algorithms. We also provide some examples of applying CEP for image synthesis to demonstrate the scalability of CEP on high-dimensional data. Code is available at https://github.com/thu-ml/CEP-energy-guided-diffusion.",,42,0.0,,,SJTU HPC,61620106010,"Center for High Performance Computing, Shanghai Jiao Tong University"
2-s2.0-85161924998,,,,Contrastive Graph Structure Learning via Information Bottleneck for Recommendation,cp,Conference Paper,Wei C.,60007997;60118460,Weill Cornell Medicine;Alibaba Group Holding Limited,New York;Hangzhou,United States;China,4.0,"Wei, Chunyu;Liang, Jian;Liu, Di;Wang, Fei",57221500464;57214441373;57226016897;56177292700,60118460;60118460;60118460;60007997,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Graph convolution networks (GCNs) for recommendations have emerged as an important research topic due to their ability to exploit higher-order neighbors. Despite their success, most of them suffer from the popularity bias brought by a small number of active users and popular items. Also, a real-world user-item bipartite graph contains many noisy interactions, which may hamper the sensitive GCNs. Graph contrastive learning show promising performance for solving the above challenges in recommender systems. Most existing works typically perform graph augmentation to create multiple views of the original graph by randomly dropping edges/nodes or relying on predefined rules, and these augmented views always serve as an auxiliary task by maximizing their correspondence. However, we argue that the graph structures generated from these vanilla approaches may be suboptimal, and maximizing their correspondence will force the representation to capture information irrelevant for the recommendation task. Here, we propose a Contrastive Graph Structure Learning via Information Bottleneck (CGI) for recommendation, which adaptively learns whether to drop an edge or node to obtain optimized graph structures in an end-to-end manner. Moreover, we innovatively introduce the Information Bottleneck into the contrastive learning process to avoid capturing irrelevant information among different views and help enrich the final representation for recommendation. Extensive experiments on public datasets are provided to show that our model significantly outperforms strong baselines.",,86,0.0,,,,,
2-s2.0-85196894564,,,,Contrastive Representation for Data Filtering in Cross-Domain Offline Reinforcement Learning,cp,Conference Paper,Wen X.,60025278;60009860;60019616;60003977;60280914;60001081,Tsinghua University;Fudan University;Harbin Institute of Technology;Northwestern Polytechnical University;Shanghai Artificial Intelligence Laboratory;China Telecom Corporation Limited,Beijing;Shanghai;Harbin;Xi'an;Shanghai;Beijing,China;China;China;China;China;China,7.0,"Wen, Xiaoyu;Bai, Chenjia;Xu, Kang;Yu, Xudong;Zhang, Yang;Li, Xuelong;Wang, Zhen",58664669000;57208819711;57423324300;57441973800;59148549300;55936260100;57192377511,60003977;60280914-60003977;60009860;60019616;60025278;60001081;60003977,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,52720-52743,"Cross-domain offline reinforcement learning leverages source domain data with diverse transition dynamics to alleviate the data requirement for the target domain. However, simply merging the data of two domains leads to performance degradation due to the dynamics mismatch. Existing methods address this problem by measuring the dynamics gap via domain classifiers while relying on the assumptions of the transferability of paired domains. In this paper, we propose a novel representation-based approach to measure the domain gap, where the representation is learned through a contrastive objective by sampling transitions from different domains. We show that such an objective recovers the mutual-information gap of transition functions in two domains without suffering from the unbounded issue of the dynamics gap in handling significantly different domains. Based on the representations, we introduce a data filtering algorithm that selectively shares transitions from the source domain according to the contrastive score functions. Empirical results on various tasks demonstrate that our method achieves superior performance, using only 10% of the target data to achieve 89.2% of the performance on 100% target dataset with state-of-the-art methods.",,3,0.0,,,NSFC,62025602,National Science Fund for Distinguished Young Scholars
2-s2.0-85204310885,,,,Contrastive and View-Interaction Structure Learning for Multi-view Clustering,cp,Conference Paper,Wang J.,60022381,Beijing Jiaotong University,Beijing,China,2.0,"Wang, Jing;Feng, Songhe",55265432200;7402531247,60022381;60022381,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,5055-5063,"Existing Deep Multi-view Clustering (DMVC) approaches typically concentrate on capturing consensus semantics from multiple views, where contrastive learning is widely used to align view-specific representations of each view. Unfortunately, view-specific representations are extracted from the content information of the corresponding instance, neglecting the relationships among different instances. Furthermore, existing contrastive loss imports numerous false negative pairs that conflict with the clustering objectives. In response to these challenges, we propose a contraStive and viEw-interaction stRucture learning framework for multI-viEw cluStering (SERIES). Our method takes into account the structural relations among instances and boosts the contrastive loss to improve intra-class compactness. Meanwhile, a cross-view dual relation generation mechanism is introduced to achieve the consensus structural graph across multiple views for clustering. Specifically, we initially acquire view-specific representations using multiple graph autoencoders to exploit both content information and structural information. Furthermore, to pull together the same cluster instances, a soft negative pair aware contrastive loss is employed to distinguish the dissimilar instances while attracting similar instances. Thereafter, the view-specific representations are fed into cross-view dual relation generation layers to generate the affinity matrices of each other, aiming to reveal a consistent structural graph across various views. Extensive experiments conducted on six benchmarks illustrate the superiority of our method compared to other state-of-the-art approaches.",,4,0.0,,,,2022JBZY019,Fundamental Research Funds for the Central Universities
2-s2.0-85203789327,,,,Convergence Guarantees for the DeepWalk Embedding on Block Models,cp,Conference Paper,Harker C.,60025488,The University of Utah,Salt Lake City,United States,2.0,"Harker, Christopher;Bhaskara, Aditya",58493708100;36175618600,60025488;60025488,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,17636-17656,"Graph embeddings have emerged as a powerful tool for understanding the structure of graphs. Unlike classical spectral methods, recent methods such as DeepWalk, Node2Vec, etc. are based on solving nonlinear optimization problems on the graph, using local information obtained by performing random walks. These techniques have empirically been shown to produce “better” embeddings than their classical counterparts. However, due to their reliance on solving a nonconvex optimization problem, obtaining theoretical guarantees on the properties of the solution has remained a challenge, even for simple classes of graphs. In this work, we show convergence properties for the DeepWalk algorithm on graphs obtained from the Stochastic Block Model (SBM). Despite being simplistic, the SBM has proved to be a classic model for analyzing the behavior of algorithms on large graphs. Our results mirror the existing ones for spectral embeddings on SBMs, showing that even in the case of one-dimensional embeddings, the output of the DeepWalk algorithm provably recovers the cluster structure with high probability.",,0,0.0,,,NSF,CCF-2008688,National Science Foundation
2-s2.0-105018473015,,,,"Convergence for nonconvex ADMM, with applications to CT imaging",ar,Article,Barber R.F.,60023086;60117720,"Department of Radiology, The University of Chicago;Department of Statistics, The University of Chicago",Chicago;Chicago,United States;United States,2.0,"Barber, Rina Foygel;Sidky, Emil Y.",56581332700;7003283807,60117720;60023086,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"The alternating direction method of multipliers (ADMM) algorithm is a powerful and flexible tool for complex optimization problems of the form min{f(x) + g(y): Ax + By = c}. ADMM exhibits robust empirical performance across a range of challenging settings including nonsmoothness and nonconvexity of the objective functions f and g, and provides a simple and natural approach to the inverse problem of image reconstruction for computed tomography (CT) imaging. From the theoretical point of view, existing results for convergence in the nonconvex setting generally assume smoothness in at least one of the component functions in the objective. In this work, our new theoretical results provide convergence guarantees under a restricted strong convexity assumption without requiring smoothness or differentiability, while still allowing differentiable terms to be treated approximately if needed. We validate these theoretical results empirically, with a simulated example where both f and g are nondifferentiable—and thus outside the scope of existing theory—as well as a simulated CT image reconstruction problem.",ADMM | CT imaging | nonconvex optimization,7,0.0,,,NIH,R01-023968,National Institutes of Health
2-s2.0-85170400705,10.24963/ijcai.2023/310,,,Convergence of Multi-Issue Iterative Voting under Uncertainty,cp,Conference Paper,Kavner J.,60022403;60025534;60017366,Technion - Israel Institute of Technology;Rensselaer Polytechnic Institute;IBM Thomas J. Watson Research Center,Haifa;Troy;Yorktown Heights,Israel;United States;United States,4.0,"Kavner, Joshua;Meir, Reshef;Rossi, Francesca;Xia, Lirong",57508322200;25634412900;56066648900;23011050500,60025534;60022403;60017366;60025534,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,2783-2791,"We study strategic behavior in iterative plurality voting for multiple issues under uncertainty. We introduce a model synthesizing simultaneous multi-issue voting with local dominance theory, in which agents repeatedly update their votes based on sets of vote profiles they deem possible, and determine its convergence properties. After demonstrating that local dominance improvement dynamics may fail to converge, we present two sufficient model refinements that guarantee convergence from any initial vote profile for binary issues: constraining agents to have O-legal preferences, where issues are ordered by importance, and endowing agents with less uncertainty about issues they are modifying than others. Our empirical studies demonstrate that while cycles are common for agents without uncertainty, introducing uncertainty makes convergence almost guaranteed in practice.",,0,1.0,all publisherfullgold,All Open Access Gold,NSF,1453542,National Science Foundation
2-s2.0-85136560799,10.1613/jair.1.13519,,,Cooperation and Learning Dynamics under Wealth Inequality and Diversity in Individual Risk Perception,ar,Article,Merhej R.,60001422;60002483;60004956,Sorbonne Université;Universiteit van Amsterdam;Instituto Superior Técnico,Paris;Amsterdam;Lisbon,France;Netherlands;Portugal,4.0,"Merhej, Ramona;Santos, Fernando P.;Melo, Francisco S.;Santos, Francisco C.",57226723243;57196074180;35569256100;9845451400,60004956-60001422;60002483;60004956;60004956,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,733-764,"We examine how wealth inequality and diversity in the perception of risk of a collective disaster impact cooperation levels in the context of a public goods game with uncertain and non-linear returns. In this game, individuals face a collective-risk dilemma where they may contribute or not to a common pool to reduce their chances of future losses. We draw our conclusions based on social simulations with populations of independent reinforcement learners with diverse levels of risk and wealth. We find that both wealth inequality and diversity in risk assessment can hinder cooperation and augment collective losses. Additionally, wealth inequality further exacerbates long term inequality, causing rich agents to become richer and poor agents to become poorer. On the other hand, diversity in risk only amplifies inequality when combined with bias in group assortment—i.e., high probability that agents from the same risk class play together. Our results also suggest that taking wealth inequality into account can help to design effective policies aiming at leveraging cooperation in large group sizes, a configuration where collective action is harder to achieve. Finally, we characterize the circumstances under which risk perception alignment is crucial and those under which reducing wealth inequality constitutes a deciding factor for collective welfare.",,6,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,FCT,PTDC/CCI-INF/7366/2020,Fundação para a Ciência e a Tecnologia
2-s2.0-85162160615,10.1613/JAIR.1.14074,,,Coopetition Against an Amazon,ar,Article,Gradwohl R.,60022403;60080064,Technion - Israel Institute of Technology;Ariel University,Haifa;Ariel,Israel;Israel,2.0,"Gradwohl, Ronen;Tennenholtz, Moshe",23388808500;7003406059,60080064;60022403,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,1077-1116,"This paper analyzes cooperative data-sharing between competitors vying to predict a consumer's tastes. We design optimal data-sharing schemes both for when they compete only with each other, and for when they additionally compete with an Amazon-a company with more, better data. We show that simple schemes-threshold rules that probabilistically induce either full data-sharing between competitors, or the full transfer of data from one competitor to another-are either optimal or approximately optimal, depending on properties of the information structure. We also provide conditions under which firms share more data when they face stronger outside competition, and describe situations in which this conclusion is reversed.",,5,1.0,all publisherfullgold,All Open Access Gold,NSF,1718670,National Science Foundation
2-s2.0-85122913369,10.1609/aaai.v36i7.20742,,,Coordinate Descent on the Orthogonal Group for Recurrent Neural Network Training,cp,Conference Paper,Massart E.,60026851;60010785;60105479,"University of Oxford;National Physical Laboratory;Indraprastha Institute of Information Technology, Delhi",Oxford;Middlesex;New Delhi,United Kingdom;United Kingdom;India,2.0,"Massart, Estelle;Abrol, Vinayak",57191835168;55574319000,60026851-60010785;60105479,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,7744-7751,"We address the poor scalability of learning algorithms for orthogonal recurrent neural networks via the use of stochastic coordinate descent on the orthogonal group, leading to a cost per iteration that increases linearly with the number of recurrent states. This contrasts with the cubic dependency of typical feasible algorithms such as stochastic Riemannian gradient descent, which prohibits the use of big network architectures. Coordinate descent rotates successively two columns of the recurrent matrix. When the coordinate (i.e., indices of rotated columns) is selected uniformly at random at each iteration, we prove convergence of the algorithm under standard assumptions on the loss function, stepsize and minibatch noise. In addition, we numerically show that the Riemannian gradient has an approximately sparse structure. Leveraging this observation, we propose a variant of our proposed algorithm that relies on the Gauss-Southwell coordinate selection rule. Experiments on a benchmark recurrent neural network training problem show that the proposed approach is a very promising step towards the training of orthogonal recurrent neural networks with big architectures.",,9,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,NPL,,National Physical Laboratory
2-s2.0-85163124092,,,,Coordinated Double Machine Learning,cp,Conference Paper,Fingerhut N.,60029311;126790806,University of Southern California;Technion,Los Angeles;,United States;Israel,3.0,"Fingerhut, Nitai;Sesia, Matteo;Romano, Yaniv",57219588445;57207909654;56102421600,126790806;60029311;126790806,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,6499-6513,"Double machine learning is a statistical method for leveraging complex black-box models to construct approximately unbiased treatment effect estimates given observational data with high-dimensional covariates, under the assumption of a partially linear model. The idea is to first fit on a subset of the samples two non-linear predictive models, one for the continuous outcome of interest and one for the observed treatment, and then to estimate a linear coefficient for the treatment using the remaining samples through a simple orthogonalized regression. While this methodology is flexible and can accommodate arbitrary predictive models, typically trained independently of one another, this paper argues that a carefully coordinated learning algorithm for deep neural networks may reduce the estimation bias. The improved empirical performance of the proposed method is demonstrated through numerical experiments on both simulated and real data.",,3,0.0,,,ISF,729/21,Israel Science Foundation
2-s2.0-85164374211,10.1609/aaai.v37i10.2636126395,,,Correct for Whom? Subjectivity and the Evaluation of Personalized Image Aesthetics Assessment Models,cp,Conference Paper,Goree S.,60010875,"Luddy School of Informatics, Computing, and Engineering",Bloomington,United States,3.0,"Goree, Samuel;Khoo, Weslie;Crandall, David J.",57202911354;58586258600;8732077700,60010875;60010875;60010875,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,11818-11827,"The problem of image aesthetic quality assessment is surprisingly difficult to define precisely. Most early work attempted to estimate the average aesthetic rating of a group of observers, while some recent work has shifted to an approach based on few-shot personalization. In this paper, we connect few-shot personalization, via Immanuel Kant’s concept of disinterested judgment, to an argument from feminist aesthetics about the biased tendencies of objective standards for subjective pleasures. To empirically investigate this philosophical debate, we introduce PR-AADB, a relabeling of the existing AADB dataset with labels for pairs of images, and measure how well the existing ground truth predicts our new pairwise labels. We find, consistent with the feminist critique, that both the existing ground truth and few-shot personalized predictions represent some users’ preferences significantly better than others, but that it is difficult to predict when and for whom the existing ground truth will be correct. We thus advise against using benchmark datasets to evaluate models for personalized IAQA, and recommend caution when attempting to account for subjective difference using machine learning more generally.",,5,0.0,,,NSF,2112635,National Science Foundation
2-s2.0-85191197285,,,,Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry,cp,Conference Paper,Bozkurt B.,60077572;60006369;60084280;130472003,Harvard John A. Paulson School of Engineering and Applied Sciences;Koç University;Gatsby Computational Neuroscience Unit;Kempner Institute for the Study of Natural and Artificial Intelligence,Cambridge;Istanbul;London;Kempner,United States;Turkey;United Kingdom;United States,3.0,"Bozkurt, Bariscan;Pehlevan, Cengiz;Erdogan, Alper T.",57678268400;25924515000;7005311430,60084280-60006369;60077572-130472003;60006369,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"The backpropagation algorithm has experienced remarkable success in training large-scale artificial neural networks; however, its biological plausibility has been strongly criticized, and it remains an open question whether the brain employs supervised learning mechanisms akin to it. Here, we propose correlative information maximization between layer activations as an alternative normative approach to describe the signal propagation in biological neural networks in both forward and backward directions. This new framework addresses many concerns about the biological-plausibility of conventional artificial neural networks and the backpropagation algorithm. The coordinate descent-based optimization of the corresponding objective, combined with the mean square error loss function for fitting labeled supervision data, gives rise to a neural network structure that emulates a more biologically realistic network of multi-compartment pyramidal neurons with dendritic processing and lateral inhibitory neurons. Furthermore, our approach provides a natural resolution to the weight symmetry problem between forward and backward signal propagation paths, a significant critique against the plausibility of the conventional backpropagation algorithm. This is achieved by leveraging two alternative, yet equivalent forms of the correlative mutual information objective. These alternatives intrinsically lead to forward and backward prediction networks without weight symmetry issues, providing a compelling solution to this long-standing challenge.",,1,0.0,,,NSF,DMS-2134157,Gatsby Charitable Foundation
2-s2.0-85170378133,10.24963/ijcai.2023/67,,,CostFormer: Cost Transformer for Cost Aggregation in Multi-view Stereo,cp,Conference Paper,Chen W.,60024542;60118460;126489570,South China University of Technology;Alibaba Group Holding Limited;Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ),Guangzhou;Hangzhou;Guangdong,China;China;China,7.0,"Chen, Weitao;Xu, Hongbin;Zhou, Zhipeng;Liu, Yang;Sun, Baigui;Kang, Wenxiong;Xie, Xuansong",57417186800;57223869954;57219636361;57191902565;57191895334;30267694900;57210375524,60118460;60118460-60024542;60118460;60118460;60118460;60024542-126489570;60118460,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,599-608,"The core of Multi-view Stereo(MVS) is the matching process among reference and source pixels. Cost aggregation plays a significant role in this process, while previous methods focus on handling it via CNNs. This may inherit the natural limitation of CNNs that fail to discriminate repetitive or incorrect matches due to limited local receptive fields. To handle the issue, we aim to involve Transformer into cost aggregation. However, another problem may occur due to the quadratically growing computational complexity caused by Transformer, resulting in memory overflow and inference latency. In this paper, we overcome these limits with an efficient Transformer-based cost aggregation network, namely CostFormer. The Residual Depth-Aware Cost Transformer(RDACT) is proposed to aggregate long-range features on cost volume via self-attention mechanisms along the depth and spatial dimensions. Furthermore, Residual Regression Transformer(RRT) is proposed to enhance spatial attention. The proposed method is a universal plug-in to improve learning-based MVS methods.",,18,1.0,all publisherfullgold,All Open Access Gold,NSFC,61976095,National Natural Science Foundation of China
2-s2.0-105000547866,,,,Counterfactual Fairness by Combining Factual and Counterfactual Predictions,cp,Conference Paper,Zhou Z.,60148444,College of Engineering,West Lafayette,United States,6.0,"Zhou, Zeyu;Liu, Tianci;Bai, Ruqi;Gao, Jing;Kocaoglu, Murat;Inouye, David I.",59066681900;57764941700;57221863759;55702655600;55234129200;59843938800,60148444;;;;;,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"In high-stakes domains such as healthcare and hiring, the role of machine learning (ML) in decision-making raises significant fairness concerns. This work focuses on Counterfactual Fairness (CF), which posits that an ML model's outcome on any individual should remain unchanged if they had belonged to a different demographic group. Previous works have proposed methods that guarantee CF. Notwithstanding, their effects on the model's predictive performance remain largely unclear. To fill this gap, we provide a theoretical study on the inherent trade-off between CF and predictive performance in a model-agnostic manner. We first propose a simple but effective method to cast an optimal but potentially unfair predictor into a fair one with minimal performance degradation. By analyzing the excess risk incurred by perfect CF, we quantify this inherent trade-off. Further analysis on our method's performance with access to only incomplete causal knowledge is also conducted. Built upon this, we propose a practical algorithm that can be applied in such scenarios. Experiments on both synthetic and semi-synthetic datasets demonstrate the validity of our analysis and methods.",,2,0.0,,,NSF,,National Science Foundation
2-s2.0-85197347726,10.1613/jair.1.16210,,,Counting Complexity for Reasoning in Abstract Argumentation,ar,Article,Fichte J.K.,60009358;60004935;60006320,Linköpings Universitet;Gottfried Wilhelm Leibniz Universität Hannover;MIT Computer Science & Artificial Intelligence Laboratory,Linkoping;Hannover;Cambridge,Sweden;Germany;United States,3.0,"Fichte, Johannes K.;Hecher, Markus;Meier, Arne",35181184500;56493064500;25927331400,60009358;60006320;60004935,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,805-834,"In this paper, we consider counting and projected model counting of extensions in abstract argumentation for various semantics, including credulous reasoning. When asking for projected counts, we are interested in counting the number of extensions of a given argumentation framework, while multiple extensions that are identical when restricted to the projected arguments count as only one projected extension. We establish classical complexity results and parameterized complexity results when the problems are parameterized by the treewidth of the undirected argumentation graph. To obtain upper bounds for counting projected extensions, we introduce novel algorithms that exploit small treewidth of the undirected argumentation graph of the input instance by dynamic programming. Our algorithms run in double or triple exponential time in the treewidth, depending on the semantics under consideration. Finally, we establish lower bounds of bounded treewidth algorithms for counting extensions and projected extension under the exponential time hypothesis (ETH).",,5,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,ELLIIT,J4656,Excellence Center at Linköping – Lund in Information Technology
2-s2.0-85170391064,10.24963/ijcai.2023/678,,,Coupled Point Process-based Sequence Modeling for Privacy-preserving Network Alignment,cp,Conference Paper,Luo D.,60016835;60014402;119194081,Beijing Institute of Technology;Renmin University of China;Beijing Key Laboratory of Big Data Management and Analysis Methods,Beijing;Beijing;Beijing,China;China;China,4.0,"Luo, Dixin;Cheng, Haoran;Li, Qingbin;Xu, Hongteng",25623719000;58569281700;58569599200;54788433800,60016835;60016835;60016835;60014402-119194081,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,6112-6120,"Network alignment aims at finding the correspondence of nodes across different networks, which is significant for many applications, e.g., fraud detection and crime network tracing across platforms. In practice, however, accessing the topological information of different networks is often restricted and even forbidden, considering privacy and security issues. Instead, what we observed might be the event sequences of the networks' nodes in the continuous-time domain. In this study, we develop a coupled neural point process-based (CPP) sequence modeling strategy, which provides a solution to privacy-preserving network alignment based on the event sequences. Our CPP consists of a coupled node embedding layer and a neural point process module. The coupled node embedding layer embeds one network's nodes and explicitly models the alignment matrix between the two networks. Accordingly, it parameterizes the node embeddings of the other network by the push-forward operation. Given the node embeddings, the neural point process module jointly captures the dynamics of the two networks' event sequences. We learn the CPP model in a maximum likelihood estimation framework with an inverse optimal transport (IOT) regularizer. Experiments show that our CPP is compatible with various point process backbones and is robust to the model misspecification issue, which achieves encouraging performance on network alignment. The code is available at https://github.com/Dixin-s-Lab/CNPP.",,1,1.0,all publisherfullgold,All Open Access Gold,NSFC,62102031,National Natural Science Foundation of China
2-s2.0-85204304963,,,,Cross-Domain Feature Augmentation for Domain Generalization,cp,Conference Paper,Liu Y.,60017161,National University of Singapore,Singapore City,Singapore,6.0,"Liu, Yingnan;Zou, Yingtian;Qiao, Rui;Liu, Fusheng;Lee, Mong Li;Hsu, Wynne",58953522100;57209239772;57225965022;57478275200;57746054100;7402002763,60017161;60017161;60017161;60017161;60017161;60017161,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1146-1154,"Domain generalization aims to develop models that are robust to distribution shifts. Existing methods focus on learning invariance across domains to enhance model robustness, and data augmentation has been widely used to learn invariant predictors, with most methods performing augmentation in the input space. However, augmentation in the input space has limited diversity whereas in the feature space is more versatile and has shown promising results. Nonetheless, feature semantics is seldom considered and existing feature augmentation methods suffer from a limited variety of augmented features. We decompose features into class-generic, class-specific, domain-generic, and domain-specific components. We propose a cross-domain feature augmentation method named XDomainMix that enables us to increase sample diversity while emphasizing the learning of invariant representations to achieve domain generalization. Experiments on widely used benchmark datasets demonstrate that our proposed method is able to achieve state-of-the-art performance. Quantitative analysis indicates that our feature augmentation approach facilitates the learning of effective models that are invariant across different domains.",,5,0.0,,,NRF,AISG-GC-2019-001-2B,National Research Foundation Singapore
2-s2.0-85204310010,,,,Cross-Domain Few-Shot Semantic Segmentation via Doubly Matching Transformation,cp,Conference Paper,Chen J.,60021666;60025578,Nanjing University of Aeronautics and Astronautics;Xidian University,Nanjing;Xi'an,China;China,3.0,"Chen, Jiayi;Quan, Rong;Qin, Jie",59870423300;57191077228;56461909500,60021666-60025578;60021666-60025578;60021666-60025578,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,641-649,"Cross-Domain Few-shot Semantic Segmentation (CD-FSS) aims to train generalized models that can segment classes from different domains with a few labeled images. Previous works have proven the effectiveness of feature transformation in addressing CD-FSS. However, they completely rely on support images for feature transformation, and repeatedly utilizing a few support images for each class may easily lead to overfitting and overlooking intra-class appearance differences. In this paper, we propose a Doubly Matching Transformation-based Network (DMTNet) to solve the above issue. Instead of completely relying on support images, we propose Self-Matching Transformation (SMT) to construct query-specific transformation matrices based on query images themselves to transform domain-specific query features into domain-agnostic ones. Calculating query-specific transformation matrices can prevent overfitting, especially for the meta-testing stage where only one or several images are used as support images to segment hundreds or thousands of images. After obtaining domain-agnostic features, we exploit a Dual Hypercorrelation Construction (DHC) module to explore the hypercorrelations between the query image with the foreground and background of the support image, based on which foreground and background prediction maps are generated and supervised, respectively, to enhance the segmentation result. In addition, we propose a Test-time Self-Finetuning (TSF) strategy to more accurately self-tune the query prediction in unseen domains. Extensive experiments on four popular datasets show that DMTNet achieves superior performance over state-of-the-art approaches. Code is available at https://github.com/ChenJiayi68/DMTNet.",,9,0.0,,,NSFC,62276129,National Natural Science Foundation of China
2-s2.0-85167978130,10.1609/aaai.v37i3.25400,,,Cross-Modal Contrastive Learning for Domain Adaptation in 3D Semantic Segmentation,cp,Conference Paper,Xing B.,60124572,"Key Laboratory of Machine Perception, Ministry of Education",Beijing,China,5.0,"Xing, Bowei;Ying, Xianghua;Wang, Ruibin;Yang, Jinfa;Chen, Taiyan",58121794500;7004495215;57470322900;57214872614;57658287600,60124572;60124572;60124572;60124572;60124572,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,2974-2982,"Domain adaptation for 3D point cloud has attracted a lot of interest since it can avoid the time-consuming labeling process of 3D data to some extent. A recent work named xMUDA leveraged multi-modal data to domain adaptation task of 3D semantic segmentation by mimicking the predictions between 2D and 3D modalities, and outperformed the previous single modality methods only using point clouds. Based on it, in this paper, we propose a novel cross-modal contrastive learning scheme to further improve the adaptation effects. By employing constraints from the correspondences between 2D pixel features and 3D point features, our method not only facilitates interaction between the two different modalities, but also boosts feature representations in both labeled source domain and unlabeled target domain. Meanwhile, to sufficiently utilize 2D context information for domain adaptation through cross-modal learning, we introduce a neighborhood feature aggregation module to enhance pixel features. The module employs neighborhood attention to aggregate nearby pixels in the 2D image, which relieves the mismatching between the two different modalities, arising from projecting relative sparse point cloud to dense image pixels. We evaluate our method on three unsupervised domain adaptation scenarios, including country-to-country, day-to-night, and datasetto-dataset. Experimental results show that our approach outperforms existing methods, which demonstrates the effectiveness of the proposed method.",,21,1.0,all publisherfullgold,All Open Access Gold,NSFC,61971008,National Natural Science Foundation of China
2-s2.0-85212280973,10.1613/jair.1.15736,,,Cross-domain Constituency Parsing by Leveraging Heterogeneous Data,ar,Article,Guo P.,60019533;60361251;60117660,Tianjin University;Harbin Institute of Technology Shenzhen;Westlake University,Tianjin;Shenzhen;Hangzhou,China;China;China,6.0,"Guo, Peiming;Zhang, Meishan;Chen, Yulong;Li, Jianling;Zhang, Min;Zhang, Yue",58017881200;55668233900;57224911639;58726240200;56368409100;56066648800,60361251;60361251;60117660;60019533;60361251;60117660,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,771-791,"Knowledge transfer is investigated in various natural language processing tasks except cross-domain constituency parsing. In this paper, we leverage heterogeneous data to transfer cross-domain and cross-task knowledge to constituency parsing. Concretely, we first select language modeling, named entity recognition, CCG supertagging and dependency parsing as auxiliary tasks and collect the corpora of these tasks covering various domains as cross-domain and cross-task heterogeneous data. Second, we exploit three types of prefixes: shared, task and domain prefix, to merge cross-domain and cross-task data and decompose the general, task and domain representation in the pretrained language model. Third, we convert the data formats of multi-source heterogeneous datasets and loss objectives of the auxiliary tasks into a consistent formalization closer to constituency parsing. Finally, we jointly train the model to transfer task and domain knowledge to cross-domain constituency parsing. We verify the effectiveness of our proposed model on five target domains of MCTB. Experimental results show that our knowledge transfer model outperforms various baseline models, including conventional chart-based and transition-based parsers and the current large-scale language model for zero-shot and few-shot settings.",,1,1.0,all publisherfullgold,All Open Access Gold,NNSFC,61976180,Westlake University
2-s2.0-85189544107,10.1613/jair.1.14888,,,Cultural Bias in Explainable AI Research: A Systematic Analysis,ar,Article,Peters U.,60007989;60016218,"Universiteit Utrecht;University of the Witwatersrand, Johannesburg",Utrecht;Johannesburg,Netherlands;South Africa,2.0,"Peters, Uwe;Carman, Mary",57206033587;57189994773,60007989;60016218,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,971-1000,"For synergistic interactions between humans and artificial intelligence (AI) systems, AI outputs often need to be explainable to people. Explainable AI (XAI) systems are commonly tested in human user studies. However, whether XAI researchers consider potential cultural differences in human explanatory needs remains unexplored. We highlight psychological research that found significant differences in human explanations between many people from Western, commonly individualist countries and people from non-Western, often collectivist countries. We argue that XAI research currently overlooks these variations and that many popular XAI designs implicitly and problematically assume that Western explanatory needs are shared cross-culturally. Additionally, we systematically reviewed over 200 XAI user studies and found that most studies did not consider relevant cultural variations, sampled only Western populations, but drew conclusions about human-XAI interactions more generally. We also analyzed over 30 literature reviews of XAI studies. Most reviews did not mention cultural differences in explanatory needs or flag overly broad cross-cultural extrapolations of XAI user study results. Combined, our analyses provide evidence of a cultural bias toward Western populations in XAI research, highlighting an important knowledge gap regarding how culturally diverse users may respond to widely used XAI systems that future work can and should address.",,28,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85147736745,,,,D-GCCA: Decomposition-based Generalized Canonical Correlation Analysis for Multi-view High-dimensional Data,ar,Article,Shu H.,60021784;60016639;60142450,New York University;UNC Gillings School of Global Public Health;Tulane University School of Science and Engineering,New York;Chapel Hill;New Orleans,United States;United States;United States,3.0,"Shu, Hai;Qu, Zhe;Zhu, Hongtu",56666767000;57219492674;7404664018,60021784;60142450;60016639,2022-06-01,1 June 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Modern biomedical studies often collect multi-view data, that is, multiple types of data measured on the same set of objects. A popular model in high-dimensional multi-view data analysis is to decompose each view’s data matrix into a low-rank common-source matrix generated by latent factors common across all data views, a low-rank distinctive-source matrix corresponding to each view, and an additive noise matrix. We propose a novel decomposition method for this model, called decomposition-based generalized canonical correlation analysis (D-GCCA). The D-GCCA rigorously defines the decomposition on the L<sup>2</sup> space of random variables in contrast to the Euclidean dot product space used by most existing methods, thereby being able to provide the estimation consistency for the low-rank matrix recovery. Moreover, to well calibrate common latent factors, we impose a desirable orthogonality constraint on distinctive latent factors. Existing methods, however, inadequately consider such orthogonality and may thus suffer from substantial loss of undetected common-source variation. Our D-GCCA takes one step further than generalized canonical correlation analysis by separating common and distinctive components among canonical variables, while enjoying an appealing interpretation from the perspective of principal component analysis. Furthermore, we propose to use the variable-level proportion of signal variance explained by common or distinctive latent factors for selecting the variables most influenced. Consistent estimators of our D-GCCA method are established with good finite-sample numerical performance, and have closed-form expressions leading to efficient computation especially for large-scale data. The superiority of D-GCCA over state-of-the-art methods is also corroborated in simulations and real-world data examples.",Canonical variable | common and distinctive variation structures | data integration | high-dimensional data | multi-view data,8,0.0,,,NIH,R01MH086633,National Institutes of Health
2-s2.0-85191278268,,,,D2 PRUNING: MESSAGE PASSING FOR BALANCING DIVERSITY & DIFFICULTY IN DATA PRUNING,cp,Conference Paper,Maharana A.,60142067,Department of Computer Science,Chapel Hill,United States,3.0,"Maharana, Adyasha;Yadav, Prateek;Bansal, Mohit",57194338660;57223914772;16466939600,60142067;60142067;60142067,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"In recent years, data quality has emerged as an important factor for training massive models. Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. In this work, we represent a dataset as an undirected graph and propose a novel pruning algorithm, D<sup>2</sup> PRUNING, that uses message passing over this dataset graph for coreset selection. D<sup>2</sup> PRUNING updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and NLP datasets. Results show that D<sup>2</sup> PRUNING improves coreset selection over previous state-of-the-art methods at low-to-medium pruning rates. Additionally, we find that using D<sup>2</sup> PRUNING for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models. Our work shows that D<sup>2</sup> PRUNING is a versatile framework for understanding and processing datasets.",,15,0.0,,,,,
2-s2.0-85147344174,10.1609/aaai.v37i10.2636126389,,,DACOM: Learning Delay-Aware Communication for Multi-Agent Reinforcement Learning,cp,Conference Paper,Yuan T.,60010348;60031514;60016930;130088777,Universitetet i Oslo;Georg-August-Universität Göttingen;Beijing University of Posts and Telecommunications;Ltd.,Oslo;Gottingen;Beijing;,Norway;Germany;China;,4.0,"Yuan, Tingting;Chung, Hwei Ming;Yuan, Jie;Fu, Xiaoming",57199297586;56888201500;57200687838;8856627300,60031514;60010348-130088777;60016930;60031514,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,11763-11771,"Communication is supposed to improve multi-agent collaboration and overall performance in cooperative Multi-agent reinforcement learning (MARL). However, such improvements are prevalently limited in practice since most existing communication schemes ignore communication overheads (e.g., communication delays). In this paper, we demonstrate that ignoring communication delays has detrimental effects on collaborations, especially in delay-sensitive tasks such as autonomous driving. To mitigate this impact, we design a delay-aware multi-agent communication model (DACOM) to adapt communication to delays. Specifically, DACOM introduces a component, TimeNet, that is responsible for adjusting the waiting time of an agent to receive messages from other agents such that the uncertainty associated with delay can be addressed. Our experiments reveal that DACOM has a non-negligible performance improvement over other mechanisms by making a better trade-off between the benefits of communication and the costs of waiting for messages.",,18,0.0,,,AvH,101092696,Alexander von Humboldt-Stiftung
2-s2.0-85137618534,10.1609/aaai.v36i4.20309,,,DANETs: Deep Abstract Networks for Tabular Data Classification and Regression,cp,Conference Paper,Chen J.,60025761;60029310;60155914;60117751,"Huazhong University of Science and Technology;Zhejiang University School of Medicine;College of Engineering;College of Computer Science and Technology, Zhejiang University",Wuhan;Hangzhou;Notre Dame;Hangzhou,China;China;United States;China,5.0,"Chen, Jintai;Liao, Kuanlun;Wan, Yao;Chen, Danny Z.;Wu, Jian",57211999917;57375681100;57089582400;7405453271;56313515800,60117751;60117751;60025761;60155914;60029310,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,3930-3938,"Tabular data are ubiquitous in real world applications. Although many commonly-used neural components (e.g., convolution) and extensible neural networks (e.g., ResNet) have been developed by the machine learning community, few of them were effective for tabular data and few designs were adequately tailored for tabular data structures. In this paper, we propose a novel and flexible neural component for tabular data, called Abstract Layer (ABSTLAY), which learns to explicitly group correlative input features and generate higher-level features for semantics abstraction. Also, we design a structure re-parameterization method to compress the trained ABSTLAY, thus reducing the computational complexity by a clear margin in the reference phase. A special basic block is built using ABSTLAYs, and we construct a family of Deep Abstract Networks (DANETs) for tabular data classification and regression by stacking such blocks. In DANETs, a special shortcut path is introduced to fetch information from raw tabular features, assisting feature interactions across different levels. Comprehensive experiments on seven real-world tabular datasets show that our ABSTLAY and DANETs are effective for tabular data classification and regression, and the computational complexity is superior to competitive methods. Besides, we evaluate the performance gains of DANET as it goes deep, verifying the extendibility of our method. Our code is available at https://github.com/WhatAShot/DANet.",,58,1.0,all publisherfullgold,All Open Access Gold,NSF,62102157,National Science Foundation
2-s2.0-85137945707,10.24963/ijcai.2022/137,,,DANet: Image Deraining via Dynamic Association Learning,cp,Conference Paper,Jiang K.,60029306;60018029;60130479,"Wuhan University;National Tsing Hua University;School of Computer Science and Technology, Harbin Institute of Technology",Wuhan;Hsinchu;Harbin,China;Taiwan;China,7.0,"Jiang, Kui;Wang, Zhongyuan;Wang, Zheng;Yi, Peng;Jiang, Junjun;Xiao, Jinsheng;Lin, Chia Wen",57203871718;57325546400;24175350400;57203880354;54902306100;8526666500;57210925364,60029306;60029306;60029306;60029306;60130479;60029306;60018029,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,980-986,"Rain streaks and background components in a rainy input are highly correlated, making the deraining task a composition of the rain streak removal and background restoration. However, the correlation of these two components is barely considered, leading to unsatisfied deraining results. To this end, we propose a dynamic associated network (DANet) to achieve the association learning between rain streak removal and background recovery. There are two key aspects to fulfill the association learning: 1) DANet unveils the latent association knowledge between rain distribution and background texture recovery, and leverages it as an extra prior via an associated learning module (ALM) to promote the texture recovery. 2) DANet introduces the parametric association constraint for enhancing the compatibility of deraining model with background reconstruction, enabling it to be automatically learned from the training data. Moreover, we observe that the sampled rainy image enjoys the similar distribution to the original one. We thus propose to learn the rain distribution at the sampling space, and exploit super-resolution to reconstruct background details for computation and memory reduction. Our proposed DANet achieves the approximate deraining performance to the state-of-the-art MPRNet but only accounts for 52.6% and 23% inference time and computational cost, respectively.",,25,1.0,all publisherfree2read,All Open Access Bronze,NSFC,GML-KF-22-16,National Natural Science Foundation of China
2-s2.0-105000464324,,,,DDN: Dual-domain Dynamic Normalization for Non-stationary Time Series Forecasting,cp,Conference Paper,Dai T.,60025278;60000937,Tsinghua University;Shenzhen University,Beijing;Shenzhen,China;China,7.0,"Dai, Tao;Wu, Beiliang;Liu, Peiyuan;Li, Naiqi;Yuerong, Xue;Xia, Shu Tao;Zhu, Zexuan",56940086700;58631279900;59076855200;57219548667;59701024100;7202892509;7404802556,60000937;60000937;60025278;60025278;60025278;60025278;60000937,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Deep neural networks (DNNs) have recently achieved remarkable advancements in time series forecasting (TSF) due to their powerful ability of sequence dependence modeling. To date, existing DNN-based TSF methods still suffer from unreliable predictions for real-world data due to its non-stationarity characteristics, i.e., data distribution varies quickly over time. To mitigate this issue, several normalization methods (e.g., SAN) have recently been specifically designed by normalization in a fixed period/window in the time domain. However, these methods still struggle to capture distribution variations, due to the complex time patterns of time series in the time domain. Based on the fact that wavelet transform can decompose time series into a linear combination of different frequencies, which exhibits distribution variations with time-varying periods, we propose a novel Dual-domain Dynamic Normalization (DDN) to dynamically capture distribution variations in both time and frequency domains. Specifically, our DDN tries to eliminate the non-stationarity of time series via both frequency and time domain normalization in a sliding window way. Besides, our DDN can serve as a plug-in-play module, and thus can be easily incorporated into other forecasting models. Extensive experiments on public benchmark datasets under different forecasting models demonstrate the superiority of our DDN over other normalization methods. Code is available at https://github.com/Hank0626/DDN.",,9,0.0,,,NSFC,PCL2023AS6-1,"Science, Technology and Innovation Commission of Shenzhen Municipality"
2-s2.0-85191150751,,,,DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models,cp,Conference Paper,Wang B.,60025038;60012708;60000745;60002798;60026532;129875245,"University of California, Berkeley;Stanford University;University of Illinois Urbana-Champaign;Chinese University of Hong Kong;Microsoft Corporation;Center for AI Safety",Berkeley;Stanford;Urbana;Hong Kong;Redmond;San Francisco,United States;United States;United States;Hong Kong;United States;United States,19.0,"Wang, Boxin;Chen, Weixin;Pei, Hengzhi;Xie, Chulin;Kang, Mintong;Zhang, Chenhui;Xu, Chejian;Xiong, Zidi;Dutta, Ritik;Schaeffer, Rylan;Truong, Sang T.;Arora, Simran;Mazeika, Mantas;Hendrycks, Dan;Lin, Zinan;Cheng, Yu;Koyejo, Sanmi;Song, Dawn;Li, Bo",57216202353;58151407100;57219591134;57210367947;57223083423;57148917300;57343006700;58420467200;57210725485;57221350848;57231362600;57219752635;57208440794;57204810366;57200653813;55421399200;57846623900;7402443870;57188689924,60000745;60000745;60000745;60000745;60000745;60000745;60000745;60000745;60000745;60012708;60012708;60012708;60000745;60025038-129875245;60026532;60002798;60012708;60025038;60000745,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance - where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives - including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially because GPT-4 follows (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/.",,147,0.0,,,DARPA,80NSSC20M0229,Google
2-s2.0-85170377194,10.24963/ijcai.2023/76,,,DFVSR: Directional Frequency Video Super-Resolution via Asymmetric and Enhancement Alignment Network,cp,Conference Paper,Dong S.,60025278;60271961,Tsinghua University;Peng Cheng Laboratory,Beijing;Shenzhen,China;China,4.0,"Dong, Shuting;Lu, Feng;Wu, Zhe;Yuan, Chun",57204723279;59041819800;57191159554;57434559000,60025278-60271961;60025278-60271961;60271961;60025278-60271961,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,681-689,"Recently, techniques utilizing frequency-based methods have gained significant attention, as they exhibit exceptional restoration capabilities for detail and structure in video super-resolution tasks. However, most of these frequency-based methods mainly have three major limitations: 1) insufficient exploration of object motion information, 2) inadequate enhancement for high-fidelity regions, and 3) loss of spatial information during convolution. In this paper, we propose a novel network, Directional Frequency Video Super-Resolution (DFVSR), to address these limitations. Specifically, we reconsider object motion from a new perspective and propose Directional Frequency Representation (DFR), which not only borrows the property of frequency representation of detail and structure information but also contains the direction information of the object motion that is extremely significant in videos. Based on this representation, we propose a Directional Frequency-Enhanced Alignment (DFEA) to use double enhancements of task-related information for ensuring the retention of high-fidelity frequency regions to generate the high-quality alignment feature. Furthermore, we design a novel Asymmetrical U-shaped network architecture to progressively fuse these alignment features and output the final output. This architecture enables the intercommunication of the same level of resolution in the encoder and decoder to achieve the supplement of spatial information. Powered by the above designs, our method achieves superior performance over state-of-the-art models on both quantitative and qualitative evaluations.",,10,1.0,all publisherfullgold,All Open Access Gold,NSFC,JCYJ20190809172201639,National Natural Science Foundation of China
2-s2.0-85142510177,,,,DIRECT THEN DIFFUSE: INCREMENTAL UNSUPERVISED SKILL DISCOVERY FOR STATE COVERING AND GOAL REACHING,cp,Conference Paper,Kamienny P.A.,60001422;60355330;127864951,Sorbonne Université;Meta Ai;Inria Scool,Paris;Menlo Park;,France;United States;France,5.0,"Kamienny, Pierre Alexandre;Tarbouriech, Jean;Lamprier, Sylvain;Lazaric, Alessandro;Denoyer, Ludovic",57219688649;57216863188;22734104000;14834304600;23090480900,60355330-60001422;60355330-127864951;60001422;60355330;60355330,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Learning meaningful behaviors in the absence of reward is a difficult problem in reinforcement learning. A desirable and challenging unsupervised objective is to learn a set of diverse skills that provide a thorough coverage of the state space while being directed, i.e., reliably reaching distinct regions of the environment. In this paper, we build on the mutual information framework for skill discovery and introduce UPSIDE, which addresses the coverage-directedness trade-off in the following ways: 1) We design policies with a decoupled structure of a directed skill, trained to reach a specific region, followed by a diffusing part that induces a local coverage. 2) We optimize policies by maximizing their number under the constraint that each of them reaches distinct regions of the environment (i.e., they are sufficiently discriminable) and prove that this serves as a lower bound to the original mutual information objective. 3) Finally, we compose the learned directed skills into a growing tree that adaptively covers the environment. We illustrate in several navigation and control environments how the skills learned by UPSIDE solve sparse-reward downstream tasks better than existing baselines.",,6,0.0,,,,,
2-s2.0-85187396603,,,,DIRICHLET-BASED PER-SAMPLE WEIGHTING BY TRANSITION MATRIX FOR NOISY LABEL LEARNING,cp,Conference Paper,Bae H.S.,60032144;131533713,Korea Advanced Institute of Science and Technology;summary.ai,Daejeon;Seoul,South Korea;South Korea,4.0,"Bae, Hee Sun;Shin, Seungjae;Na, Byeonghu;Moon, Il Chul",57699383100;57217871438;57219876296;56038773000,60032144;60032144;60032144;60032144-131533713,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"For learning with noisy labels, the transition matrix, which explicitly models the relation between noisy label distribution and clean label distribution, has been utilized to achieve the statistical consistency of either the classifier or the risk. Previous researches have focused more on how to estimate this transition matrix well, rather than how to utilize it. We propose good utilization of the transition matrix is crucial and suggest a new utilization method based on resampling, coined RENT. Specifically, we first demonstrate current utilizations can have potential limitations for implementation. As an extension to Reweighting, we suggest the Dirichlet distribution-based per-sample Weight Sampling (DWS) framework, and compare reweighting and resampling under DWS framework. With the analyses from DWS, we propose RENT, a REsampling method with Noise Transition matrix. Empirically, RENT consistently outperforms existing transition matrix utilization methods, which includes reweighting, on various benchmark datasets. Our code is available at https://github.com/BaeHeeSun/RENT.",,7,0.0,,,NRF,2021R1A2C200981613,National Research Foundation of Korea
2-s2.0-85192437573,,,,DISTRIBUTED DIFFERENTIAL PRIVACY IN MULTI-ARMED BANDITS,cp,Conference Paper,Chowdhury S.R.,60021726;60157515,Microsoft Research;James and Patricia Anderson College of Engineering,Redmond;Detroit,United States;United States,2.0,"Chowdhury, Sayak Ray;Zhou, Xingyu",57195955397;57203513849,60021726;60157515,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"We consider the standard K-armed bandit problem under a distributed trust model of differential privacy (DP), which enables to guarantee privacy without a trustworthy server. Under this trust model, previous work on private bandits largely focus on achieving privacy using a shuffle protocol, where a batch of users data are randomly permuted before sending to a central server. This protocol achieves (ε, δ) or approximate-DP guarantee by sacrificing an additive O (equation presented) ε factor in T-step cumulative regret. In contrast, the optimal privacy cost to achieve a stronger (ε, 0) or pure-DP guarantee under the widely used central trust model is only (equation presented), where, however, a trusted server is required. In this work, we aim to obtain a pure-DP guarantee under distributed trust model while sacrificing no more regret than that under the central trust model. We achieve this by designing a generic bandit algorithm based on successive arm elimination, where privacy is guaranteed by corrupting rewards with an equivalent discrete Laplace noise ensured by a secure computation protocol. We also show that our algorithm, when instantiated with Skellam noise and the secure protocol, ensures Rényi differential privacy - a stronger notion than approximate DP - under distributed trust model with a privacy cost of (equation presented). Our theoretical findings are corroborated by numerical evaluations on both synthetic and real-world data.",,2,0.0,,,,CNS-2153220,
2-s2.0-85150379116,,,,DISTRIBUTIONALLY ROBUST MODELS WITH PARAMETRIC LIKELIHOOD RATIOS,cp,Conference Paper,Michel P.,60141508;60000179;60136640,Stanford Engineering;École Normale Supérieure;School of Computer Science,Stanford;Paris;Pittsburgh,United States;France;United States,3.0,"Michel, Paul;Hashimoto, Tatsunori;Neubig, Graham",57204047891;57202060550;36141167700,60000179;60141508;60136640,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"As machine learning models are deployed ever more broadly, it becomes increasingly important that they are not only able to perform well on their training distribution, but also yield accurate predictions when confronted with distribution shift. The Distributionally Robust Optimization (DRO) framework proposes to address this issue by training models to minimize their expected risk under a collection of distributions, to imitate test-time shifts. This is most commonly achieved by instance-level re-weighting of the training objective to emulate the likelihood ratio with possible test distributions, which allows for estimating their empirical risk via importance sampling (assuming that they are subpopulations of the training distribution). However, re-weighting schemes in the literature are usually limited due to the difficulty of keeping the optimization problem tractable and the complexity of enforcing normalization constraints. In this paper, we show that three simple ideas - mini-batch level normalization, a KL penalty and simultaneous gradient updates - allow us to train models with DRO using a broader class of parametric likelihood ratios. In a series of experiments on both image and text classification benchmarks, we find that models trained with the resulting parametric adversaries are consistently more robust to subpopulation shifts when compared to other DRO approaches, and that the method performs reliably well with little hyper-parameter tuning.",,14,0.0,,,,,
2-s2.0-85150375964,,,,DIVA: DATASET DERIVATIVE OF A LEARNING TASK,cp,Conference Paper,Dukler Y.,60027550;60076757,"University of California, Los Angeles;Amazon.com, Inc.",Los Angeles;Seattle,United States;United States,6.0,"Dukler, Yonatan;Achille, Alessandro;Paolini, Giovanni;Ravichandran, Avinash;Polito, Marzia;Soatto, Stefano",57214896876;57200414453;57193899601;8732051900;57221919430;7004080670,60076757-60027550;60076757;60076757;60076757;60076757;60076757,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"We present a method to compute the derivative of a learning task with respect to a dataset. A learning task is a function from a training set to the validation error, which can be represented by a trained deep neural network (DNN). The “dataset derivative” is a linear operator, computed around the trained model, that informs how perturbations of the weight of each training sample affect the validation error, usually computed on a separate validation dataset. Our method, DIVA (Differentiable Validation) hinges on a closed-form differentiable expression of the leave-one-out cross-validation error around a pre-trained DNN. Such expression constitutes the dataset derivative. DIVA could be used for dataset auto-curation, for example removing samples with faulty annotations, augmenting a dataset with additional relevant samples, or rebalancing. More generally, DIVA can be used to optimize the dataset, along with the parameters of the model, as part of the training process without the need for a separate validation dataset, unlike bi-level optimization methods customary in AutoML. To illustrate the flexibility of DIVA, we report experiments on sample auto-curation tasks such as outlier rejection, dataset extension, and automatic aggregation of multi-modal data.",,3,0.0,,,,,
2-s2.0-85148269604,,,,DP-PCA: Statistically Optimal and Differentially Private PCA,cp,Conference Paper,Liu X.,60028661;60006191,UW College of Engineering;Google LLC,Seattle;Mountain View,United States;United States,4.0,"Liu, Xiyang;Kong, Weihao;Jain, Prateek;Oh, Sewoong",57218718279;57197795547;58179917100;55625388200,60028661;60006191;60006191;60028661-60006191,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"We study the canonical statistical task of computing the principal component from n i.i.d. data in d dimensions under (ε, δ)-differential privacy. Although extensively studied in literature, existing solutions fall short on two key aspects: (i) even for Gaussian data, existing private algorithms require the number of samples n to scale super-linearly with d, i.e., n = Ω(d<sup>3</sup>/<sup>2</sup>), to obtain non-trivial results while non-private PCA requires only n = O(d), and (ii) existing techniques suffer from a non-vanishing error even when the randomness in each data point is arbitrarily small. We propose DP-PCA, which is a single-pass algorithm that overcomes both limitations. It is based on a private minibatch gradient ascent method that relies on private mean estimation to add minimal noise required to ensure privacy by adapting to the geometry of a given minibatch of gradients. For sub-Gaussian data, we provide nearly optimal statistical error rates even for n = Õ(d). Furthermore, we provide a lower bound showing that sub-Gaussian style assumption is necessary in obtaining the optimal error rate.",,15,0.0,,,NSF,CNS-2112471,National Science Foundation
2-s2.0-85168543274,,,,DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION,cp,Conference Paper,Poole B.,60025038;60006191,"University of California, Berkeley;Google LLC",Berkeley;Mountain View,United States;United States,4.0,"Poole, Ben;Jain, Ajay;Barron, Jonathan T.;Mildenhall, Ben",56462379000;57219589246;50561062400;56964454100,60006191;60025038;60006191;60006191,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors. See dreamfusionpaper.github.io for a more immersive view into our 3D results.",,484,0.0,,,,,
2-s2.0-85189562566,10.1609/aaai.v38i6.28394,,,DTMFormer: Dynamic Token Merging for Boosting Transformer-Based Medical Image Segmentation,cp,Conference Paper,Wang Z.,60025761;60008592,Huazhong University of Science and Technology;Hong Kong University of Science and Technology,Wuhan;Hong Kong,China;Hong Kong,6.0,"Wang, Zhehao;Lin, Xian;Wu, Nannan;Yu, Li;Cheng, Kwang Ting;Yan, Zengqiang",58965948000;57788204100;57788541100;57208196864;7402997957;57199176182,60025761;60025761;60025761;60025761;60008592;60025761,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,6.0,,5814-5822,"Despite the great potential in capturing long-range dependency, one rarely-explored underlying issue of transformer in medical image segmentation is attention collapse, making it often degenerate into a bypass module in CNN-Transformer hybrid architectures. This is due to the high computational complexity of vision transformers requiring extensive training data while well-annotated medical image data is relatively limited, resulting in poor convergence. In this paper, we propose a plug-n-play transformer block with dynamic token merging, named DTMFormer, to avoid building long-range dependency on redundant and duplicated tokens and thus pursue better convergence. Specifically, DTMFormer consists of an attention-guided token merging (ATM) module to adaptively cluster tokens into fewer semantic tokens based on feature and dependency similarity and a light token reconstruction module to fuse ordinary and semantic tokens. In this way, as self-attention in ATM is calculated based on fewer tokens, DTMFormer is of lower complexity and more friendly to converge. Extensive experiments on publicly-available datasets demonstrate the effectiveness of DTMFormer working as a plug-n-play module for simultaneous complexity reduction and performance improvement. We believe it will inspire future work on rethinking transformers in medical image segmentation. Code: https://github.com/iam-nacl/DTMFormer.",,9,1.0,all publisherfullgold,All Open Access Gold,NSFC,62271220,National Natural Science Foundation of China
2-s2.0-105003887774,,,,DYNAMIC UPDATE-TO-DATA RATIO: MINIMIZING WORLD MODEL OVERFITTING,cp,Conference Paper,Dorka N.,60025641;60292337,Universität Freiburg;University of Technology Nuremberg,Freiburg im Breisgau;Nurnberg,Germany;Germany,3.0,"Dorka, Nicolai;Welschehold, Tim;Burgard, Wolfram",57219756194;56974419500;7003610380,60025641;60025641;60292337,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari 100k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search which is not feasible for many applications. Our method eliminates the need to set the UTD hyperparameter by hand and even leads to a higher robustness with regard to other learning-related hyperparameters further reducing the amount of necessary tuning.",,2,0.0,,,,871449-OpenDR,
2-s2.0-85197087064,,,,DYNAMICS-INFORMED PROTEIN DESIGN WITH STRUCTURE CONDITIONING,cp,Conference Paper,Komorowska U.J.,60119999,Department of Computer Science and Technology,Cambridge,United Kingdom,6.0,"Komorowska, Urszula Julia;Mathis, Simon V.;Didi, Kieran;Vargas, Francisco;Lio, Pietro;Jamnik, Mateja",58058403400;57222303394;57572453300;57225472257;7004223170;8403010100,60119999;60119999;60119999;60119999;60119999;60119999,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Current protein generative models are able to design novel backbones with desired shapes or functional motifs. However, despite the importance of a protein's dynamical properties for its function, conditioning on these dynamics remains elusive. We present a new approach to include dynamical properties in protein generative modeling by leveraging Normal Mode Analysis. We introduce a method for conditioning diffusion probabilistic models on protein dynamics, specifically on the lowest non-trivial normal mode of oscillation. Our method, similar to classifier guidance conditioning, formulates the sampling process as being driven by conditional and unconditional terms. However, unlike previous works, we approximate the conditional term with a simple analytical function rather than an external neural network, thus making the eigenvector calculations approachable. We present the corresponding SDE theory as a formal justification of our approach. We extend our framework to conditioning on structure and dynamics at the same time, enabling scaffolding of dynamical motifs. We demonstrate the empirical effectiveness of our method by turning the open-source unconditional protein diffusion model Genie into a normal-mode-dynamics-conditional model with no retraining. Generated proteins exhibit the desired dynamical and structural properties while still being biologically plausible. Our work represents a first step towards incorporating dynamical behaviour in protein design and may open the door to designing more flexible and functional proteins in the future.",,4,0.0,,,,,
2-s2.0-85148452866,10.1613/JAIR.1.13683,,,Data-Driven Revision of Conditional Norms in Multi-Agent Systems,ar,Article,Dell'Anna D.,60007989;60006288;60015875,Universiteit Utrecht;Delft University of Technology;University of Aberdeen,Utrecht;Delft;Aberdeen,Netherlands;Netherlands;United Kingdom,5.0,"Dell'Anna, Davide;Alechina, Natasha;Dalpiaz, Fabiano;Dastani, Mehdi;Logan, Brian",57201642596;6603450996;24400635100;6603932250;56092567900,60006288;60007989;60007989;60007989;60007989-60015875,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,1549-1593,"In multi-agent systems, norm enforcement is a mechanism for steering the behavior of individual agents in order to achieve desired system-level objectives. Due to the dynamics of multi-agent systems, however, it is hard to design norms that guarantee the achievement of the objectives in every operating context. Also, these objectives may change over time, thereby making previously defined norms ineffective. In this paper, we investigate the use of system execution data to automatically synthesise and revise conditional prohibitions with deadlines, a type of norms aimed at prohibiting agents from exhibiting certain patterns of behaviors. We propose DDNR (Data-Driven Norm Revision), a data-driven approach to norm revision that synthesises revised norms with respect to a data set of traces describing the behavior of the agents in the system. We evaluate DDNR using a state-of-the-art, offthe- shelf urban traffic simulator. The results show that DDNR synthesises revised norms that are significantly more accurate than the original norms in distinguishing adequate and inadequate behaviors for the achievement of the system-level objectives.",,3,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85137918815,10.24963/ijcai.2022/554,,,Data-Efficient Backdoor Attacks,cp,Conference Paper,Xia P.,60019118,University of Science and Technology of China,Hefei,China,4.0,"Xia, Pengfei;Li, Ziqiang;Zhang, Wei;Li, Bin",57205762330;56857136700;57775454600;57129082300,60019118;60019118;60019118;60019118,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3992-3998,"Recent studies have proven that deep neural networks are vulnerable to backdoor attacks. Specifically, by mixing a small number of poisoned samples into the training set, the behavior of the trained model can be maliciously controlled. Existing attack methods construct such adversaries by randomly selecting some clean data from the benign set and then embedding a trigger into them. However, this selection strategy ignores the fact that each poisoned sample contributes inequally to the backdoor injection, which reduces the efficiency of poisoning. In this paper, we formulate improving the poisoned data efficiency by the selection as an optimization problem and propose a Filtering-and-Updating Strategy (FUS) to solve it. The experimental results on CIFAR-10 and ImageNet-10 indicate that the proposed method is effective: the same attack success rate can be achieved with only 47% to 75% of the poisoned sample volume compared to the random selection strategy. More importantly, the adversaries selected according to one setting can generalize well to other settings, exhibiting strong transferability. The prototype code of our method is now available at https://github.com/xpf/Data-Efficient-Backdoor-Attacks.",,20,1.0,all publisherfree2read,All Open Access Bronze,NSFC,61836011,National Natural Science Foundation of China
2-s2.0-85189562349,10.1609/aaai.v38i7.28510,,,Data-Free Hard-Label Robustness Stealing Attack,cp,Conference Paper,Yuan X.,60019118;60005510,University of Science and Technology of China;Nanyang Technological University,Hefei;Singapore City,China;Singapore,6.0,"Yuan, Xiaojian;Chen, Kejiang;Huang, Wen;Zhang, Jie;Zhang, Weiming;Yu, Nenghai",58115827900;57193571285;58744688100;55986519600;34769199700;57201634030,60019118;60019118;60019118;60005510;60019118;60019118,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,7.0,,6853-6861,"The popularity of Machine Learning as a Service (MLaaS) has led to increased concerns about Model Stealing Attacks (MSA), which aim to craft a clone model by querying MLaaS. Currently, most research on MSA assumes that MLaaS can provide soft labels and that the attacker has a proxy dataset with a similar distribution. However, this fails to encapsulate the more practical scenario where only hard labels are returned by MLaaS and the data distribution remains elusive. Furthermore, most existing work focuses solely on stealing the model accuracy, neglecting the model robustness, while robustness is essential in security-sensitive scenarios, e.g., face-scan payment. Notably, improving model robustness often necessitates the use of expensive techniques such as adversarial training, thereby further making stealing robustness a more lucrative prospect. In response to these identified gaps, we introduce a novel Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which enables the stealing of both model accuracy and robustness by simply querying hard labels of the target model without the help of any natural data. Comprehensive experiments demonstrate the effectiveness of our method. The clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51% against AutoAttack, which are only 4.71% and 8.40% lower than the target model on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is available at: https://github.com/LetheSec/DFHL-RS-Attack.",,5,1.0,all publisherfullgold,All Open Access Gold,NSFC,62072421,National Natural Science Foundation of China
2-s2.0-85203786857,,,,Debating with More Persuasive LLMs Leads to More Truthful Answers,cp,Conference Paper,Khan A.,60022148;126611673;129304118;125240226;131684186,University College London;Anthropic;FAR AI;Speechmatics;MATS,London;;San Diego;Cambridge;,United Kingdom;United States;United States;United Kingdom;,10.0,"Khan, Akbir;Hughes, John;Valentine, Dan;Ruis, Laura;Sachan, Kshitij;Radhakrishnan, Ansh;Grefenstette, Edward;Bowman, Samuel R.;Rocktäschel, Tim;Perez, Ethan",58150834400;59111656900;58632060700;57219630873;57938246700;58508828800;51664755300;57155948600;55899274800;57204288235,60022148;125240226-131684186;131684186;60022148;126611673;126611673;60022148;126611673;60022148;126611673-129304118,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,23662-23733,"Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information but are otherwise as capable. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. On the QuALITY comprehension task, we find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.",,8,0.0,,,UCL,EP/S021566/1,University College London
2-s2.0-85143077207,,,,Decentralized Gossip-Based Stochastic Bilevel Optimization over Communication Networks,cp,Conference Paper,Yang S.,60003269;60008592,Princeton University;Hong Kong University of Science and Technology,Princeton;Hong Kong,United States;Hong Kong,3.0,"Yang, Shuoguang;Zhang, Xuezhou;Wang, Mengdi",57191073797;57207319373;57215322066,60008592;60003269;60003269,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Bilevel optimization have gained growing interests, with numerous applications being found in meta learning, minimax games, reinforcement learning, and nested composition optimization. This paper studies the problem of decentralized distributed stochastic bilevel optimization over a network where each agent can only communicate with its neighbors, and gives examples from multi-task, multi-agent learning and federated learning. In this paper, we propose a gossip-based decentralized bilevel learning algorithm that allows networked agents to solve both the inner and outer optimization problems in a single timescale and share information through network propagation. We show that our algorithm enjoys the Õ(1/Kε2) per-agent sample complexity for general nonconvex bilevel optimization and Õ(1/Kε) for Polyak-Łojasiewicz objectives, achieving a speedup that scales linearly with the network size K. The sample complexities are optimal in both ε and K. We test our algorithm on the examples of hyperparameter tuning and decentralized reinforcement learning. Simulated experiments confirmed that our algorithm achieves the state-of-the-art training efficiency and test accuracy.",,33,0.0,,,NSF,CMMI-1653435,National Science Foundation
2-s2.0-85153676751,10.1613/JAIR.1.14061,,,Deciding FO-rewritability of Regular Languages and Ontology-mediated Queries in Linear Temporal Logic,ar,Article,Kurucz A.,60011520;60025225;60009016,"King's College London;University of Southampton;Birkbeck, University of London",London;Southampton;London,United Kingdom;United Kingdom;United Kingdom,4.0,"Kurucz, Agi;Ryzhikov, Vladislav;Savateev, Yury;Zakharyaschev, Michael",8732161000;23467849100;24339205900;6602982360,60011520;60009016;60025225;60009016,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,645-703,"Our concern is the problem of determining the data complexity of answering an ontology-mediated query (OMQ) formulated in linear temporal logic LTL over (Z, <) and deciding whether it is rewritable to an FO(<)-query, possibly with some extra predicates. First, we observe that, in line with the circuit complexity and FO-definability of regular languages, OMQ answering in AC<sup>0</sup>, ACC<sup>0</sup> and NC<sup>1</sup> coincides with FO(<, ≡)-rewritability using unary predicates x ≡ 0 (mod n), FO(<, MOD)-rewritability, and FO(RPR)-rewritability using relational primitive recursion, respectively. We prove that, similarly to known PSpace-completeness of recognising FO(<)-definability of regular languages, deciding FO(<, ≡)- and FO(<, MOD)-definability is also PSpace-complete (unless ACC<sup>0</sup> = NC<sup>1</sup>). We then use this result to show that deciding FO(<)-, FO(<, ≡)- and FO(<, MOD)-rewritability of LTL OMQs is ExpSpace-complete, and that these problems become PSpace-complete for OMQs with a linear Horn ontology and an atomic query, and also a positive query in the cases of FO(<)- and FO(<, ≡)-rewritability. Further, we consider FO(<)-rewritability of OMQs with a binary-clause ontology and identify OMQ classes, for which deciding it is PSpace-, Π<sup>p</sup><inf>2</inf>- and coNP-complete.",,3,1.0,all publisherfullgold,All Open Access Gold,EPSRC,EP/S032282,Engineering and Physical Sciences Research Council
2-s2.0-105018585511,,,,Decorrelated Variable Importance,ar,Article,Verdinelli I.,60027950,Carnegie Mellon University,Pittsburgh,United States,2.0,"Verdinelli, Isabella;Wasserman, Larry",6507381046;57191231290,60027950;60027950,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Because of the widespread use of black box prediction methods such as random forests and neural nets, there is renewed interest in developing methods for quantifying variable importance as part of the broader goal of interpretable prediction. A popular approach is to define a variable importance parameter - known as LOCO (Leave Out COvariates) - based on dropping covariates from a regression model. This is essentially a nonparametric version of R<sup>2</sup>. This parameter is very general and can be estimated nonparametrically, but it can be hard to interpret because it is affected by correlation between covariates. We propose a method for mitigating the effect of correlation by defining a modified version of LOCO. This new parameter is difficult to estimate nonparametrically, but we show how to estimate it using semiparametric models.",Correlation | Nonparametric Estimators | Prediction | Variable Importance,15,0.0,,,,,
2-s2.0-105018453948,,,,Deep Network Approximation: Beyond ReLU to Diverse Activation Functions,ar,Article,Zhang S.,60008724,Duke University,Durham,United States,3.0,"Zhang, Shijun;Lu, Jianfeng;Zhao, Hongkai",57210290570;55511129200;7404778855,60008724;60008724;60008724,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set A is defined to encompass the majority of commonly used activation functions, such as ReLU, LeakyReLU, ReLU<sup>2</sup>, ELU, CELU, SELU, Softplus, GELU, SiLU, Swish, Mish, Sigmoid, Tanh, Arctan, Softsign, dSiLU, and SRS. We demonstrate that for any activation function % ∈ A , a ReLU network of width N and depth L can be approximated to arbitrary precision by a %-activated network of width 3N and depth 2L on any bounded set. This finding enables the extension of most approximation results achieved with ReLU networks to a wide variety of other activation functions, albeit with slightly increased constants. Significantly, we establish that the (width, depth) scaling factors can be further reduced from (3, 2) to (1, 1) if % falls within a specific subset of A . This subset includes activation functions such as ELU, CELU, SELU, Softplus, GELU, SiLU, Swish, and Mish.",deep neural networks | diverse activation functions | expressive power | nonlinear approximation | rectified linear unit,33,0.0,,,NSF,CCF-1910571,National Science Foundation
2-s2.0-85141903606,10.1609/aaai.v36i2.20085,,,Deep Neural Networks Learn Meta-Structures from Noisy Labels in Semantic Segmentation,cp,Conference Paper,Luo Y.,60027363;60018486,University of Chinese Academy of Sciences;Institute of Automation Chinese Academy of Sciences,Beijing;Beijing,China;China,4.0,"Luo, Yaoru;Liu, Guole;Guo, Yuanhao;Yang, Ge",57222903972;57190286318;57188851234;56903300600,60018486-60027363;60018486-60027363;60018486-60027363;60018486,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,1908-1916,"How deep neural networks (DNNs) learn from noisy labels has been studied extensively in image classification but much less in image segmentation. So far, our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited. In this study, we address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images. We generate extremely noisy labels by randomly sampling a small fraction (e.g., 10%) or flipping a large fraction (e.g., 90%) of the ground truth labels. When trained with these noisy labels, DNNs provide largely the same segmentation performance as trained by the original ground truth. This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation. We refer to these hidden structures in labels as meta-structures. When DNNs are trained by labels with different perturbations to the meta-structure, we find consistent degradation in their segmentation performance. In contrast, incorporation of meta-structure information substantially improves performance of an unsupervised segmentation model developed for binary semantic segmentation. We define meta-structures mathematically as spatial density distributions and show both theoretically and experimentally how this formulation explains key observed learning behavior of DNNs.",,9,1.0,all publisherfullgold,All Open Access Gold,NSFC,31971289,National Natural Science Foundation of China
2-s2.0-105018578125,,,,Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces,ar,Article,Liu H.,60019647;60136858;60141284;60014347;60151568,Georgia Institute of Technology;College of Engineering;School of Engineering and Applied Science;Hong Kong Baptist University;Department of Computer Science,Atlanta;Atlanta;Princeton;Hong Kong;College Park,United States;United States;United States;Hong Kong;United States,5.0,"Liu, Hao;Yang, Haizhao;Chen, Minshuo;Zhao, Tuo;Liao, Wenjing",57195532958;55174878900;57208438306;55178265500;54389478900,60014347;60151568;60141284;60136858;60019647,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Learning operators between infinitely dimensional spaces is an important learning task arising in machine learning, imaging science, mathematical modeling and simulations, etc. This paper studies the nonparametric estimation of Lipschitz operators using deep neural networks. Non-asymptotic upper bounds are derived for the generalization error of the empirical risk minimizer over a properly chosen network class. Under the assumption that the target operator exhibits a low dimensional structure, our error bounds decay as the training sample size increases, with an attractive fast rate depending on the intrinsic dimension in our estimation. Our assumptions cover most scenarios in real applications and our results give rise to fast rates by exploiting low dimensional structures of data in operator estimation. We also investigate the influence of network structures (e.g., network width, depth, and sparsity) on the generalization error of the neural network estimator and propose a general suggestion on the choice of network structures to maximize the learning efficiency quantitatively.",Deep neural networks | Generalization error analysis | Nonparametric estimation | Operator learning,14,0.0,,,NNSF,12201530,National Natural Science Foundation of China
2-s2.0-85174424038,,,,Deep Perturbation Learning: Enhancing the Network Performance via Image Perturbations,cp,Conference Paper,Song Z.,60073652;60033100;60025578;130353952,Tongji University;Nanjing University;Xidian University;Oosto,Shanghai;Nanjing;Xi'an;Belfast,China;China;China;United Kingdom,4.0,"Song, Zifan;Gong, Xiao;Hu, Guosheng;Zhao, Cairong",58534498800;57222091491;55925786500;7403564629,60073652-60025578;60033100;130353952;60073652-60025578,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,32273-32287,"Image perturbation technique is widely used to generate adversarial examples to attack networks, greatly decreasing the performance of networks. Unlike the existing works, in this paper, we introduce a novel framework Deep Perturbation Learning (DPL), the new insights into understanding image perturbations, to enhance the performance of networks rather than decrease the performance. Specifically, we learn image perturbations to amend the data distribution of training set to improve the performance of networks. This optimization w.r.t data distribution is non-trivial. To approach this, we tactfully construct a differentiable optimization target w.r.t. image perturbations via minimizing the empirical risk. Then we propose an alternating optimization of the network weights and perturbations. DPL can easily be adapted to a wide spectrum of downstream tasks and backbone networks. Extensive experiments demonstrate the effectiveness of our DPL on 6 datasets (CIFAR-10, CIFAR-100, ImageNet, MS-COCO, PASCAL VOC, and SBD) over 3 popular vision tasks (image classification, object detection, and semantic segmentation) with different backbone architectures (e.g., ResNet, MobileNet, and ViT).",,5,0.0,,,NSFC,20511100700,Natural Science Foundation of Shanghai Municipality
2-s2.0-85162086234,,,,Defending Against Adversarial Attacks via Neural Dynamic System,cp,Conference Paper,Li X.,60029306,Wuhan University,Wuhan,China,3.0,"Li, Xiyuan;Zou, Xin;Liu, Weiwei",59452001700;58119680600;56437794000,60029306;60029306;60029306,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Although deep neural networks (DNN) have achieved great success, their applications in safety-critical areas are hindered due to their vulnerability to adversarial attacks. Some recent works have accordingly proposed to enhance the robustness of DNN from a dynamic system perspective. Following this line of inquiry, and inspired by the asymptotic stability of the general nonautonomous dynamical system, we propose to make each clean instance be the asymptotically stable equilibrium points of a slowly time-varying system in order to defend against adversarial attacks. We present a theoretical guarantee that if a clean instance is an asymptotically stable equilibrium point and the adversarial instance is in the neighborhood of this point, the asymptotic stability will reduce the adversarial noise to bring the adversarial instance close to the clean instance. Motivated by our theoretical results, we go on to propose a nonautonomous neural ordinary differential equation (ASODE) and place constraints on its corresponding linear time-variant system to make all clean instances act as its asymptotically stable equilibrium points. Our analysis suggests that the constraints can be converted to regularizers in implementation. The experimental results show that ASODE improves robustness against adversarial attacks and outperforms the state-of-the-art methods.",,21,0.0,,,NSFC,61976161,National Natural Science Foundation of China
2-s2.0-85204300620,,,,Deploying Mobility-On-Demand for All by Optimizing Paratransit Services,cp,Conference Paper,Pavia S.,60001439;60007776;60003915;114664959,Pennsylvania State University;Cornell University;Vanderbilt University;Chattanooga Area Regional Transportation Authority,University Park;Ithaca;Nashville;Chattanooga,United States;United States;United States;United States,11.0,"Pavia, Sophie;Rogers, David;Sivagnanam, Amutheezan;Wilbur, Michael;Edirimanna, Danushka;Kim, Youngseo;Pugliese, Philip;Samaranayake, Samitha;Laszka, Aron;Mukhopadhyay, Ayan;Dubey, Abhishek",57468519800;59333097100;57219692506;57210552279;58146590900;57222579649;57219692936;6602435439;57294198700;57191923091;57198233972,60003915;60003915;60001439;60003915;60007776;60007776;114664959;60007776;60001439;60003915;60003915,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,7430-7437,"While on-demand ride-sharing services have become popular in recent years, traditional on-demand transit services cannot be used by everyone, e.g., people who use wheelchairs. Paratransit services, operated by public transit agencies, are a critical infrastructure that offers door-to-door transportation assistance for individuals who face challenges in using standard transit routes. However, with declining ridership and mounting financial pressure, public transit agencies in the USA struggle to operate existing services. We collaborate with a public transit agency from the southern USA, highlight the specific nuances of paratransit optimization, and present a vehicle routing problem formulation for optimizing paratransit. We validate our approach using real-world data from the transit agency, present results from an actual pilot deployment of the proposed approach in the city, and show how the proposed approach comprehensively outperforms existing approaches used by the transit agency. To the best of our knowledge, this work presents one of the first examples of using open-source algorithmic approaches for paratransit optimization.",,1,0.0,,,NSF,CNS-1952011,National Science Foundation
2-s2.0-85189785809,,,,DesCo: Learning Object Recognition with Rich Language Descriptions,cp,Conference Paper,Li L.H.,60027550,"University of California, Los Angeles",Los Angeles,United States,4.0,"Li, Liunian Harold;Dou, Zi Yi;Peng, Nanyun;Chang, Kai Wei",57219620836;57550515100;57204466260;24502911300,60027550;60027550;60027550;60027550,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Recent development in vision-language approaches has instigated a paradigm shift in learning visual recognition models from language supervision. These approaches align objects with language queries (e.g. “a photo of a cat”) and thus improve the models' adaptability to novel objects and domains. Recent studies have attempted to query these models with complex language expressions that include specifications of fine-grained details, such as colors, shapes, and relations. However, simply incorporating language descriptions into queries does not guarantee accurate interpretation by the models. In fact, our experiments show that GLIP, a state-of-the-art vision-language model for object detection, often disregards contextual information in the language descriptions and instead relies heavily on detecting objects solely by their names. To tackle the challenge, we propose a new description-conditioned (DesCo) paradigm of learning object recognition models with rich language descriptions consisting of two innovations: 1) we employ a large language model as a commonsense knowledge engine to generate rich language descriptions of objects; 2) we design context-sensitive queries to improve the model's ability in deciphering intricate nuances embedded within descriptions and enforce the model to focus on context rather than object names alone. On two novel object detection benchmarks, LVIS and OminiLabel, under the zero-shot detection setting, our approach achieves 34.8 APr minival (+9.1) and 29.3 AP (+3.6), respectively, surpassing the prior state-of-the-art models, GLIP and FIBER, by a large margin.",,10,0.0,,,UCLA,N660011924032,"University of California, Los Angeles"
2-s2.0-85204281503,,,,Design a Win-Win Strategy That Is Fair to Both Service Providers and Tasks When Rejection Is Not an Option,cp,Conference Paper,Trabelsi Y.,60002765;60022904,Bar-Ilan University;New Jersey Institute of Technology,Ramat Gan;Newark,Israel;United States,3.0,"Trabelsi, Yohai;Xu, Pan;Kraus, Sarit",55366306600;40762795300;7103086512,60002765;60022904;60002765,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,257-264,"Assigning tasks to service providers is a frequent procedure across various applications. Often the tasks arrive dynamically while the service providers remain static. Preventing task rejection caused by service provider overload is of utmost significance. To ensure a positive experience in relevant applications for both service providers and tasks, fairness must be considered. To address the issue, we model the problem as an online matching within a bipartite graph and tackle two minimax problems: one focuses on minimizing the highest waiting time of a task, while the other aims to minimize the highest workload of a service provider. We show that the second problem can be expressed as a linear program and thus solved efficiently while maintaining a reasonable approximation to the objective of the first problem. We developed novel methods that utilize the two minimax problems. We conducted extensive simulation experiments using real data and demonstrated that our novel heuristics, based on the linear program, performed remarkably well.",,0,0.0,,,ISF,1958/20,Israel Science Foundation
2-s2.0-85156138615,,,,Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them,cp,Conference Paper,Tramèr F.,60006191,Google LLC,Mountain View,United States,1.0,"Tramèr, Florian",56878876400,60006191,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,21692-21702,"Making classifiers robust to adversarial examples is challenging. Thus, many works tackle the seemingly easier task of detecting perturbed inputs. We show a barrier towards this goal. We prove a hardness reduction between detection and classification of adversarial examples: given a robust detector for attacks at distance ∊ (in some metric), we show how to build a similarly robust (but computationally inefficient) classifier for attacks at distance ∊/2. Our reduction is computationally inefficient, but preserves the sample complexity of the original detector. The reduction thus cannot be directly used to build practical classifiers. Instead, it is a useful sanity check to test whether empirical detection results imply something much stronger than the authors presumably anticipated (namely a highly robust and data-efficient classifier). To illustrate, we revisit 14 empirical detector defenses published over the past years. For 12/14 defenses, we show that the claimed detection results imply an inefficient classifier with robustness far beyond the state-of-the-art.",,32,0.0,,,,,
2-s2.0-85183857200,10.1613/jair.1.15762,,,Detecting Change Intervals with Isolation Distributional Kernel,ar,Article,Cao Y.,60028333;60033100;60018805,UNSW Sydney;Nanjing University;Deakin University,Sydney;Nanjing;Geelong,Australia;China;Australia,7.0,"Cao, Yang;Zhu, Ye;Ting, Kai Ming;Salim, Flora D.;Li, Hong Xian;Yang, Luxing;Li, Gang",57966649400;56582852200;7101844670;7801420000;36701739500;52063906100;56336374700,60018805;60018805;60033100;60028333;60018805;60018805;60018805,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,273-306,"Detecting abrupt changes in data distribution is one of the most significant tasks in streaming data analysis. Although many unsupervised Change-Point Detection (CPD) methods have been proposed recently to identify those changes, they still suffer from missing subtle changes, poor scalability, or/and sensitivity to outliers. To meet these challenges, we are the first to generalise the CPD problem as a special case of the Change-Interval Detection (CID) problem. Then we propose a CID method, named iCID, based on a recent Isolation Distributional Kernel (IDK). iCID identifies the change interval if there is a high dissimilarity score between two non-homogeneous temporal adjacent intervals. The data-dependent property and finite feature map of IDK enabled iCID to efficiently identify various types of change-points in data streams with the tolerance of outliers. Moreover, the proposed online and offline versions of iCID have the ability to optimise key parameter settings. The effectiveness and efficiency of iCID have been systematically verified on both synthetic and real-world datasets.",,4,1.0,all publisherfullgold,All Open Access Gold,NSFC,62076120,National Natural Science Foundation of China
2-s2.0-85163110029,,,,Detecting Corrupted Labels Without Training a Model to Predict,cp,Conference Paper,Zhu Z.,60137794,Baskin School of Engineering,Santa Cruz,United States,3.0,"Zhu, Zhaowei;Dong, Zihao;Liu, Yang",57200224004;57311060400;57206665385,60137794;60137794;60137794,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,27412-27427,"Label noise in real-world datasets encodes wrong correlation patterns and impairs the generalization of deep neural networks (DNNs). It is critical to find efficient ways to detect corrupted patterns. Current methods primarily focus on designing robust training techniques to prevent DNNs from memorizing corrupted patterns. These approaches often require customized training processes and may overfit corrupted patterns, leading to a performance drop in detection. In this paper, from a more data-centric perspective, we propose a training-free solution to detect corrupted labels. Intuitively, “closer” instances are more likely to share the same clean label. Based on the neighborhood information, we propose two methods: the first one uses “local voting” via checking the noisy label consensuses of nearby features. The second one is a ranking-based approach that scores each instance and filters out a guaranteed number of instances that are likely to be corrupted. We theoretically analyze how the quality of features affects the local voting and provide guidelines for tuning neighborhood size. We also prove the worst-case error bound for the ranking-based method. Experiments with both synthetic and real-world label noise demonstrate our training-free solutions consistently and significantly improve most of the training-based baselines. Code is available at github.com/UCSC-REAL/SimiFeat.",,51,0.0,,,NSF,IIS-2007951,National Science Foundation
2-s2.0-85174393888,,,,Detecting Out-of-distribution Data through In-distribution Class Prior,cp,Conference Paper,Jiang X.,60026553;60025709;60023932;60105683;60032955;60014347;60195969,University of Melbourne;The University of Sydney;University of Technology Sydney;Southern University of Science and Technology;Huazhong Agricultural University;Hong Kong Baptist University;Mohamed Bin Zayed University of Artificial Intelligence,Melbourne;Sydney;Sydney;Shenzhen;Wuhan;Hong Kong;Abu Dhabi,Australia;Australia;Australia;China;China;Hong Kong;United Arab Emirates,7.0,"Jiang, Xue;Liu, Feng;Fang, Zhen;Chen, Hong;Liu, Tongliang;Zheng, Feng;Han, Bo",58088227100;57118471000;57211269319;56658950800;56297951800;57210574274;57191281044,60105683-60014347;60026553;60023932;60032955;60195969-60025709;60105683;60014347,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,15067-15088,"Given a pre-trained in-distribution (ID) model, the inference-time out-of-distribution (OOD) detection aims to recognize OOD data during the inference stage. However, some representative methods share an unproven assumption that the probability that OOD data belong to every ID class should be the same, i.e., these OOD-to-ID probabilities actually form a uniform distribution. In this paper, we show that this assumption makes the above methods incapable when the ID model is trained with class-imbalanced data. Fortunately, by analyzing the causal relations between ID/OOD classes and features, we identify several common scenarios where the OOD-to-ID probabilities should be the ID-class-prior distribution and propose two strategies to modify existing inference-time detection methods: 1) replace the uniform distribution with the ID-class-prior distribution if they explicitly use the uniform distribution; 2) otherwise, reweight their scores according to the similarity between the ID-class-prior distribution and the softmax outputs of the pre-trained model. Extensive experiments show that both strategies can improve the OOD detection performance when the ID model is pre-trained with imbalanced data, reflecting the importance of ID-class prior in OOD detection. The codes are available at https://github.com/tmlr-group/class_prior.",,19,0.0,,,NSFC,12071166,National Natural Science Foundation of China
2-s2.0-85185288272,,,,DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models,cp,Conference Paper,Xing X.,60013789;60006541,Beihang University;The University of Hong Kong,Beijing;Hong Kong,China;Hong Kong,6.0,"Xing, Ximing;Wang, Chuang;Zhou, Haitao;Zhang, Jing;Yu, Qian;Xu, Dong",58489398700;58120016700;58489398800;57221657752;57190392236;35231499000,60013789;60013789;60013789;60013789;60013789;60006541,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates vectorized free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of Bézier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than prior work. The code and demo of DiffSketcher can be found at https://ximinng.github.io/DiffSketcher-project/.",,43,0.0,,,研究資助局,62132001,"Research Grants Council, University Grants Committee"
2-s2.0-85203809259,,,,DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching,cp,Conference Paper,Li G.,60025084;60007711,Shanghai Jiao Tong University;Jilin University,Shanghai;Changchun,China;China,5.0,"Li, Guanghe;Shan, Yixiang;Zhu, Zhengbang;Long, Ting;Zhang, Weinan",58897673700;57933809100;57539861100;57226487356;56108513500,60007711;60007711;60025084;60007711;60025084,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,28597-28609,"In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and trajectory optimization methods (DT). Our code is publicly available at https://github.com/guangheli12/DiffStitch.",,6,0.0,,,NSFC,62322603,National Natural Science Foundation of China
2-s2.0-85170369186,10.24963/ijcai.2023/293,,,Differentiable Economics for Randomized Affine Maximizer Auctions,cp,Conference Paper,Curry M.,60020304;60012614;60027950,"University of Maryland, College Park;Universität Zürich;Carnegie Mellon University",College Park;Zurich;Pittsburgh,United States;Switzerland;United States,3.0,"Curry, Michael;Sandholm, Tuomas;Dickerson, John",57212527170;57203083791;55363539800,60012614;60027950;60020304,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,2633-2641,"A recent approach to automated mechanism design, differentiable economics, represents auctions by rich function approximators and optimizes their performance by gradient descent. The ideal auction architecture for differentiable economics would be perfectly strategyproof, support multiple bidders and items, and be rich enough to represent the optimal (i.e. revenue-maximizing) mechanism. So far, such an architecture does not exist. There are single-bidder approaches (MenuNet, RochetNet) which are always strategyproof and can represent optimal mechanisms. RegretNet is multi-bidder and can approximate any mechanism, but is only approximately strategyproof. We present an architecture that supports multiple bidders and is perfectly strategyproof, but cannot necessarily represent the optimal mechanism. This architecture is the classic affine maximizer auction (AMA), modified to offer lotteries. By using the gradient-based optimization tools of differentiable economics, we can now train lottery AMAs, competing with or outperforming prior approaches in revenue.",,15,1.0,all publisherfullgold,All Open Access Gold,ERC,4334192,European Research Council
2-s2.0-85159928477,10.1609/aaai.v37i7.26026,,,Differentiable Meta Multigraph Search with Partial Message Propagation on Heterogeneous Information Networks,cp,Conference Paper,Li C.,60025761,Huazhong University of Science and Technology,Wuhan,China,3.0,"Li, Chao;Xu, Hao;He, Kun",59635080400;58428871100;57204773777,60025761;60025761;60025761,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,8518-8526,"Heterogeneous information networks (HINs) are widely employed for describing real-world data with intricate entities and relationships. To automatically utilize their semantic information, graph neural architecture search has recently been developed on various tasks of HINs. Existing works, on the other hand, show weaknesses in instability and inflexibility. To address these issues, we propose a novel method called Partial Message Meta Multigraph search (PMMM) to automatically optimize the neural architecture design on HINs. Specifically, to learn how graph neural networks (GNNs) propagate messages along various types of edges, PMMM adopts an efficient differentiable framework to search for a meaningful meta multigraph, which can capture more flexible and complex semantic relations than a meta graph. The differentiable search typically suffers from performance instability, so we further propose a stable algorithm called partial message search to ensure that the searched meta multigraph consistently surpasses the manually designed meta-structures, i.e., meta-paths. Extensive experiments on six benchmark datasets over two representative tasks, including node classification and recommendation, demonstrate the effectiveness of the proposed method. Our approach outperforms the state-of-the-art heterogeneous GNNs, finds out meaningful meta multigraphs, and is significantly more stable. Our code is available at https://github.com/JHL-HUST/PMMM.",,12,1.0,all publisherfullgold,All Open Access Gold,NSFC,U22B2017,National Natural Science Foundation of China
2-s2.0-85213297705,10.1613/jair.1.15985,,,Differentially Private Neural Tangent Kernels (DP-NTK) for Privacy-Preserving Data Generation,ar,Article,Yang Y.,60010365;60025858;60011373;60278837;60193824,The University of British Columbia;ETH Zürich;Technical University of Denmark;Vector Institute;Alberta Machine Intelligence Institute,Vancouver;Zurich;Lyngby;Toronto;Edmonton,Canada;Switzerland;Denmark;Canada;Canada,5.0,"Yang, Yilin;Adamczewski, Kamil;Li, Xiaoxiao;Sutherland, Danica J.;Park, Mijung",58143349400;57194909647;57202387155;57222016238;53981759100,60010365;60025858;60010365-60278837;60010365-60193824;60010365-60193824-60011373,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,683-700,"Maximum mean discrepancy (MMD) is a particularly useful distance metric for differentially private data generation: when used with finite-dimensional features, it allows us to summarize and privatize the data distribution once, which we can repeatedly use during generator training without further privacy loss. An important question in this framework is, then, what features are useful to distinguish between real and synthetic data distributions, and whether those enable us to generate quality synthetic data. This work considers using the features of neural tangent kernels (NTKs), more precisely empirical NTKs (e-NTKs). We find that, perhaps surprisingly, the expressiveness of the untrained e-NTK features is comparable to that of the features taken from pre-trained perceptual features using public data. As a result, our method improves the privacy-accuracy trade-off compared to other state-of-the-art methods, without relying on any public data, as demonstrated on several tabular and image benchmark datasets.",,0,1.0,all publisherfullgold,All Open Access Gold,NSERC,,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85203844178,,,,Differentially Private Synthetic Data via Foundation Model APIs 2: Text,cp,Conference Paper,Xie C.,60000745;60021182;60029278;60021726,University of Illinois Urbana-Champaign;Sun Yat-Sen University;The University of Chicago;Microsoft Research,Urbana;Guangzhou;Chicago;Redmond,United States;China;United States;United States,12.0,"Xie, Chulin;Lin, Zinan;Backurs, Arturs;Gopi, Sivakanth;Yu, Da;Inan, Huseyin;Nori, Harsha;Jiang, Haotian;Zhang, Huishuai;Lee, Yin Tat;Li, Bo;Yekhanin, Sergey",57210367947;57200653813;55560610600;55480761500;57215968837;55370178200;57205550512;57205471520;56338559400;55785529000;57188689924;6506493185,60000745;60021726;60021726;60021726;60021182;60021726;60021726;60021726;60021726;60021726;60000745-60029278;60021726,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,54531-54560,"Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named AUG-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that AUG-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications. Our code and data are available at https://github.com/AI-secure/aug-pe.",,4,0.0,,,,,
2-s2.0-85205823317,10.1613/jair.1.16174,,,Digraph k-Coloring Games: New Algorithms and Experiments,ar,Article,D'Ascenzo A.,60018783;60020261;60004638;60016374;60136210,Università degli Studi dell'Aquila;Università della Calabria;University of G. d'Annunzio Chieti and Pescara;LUISS University;Gran Sasso Science Institute,L'Aquila;Rende;Chieti;Rome;L'Aquila,Italy;Italy;Italy;Italy;Italy,4.0,"D'Ascenzo, Andrea;D'Emidio, Mattia;Flammini, Michele;Monaco, Gianpiero",58630013200;57207254487;22333318800;16549965100,60016374;60018783;60136210-60020261;60004638,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,163-202,"We study digraph k-coloring games where strategic agents are vertices of a digraph and arcs represent agents' mutual unidirectional conflicts/idiosyncrasies. Each agent can select, as strategy, one of k different colors, and her payoff in a given state (a k-coloring) is given by the number of outgoing neighbors with a color different from her one. Such games model lots of strategic real-world scenarios and are related to several fundamental classes of anti-coordination games. Unfortunately, the problem of understanding whether an instance of the game admits a pure Nash equilibrium (NE), i.e., a state where no agent can improve her payoff by changing strategy, is NP-complete. Thus, in this paper, we focus on algorithms to compute an approximate NE: informally, a coloring is an approximate γ-NE, for some γ ≥ 1, if no agent can improve her payoff, by changing strategy, by a multiplicative factor of . Our contribution is manifold and of both theoretical and experimental nature. First, we characterize the hardness of finding pure and approximate equilibria in both general and special classes of digraphs. Second, we design and analyze three approximation algorithms with different theoretical guarantees on the approximation ratio, under different conditions; (i) algorithm approx-1 which computes, for any k ≥ 3, a Δo-NE for any n vertex graph having a maximum outdegree of Δo, in polynomial time; (ii) algorithm lll-spe, a randomized algorithm that, for any constant k ≥ 2, determines a -NE for some constant but only in digraphs whose minimum outdegree is sufficiently large, in polynomial time in expectation; (iii) algorithm approx-3 which, for any ϵ > 0, computes a (1+ϵ)-NE by using O( log n ϵ ) colors, for any n-vertex digraph. Note that, the latter shows that a (1 + ϵ)-NE exists and can be computed in polynomial time for k = O(log n). Finally, to assess how proposed algorithms behave in the typical case, we complete our study with an extensive experimental evaluation showing that, while newly introduced algorithms achieve bounded worst case behavior, they generally perform poorly in practice. Motivated by such unsatisfactory performance, we shift our attention to the best-response paradigm, successfully applied to other classes of games, and design and experimentally evaluate it a heuristic based on such paradigm. Our experiments provide strong evidences of such approach outperforming, in terms of approximation and computational time, all other methods and hence identify it as the most suited candidate for practical usage. More remarkably, it is also able to compute exact, pure NE in the great majority of cases. This suggests that, while these games are known to not always possess a pure NE, such an equilibrium often exists and can be efficiently computed, even by a distributed uncoordinated interaction of the agents.",,1,1.0,all publisherfullgold,All Open Access Gold,EC,CUP J33C22002880001,European Commission
2-s2.0-85214099621,,,,Dimension Reduction and MARS,ar,Article,Liu Y.,60017161;60005465;60016418,National University of Singapore;University of Electronic Science and Technology of China;University of York,Singapore City;Chengdu;York,Singapore;China;United Kingdom,3.0,"Liu, Yu;Li, Degui;Xia, Yingcun",59853309700;16199449400;7403027730,60005465;60016418;60005465-60017161,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,309,,"The multivariate adaptive regression spline (MARS) is one of the popular estimation methods for nonparametric multivariate regression. However, as MARS is based on marginal splines, to incorporate interactions of covariates, products of the marginal splines must be used, which often leads to an unmanageable number of basis functions when the order of interaction is high and results in low estimation efficiency. In this paper, we improve the performance of MARS by using linear combinations of the covariates which achieve sufficient dimension reduction. The special basis functions of MARS facilitate calculation of gradients of the regression function, and estimation of these linear combinations is obtained via eigen-analysis of the outer-product of the gradients. Under some technical conditions, the consistency property is established for the proposed estimation method. Numerical studies including both simulation and empirical applications show its effectiveness in dimension reduction and improvement over MARS and other commonly-used nonparametric methods in regression estimation and prediction.",consistency | gradient estimation | multivariate adaptive regression spline | nonparametric regression | sufficient dimension reduction,0,0.0,,,NNSFC,72033002,National Natural Science Foundation of China
2-s2.0-85196903988,,,,Direct Preference Optimization: Your Language Model is Secretly a Reward Model,cp,Conference Paper,Rafailov R.,60012708;121689737,Stanford University;CZ Biohub,Stanford;San Francisco,United States;United States,6.0,"Rafailov, Rafael;Sharma, Archit;Mitchell, Eric;Ermon, Stefano;Manning, Christopher D.;Finn, Chelsea",57219787180;57214356205;57215559556;35791579200;35280197500;55938427500,60012708;60012708;60012708;60012708-121689737;60012708;60012708,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",,1783,0.0,,,HAI,N00014-20-1-2675,"Stanford Institute for Human-Centered Artificial Intelligence, Stanford University"
2-s2.0-105000536341,,,,Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer,cp,Conference Paper,Wu S.,60026851;60033100;131711337,University of Oxford;Nanjing University;DreamTech,Oxford;Nanjing;,United Kingdom;China;China,8.0,"Wu, Shuang;Lin, Youtian;Zhang, Feihu;Zeng, Yifei;Xu, Jingxi;Torr, Philip;Cao, Xun;Yao, Yao",59174527200;57213189392;57206770924;58453536900;59174614000;56821543600;36985370300;57202573176,131711337-60033100;60033100;131711337;131711337-60033100;131711337;60026851;60033100;60033100,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Generating high-quality 3D assets from text and images has long been challenging, primarily due to the absence of scalable 3D representations capable of capturing intricate geometry distributions. In this work, we introduce Direct3D, a native 3D generative model scalable to in-the-wild input images, without requiring a multiview diffusion model or SDS optimization. Our approach comprises two primary components: a Direct 3D Variational Auto-Encoder (D3D-VAE) and a Direct 3D Diffusion Transformer (D3D-DiT). D3D-VAE efficiently encodes high-resolution 3D shapes into a compact and continuous latent triplane space. Notably, our method directly supervises the decoded geometry using a semi-continuous surface sampling strategy, diverging from previous methods that rely on rendered images as supervision signals. D3D-DiT models the distribution of encoded 3D latents and is specifically designed to fuse positional information from the three feature maps of the triplane latent, enabling a native 3D generative model scalable to large-scale 3D datasets. Additionally, we introduce an innovative image-to-3D generation pipeline incorporating semantic-level and pixel-level image conditions, allowing the model to produce 3D shapes consistent with the provided conditional image input. Extensive experiments demonstrate the superiority of our large-scale pre-trained Direct3D over previous image-to-3D approaches, achieving significantly better generation quality and generalization ability, thus establishing a new state-of-the-art for 3D content creation. Project page: https://www.neural4d.com/research/direct3d.",,13,0.0,,,NSFC,62441204,National Natural Science Foundation of China
2-s2.0-85203798201,,,,Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution,cp,Conference Paper,Lou A.,60012708;130815882,Stanford University;Pika Labs,Stanford;,United States;United States,3.0,"Lou, Aaron;Meng, Chenlin;Ermon, Stefano",57219694140;57210642221;35791579200,60012708;60012708-130815882;60012708,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,32819-32848,"Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by 25-75%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around 6-8× better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with 32× fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).",,18,0.0,,,HAI,1651565,"Stanford Institute for Human-Centered Artificial Intelligence, Stanford University"
2-s2.0-85174402977,,,,Discrete Key-Value Bottleneck,cp,Conference Paper,Träuble F.,60017161;60009507;60006191;60030569;60111161;123045532;127854087,National University of Singapore;University of Montreal;Google LLC;Max Planck Institute for Intelligent Systems;DeepMind Technologies Limited;Mila;CIFAR,Singapore City;Montreal;Mountain View;Tubingen;London;Quebec;Montreal,Singapore;Canada;United States;Germany;United Kingdom;Canada;Canada,7.0,"Träuble, Frederik;Goyal, Anirudh;Rahaman, Nasim;Mozer, Michael;Kawaguchi, Kenji;Bengio, Yoshua;Schölkopf, Bernhard",57219758348;57194150283;57193169408;7003407081;57189097784;7003958245;7004460308,60030569;60111161;60030569-123045532;60006191;60017161;123045532-60009507-127854087;60030569-127854087,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,34431-34455,"Deep neural networks perform well on classification tasks where data streams are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary training data streams such as continual learning. One powerful approach that has addressed this challenge involves pre-training of large encoders on volumes of readily available data, followed by task-specific tuning. Given a new task, however, updating the weights of these encoders is challenging as a large number of weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. In the present work, we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes. Our paradigm will be to encode; process the representation via a discrete bottleneck; and decode. Here, the input is fed to the pre-trained encoder, the output of the encoder is used to select the nearest keys, and the corresponding values are fed to the decoder to solve the current task. The model can only fetch and re-use a sparse number of these key-value pairs during inference, enabling localized and context-dependent model updates. We theoretically investigate the ability of the discrete key-value bottleneck to minimize the effect of learning under distribution shifts and show that it reduces the complexity of the hypothesis class. We empirically verify the proposed method under challenging class-incremental learning scenarios and show that the proposed model - without any task boundaries - reduces catastrophic forgetting across a wide variety of pre-trained models, outperforming relevant baselines on this task.",,12,0.0,,,,,
2-s2.0-85170377262,10.24963/ijcai.2023/296,,,Discrete Two Player All-Pay Auction with Complete Information,cp,Conference Paper,Dziubiński M.,60003675;60112534,"Politechnika Warszawska;University of Warsaw, Institute of Informatics",Warsaw;Warsaw,Poland;Poland,2.0,"Dziubiński, Marcin;Jahn, Krzysztof",36886963500;58303873400,60112534;60003675,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,2659-2666,"We study discrete two player all-pay auction with complete information. We provide full characterization of mixed strategy Nash equilibria and show that they constitute a subset of Nash equilibria of discrete General Lotto game. We show that equilibria are not unique in general but they are interchangeable and sets of equilibrium strategies are convex. We also show that equilibrium payoffs are unique, unless valuation of at least one of the players is an even integer number. If equilibrium payoffs are not unique, continuum of equilibrium payoffs are possible.",,1,1.0,all publisherfullgold,All Open Access Gold,NCN,2018/29/B/ST6/00174,Narodowe Centrum Nauki
2-s2.0-85189045515,,,,Discrete-Smoothness in Online Algorithms with Predictions,cp,Conference Paper,Azar Y.,60005681;60008724;131185488,Tel Aviv University;Duke University;Amazon,Tel Aviv-Yafo;Durham;,Israel;United States;,3.0,"Azar, Yossi;Panigrahi, Debmalya;Touitou, Noam",7006362491;24776540000;57205377012,60005681;60008724;131185488,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"In recent years, there has been an increasing focus on designing online algorithms with (machine-learned) predictions. The ideal learning-augmented algorithm is comparable to the optimum when given perfect predictions (consistency), to the best online approximation for arbitrary predictions (robustness), and should interpolate between these extremes as a smooth function of the prediction error. In this paper, we quantify these guarantees in terms of a general property that we call discrete-smoothness and achieve discrete-smooth algorithms for online covering, specifically the facility location and set cover problems. For set cover, our work improves the results of Bamas, Maggiori, and Svensson (2020) by augmenting consistency and robustness with smoothness guarantees. For facility location, our work improves on prior work by Almanza et al. (2021) by generalizing to nonuniform costs and also providing smoothness guarantees by augmenting consistency and robustness.",,3,0.0,,,ISF,2304/20,Israel Science Foundation
2-s2.0-85170372052,10.24963/ijcai.2023/361,,,Disentanglement of Latent Representations via Causal Interventions,cp,Conference Paper,Gendron G.,60005686,University of Auckland,Auckland,New Zealand,3.0,"Gendron, Gaël;Witbrock, Michael;Dobbie, Gillian",57407140100;6602830183;6602511428,60005686;60005686;60005686,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,3239-3247,"The process of generating data such as images is controlled by independent and unknown factors of variation. The retrieval of these variables has been studied extensively in the disentanglement, causal representation learning, and independent component analysis fields. Recently, approaches merging these domains together have shown great success. Instead of directly representing the factors of variation, the problem of disentanglement can be seen as finding the interventions on one image that yield a change to a single factor. Following this assumption, we introduce a new method for disentanglement inspired by causal dynamics that combines causality theory with vector-quantized variational autoencoders. Our model considers the quantized vectors as causal variables and links them in a causal graph. It performs causal interventions on the graph and generates atomic transitions affecting a unique factor of variation in the image. We also introduce a new task of action retrieval that consists of finding the action responsible for the transition between two images. We test our method on standard synthetic and real-world disentanglement datasets. We show that it can effectively disentangle the factors of variation and perform precise interventions on high-level semantic attributes of an image without affecting its quality, even with imbalanced data distributions.",,4,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85163127150,,,,Disentangling Disease-related Representation from Obscure for Disease Prediction,cp,Conference Paper,Wang C.,60014966;60006541;120802162,Peking University;The University of Hong Kong;Deepwise Healthcare,Beijing;Hong Kong;Beijing,China;Hong Kong;China,6.0,"Wang, Churan;Gao, Fei;Zhang, Fandong;Zhong, Fangwei;Yu, Yizhou;Wang, Yizhou",57211168350;57220161918;57195435596;57197844750;8554163500;7601522356,60014966;60014966;60014966;60014966;120802162-60006541;60014966,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,22652-22664,"Disease-related representations play a crucial role in image-based disease prediction such as cancer diagnosis, due to its considerable generalization capacity. However, it is still a challenge to identify lesion characteristics in obscured images, as many lesions are obscured by other tissues. In this paper, to learn the representations for identifying obscured lesions, we propose a disentanglement learning strategy under the guidance of alpha blending generation in an encoder-decoder framework (DAB-Net). Specifically, we take mammogram mass benign/malignant classification as an example. In our framework, composite obscured mass images are generated by alpha blending and then explicitly disentangled into disease-related mass features and interference glands features. To achieve disentanglement learning, features of these two parts are decoded to reconstruct the mass and the glands with corresponding reconstruction losses, and only disease-related mass features are fed into the classifier for disease prediction. Experimental results on one public dataset DDSM and three in-house datasets demonstrate that the proposed strategy can achieve state-of-the-art performance. DAB-Net achieves substantial improvements of 3.9%∼4.4% AUC in obscured cases. Besides, the visualization analysis shows the model can better disentangle the mass and glands in the obscured image, suggesting the effectiveness of our solution in exploring the hidden characteristics in this challenging problem.",,5,0.0,,,NKRDPC,2020C03073,National Postdoctoral Program for Innovative Talents
2-s2.0-85204292382,,,,Disentangling Domain and General Representations for Time Series Classification,cp,Conference Paper,Chen Y.,60003970;60014402;60119391,Zhejiang University;Renmin University of China;Huawei Noah's Ark Lab,Hangzhou;Beijing;Hong Kong,China;China;Hong Kong,7.0,"Chen, Youmin;Yan, Xinyu;Yang, Yang;Zhang, Jianfeng;Zhang, Jing;Pan, Lujia;Li, Juren",59332650900;59332799000;57206628653;59071799000;57216205138;55235702700;57226488807,60003970;60003970;60003970;60119391;60014402;60119391;60003970,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3834-3842,"Modeling time series data has become a very attractive research topic due to its wide application, such as human activity recognition, financial forecasting and sensor-based automatic system monitoring. Recently deep learning models have shown great advances in modeling the time series data, but they heavily depend on a large amount of labeled data. To avoid costly labeling, this paper explores domain adaptation from a labeled source domain to the unlabeled target domain on time series data. To achieve the goal, we propose a disentangled representation learning framework named CADT to disentangle the domain-invariant features from the domain-specific ones. Particularly, CADT is injected with a novel class-wise hypersphere loss to improve the generalization of the classifier from the source domain to the target domain. Intuitively, it restricts the source data of the same class within the same hypersphere and minimizes the radius of it, which in turn enlarges the margin between different classes and makes the decision boundary of both domains easier. We further devise several kinds of domain-preserving data augmentation methods to better capture the domain-specific patterns. Extensive experiments on two public datasets and three real-world applications demonstrate the effectiveness of the proposed model against several state-of-the-art baselines.",,1,0.0,,,NSFC,62176233,National Natural Science Foundation of China
2-s2.0-85203786393,,,,Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion,cp,Conference Paper,Rawal I.S.,60025710;60002668;60004678,"Agency for Science, Technology and Research, Singapore;A-Star, Institute for Infocomm Research;A-Star, Institute of High Performance Computing",Singapore City;Singapore City;Singapore City,Singapore;Singapore;Singapore,5.0,"Rawal, Ishaan Singh;Matyasko, Alexander;Jaiswal, Shantanu;Fernando, Basura;Tan, Cheston",59552934000;56816988100;57365621200;36975184500;55965626200,60025710-60004678;60025710-60004678;60025710-60004678;60025710-60004678;60025710-60004678-60002668,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,42213-42244,"While VideoQA Transformer models demonstrate competitive performance on standard benchmarks, the reasons behind their success are not fully understood. Do these models capture the rich multimodal structures and dynamics from video and text jointly? Or are they achieving high scores by exploiting biases and spurious features? Hence, to provide insights, we design QUAG (QUadrant AveraGe), a lightweight and non-parametric probe, to conduct dataset-model combined representation analysis by impairing modality fusion. We find that the models achieve high performance on many datasets without leveraging multimodal representations. To validate QUAG further, we design QUAG-attention, a less-expressive replacement of self-attention with restricted token interactions. Models with QUAG-attention achieve similar performance with significantly fewer multiplication operations without any finetuning. Our findings raise doubts about the current models' abilities to learn highly-coupled multimodal representations. Hence, we design the CLAVI (Complements in LAnguage and VIdeo) dataset, a stress-test dataset curated by augmenting real-world videos to have high modality coupling. Consistent with the findings of QUAG, we find that most of the models achieve near-trivial performance on CLAVI. This reasserts the limitations of current models for learning highly-coupled multimodal representations, that is not evaluated by the current datasets (project page: https://dissect-videoqa.github.io).",,1,0.0,,,A*STAR,NRF-NRFF14-2022-0001,"Agency for Science, Technology and Research"
2-s2.0-85203806336,,,,Distinguishing the Knowable from the Unknowable with Language Models,cp,Conference Paper,Ahdritz G.,60009982,Harvard University,Cambridge,United States,5.0,"Ahdritz, Gustaf;Qin, Tian;Vyas, Nikhil;Barak, Boaz;Edelman, Benjamin L.",57914355500;57979181700;57188557129;55909951700;57216593894,60009982;60009982;60009982;60009982;60009982,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,503-549,"We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings. Code can be found at: https://github.com/KempnerInstitute/llm_uncertainty.",,3,0.0,,,ONR,IIS 2229881,Office of Naval Research
2-s2.0-105018574975,,,,Distributed Gaussian Mean Estimation under Communication Constraints: Optimal Rates and Communication-Efficient Algorithms,ar,Article,Cai T.T.,60022452,Wharton School of the University of Pennsylvania,Philadelphia,United States,2.0,"Cai, T. Tony;Wei, Hongji",58609924300;57219493933,60022452;60022452,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Distributed estimation of a Gaussian mean under communication constraints is studied in a decision theoretical framework. Minimax rates of convergence, which characterize the tradeoff between communication costs and statistical accuracy, are established under the independent protocols. Communication-efficient and statistically optimal procedures are developed. In the univariate case, the optimal rate depends only on the total communication budget, so long as each local machine has at least one bit. However, in the multivariate case, the minimax rate depends on the specific allocations of the communication budgets among the local machines. Although optimal estimation of a Gaussian mean is relatively simple in the conventional setting, it is quite involved under communication constraints, both in terms of the optimal procedure design and the lower bound argument. An essential step is the decomposition of the minimax estimation problem into two stages, localization and refinement. This critical decomposition provides a framework for both the lower bound analysis and optimal procedure design. The optimality results and techniques developed in the present paper can be useful for solving other problems such as distributed nonparametric function estimation and sparse signal recovery.",Communication constraints | distributed learning | Gaussian mean | minimax lower bound | optimal rate of convergence,1,0.0,,,NSF,DMS-1712735,National Science Foundation
2-s2.0-85198921488,,,,Distribution Alignment Optimization through Neural Collapse for Long-tailed Classification,cp,Conference Paper,Gao J.,60007711;60029470;60108865,"Jilin University;Commonwealth Scientific and Industrial Research Organisation;The Chinese University of Hong Kong, Shenzhen",Changchun;Canberra;Shenzhen,China;Australia;China,4.0,"Gao, Jintong;Zhao, He;Guo, Dandan;Zha, Hongyuan",58093316600;57201215659;58995023900;57201737688,60007711;60029470;60007711;60108865,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,14969-14987,"A well-trained deep neural network on balanced datasets usually exhibits the Neural Collapse (NC) phenomenon, which is an informative indicator of the model achieving good performance. However, NC is usually hard to be achieved for a model trained on long-tailed datasets, leading to the deteriorated performance of test data. This work aims to induce the NC phenomenon in imbalanced learning from the perspective of distribution matching. By enforcing the distribution of last-layer representations to align the ideal distribution of the ETF structure, we develop a Distribution Alignment Optimization (DisA) loss, acting as a plug-and-play method can be combined with most of the existing long-tailed methods, we further instantiate it to the cases of fixing classifier and learning classifier. The extensive experiments show the effectiveness of DisA, providing a promising solution to the imbalanced issue. Our code is available at DisA.",,5,0.0,,,NSFC,62306125,National Natural Science Foundation of China
2-s2.0-85172615457,,,,Diversity-enhancing Generative Network for Few-shot Hypothesis Adaptation,cp,Conference Paper,Dong R.,60025272;60025709;60014347;60277287;60195969;60118644;125172111,The University of Tokyo;The University of Sydney;Hong Kong Baptist University;RIKEN Center for Advanced Intelligence Project;Mohamed Bin Zayed University of Artificial Intelligence;School of Mathematics and Statistics;Tianjin Artificial Intelligence Innovation Center,Tokyo;Sydney;Hong Kong;Tokyo;Abu Dhabi;Melbourne;Tianjin,Japan;Australia;Hong Kong;Japan;United Arab Emirates;Australia;China,8.0,"Dong, Ruijiang;Liu, Feng;Chi, Haoang;Liu, Tongliang;Gong, Mingming;Niu, Gang;Sugiyama, Masashi;Han, Bo",58509539900;57118471000;57222260713;56297951800;36617384500;26431056600;7402826969;57191281044,60014347;60118644-60277287;125172111;60277287-60195969-60025709;60118644;60277287;60277287-60025272;60014347-60277287,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,8260-8275,"Generating unlabeled data has been recently shown to help address the few-shot hypothesis adaptation (FHA) problem, where we aim to train a classifier for the target domain with a few labeled target-domain data and a well-trained source-domain classifier (i.e., a source hypothesis), for the additional information of the highly-compatible unlabeled data. However, the generated data of the existing methods are extremely similar or even the same. The strong dependency among the generated data will lead the learning to fail. In this paper, we propose a diversity-enhancing generative network (DEG-Net) for the FHA problem, which can generate diverse unlabeled data with the help of a kernel independence measure: the Hilbert-Schmidt independence criterion (HSIC). Specifically, DEG-Net will generate data via minimizing the HSIC value (i.e., maximizing the independence) among the semantic features of the generated data. By DEG-Net, the generated unlabeled data are more diverse and more effective for addressing the FHA problem. Experimental results show that the DEG-Net outperforms existing FHA baselines and further verifies that generating diverse data plays a vital role in addressing the FHA problem.",,6,0.0,,,HKBU,62006202,Hong Kong Baptist University
2-s2.0-85163116548,,,,Do Differentiable Simulators Give Better Policy Gradients?,cp,Conference Paper,Suh H.J.T.,60141072,MIT Department of Electrical Engineering and Computer Science,Cambridge,United States,4.0,"Suh, H. J.Terry;Simchowitz, Max;Zhang, Kaiqing;Tedrake, Russ",57219586873;57192165585;57190579283;6507809929,60141072;60141072;60141072;60141072,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,20668-20696,"Differentiable simulators promise faster computation time for reinforcement learning by replacing zeroth-order gradient estimates of a stochastic objective with an estimate based on first-order gradients. However, it is yet unclear what factors decide the performance of the two estimators on complex landscapes that involve long-horizon planning and control on physical systems, despite the crucial relevance of this question for the utility of differentiable simulators. We show that characteristics of certain physical systems, such as stiffness or discontinuities, may compromise the efficacy of the first-order estimator, and analyze this phenomenon through the lens of bias and variance. We additionally propose an α-order gradient estimator, with α ∈ [0, 1], which correctly utilizes exact gradients to combine the efficiency of first-order estimates with the robustness of zero-order methods. We demonstrate the pitfalls of traditional estimators and the advantages of the α-order estimator on some numerical examples.",,64,0.0,,,NSF,EFMA-1830901,National Science Foundation
2-s2.0-85199672379,10.1613/jair.1.15461,,,Does CLIP Know My Face?,ar,Article,Hintersdorf D.,60011226,Technische Universität Darmstadt,Darmstadt,Germany,6.0,"Hintersdorf, Dominik;Struppek, Lukas;Brack, Manuel;Friedrich, Felix;Schramowski, Patrick;Kersting, Kristian",57350223800;57350263600;57860962000;57310236900;57209565243;57189007679,60011226;60011226;60011226;60011226;60011226;60011226,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,1033-1062,"With the rise of deep learning in various applications, privacy concerns around the protection of training data have become a critical area of research. Whereas prior studies have focused on privacy risks in single-modal models, we introduce a novel method to assess privacy for multi-modal models, specifically vision-language models like CLIP. The proposed Identity Inference Attack (IDIA) reveals whether an individual was included in the training data by querying the model with images of the same person. Letting the model choose from a wide variety of possible text labels, the model reveals whether it recognizes the person and, therefore, was used for training. Our large-scale experiments on CLIP demonstrate that individuals used for training can be identified with very high accuracy. We confirm that the model has learned to associate names with depicted individuals, implying the existence of sensitive information that can be extracted by adversaries. Our results highlight the need for stronger privacy protection in large-scale models and suggest that IDIAs can be used to prove the unauthorized use of data for training and to enforce privacy laws.",,12,1.0,all publisherfullgold,All Open Access Gold,DFKI,13N15343,Hessisches Ministerium für Wissenschaft und Kunst
2-s2.0-85147603632,10.1609/aaai.v36i3.20196,,,Dual Attention Networks for Few-Shot Fine-Grained Recognition,cp,Conference Paper,Xu S.L.,60033100;60025578;60010080;131846613,Nanjing University;Xidian University;Nanjing University of Science and Technology;Ltd,Nanjing;Xi'an;Nanjing;,China;China;China;,4.0,"Xu, Shu Lin;Zhang, Faen;Wei, Xiu Shen;Wang, Jianhua",57698057300;57209457629;56718883700;58094868900,60010080-60025578;131846613;60010080-60025578-60033100;131846613,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,2911-2919,"The task of few-shot fine-grained recognition is to classify images belonging to subordinate categories merely depending on few examples. Due to the fine-grained nature, it is desirable to capture subtle but discriminative part-level patterns from limited training data, which makes it a challenging problem. In this paper, to generate fine-grained tailored representations for few-shot recognition, we propose a Dual Attention Network (DUAL ATT-NET) consisting of two dual branches of both hard- and soft-attentions. Specifically, by producing attention guidance from deep activations of input images, our hard-attention is realized by keeping a few useful deep descriptors and forming them as a bag of multi-instance learning. Since these deep descriptors could correspond to objects' parts, the advantage of modeling as a multi-instance bag is able to exploit inherent correlation of these fine-grained parts. On the other side, a soft attended activation representation can be obtained by applying attention guidance upon original activations, which brings comprehensive attention information as the counterpart of hard-attention. After that, both outputs of dual branches are aggregated as a holistic image embedding w.r.t. input images. By performing meta-learning, we can learn a powerful image embedding in such a metric space to generalize to novel classes. Experiments on three popular fine-grained benchmark datasets show that our DUAL ATT-NET obviously outperforms other existing state-of-the-art methods.",,43,1.0,all publisherfullgold,All Open Access Gold,NKRDPC,BK20210340,Natural Science Foundation of Jiangsu Province
2-s2.0-85203826754,,,,DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems,cp,Conference Paper,Schiff Y.,60032179;60006191;60104837,University of Wisconsin-Madison;Google LLC;Cornell Tech,Madison;Mountain View;New York,United States;United States;United States,7.0,"Schiff, Yair;Wan, Zhong Yi;Parker, Jeffrey B.;Hoyer, Stephan;Kuleshov, Volodymyr;Sha, Fei;Zepeda-Núñez, Leonardo",57221860490;57193069867;58904275200;57201197294;36651233400;35743855800;57035317700,60104837;60006191;60006191;60006191;60104837;60006191;60006191-60032179,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,43649-43684,"Learning dynamics from dissipative chaotic systems is notoriously difficult due to their inherent instability, as formalized by their positive Lyapunov exponents, which exponentially amplify errors in the learned dynamics. However, many of these systems exhibit ergodicity and an attractor: a compact and highly complex manifold, to which trajectories converge in finite-time, that supports an invariant measure, i.e., a probability distribution that is invariant under the action of the dynamics, which dictates the long-term statistical behavior of the system. In this work, we leverage this structure to propose a new framework that targets learning the invariant measure as well as the dynamics, in contrast with typical methods that only target the misfit between trajectories, which often leads to divergence as the trajectories' length increases. We use our framework to propose a tractable and sample efficient objective that can be used with any existing learning objectives. Our Dynamics Stable Learning by Invariant Measure (DySLIM) objective enables model training that achieves better point-wise tracking and long-term statistical accuracy relative to other learning objectives. By targeting the distribution with a scalable regularization term, we hope that this approach can be extended to more complex systems exhibiting slowly-variant distributions, such as weather and climate models. Code to reproduce our experiments is available here.",,2,0.0,,,,,
2-s2.0-85189359438,,,,DynPoint: Dynamic Neural Point For View Synthesis,cp,Conference Paper,Zhou K.,60026851,University of Oxford,Oxford,United Kingdom,7.0,"Zhou, Kaichen;Zhong, Jia Xing;Shin, Sangyun;Lu, Kai;Yang, Yiyuan;Markham, Andrew;Trigoni, Niki",57219448771;57658451100;57557927900;58411208700;57217051309;35105425400;23502091400,60026851;60026851;60026851;60026851;60026851;60026851;60026851,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"The introduction of neural radiance fields has greatly improved the effectiveness of view synthesis for monocular videos. However, existing algorithms face difficulties when dealing with uncontrolled or lengthy scenarios, and require extensive training time specific to each new scenario. To tackle these limitations, we propose DynPoint, an algorithm designed to facilitate the rapid synthesis of novel views for unconstrained monocular videos. Rather than encoding the entirety of the scenario information into a latent representation, DynPoint concentrates on predicting the explicit 3D correspondence between neighboring frames to realize information aggregation. Specifically, this correspondence prediction is achieved through the estimation of consistent depth and scene flow information across frames. Subsequently, the acquired correspondence is utilized to aggregate information from multiple reference frames to a target frame, by constructing hierarchical neural point clouds. The resulting framework enables swift and accurate view synthesis for desired views of target frames. The experimental results obtained demonstrate the considerable acceleration of training time achieved - typically an order of magnitude - by our proposed method while yielding comparable outcomes compared to prior approaches. Furthermore, our method exhibits strong robustness in handling long-duration videos without learning a canonical representation of video content.",,9,0.0,,,AWS,EP/S030832/1,Amazon Web Services
2-s2.0-85170409406,10.1613/jair.1.13065,,,Dynamic Controllability of Temporal Plans in Uncertain and Partially Observable Environments,ar,Article,Bit-Monnot A.,60004179;60102124,NASA Ames Research Center;Communauté d'universités et établissements de Toulouse,Moffett Field;Toulouse,United States;France,2.0,"Bit-Monnot, Arthur;Morris, Paul",56035400000;10242799500,60102124;60004179,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,1311-1369,"The formalism of Simple Temporal Networks (STNs) provides methods for evaluating the feasibility of temporal plans. The basic formalism deals with the consistency of quantitative temporal requirements on scheduled events. This implicitly assumes a single agent has full control over the timing of events. The extension of Simple Temporal Networks with Uncertainty (STNU) introduces uncertainty into the timing of some events. Two main approaches to the feasibility of STNUs involve (1) where a single schedule works irrespective of the duration outcomes, called Strong Controllability, and (2) whether a strategy exists to schedule future events based on the outcomes of past events, called Dynamic Controllability. Case (1) essentially assumes the timing of uncertain events cannot be observed by the agent while case (2) assumes full observability. The formalism of Partially Observable Simple Temporal Networks with Uncertainty (POSTNU) provides an intermediate stance between these two extremes, where a known subset of the uncertain events can be observed when they occur. A sound and complete polynomial algorithm to determining the Dynamic Controllability of POSTNUs has not previously been known; we present one in this paper. This answers an open problem that has been posed in the literature. The approach we take factors the problem into Strong Controllability micro-problems in an overall Dynamic Controllability macro-problem framework. It generalizes the notion of labeled distance graph from STNUs. The generalized labels are expressed as max/min expressions involving the observables. The paper introduces sound generalized reduction rules that act on the generalized labels. These incorporate tightenings based on observability that preserve dynamic viable strategies. It is shown that if the generalized reduction rules reach quiescence without exposing an inconsistency, then the POSTNU is Dynamically Controllable (DC). The paper also presents algorithms that apply the reduction rules in an organized way and reach quiescence in a polynomial number of steps if the POSTNU is Dynamically Controllable. Remarkably, the generalized perspective leads to a simpler and more uniform framework that applies also to the STNU special case. It helps illuminate the previous methods inasmuch as the max/min label representation is more semantically clear than the ad-hoc upper/lower case labels previously used.",,5,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,H2020,101016442,Horizon 2020
2-s2.0-85203827717,,,,Dynamic Evaluation of Large Language Models by Meta Probing Agents,cp,Conference Paper,Zhu K.,60019118;60021726,University of Science and Technology of China;Microsoft Research,Hefei;Redmond,China;United States,5.0,"Zhu, Kaijie;Wang, Jindong;Zhao, Qinlin;Xu, Ruochen;Xie, Xing",58481448300;57190969217;58684668700;57220778246;57221820833,60021726;60021726;60019118;60021726;60021726,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,62599-62617,"Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination.Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios.Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities.In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs.MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge.These basic abilities are also dynamically configurable, allowing multifaceted analysis.We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement.Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities.MPA can also be used as a data augmentation approach to enhance LLMs.Code is available at: https://github.com/microsoft/promptbench.",,1,0.0,,,,,
2-s2.0-85189644500,10.1609/aaai.v38i11.29079,,,Dynamic Sub-graph Distillation for Robust Semi-supervised Continual Learning,cp,Conference Paper,Fan Y.,60019533,Tianjin University,Tianjin,China,4.0,"Fan, Yan;Wang, Yu;Zhu, Pengfei;Hu, Qinghua",58798751500;58611250600;35779388800;7403214664,60019533;60019533;60019533;60019533,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,11.0,,11927-11935,"Continual learning (CL) has shown promising results and comparable performance to learning at once in a fully supervised manner. However, CL strategies typically require a large number of labeled samples, making their real-life deployment challenging. In this work, we focus on semi-supervised continual learning (SSCL), where the model progressively learns from partially labeled data with unknown categories. We provide a comprehensive analysis of SSCL and demonstrate that unreliable distributions of unlabeled data lead to unstable training and refinement of the progressing stages. This problem severely impacts the performance of SSCL. To address the limitations, we propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for semi-supervised continual learning, which leverages both semantic and structural information to achieve more stable knowledge distillation on unlabeled data and exhibit robustness against distribution bias. Firstly, we formalize a general model of structural distillation and design a dynamic graph construction for the continual learning progress. Next, we define a structure distillation vector and design a dynamic sub-graph distillation algorithm, which enables end-to-end training and adaptability to scale up tasks. The entire proposed method is adaptable to various CL methods and supervision settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100, and ImageNet-100, with varying supervision ratios, demonstrate the effectiveness of our proposed approach in mitigating the catastrophic forgetting problem in semi-supervised continual learning scenarios. Our code is available: https://github.com/fanyan0411/DSGD.",,8,1.0,all publisherfullgold,All Open Access Gold,NSFC,62106174,National Natural Science Foundation of China
2-s2.0-85191187603,,,,ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram,cp,Conference Paper,Oh J.,60032144;129550309,Korea Advanced Institute of Science and Technology;Medical AI Inc.,Daejeon;Seoul,South Korea;South Korea,5.0,"Oh, Jungwoo;Lee, Gyubok;Bae, Seongsu;Kwon, Joon Myoung;Choi, Edward",57232508200;57224896115;57362561400;57202893940;57188811144,60032144;60032144;60032144;129550309;60032144,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or structured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the development of intelligent QA systems capable of assisting clinicians in ECG interpretations.",,11,0.0,,,MSIP,NRF-2020H1D3A2A03100945,"Ministry of Science, ICT and Future Planning"
2-s2.0-85200553948,,,,EFFECTIVE STRUCTURAL ENCODINGS VIA LOCAL CURVATURE PROFILES,cp,Conference Paper,Fesser L.,60006303;60077572,Harvard Faculty of Arts and Sciences;Harvard John A. Paulson School of Engineering and Applied Sciences,Cambridge;Cambridge,United States;United States,2.0,"Fesser, Lukas;Weber, Melanie",58476571800;57195473864,60006303;60077572,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Structural and Positional Encodings can significantly improve the performance of Graph Neural Networks in downstream tasks. Recent literature has begun to systematically investigate differences in the structural properties that these approaches encode, as well as performance trade-offs between them. However, the question of which structural properties yield the most effective encoding remains open. In this paper, we investigate this question from a geometric perspective. We propose a novel structural encoding based on discrete Ricci curvature (Local Curvature Profiles, short LCP) and show that it significantly outperforms existing encoding approaches. We further show that combining local structural encodings, such as LCP, with global positional encodings improves downstream performance, suggesting that they capture complementary geometric information. Finally, we compare different encoding types with (curvature-based) rewiring techniques. Rewiring has recently received a surge of interest due to its ability to improve the performance of GNNs by mitigating over-smoothing and over-squashing effects. Our results suggest that utilizing curvature information for structural encodings delivers significantly larger performance increases than rewiring.",,3,0.0,,,NSF,2112085,National Science Foundation
2-s2.0-85185125145,,,,EFFICIENT EDGE INFERENCE BY SELECTIVE QUERY,cp,Conference Paper,Kag A.,60027950;60019674;60355330;60357266,Carnegie Mellon University;Boston University;Meta Ai;Qualcomm AI Research,Pittsburgh;Boston;Menlo Park;San Diego,United States;United States;United States;United States,5.0,"Kag, Anil;Fedorov, Igor;Gangrade, Aditya;Whatmough, Paul;Saligrama, Venkatesh",57202036238;57220410276;57202349965;26031186900;13609751200,60019674;60355330;60027950;60357266;60019674,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Edge devices provide inference on predictive tasks to many end-users. However, deploying deep neural networks that achieve state-of-the-art accuracy on these devices is infeasible due to edge resource constraints. Nevertheless, cloud-only processing, the de-facto standard, is also problematic, since uploading large amounts of data imposes severe communication bottlenecks. We propose a novel end-to-end hybrid learning framework that allows the edge to selectively query only those hard examples that the cloud can classify correctly. Our framework optimizes over neural architectures and trains edge predictors and routing models so that the overall accuracy remains high while minimizing the overall latency. Training a hybrid learner is difficult since we lack annotations of hard edge-examples. We introduce a novel proxy supervision in this context and show that our method adapts seamlessly and near optimally across different latency regimes. On the ImageNet dataset, our proposed method deployed on a micro-controller unit exhibits 25% reduction in latency compared to cloud-only processing while suffering no excess loss.",,6,0.0,,,ARO,W911NF2110246,Army Research Office
2-s2.0-85196705790,,,,EFFICIENT MODULATION FOR VISION NETWORKS,cp,Conference Paper,Ma X.,60028628;60026532,Northeastern University;Microsoft Corporation,Boston;Redmond,United States;United States,7.0,"Ma, Xu;Dai, Xiyang;Yang, Jianwei;Xiao, Bin;Chen, Yinpeng;Fu, Yun;Yuan, Lu",57202844335;57200615333;57191074527;57201773066;55924877900;7404432812;7402013543,60028628;60026532;60026532;60026532;60026532;60028628;60026532,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"In this work, we present efficient modulation, a novel design for efficient vision networks. We revisit the modulation mechanism, which operates input through convolutional context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block. We demonstrate that the modulation mechanism is particularly well suited for efficient networks and further tailor the modulation design by proposing the efficient modulation (EfficientMod) block, which is considered the essential building block for our networks. Benefiting from the prominent representational ability of modulation mechanism and the proposed efficient design, our network can accomplish better trade-offs between accuracy and efficiency and set new state-of-the-art performance in the zoo of efficient networks. When integrating EfficientMod with the vanilla self-attention block, we obtain the hybrid architecture which further improves the performance without loss of efficiency. We carry out comprehensive experiments to verify EfficientMod's performance. With fewer parameters, our EfficientMod-s performs 0.6 top-1 accuracy better than EfficientFormerV2-s2 and is 25% faster on GPU, and 2.9 better than MobileViTv2-1.0 at the same GPU latency. Additionally, our method presents a notable improvement in downstream tasks, outperforming EfficientFormerV2-s by 3.6 mIoU on the ADE20K benchmark. Code and checkpoints are available at https://github.com/ma-xu/EfficientMod.",,14,0.0,,,,,
2-s2.0-85147604574,10.1609/aaai.v36i2.20038,,,ELMA: Energy-Based Learning for Multi-Agent Activity Forecasting,cp,Conference Paper,Li Y.K.,60026851;60025038;60029306,"University of Oxford;University of California, Berkeley;Wuhan University",Oxford;Berkeley;Wuhan,United Kingdom;United States;China,5.0,"Li, Yu Ke;Wang, Pin;Chen, Li Xiong;Wang, Zheng;Chan, Ching Yao",59786177700;56193114700;57200616103;24175350400;7404814589,60025038;60025038;60026851;60029306;60025038,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,1482-1490,"This paper describes an energy-based learning method that predicts the activities of multiple agents simultaneously. It aims to forecast both upcoming actions and paths of all agents in a scene based on their past activities, which can be jointly formulated by a probabilistic model over time. Learning this model is challenging because: 1) it has a large number of time-dependent variables that must scale with the forecast horizon and the number of agents; 2) distribution functions have to contain multiple modes in order to capture the spatiotemporal complexities of each agent's activities. To address these challenges, we put forth a novel Energy-based Learning approach for Multi-Agent activity forecasting (ELMA) to estimate this complex model via maximum log-likelihood estimation. Specifically, by sampling from a sequence of factorized marginalized multi-modal distributions, ELMA generates the possible future actions efficiently. Moreover, by graph-based representations, ELMA also explicitly resolves the spatio-temporal dependencies of all agents' activities in a single pass. Our experiments on two large-scale datasets prove that ELMA outperforms recent leading studies by an obvious margin.",,8,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85199863188,,,,EMERGENCE OF MAPS IN THE MEMORIES OF BLIND NAVIGATION AGENTS,cp,Conference Paper,Wijmans E.,60019647;60013402;60018491;60006191;60355330,Georgia Institute of Technology;Oregon State University;Simon Fraser University;Google LLC;Meta Ai,Atlanta;Corvallis;Burnaby;Mountain View;Menlo Park,United States;United States;Canada;United States;United States,6.0,"Wijmans, Erik;Savva, Manolis;Essa, Irfan;Lee, Stefan;Morcos, Ari S.;Batra, Dhruv",57201316320;47962533500;6701806882;56402182300;56946640900;24764919500,60019647-60355330;60355330-60018491;60019647-60006191;60013402;60355330;60019647-60355330,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Animal navigation research posits that organisms build and maintain internal spatial representations, or maps, of their environment. We ask if machines - specifically, artificial intelligence (AI) navigation agents - also build implicit (or 'mental') maps. A positive answer to this question would (a) explain the surprising phenomenon in recent literature of ostensibly map-free neural-networks achieving strong performance, and (b) strengthen the evidence of mapping as a fundamental mechanism for navigation by intelligent embodied agents, whether they be biological or artificial. Unlike animal navigation, we can judiciously design the agent's perceptual system and control the learning paradigm to nullify alternative navigation mechanisms. Specifically, we train 'blind' agents - with sensing limited to only egomotion and no other sensing of any kind - to perform PointGoal navigation ('go to ∆x, ∆y') via reinforcement learning. Our agents are composed of navigation-agnostic components (fully-connected and recurrent neural networks), and our experimental setup provides no inductive bias towards mapping. Despite these harsh conditions, we find that blind agents are (1) surprisingly effective navigators in new environments (∼95% success); (2) they utilize memory over long horizons (remembering ∼1,000 steps of past experience in an episode); (3) this memory enables them to exhibit intelligent behavior (following walls, detecting collisions, taking shortcuts); (4) there is emergence of maps and collision detection neurons in the representations of the environment built by a blind agent as it navigates; and (5) the emergent maps are selective and task dependent (e.g. the agent 'forgets' exploratory detours). Overall, this paper presents no new techniques for the AI audience, but a surprising finding, an insight, and an explanation.",,10,0.0,,,,,
2-s2.0-85152123964,,,,EMPOWERING GRAPH REPRESENTATION LEARNING WITH TEST-TIME GRAPH TRANSFORMATION,cp,Conference Paper,Jin W.,60031707;60120916,Michigan State University;Snap Inc.,East Lansing;Santa Monica,United States;United States,6.0,"Jin, Wei;Zhao, Tong;Ding, Jiayuan;Liu, Yozen;Tang, Jiliang;Shah, Neil",60355043100;57207568082;57312265800;57210646358;56245477300;56719006400,60031707;60120916;60031707;60120916;60031707;60120916,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"As powerful tools for representation learning on graphs, graph neural networks (GNNs) have facilitated various applications from drug discovery to recommender systems. Nevertheless, the effectiveness of GNNs is immensely challenged by issues related to data quality, such as distribution shift, abnormal features and adversarial attacks. Recent efforts have been made on tackling these issues from a modeling perspective which requires additional cost of changing model architectures or re-training model parameters. In this work, we provide a data-centric view to tackle these issues and propose a graph transformation framework named GTRANS which adapts and refines graph data at test time to achieve better performance. We provide theoretical analysis on the design of the framework and discuss why adapting graph data works better than adapting the model. Extensive experiments have demonstrated the effectiveness of GTRANS on three distinct scenarios for eight benchmark datasets where suboptimal data is presented. Remarkably, GTRANS performs the best in most cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on three experimental settings. Code is released at https://github.com/ChandlerBang/GTrans.",,31,0.0,,,HD,IIS1845081,Home Depot
2-s2.0-85189889987,,,,ENHANCING HIGH-RESOLUTION 3D GENERATION THROUGH PIXEL-WISE GRADIENT CLIPPING,cp,Conference Paper,Pan Z.,60009860;60021097,Fudan University;University of Surrey,Shanghai;Guildford,China;United Kingdom,4.0,"Pan, Zijie;Lu, Jiachen;Zhu, Xiatian;Zhang, Li",58669021100;57222147539;56050744800;57208268367,60009860;60009860;60021097;60009860,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"High-resolution 3D object generation remains a challenging task primarily due to the limited availability of comprehensive annotated training data. Recent advancements have aimed to overcome this constraint by harnessing image generative models, pretrained on extensive curated web datasets, using knowledge transfer techniques like Score Distillation Sampling (SDS). Efficiently addressing the requirements of high-resolution rendering often necessitates the adoption of latent representation-based models, such as the Latent Diffusion Model (LDM). In this framework, a significant challenge arises: To compute gradients for individual image pixels, it is necessary to backpropagate gradients from the designated latent space through the frozen components of the image model, such as the VAE encoder used within LDM. However, this gradient propagation pathway has never been optimized, remaining uncontrolled during training. We find that the unregulated gradients adversely affect the 3D model's capacity in acquiring texture-related information from the image generative model, leading to poor quality appearance synthesis. To address this overarching challenge, we propose an innovative operation termed Pixel-wise Gradient Clipping (PGC) designed for seamless integration into existing 3D generative models, thereby enhancing their synthesis quality. Specifically, we control the magnitude of stochastic gradients by clipping the pixel-wise gradients efficiently, while preserving crucial texture-related gradient directions. Despite this simplicity and minimal extra cost, extensive experiments demonstrate the efficacy of our PGC in enhancing the performance of existing 3D generative models for high-resolution object rendering.",,6,0.0,,,NSFC,22ZR1407500,Natural Science Foundation of Shanghai Municipality
2-s2.0-85167680279,10.1609/aaai.v37i1.25079,,,ESL-SNNs: An Evolutionary Structure Learning Strategy for Spiking Neural Networks,cp,Conference Paper,Shen J.,60012070;60004538;60117751;129966488,"University of Leeds;Dalian University of Technology;College of Computer Science and Technology, Zhejiang University;Research Institute of Intelligent Computing",Leeds;Dalian;Hangzhou;Chengdu,United Kingdom;China;China;China,6.0,"Shen, Jiangrong;Xu, Qi;Liu, Jian K.;Wang, Yueming;Pan, Gang;Tang, Huajin",57196190369;55636690500;57202579249;35231746900;35254246400;7403124759,60117751;60004538;60012070;60117751;60117751;60117751-129966488,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,86-93,"Spiking neural networks (SNNs) have manifested remarkable advantages in power consumption and event-driven property during the inference process. To take full advantage of low power consumption and improve the efficiency of these models further, the pruning methods have been explored to find sparse SNNs without redundancy connections after training. However, parameter redundancy still hinders the efficiency of SNNs during training. In the human brain, the rewiring process of neural networks is highly dynamic, while synaptic connections maintain relatively sparse during brain development. Inspired by this, here we propose an efficient evolutionary structure learning (ESL) framework for SNNs, named ESL-SNNs, to implement the sparse SNN training from scratch. The pruning and regeneration of synaptic connections in SNNs evolve dynamically during learning, yet keep the structural sparsity at a certain level. As a result, the ESL-SNNs can search for optimal sparse connectivity by exploring all possible parameters across time. Our experiments show that the proposed ESL-SNNs framework is able to learn SNNs with sparse structures effectively while reducing the limited accuracy. The ESL-SNNs achieve merely 0.28% accuracy loss with 10% connection density on the DVS-Cifar10 dataset. Our work presents a brand-new approach for sparse training of SNNs from scratch with biologically plausible evolutionary mechanisms, closing the gap in the expressibility between sparse training and dense training. Hence, it has great potential for SNN lightweight training and inference with low power consumption and small memory usage.",,54,1.0,all publisherfullgold,All Open Access Gold,NSFC,2021KC0AC01,National Natural Science Foundation of China
2-s2.0-85150196518,,,,EVALUATING DISENTANGLEMENT OF STRUCTURED REPRESENTATIONS,cp,Conference Paper,Dang-Nhu R.,,,,,1.0,"Dang-Nhu, Raphaël",57203128758,,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"We introduce the first metric for evaluating disentanglement at individual hierarchy levels of a structured latent representation. Applied to object-centric generative models, this offers a systematic, unified approach to evaluating (i) object separation between latent slots (ii) disentanglement of object properties inside individual slots (iii) disentanglement of intrinsic and extrinsic object properties. We theoretically show that for structured representations, our framework gives stronger guarantees of selecting a good model than previous disentanglement metrics. Experimentally, we demonstrate that viewing object compositionality as a disentanglement problem addresses several issues with prior visual metrics of object separation. As a core technical component, we present the first representation probing algorithm handling slot permutation invariance.",,1,0.0,,,ETH,,Eidgenössische Technische Hochschule Zürich
2-s2.0-85144779831,,,,EXPLAINABLE GNN-BASED MODELS OVER KNOWLEDGE GRAPHS,cp,Conference Paper,Cucala D.T.,60026851;60010348,University of Oxford;Universitetet i Oslo,Oxford;Oslo,United Kingdom;Norway,4.0,"Cucala, David Tena;Grau, Bernardo Cuenca;Kostylev, Egor V.;Motik, Boris",57195404827;22834310900;26433628600;23101071600,60026851;60026851;60010348;60026851,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Graph Neural Networks (GNNs) are often used to learn transformations of graph data. While effective in practice, such approaches make predictions via numeric manipulations so their output cannot be easily explained symbolically. We propose a new family of GNN-based transformations of graph data that can be trained effectively, but where all predictions can be explained symbolically as logical inferences in Datalog-a well-known rule-based formalism. In particular, we show how to encode an input knowledge graph into a graph with numeric feature vectors, process this graph using a GNN, and decode the result into an output knowledge graph. We use a new class of monotonic GNNs (MGNNs) to ensure that this process is equivalent to a round of application of a set of Datalog rules. We also show that, given an arbitrary MGNN, we can automatically extract rules that completely characterise the transformation. We evaluate our approach by applying it to classification tasks in knowledge graph completion.",,15,0.0,,,EPSRC,EP/S019111/1,Engineering and Physical Sciences Research Council
2-s2.0-85197706104,,,,EXPLAINING TIME SERIES VIA CONTRASTIVE AND LOCALLY SPARSE PERTURBATIONS,cp,Conference Paper,Liu Z.,60025278;60001439;60033100;60006541;60015206;60025710;60022904;130780795,"Tsinghua University;Pennsylvania State University;Nanjing University;The University of Hong Kong;Florida International University;Agency for Science, Technology and Research, Singapore;New Jersey Institute of Technology;Ailibaba Group",Beijing;University Park;Nanjing;Hong Kong;Miami;Singapore City;Newark;Beijing,China;United States;China;Hong Kong;United States;Singapore;United States;China,11.0,"Liu, Zichuan;Zhang, Yingying;Wang, Tianchun;Wang, Zefan;Luo, Dongsheng;Du, Mengnan;Wu, Min;Wang, Yi;Chen, Chunlin;Fan, Lunting;Wen, Qingsong",57224578392;57219694873;57958545500;59837589400;57202790937;57203397578;58848466700;57223032385;24729216100;57340425900;57211755653,60033100-130780795;130780795;60001439;130780795-60025278;60015206;60022904;60025710;60006541;60033100;130780795;130780795,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns. Although previous saliency-based methods addressed the challenges, their perturbation may not alleviate the distribution shift issue, which is inevitable especially in heterogeneous samples. We present ContraLSP, a locally sparse model that introduces counterfactual samples to build uninformative perturbations but keeps distribution using contrastive learning. Furthermore, we incorporate sample-specific sparse gates to generate more binary-skewed and smooth masks, which easily integrate temporal trends and select the salient features parsimoniously. Empirical studies on both synthetic and real-world datasets show that ContraLSP outperforms state-of-the-art models, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at https://github.com/zichuan-liu/ContraLSP.",,8,0.0,,,,,
2-s2.0-85193442942,,,,EXPLICIT BOX DETECTION UNIFIES END-TO-END MULTI-PERSON POSE ESTIMATION,cp,Conference Paper,Yang J.,60108865;131451135,"The Chinese University of Hong Kong, Shenzhen;International Digital Economy Academy (IDEA)",Shenzhen;Shenzhen,China;China,6.0,"Yang, Jie;Zeng, Ailing;Liu, Shilong;Li, Feng;Zhang, Ruimao;Zhang, Lei",57782312400;57211029797;57224726034;56668990100;54923549900;36078589800,131451135-60108865;131451135;131451135;131451135;60108865;131451135,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"This paper presents a novel end-to-end framework with Explicit box Detection for multi-person Pose estimation, called ED-Pose, where it unifies the contextual learning between human-level (global) and keypoint-level (local) information. Different from previous one-stage methods, ED-Pose re-considers this task as two explicit box detection processes with a unified representation and regression supervision. First, we introduce a human detection decoder from encoded tokens to extract global features. It can provide a good initialization for the latter keypoint detection, making the training process converge fast. Second, to bring in contextual information near keypoints, we regard pose estimation as a keypoint box detection problem to learn both box positions and contents for each keypoint. A human-to-keypoint detection decoder adopts an interactive learning strategy between human and keypoint features to further enhance global and local feature aggregation. In general, ED-Pose is conceptually simple without post-processing and dense heatmap supervision. It demonstrates its effectiveness and efficiency compared with both two-stage and one-stage methods. Notably, explicit box detection boosts the pose estimation performance by 4.5 AP on COCO and 9.9 AP on CrowdPose. For the first time, as a fully end-to-end framework with a L1 regression loss, ED-Pose surpasses heatmap-based Top-down methods under the same backbone by 1.2 AP on COCO and achieves the state-of-the-art with 76.6 AP on CrowdPose without bells and whistles. Code is available at https://github.com/IDEA-Research/ED-Pose.",,50,0.0,,,NSFC,62106154,National Natural Science Foundation of China
2-s2.0-85143260324,,,,EXPRESSIVENESS AND APPROXIMATION PROPERTIES OF GRAPH NEURAL NETWORKS,cp,Conference Paper,Geerts F.,60012937;60029681;60281481,Universiteit Antwerpen;Pontificia Universidad Católica de Chile;Instituto Milenio Fundamentos de los Datos,Antwerpen;Santiago;Santiago,Belgium;Chile;Chile,2.0,"Geerts, Floris;Reutter, Juan L.",55971823600;34880850800,60012937;60029681-60281481,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNN architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to measure the separation power of GNNs. The crux is to view GNNs as expressions in a procedural tensor language describing the computations in the layers of the GNNs. Then, by a simple analysis of the obtained expressions, in terms of the number of indexes and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. We use tensor language to define Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural extension of MPNNs. Furthermore, the tensor language point of view allows for the derivation of universality results for classes of GNNs in a natural way. Our approach provides a toolbox with which GNN architecture designers can analyze the separation power of their GNNs, without needing to know the intricacies of the WL-tests. We also provide insights in what is needed to boost the separation power of GNNs.",,52,0.0,,,ANID,ICN17 002,Agencia Nacional de Investigación y Desarrollo
2-s2.0-85150387385,,,,EXT5: TOWARDS EXTREME MULTI-TASK SCALING FOR TRANSFER LEARNING,cp,Conference Paper,Aribandi V.,60006191;123683234;129320562;129320814,Google LLC;CMU;DeepMind;Google AI Resident,Mountain View;Philadelphia;;,United States;United States;;,14.0,"Aribandi, Vamsi;Tay, Yi;Schuster, Tal;Rao, Jinfeng;Zheng, Huaixiu Steven;Mehta, Sanket Vaibhav;Zhuang, Honglei;Tran, Vinh Q.;Bahri, Dara;Ni, Jianmo;Gupta, Jai;Hui, Kai;Ruder, Sebastian;Metzler, Donald",57222420079;57193627780;57205163573;57819866800;57358362900;57215715184;49862566500;57222289048;57208443989;56903531900;57209227084;56732503400;57197872736;57204248523,60006191-129320814;60006191;60006191;60006191;60006191;60006191-123683234;60006191;60006191;60006191;60006191;60006191;60006191;60006191-129320562;60006191,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces EXMIX (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using EXMIX, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose EXT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised EXMIX. Via extensive experiments, we show that EXT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of EXMIX. EXT5 also significantly improves sample efficiency while pre-training.",,75,0.0,,,,,
2-s2.0-105018669726,,,,Effect-Invariant Mechanisms for Policy Generalization,ar,Article,Saengkyongam S.,60025778;60025858;60030840;60077572,"University of Michigan, Ann Arbor;ETH Zürich;Københavns Universitet;Harvard John A. Paulson School of Engineering and Applied Sciences",Ann Arbor;Zurich;Copenhagen;Cambridge,United States;Switzerland;Denmark;United States,5.0,"Saengkyongam, Sorawit;Pfister, Niklas;Klasnja, Predrag;Murphy, Susan;Peters, Jonas",57211438005;57194283500;22834990200;7402778674;55705328700,60025858;60030840;60025778;60077572;60025858,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Policy learning is an important component of many real-world learning systems. A major challenge in policy learning is how to adapt efficiently to unseen environments or tasks. Recently, it has been suggested to exploit invariant conditional distributions to learn models that generalize better to unseen environments. However, assuming invariance of entire conditional distributions (which we call full invariance) may be too strong of an assumption in practice. In this paper, we introduce a relaxation of full invariance called effect-invariance (e-invariance for short) and prove that it is sufficient, under suitable assumptions, for zero-shot policy generalization. We also discuss an extension that exploits e-invariance when we have a small sample from the test environment, enabling few-shot policy generalization. Our work does not assume an underlying causal graph or that the data are generated by a structural causal model; instead, we develop testing procedures to test e-invariance directly from data. We present empirical results using simulated data and a mobile health intervention dataset to demonstrate the effectiveness of our approach.",causality | distribution generalization | domain adaptation | invariance | policy learning,0,0.0,,,NNF,R01HL125440,Novo Nordisk Fonden
2-s2.0-85194195917,10.1613/jair.1.14741,,,"Effectiveness of Tree-based Ensembles for Anomaly Discovery: Insights, Batch and Streaming Active Learning",ar,Article,Das S.,60160236,Voiland College of Engineering and Architecture,Pullman,United States,4.0,"Das, Shubhomoy;Islam, Md Rakibul;Jayakodi, Nitthilan Kannappan;Doppa, Janardhan Rao",37035771600;57201773715;57203222491;35324430700,60160236;60160236;60160236;60160236,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,127-170,"Anomaly detection (AD) task corresponds to identifying the true anomalies among a given set of data instances. AD algorithms score the data instances and produce a ranked list of candidate anomalies. The ranked list of anomalies is then analyzed by a human to discover the true anomalies. Ensemble of tree-based anomaly detectors trained in an unsupervised manner and scoring based on uniform weights for ensembles are shown to work well in practice. However, the manual process of analysis can be laborious for the human analyst when the number of false-positives is very high. Therefore, in many real-world AD applications including computer security and fraud prevention, the anomaly detector must be configurable by the human analyst to minimize the effort on false positives. One important way to configure the detector is by providing true labels (nominal or anomaly) for a few instances. Recent work on active anomaly discovery has shown that greedily querying the top-scoring instance and tuning the weights of ensemble detectors based on label feedback allows us to quickly discover true anomalies. This paper makes four main contributions to improve the state-of-the-art in anomaly discovery using tree-based ensembles. First, we provide an important insight that explains the practical successes of unsupervised tree-based ensembles and active learning based on greedy query selection strategy. We also present empirical results on real-world data to support our insights and theoretical analysis to support active learning. Second, we develop a novel batch active learning algorithm to improve the diversity of discovered anomalies based on a formalism called compact description to describe the discovered anomalies. Third, we develop a novel active learning algorithm to handle streaming data setting. We present a data drift detection algorithm that not only detects the drift robustly, but also allows us to take corrective actions to adapt the anomaly detector in a principled manner. Fourth, we present extensive experiments to evaluate our insights and our tree-based active anomaly discovery algorithms in both batch and streaming data settings. Our results show that active learning allows us to discover significantly more anomalies than state-of-the-art unsupervised baselines, our batch active learning algorithm discovers diverse anomalies, and our algorithms under the streaming-data setup are competitive with the batch setup.",,2,1.0,all publisherfullgold,All Open Access Gold,NSF,2021-67021-35344,National Science Foundation
2-s2.0-85156088807,,,,Effectiveness of Vision Transformer for Fast and Accurate Single-Stage Pedestrian Detection,cp,Conference Paper,Yuan J.,60015150;60176024,Imperial College London;UCL Engineering,London;London,United Kingdom;United Kingdom,3.0,"Yuan, Jing;Barmpoutis, Panagiotis;Stathaki, Tania",57665365100;55973201200;7003386658,60015150;60176024;60015150,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Vision transformers have demonstrated remarkable performance on a variety of computer vision tasks. In this paper, we illustrate the effectiveness of the deformable vision transformer for single-stage pedestrian detection and propose a spatial and multi-scale feature enhancement module, which aims to achieve the optimal balance between speed and accuracy. Performance improvement with vision transformers on various commonly used single-stage structures is demonstrated. The design of the proposed architecture is investigated in depth. Comprehensive comparisons with state-of-the-art single- and two-stage detectors on different pedestrian datasets are performed. The proposed detector achieves leading performance on Caltech and Citypersons datasets among single- and two-stage methods using fewer parameters than the baseline. The log-average miss rates for Reasonable and Heavy are decreased to 2.6% and 28.0% on the Caltech test set, and 10.9% and 38.6% on the Citypersons validation set, respectively. The proposed method outperforms SOTA two-stage detectors in the Heavy subset on the Citypersons validation set with considerably faster inference speed.",,15,0.0,,,,,
2-s2.0-85152534945,10.1609/aaai.v37i4.25588,,,Efficient Embeddings of Logical Variables for Query Answering over Incomplete Knowledge Graphs,cp,Conference Paper,Wang D.,60026851;60018308,University of Oxford;Xi'an Jiaotong University,Oxford;Xi'an,United Kingdom;China,3.0,"Wang, Dingmin;Chen, Yeyuan;Grau, Bernardo Cuenca",57222286041;59814991900;22834310900,60026851;60018308;60026851,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,4652-4659,"The problem of answering complex First-order Logic queries over incomplete knowledge graphs is receiving growing attention in the literature. A promising recent approach to this problem has been to exploit neural link predictors, which can be effective in identifying individual missing triples in the incomplete graph, in order to efficiently answer complex queries. A crucial advantage of this approach over other methods is that it does not require example answers to complex queries for training, as it relies only on the availability of a trained link predictor for the knowledge graph at hand. This approach, however, can be computationally expensive during inference, and cannot deal with queries involving negation. In this paper, we propose a novel approach that addresses all of these limitations. Experiments on established benchmark datasets demonstrate that our approach offers superior performance while significantly reducing inference times.",,13,1.0,all publisherfullgold,All Open Access Gold,EPSRC,EP/S019111/1,Engineering and Physical Sciences Research Council
2-s2.0-85142073494,10.1613/JAIR.1.13482,,,Efficient Learning of Interpretable Classification Rules,ar,Article,Ghosh B.,60017161,National University of Singapore,Singapore City,Singapore,3.0,"Ghosh, Bishwamittra;Malioutov, Dmitry;Meel, Kuldeep S.",57215334392;6506041378;55813479500,60017161;60017161;60017161,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,1823-1863,"Machine learning has become omnipresent with applications in various safety-critical domains such as medical, law, and transportation. In these domains, high-stake decisions provided by machine learning necessitate researchers to design interpretable models, where the prediction is understandable to a human. In interpretable machine learning, rule-based classifiers are particularly effective in representing the decision boundary through a set of rules comprising input features. Examples of such classifiers include decision trees, decision lists, and decision sets. The interpretability of rule-based classifiers is in general related to the size of the rules, where smaller rules are considered more interpretable. To learn such a classifier, the brute-force direct approach is to consider an optimization problem that tries to learn the smallest classification rule that has close to maximum accuracy. This optimization problem is computationally intractable due to its combinatorial nature and thus, the problem is not scalable in large datasets. To this end, in this paper we study the triangular relationship among the accuracy, interpretability, and scalability of learning rule-based classifiers. The contribution of this paper is an interpretable learning framework IMLI, that is based on maximum satisfiability (MaxSAT) for synthesizing classification rules expressible in proposition logic. IMLI considers a joint objective function to optimize the accuracy and the interpretability of classification rules and learns an optimal rule by solving an appropriately designed MaxSAT query. Despite the progress of MaxSAT solving in the last decade, the straightforward MaxSAT-based solution cannot scale to practical classification datasets containing thousands to millions of samples. Therefore, we incorporate an efficient incremental learning technique inside the MaxSAT formulation by integrating mini-batch learning and iterative rule-learning. The resulting framework learns a classifier by iteratively covering the training data, wherein in each iteration, it solves a sequence of smaller MaxSAT queries corresponding to each mini-batch. In our experiments, IMLI achieves the best balance among prediction accuracy, interpretability, and scalability. For instance, IMLI attains a competitive prediction accuracy and interpretability w.r.t. existing interpretable classifiers and demonstrates impressive scalability on large datasets where both interpretable and non-interpretable classifiers fail. As an application, we deploy IMLI in learning popular interpretable classifiers such as decision lists and decision sets. The source code is available at https://github.com/meelgroup/mlic.",,10,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,NRF,NRF-NRFFAI1-2019-0004,National Research Foundation Singapore
2-s2.0-85203836475,,,,Efficient Mixture Learning in Black-Box Variational Inference,cp,Conference Paper,Hotti A.,60027272;60002014;113660523;122554298,The University of Edinburgh;The Royal Institute of Technology (KTH);Klarna;Science for Life Laboratory,Edinburgh;Stockholm;Stockholm;Stockholm,United Kingdom;Sweden;Sweden;Sweden,5.0,"Hotti, Alexandra;Kviman, Oskar;Molén, Ricky;Elvira, Víctor;Lagergren, Jens",57343015300;57211170113;57928730400;56369475400;7005123897,60002014-122554298-113660523;60002014-122554298;60002014-122554298;60027272;60002014-122554298,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,18972-18991,"Mixture variational distributions in black box variational inference (BBVI) have demonstrated impressive results in challenging density estimation tasks. However, currently scaling the number of mixture components can lead to a linear increase in the number of learnable parameters and a quadratic increase in inference time due to the evaluation of the evidence lower bound (ELBO). Our two key contributions address these limitations. First, we introduce the novel Multiple Importance Sampling Variational Autoencoder (MISVAE), which amortizes the mapping from input to mixture-parameter space using one-hot encodings. Fortunately, with MISVAE, each additional mixture component incurs a negligible increase in network parameters. Second, we construct two new estimators of the ELBO for mixtures in BBVI, enabling a tremendous reduction in inference time with marginal or even improved impact on performance. Collectively, our contributions enable scalability to hundreds of mixture components and provide superior estimation performance in shorter time, with fewer network parameters compared to previous Mixture VAEs. Experimenting with MISVAE, we achieve astonishing, SOTA results on MNIST. Furthermore, we empirically validate our estimators in other BBVI settings, including Bayesian phylogenetic inference, where we improve inference times for the SOTA mixture model on eight data sets.",,0,0.0,,,SSF,BD15-0043,Stiftelsen för Strategisk Forskning
2-s2.0-105018457846,,,,Efficient Modality Selection in Multimodal Learning,ar,Article,He Y.,60000745;60027950,University of Illinois Urbana-Champaign;Carnegie Mellon University,Urbana;Pittsburgh,United States;United States,5.0,"He, Yifei;Cheng, Runxiang;Balasubramaniam, Gargi;Tsai, Yao Hung Hubert;Zhao, Han",59043958200;57215717360;57952183200;56683697300;57001574800,60000745;60000745;60000745;60027950;60000745,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Multimodal learning aims to learn from data of different modalities by fusing information from heterogeneous sources. Although it is beneficial to learn from more modalities, it is often infeasible to use all available modalities under limited computational resources. Modeling with all available modalities can also be inefficient and unnecessary when information across input modalities overlaps. In this paper, we study the modality selection problem, which aims to select the most useful subset of modalities for learning under a cardinality constraint. To that end, we propose a unified theoretical framework to quantify the learning utility of modalities, and we identify dependence assumptions to flexibly model the heterogeneous nature of multimodal data, which also allows efficient algorithm design. Accordingly, we derive a greedy modality selection algorithm via submodular maximization, which selects the most useful modalities with an optimality guarantee on learning performance. We also connect marginal-contribution-based feature importance scores, such as Shapley value, from the feature selection domain to the context of modality selection, to efficiently compute the importance of individual modality. We demonstrate the efficacy of our theoretical results and modality selection algorithms on 2 synthetic and 4 real-world data sets on a diverse range of multimodal data.",Feature Importance | Modality Selection | Multimodal Learning | Submodular Optimization,13,0.0,,,DARPA,HR00112320012,Defense Advanced Research Projects Agency
2-s2.0-85203795766,,,,Efficient PAC Learnability of Dynamical Systems Over Multilayer Networks,cp,Conference Paper,Qiu Z.,60021918;60279565,"University of Virginia;College of Nanotechnology, Science, and Engineering",Charlottesville;Albany,United States;United States,7.0,"Qiu, Zirou;Adiga, Abhijin;Marathe, Madhav V.;Ravi, S. S.;Rosenkrantz, Daniel J.;Stearns, Richard E.;Vullikanti, Anil",57760842600;24376198600;7005103606;7005265891;7004229410;7006322071;24077196600,60021918;60021918;60021918;60021918-60279565;60021918-60279565;60021918-60279565;60021918,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,41557-41581,"Networked dynamical systems are widely used as formal models of real-world cascading phenomena, such as the spread of diseases and information. Prior research has addressed the problem of learning the behavior of an unknown dynamical system when the underlying network has a single layer. In this work, we study the learnability of dynamical systems over multilayer networks, which are more realistic and challenging. First, we present an efficient PAC learning algorithm with provable guarantees to show that the learner only requires a small number of training examples to infer an unknown system. We further provide a tight analysis of the Natarajan dimension which measures the model complexity. Asymptotically, our bound on the Nararajan dimension is tight for almost all multilayer graphs. The techniques and insights from our work provide the theoretical foundations for future investigations of learning problems for multilayer dynamical systems.",,0,0.0,,,UV,SIF160,University of Virginia
2-s2.0-85149162504,,,,Efficient Structure-preserving Support Tensor Train Machine,ar,Article,Kour K.,60008069;60016025;60171512,"Technische Universität Chemnitz;Max Planck Institute for Dynamics of Complex Technical Systems;University of Bath, Department of Mathematical Sciences",Chemnitz;Magdeburg;Bath,Germany;Germany;United Kingdom,4.0,"Kour, Kirandeep;Benner, Peter;Dolgov, Sergey;Stoll, Martin",57219508412;22733643800;57200590946;8906428600,60016025;60016025;60171512-60008069;60008069,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"An increasing amount of the collected data are high-dimensional multi-way arrays (tensors), and it is crucial for efficient learning algorithms to exploit this tensorial structure as much as possible. The ever present curse of dimensionality for high dimensional data and the loss of structure when vectorizing the data motivates the use of tailored low-rank tensor classification methods. In the presence of small amounts of training data, kernel methods offer an attractive choice as they provide the possibility for a nonlinear decision boundary. We develop the Tensor Train Multi-way Multi-level Kernel (TT-MMK), which combines the simplicity of the Canonical Polyadic decomposition, the classification power of the Dual Structure-preserving Support Vector Machine, and the reliability of the Tensor Train (TT) approximation. We show by experiments that the TT-MMK method is usually more reliable computationally, less sensitive to tuning parameters, and gives higher prediction accuracy in the SVM classification when benchmarked against other state-of-the-art techniques.",Classification | High-dimensional Data | Kernel Approximation | Support Vector Machine | Tensor Decomposition,13,0.0,,,,,
2-s2.0-85208284993,10.1613/jair.1.15024,,,Efficient and Fair Healthcare Rationing,ar,Article,Aziz H.,60028333;60007493,UNSW Sydney;Universität Bonn,Sydney;Bonn,Australia;Germany,2.0,"Aziz, Haris;Brandl, Florian",24824231400;24582982500,60028333;60007493,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,337-358,"The rationing of healthcare resources has emerged as an important issue, which has been discussed by medical experts, policy-makers, and the general public. We consider a rationing problem where medical units are to be allocated to patients. Each unit is reserved for one of several categories, and each category has a priority ranking over the patients. We present a class of allocation rules that respect the priorities, comply with the eligibility requirements, allocate the largest feasible number of units, and do not penalize agents for rising in the priority ranking of a category. The rules characterize all possible allocations that satisfy the first three properties and are polynomial-time computable.",,1,1.0,all publisherfullgold,All Open Access Gold,DFG,EXC-2047,Deutsche Forschungsgemeinschaft
2-s2.0-85137855135,10.24963/ijcai.2022/184,,,Emotion-Controllable Generalized Talking Face Generation,cp,Conference Paper,Sinha S.,60021988;60283297,Indian Institute of Technology Kanpur;TCS Research,Kanpur;Mumbai,India;India,4.0,"Sinha, Sanjana;Biswas, Sandika;Yadav, Ravindra;Bhowmick, Brojeshwar",57192913784;56784993700;57216953549;26422776800,60283297;60283297;60021988;60283297,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1320-1327,"Despite the significant progress in recent years, very few of the AI-based talking face generation methods attempt to render natural emotions. Moreover, the scope of the methods is majorly limited to the characteristics of the training dataset, hence they fail to generalize to arbitrary unseen faces. In this paper, we propose a one-shot facial geometry-aware emotional talking face generation method that can generalize to arbitrary faces. We propose a graph convolutional neural network that uses speech content feature, along with an independent emotion input to generate emotion and speech-induced motion on facial geometry-aware landmark representation. This representation is further used in our optical flow-guided texture generation network for producing the texture. We propose a two-branch texture generation network, with motion and texture branches designed to consider the motion and texture content independently. Compared to the previous emotion talking face methods, our method can adapt to arbitrary faces captured in-the-wild by fine-tuning with only a single image of the target identity in neutral emotion.",,16,1.0,all publisherfree2read,All Open Access Bronze,,,
2-s2.0-85137863020,10.24963/ijcai.2022/672,,,Empirical Bayesian Approaches for Robust Constraint-based Causal Discovery under Insufficient Data,cp,Conference Paper,Cui Z.,60025534;60009509,Rensselaer Polytechnic Institute;Northeast Normal University,Troy;Changchun,United States;China,4.0,"Cui, Zijun;Yin, Naiyu;Wang, Yuru;Ji, Qiang",57221150132;57764944800;49864758200;57202596129,60025534;60025534;60009509;60025534,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4850-4856,"Causal discovery is to learn cause-effect relationships among variables given observational data and is important for many applications. Existing causal discovery methods assume data sufficiency, which may not be the case in many real world datasets. As a result, many existing causal discovery methods can fail under limited data. In this work, we propose Bayesian-augmented frequentist independence tests to improve the performance of constraint-based causal discovery methods under insufficient data: 1) We firstly introduce a Bayesian method to estimate mutual information (MI), based on which we propose a robust MI based independence test; 2) Secondly, we consider the Bayesian estimation of hypothesis likelihood and incorporate it into a well-defined statistical test, resulting in a robust statistical testing based independence test. We apply proposed independence tests to constraint-based causal discovery methods and evaluate the performance on benchmark datasets with insufficient samples. Experiments show significant performance improvement in terms of both accuracy and efficiency over SOTA methods.",,1,1.0,all publisherfree2read,All Open Access Bronze,DARPA,FA8750-17-2-0132,Defense Advanced Research Projects Agency
2-s2.0-85124218098,,,,Empirical Risk Minimization under Random Censorship,ar,Article,Ausset G.,60030553;60116488,Université de Rennes;Institut Polytechnique de Paris,Rennes;Palaiseau,France;France,3.0,"Ausset, Guillaume;Clémençon, Stephan;Portier, François",57219623283;6508167584;55444373400,60116488;60116488;60030553,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"We consider the classic supervised learning problem where a continuous non-negative random label Y (e.g. a random duration) is to be predicted based upon observing a random vector X valued in R<sup>d</sup> with d ≥ 1 by means of a regression rule with minimum least square error. In various applications, ranging from industrial quality control to public health through credit risk analysis for instance, training observations can be right censored, meaning that, rather than on independent copies of (X, Y ), statistical learning relies on a collection of n ≥ 1 independent realizations of the triplet (X, min{Y, C}, δ), where C is a nonnegative random variable with unknown distribution, modelling censoring and δ = I{Y ≤ C} indicates whether the duration is right censored or not. As ignoring censoring in the risk computation may clearly lead to a severe underestimation of the target duration and jeopardize prediction, we consider a plug-in estimate of the true risk based on a Kaplan-Meier estimator of the conditional survival function of the censoring C given X, referred to as Beran risk, in order to perform empirical risk minimization. It is established, under mild conditions, that the learning rate of minimizers of this biased/weighted empirical risk functional is of order (Formula Presented) when ignoring model bias issues inherent to plug-in estimation, as can be attained in absence of censoring. Beyond theoretical results, numerical experiments are presented in order to illustrate the relevance of the approach developed.",Censored data | Empirical risk minimization | Statistical learning theory | Survival data analysis | U-processes,3,0.0,,,,,
2-s2.0-85135725446,10.1609/aaai.v36i4.20300,,,Encoding Multi-Valued Decision Diagram Constraints as Binary Constraint Trees,cp,Conference Paper,Wang R.,60017161,National University of Singapore,Singapore City,Singapore,2.0,"Wang, Ruiwei;Yap, Roland H.C.",57190348024;7003447860,60017161;60017161,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,3850-3858,"Ordered Multi-valued Decision Diagram (MDD) is a compact representation used to model various constraints, such as regular constraints and table constraints. It can be particularly useful for representing ad-hoc problem specific constraints. Many algorithms have been proposed to enforce Generalized Arc Consistency (GAC) on MDD constraints. In this paper, we introduce a new compact representation called Binary Constraint Tree (BCT). We propose tree binary encodings to transform any MDD constraint into a BCT constraint. We also present a specialized algorithm enforcing GAC on the BCT constraint resulting from a MDD constraint. Experimental results on a large set of benchmarks show that the BCT GAC algorithm can significantly outperform state-of-the-art MDD as well as table GAC algorithms.",,7,1.0,all publisherfullgold,All Open Access Gold,NRF,AISG-RP-2018-005,National Research Foundation Singapore
2-s2.0-85147684093,10.1609/aaai.v36i4.20379,,,End-to-End Line Drawing Vectorization,cp,Conference Paper,Liu H.,60002798;60175941,Chinese University of Hong Kong;Saint Francis University,Hong Kong;Hong Kong,Hong Kong;Hong Kong,4.0,"Liu, Hanyuan;Li, Chengze;Liu, Xueting;Wong, Tien Tsin",57961984500;56589218400;55932112600;7403531614,60002798;60175941;60175941;60002798,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,4559-4566,"Vector graphics is broadly used in a variety of forms, such as illustrations, logos, posters, billboards, and printed ads. Despite its broad use, many artists still prefer to draw with pen and paper, which leads to a high demand of converting raster designs into the vector form. In particular, line drawing is a primary art and attracts many research efforts in automatically converting raster line drawings to vector form. However, the existing methods generally adopt a two-step approach, stroke segmentation and vectorization. Without vector guidance, the raster-based stroke segmentation frequently obtains unsatisfying segmentation results, such as over-grouped strokes and broken strokes. In this paper, we make an attempt in proposing an end-to-end vectorization method which directly generates vectorized stroke primitives from raster line drawing in one step. We propose a Transformer-based framework to perform stroke tracing like human does in an automatic stroke-by-stroke way with a novel stroke feature representation and multi-modal supervision to achieve vectorization with high quality and fidelity. Qualitative and quantitative evaluations show that our method achieves state of the art performance.",,9,1.0,all publisherfullgold,All Open Access Gold,CUHK,4055152,Chinese University of Hong Kong
2-s2.0-85174419276,,,,End-to-end Differentiable Clustering with Associative Memories,cp,Conference Paper,Saha B.,60025534;60011048,Rensselaer Polytechnic Institute;IBM Research,Troy;Yorktown Heights,United States;United States,4.0,"Saha, Bishwajit;Krotov, Dmitry;Zaki, Mohammed J.;Ram, Parikshit",58487719700;57147760800;7202243707;36454969300,60025534;60011048;60025534;60011048,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,29649-29670,"Clustering is a widely used unsupervised learning technique involving an intensive discrete optimization problem. Associative Memory models or AMs are differentiable neural networks defining a recursive dynamical system, which have been integrated with various deep learning architectures. We uncover a novel connection between the AM dynamics and the inherent discrete assignment necessary in clustering to propose a novel unconstrained continuous relaxation of the discrete clustering problem, enabling end-to-end differentiable clustering with AM, dubbed ClAM. Leveraging the pattern completion ability of AMs, we further develop a novel self-supervised clustering loss. Our evaluations on varied datasets demonstrate that ClAM benefits from the self-supervision, and significantly improves upon both the traditional Lloyd's k-means algorithm, and more recent continuous clustering relaxations (by upto 60% in terms of the Silhouette Coefficient).",,2,0.0,,,IBM,,International Business Machines Corporation
2-s2.0-85170380294,10.24963/ijcai.2023/226,,,Engineering an Efficient Approximate DNF-Counter,cp,Conference Paper,Soos M.,60017161;60024948;60095887,"National University of Singapore;Indian Statistical Institute, Kolkata;Centre for Quantum Technologies",Singapore City;Kolkata;Singapore City,Singapore;India;Singapore,5.0,"Soos, Mate;Aggarwal, Divesh;Chakraborty, Sourav;Meel, Kuldeep S.;Obremski, Maciej",36715713100;14007771800;35483329200;55813479500;55861204400,60017161;60017161-60095887;60024948;60017161;60095887,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,2031-2038,"Model counting is a fundamental problem in many practical applications, including query evaluation in probabilistic databases and failure-probability estimation of networks. In this work, we focus on a variant of this problem where the underlying formula is expressed in Disjunctive Normal Form (DNF), also known as #DNF. This problem has been shown to be #P-complete, making it often intractable to solve exactly. Much research has therefore focused on obtaining approximate solutions, particularly in the form of (ε, δ) approximations. The primary contribution of this paper is a new approach, called pepin, an approximate #DNF counter that significantly outperforms prior state of the art approaches. Our work is based on the recent breakthrough in the context of union of sets in the streaming model. We demonstrate the effectiveness of our approach through extensive experiments and show that it provides an affirmative answer to the challenge of efficiently computing #DNF.",,3,1.0,all publisherfullgold,All Open Access Gold,NRF,NRF-NRFFAI1-2019-0004,National Research Foundation Singapore
2-s2.0-85189565205,10.1609/aaai.v38i6.28401,,,Enhanced Fine-Grained Motion Diffusion for Text-Driven Human Motion Synthesis,cp,Conference Paper,Wei D.,60010080;127288706,Nanjing University of Science and Technology;Ltd.,Nanjing;Tianjin,China;China,7.0,"Wei, Dong;Sun, Xiaoning;Sun, Huaijiang;Hu, Shengxiang;Li, Bin;Li, Weiqing;Lu, Jianfeng",57204694544;57213269058;24170102000;58836187200;57201269232;54795253600;55547138819,60010080;60010080;60010080;60010080;127288706;60010080;60010080,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,6.0,,5876-5884,"The emergence of text-driven motion synthesis technique provides animators with great potential to create efficiently. However, in most cases, textual expressions only contain general and qualitative motion descriptions, while lack fine depiction and sufficient intensity, leading to the synthesized motions that either (a) semantically compliant but uncontrollable over specific pose details, or (b) even deviates from the provided descriptions, bringing animators with undesired cases. In this paper, we propose DiffKFC, a conditional diffusion model for text-driven motion synthesis with KeyFrames Collaborated, enabling realistic generation with collaborative and efficient dual-level control: coarse guidance at semantic level, with only few keyframes for direct and fine-grained depiction down to body posture level. Unlike existing inference-editing diffusion models that incorporate conditions without training, our conditional diffusion model is explicitly trained and can fully exploit correlations among texts, keyframes and the diffused target frames. To preserve the control capability of discrete and sparse keyframes, we customize dilated mask attention modules where only partial valid tokens participate in local-to-global attention, indicated by the dilated keyframe mask. Additionally, we develop a simple yet effective smoothness prior, which steers the generated frames towards seamless keyframe transitions at inference. Extensive experiments show that our model not only achieves state-of-the-art performance in terms of semantic fidelity, but more importantly, is able to satisfy animator requirements through fine-grained guidance without tedious labor.",,5,1.0,all publisherfullgold,All Open Access Gold,NSFC,62176125,National Natural Science Foundation of China
2-s2.0-85177456386,,,,Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization,cp,Conference Paper,Xu X.,60025272;60017161;60005686;60277287;60118847,The University of Tokyo;National University of Singapore;University of Auckland;RIKEN Center for Advanced Intelligence Project;School of Computing and Information Systems,Tokyo;Singapore City;Auckland;Tokyo;Melbourne,Japan;Singapore;New Zealand;Japan;Australia,5.0,"Xu, Xilie;Zhang, Jingfeng;Liu, Feng;Sugiyama, Masashi;Kankanhalli, Mohan",57219693194;57211755583;57118471000;7402826969;7003629165,60017161;60277287-60005686;60118847;60277287-60025272;60017161,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Adversarial contrastive learning (ACL) is a technique that enhances standard contrastive learning (SCL) by incorporating adversarial data to learn a robust representation that can withstand adversarial attacks and common corruptions without requiring costly annotations. To improve transferability, the existing work introduced the standard invariant regularization (SIR) to impose style-independence property to SCL, which can exempt the impact of nuisance style factors in the standard representation. However, it is unclear how the style-independence property benefits ACL-learned robust representations. In this paper, we leverage the technique of causal reasoning to interpret the ACL and propose adversarial invariant regularization (AIR) to enforce independence from style factors. We regulate the ACL using both SIR and AIR to output the robust representation. Theoretically, we show that AIR implicitly encourages the representational distance between different views of natural data and their adversarial variants to be independent of style factors. Empirically, our experimental results show that invariant regularization significantly improves the performance of state-of-the-art ACL methods in terms of both standard generalization and robustness on downstream tasks. To the best of our knowledge, we are the first to apply causal reasoning to interpret ACL and develop AIR for enhancing ACL-learned robust representations. Our source code is at https://github.com/GodXuxilie/Enhancing_ACL_via_AIR.",,16,0.0,,,NRF,DP230101540,National Research Foundation Singapore
2-s2.0-85204298995,,,,Enhancing Controlled Query Evaluation through Epistemic Policies,cp,Conference Paper,Cima G.,60032350;60005254,Sapienza Università di Roma;Università degli Studi di Bergamo,Rome;Bergamo,Italy;Italy,5.0,"Cima, Gianluca;Lembo, Domenico;Marconi, Lorenzo;Rosati, Riccardo;Savo, Domenico Fabio",57195406965;10239062800;57313400000;36004468500;36609120400,60032350;60032350;60032350;60032350;60005254,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3307-3314,"In this paper, we propose the use of epistemic dependencies to express data protection policies in Controlled Query Evaluation (CQE), which is a form of confidentiality-preserving query answering over ontologies and databases. The resulting policy language goes significantly beyond those proposed in the literature on CQE so far, allowing for very rich and practically interesting forms of data protection rules. We show the expressive abilities of our framework and study the data complexity of CQE for (unions of) conjunctive queries when ontologies are specified in the Description Logic DL-Lite<inf>R</inf>. Interestingly, while we show that the problem is in general intractable, we prove tractability for the case of acyclic epistemic dependencies by providing a suitable query rewriting algorithm. The latter result paves the way towards the implementation and practical application of this new approach to CQE.",,3,0.0,,,MIUR,CUP B53C22006700001,"Ministero dell’Istruzione, dell’Università e della Ricerca"
2-s2.0-85204310186,,,,Enhancing Cross-modal Completion and Alignment for Unsupervised Incomplete Text-to-Image Person Retrieval,cp,Conference Paper,Gong T.,60021666;60010080,Nanjing University of Aeronautics and Astronautics;Nanjing University of Science and Technology,Nanjing;Nanjing,China;China,3.0,"Gong, Tiantian;Wang, Junsheng;Zhang, Liyan",58075084000;58158681200;57216791964,60021666;60010080;60021666,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,794-802,"Traditional text-image person retrieval methods heavily rely on fully matched and identity-annotated multimodal data, representing an ideal yet limited scenario. The issues of handling incomplete multimodal data and the complexities of labeling multimodal data are common challenges encountered in real-world applications. In response to these challenges encountered, we consider a more robust and pragmatic setting termed unsupervised incomplete text-image person retrieval, where person images and text descriptions are not fully matched and lack the supervision of identity labels. To tackle these two problems, we propose the Enhancing Cross-modal Completion and Alignment (ECCA) method. Specifically, we propose a feature-level cross-modal completion strategy for incomplete data. This approach leverages the available cross-modal high semantic similarity features to construct relational graphs for missing modal data, which can generate more reliable completion features. Additionally, to address the cross-modal matching ambiguity, we propose weighted inter-instance granularity alignment as well as enhanced prototype-wise granularity alignment modules that can map semantically similar image-text pairs more compact in the common embedding space. Extensive experiments on public datasets, fully demonstrate the consistent superiority of our method over SOTA text-image person retrieval methods.",,3,0.0,,,NSFC,62172212,National Natural Science Foundation of China
2-s2.0-85137936559,10.24963/ijcai.2022/333,,,Enhancing Sequential Recommendation with Graph Contrastive Learning,cp,Conference Paper,Zhang Y.,60031031;60005510;60078616;60118460,Shandong University;Nanyang Technological University;School of Computer Science and Engineering;Alibaba Group Holding Limited,Jinan;Singapore City;Singapore City;Hangzhou,China;Singapore;Singapore;China,8.0,"Zhang, Yixin;Liu, Yong;Xu, Yonghui;Xiong, Hao;Lei, Chenyi;He, Wei;Cui, Lizhen;Miao, Chunyan",57214731231;56155393100;55537371300;57733521300;56377228300;57204699506;55632676200;8850060600,60031031;60005510;60031031;60118460;60118460;60031031;60031031;60005510-60078616,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2398-2405,"The sequential recommendation systems capture users' dynamic behavior patterns to predict their next interaction behaviors. Most existing sequential recommendation methods only exploit the local context information of an individual interaction sequence and learn model parameters solely based on the item prediction loss. Thus, they usually fail to learn appropriate sequence representations. This paper proposes a novel recommendation framework, namely Graph Contrastive Learning for Sequential Recommendation (GCL4SR). Specifically, GCL4SR employs a Weighted Item Transition Graph (WITG), built based on interaction sequences of all users, to provide global context information for each interaction and weaken the noise information in the sequence data. Moreover, GCL4SR uses subgraphs of WITG to augment the representation of each interaction sequence. Two auxiliary learning objectives have also been proposed to maximize the consistency between augmented representations induced by the same interaction sequence on WITG, and minimize the difference between the representations augmented by the global context on WITG and the local representation of the original sequence. Extensive experiments on real-world datasets demonstrate that GCL4SR consistently outperforms state-of-the-art sequential recommendation methods.",,53,1.0,all publisherfree2read,All Open Access Bronze,NTU,2021CXGC010108,Key Technology Research and Development Program of Shandong
2-s2.0-85203841626,,,,Enhancing Size Generalization in Graph Neural Networks through Disentangled Representation Learning,cp,Conference Paper,Huang Z.,60030612;60157272;60136817,"University of California, San Diego;Virginia Tech College of Engineering;Department of Computer Science",La Jolla;Blacksburg;Hanover,United States;United States;United States,4.0,"Huang, Zheng;Yang, Qihui;Zhou, Dawei;Yan, Yujun",59204195200;59203618200;57001852100;57202435466,60136817;60030612;60157272;60136817,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,20365-20381,"Although most graph neural networks (GNNs) can operate on graphs of any size, their classification performance often declines on graphs larger than those encountered during training. Existing methods insufficiently address the removal of size information from graph representations, resulting in sub-optimal performance and reliance on backbone models. In response, we propose DISGEN, a novel and model-agnostic framework designed to disentangle size factors from graph representations. DISGEN employs size- and task-invariant augmentations and introduces a decoupling loss that minimizes shared information in hidden representations, with theoretical guarantees for its effectiveness. Our empirical results show that DISGEN outperforms the state-of-the-art models by up to 6% on real-world datasets, underscoring its effectiveness in enhancing the size generalizability of GNNs. Our codes are available at: https://github.com/GraphmindDartmouth/DISGEN.",,2,0.0,,,,,
2-s2.0-85147716554,10.1609/aaai.v36i9.21237,,,Entropy Eestimation via Normalizing Flow,cp,Conference Paper,Ao Z.,60019702,University of Birmingham,Birmingham,United Kingdom,2.0,"Ao, Ziqiao;Li, Jinglai",57219646630;24741115300,60019702;60019702,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,9990-9998,"Entropy estimation is an important problem in information theory and statistical science. Many popular entropy estimators suffer from fast growing estimation bias with respect to dimensionality, rendering them unsuitable for high-dimensional problems. In this work we propose a transform-based method for high-dimensional entropy estimation, which consists of the following two main ingredients. First by modifying the k-NN based entropy estimator, we propose a new estimator which enjoys small estimation bias for samples that are close to a uniform distribution. Second we design a normalizing flow based mapping that pushes samples toward a uniform distribution, and the relation between the entropy of the original samples and the transformed ones is also derived. As a result the entropy of a given set of samples is estimated by first transforming them toward a uniform distribution and then applying the proposed estimator to the transformed samples. Numerical experiments demonstrate the effectiveness of the method for high-dimensional entropy estimation problems.",,7,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,CSC,,China Scholarship Council
2-s2.0-85137875472,10.24963/ijcai.2022/84,,,Environment Design for Biased Decision Makers,cp,Conference Paper,Yu G.,60010261,Washington University in St. Louis,St. Louis,United States,2.0,"Yu, Guanghui;Ho, Chien Ju",57204467475;35174650400,60010261;60010261,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,592-598,"We study the environment design problem for biased decision makers. In an environment design problem, an informed principal aims to update the decision making environment to influence the decisions made by the agent. This problem is ubiquitous in various domains, e.g., a social networking platform might want to update its website to encourage more user engagement. In this work, we focus on the scenario in which the agent might exhibit biases in decision making. We relax the common assumption that the agent is rational and aim to incorporate models of biased agents in environment design. We formulate the environment design problem under the Markov decision process (MDP) and incorporate common models of biased agents through introducing general time-discounting functions. We then formalize the environment design problem as constrained optimization problems and propose corresponding algorithms. We conduct both simulations and real human-subject experiments with workers recruited from Amazon Mechanical Turk to evaluate our proposed algorithms.",,7,1.0,all publisherfree2read,All Open Access Bronze,ONR,N00014-20-1-2240,Office of Naval Research
2-s2.0-85166194314,10.1613/jair.1.14625,,,Equivalence in Argumentation Frameworks with a Claim-centric View: Classical Results with Novel Ingredients,ar,Article,Baumann R.,60008042;60018163,Universität Leipzig;TU Wien,Leipzig;Vienna,Germany;Austria,3.0,"Baumann, Ringo;Rapberger, Anna;Ulbricht, Markus",36602190800;57211169022;57214569800,60008042;60018163;60008042,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,891-948,"A common feature of non-monotonic logics is that the classical notion of equivalence does not preserve the intended meaning in light of additional information. Consequently, the term strong equivalence was coined in the literature and thoroughly investigated. In the present paper, the knowledge representation formalism under consideration is claimaugmented argumentation frameworks (CAFs) which provide a formal basis to analyze conclusion-oriented problems in argumentation by adapting a claim-focused perspective. CAFs extend Dung AFs by associating a claim to each argument representing its conclusion. In this paper, we investigate both ordinary and strong equivalence in CAFs. Thereby, we take the fact into account that one might either be interested in the actual arguments or their claims only. The former point of view naturally yields an extension of strong equivalence for AFs to the claim-based setting while the latter gives rise to a novel equivalence notion which is genuine for CAFs. We tailor, examine and compare these notions and obtain a comprehensive study of this matter for CAFs. We conclude by investigating the computational complexity of naturally arising decision problems.",,5,1.0,all publisherfullgold,All Open Access Gold,BMBF,01/S18026A-F,Bundesministerium für Bildung und Forschung
2-s2.0-85145961098,,,,Equivariance versus Augmentation for Spherical Images,cp,Conference Paper,Gerken J.E.,60016437;60011604;60000990;60031040;60305574;127393224,Göteborgs Universitet;Technische Universität Berlin;Chalmers University of Technology;Umeå Universitet;Zenseact AB;BIFOLD - Berlin Institute for the Foundations of Learning and Data,Gothenburg;Berlin;Gothenburg;Umea;Gothenburg;Berlin,Sweden;Germany;Sweden;Sweden;Sweden;Germany,6.0,"Gerken, Jan E.;Carlsson, Oscar;Linander, Hampus;Ohlsson, Fredrik;Petersson, Christoffer;Persson, Daniel",57205418991;59897050900;55012739400;57200382785;24468663700;57202063321,60000990-60011604-127393224;60000990;60016437;60031040;60000990-60305574;60000990,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,7404-7421,"We analyze the role of rotational equivariance in convolutional neural networks (CNNs) applied to spherical images. We compare the performance of the group equivariant networks known as S2CNNs and standard non-equivariant CNNs trained with an increasing amount of data augmentation. The chosen architectures can be considered baseline references for the respective design paradigms. Our models are trained and evaluated on single or multiple items from the MNIST or FashionMNIST dataset projected onto the sphere. For the task of image classification, which is inherently rotationally invariant, we find that by considerably increasing the amount of data augmentation and the size of the networks, it is possible for the standard CNNs to reach at least the same performance as the equivariant network. In contrast, for the inherently equivariant task of semantic segmentation, the non-equivariant networks are consistently outperformed by the equivariant networks with significantly fewer parameters. We also analyze and compare the inference latency and training times of the different networks, enabling detailed tradeoff considerations between equivariant architectures and data augmentation for practical problems. The equivariant spherical networks used in the experiments are available at https://github.com/JanEGerken/sem_seg_s2cnn.",,21,0.0,,,BMBF,01GQ1115,Bundesministerium für Bildung und Forschung
2-s2.0-85194278092,10.1613/jair.1.15326,,,Estimating Agent Skill in Continuous Action Domains,ar,Article,Archibald C.,60006832;60001526,Brigham Young University;Mississippi State University,Provo;Mississippi State,United States;United States,2.0,"Archibald, Christopher;Nieves-Rivera, Delma",58639282200;57204158025,60006832;60001526,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,27-86,"Actions in most real-world continuous domains cannot be executed exactly. An agent’s performance in these domains is influenced by two critical factors: the ability to select effective actions (decision-making skill), and how precisely it can execute those selected actions (execution skill). This article addresses the problem of estimating the execution and decision-making skill of an agent, given observations. Several execution skill estimation methods are presented, each of which utilize different information from the observations and make assumptions about the agent’s decision-making ability. A final novel method forgoes these assumptions about decision-making and instead estimates the execution and decision-making skills simultaneously under a single Bayesian framework. Experimental results in several domains evaluate the estimation accuracy of the estimators, especially focusing on how robust they are as agents and their decision-making methods are varied. These results demonstrate that reasoning about both types of skill together significantly improves the robustness and accuracy of execution skill estimation. A case study is presented using the proposed methods to estimate the skill of Major League Baseball pitchers, demonstrating how these methods can be applied to real-world data sources.",,3,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85174421677,,,,Estimating Joint Treatment Effects by Combining Multiple Experiments,cp,Conference Paper,Jung Y.,60030162;60009254;60004354,Columbia University;Purdue University;Iowa State University,New York;West Lafayette;Ames,United States;United States;United States,3.0,"Jung, Yonghan;Tian, Jin;Bareinboim, Elias",57204517144;56314888000;24075591200,60009254;60004354;60030162,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,15451-15527,"Estimating the effects of multi-dimensional treatments (i.e., joint treatment effects) is critical in many data-intensive domains, including genetics and drug evaluation. The main challenges for studying the joint treatment effects include the need for large sample sizes to explore different treatment combinations as well as potentially unsafe treatment interactions. In this paper, we develop machinery for estimating joint treatment effects by combining data from multiple experimental datasets. In particular, first, we develop new identification conditions for determining whether a joint treatment effect can be computed in terms of multiple interventional distributions under various scenarios. Further, we develop estimators with statistically appealing properties, including consistency and robustness to model misspecification and slow convergence. Finally, we perform simulation studies, which corroborate the effectiveness of the proposed methods.",,3,0.0,,,NSF,IIS-2231797,National Science Foundation
2-s2.0-105018667554,,,,Estimating the Minimizer and the Minimum Value of a Regression Function under Passive Design,ar,Article,Akhavan A.,60029859;60022532,EURECOM;ENSAE Paris,Sophia Antipolis;Palaiseau,France;France,3.0,"Akhavan, Arya;Gogolashvili, Davit;Tsybakov, Alexandre B.",57219733008;57953790000;7004200599,60022532;60029859;60022532,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"We propose a new method for estimating the minimizer x<sup>∗</sup> and the minimum value f<sup>∗</sup> of a smooth and strongly convex regression function f from the observations contaminated by random noise. Our estimator z<inf>n</inf> of the minimizer x<sup>∗</sup> is based on a version of the projected gradient descent with the gradient estimated by a regularized local polynomial algorithm. Next, we propose a two-stage procedure for estimation of the minimum value f<sup>∗</sup> of regression function f. At the first stage, we construct an accurate enough estimator of x<sup>∗</sup>, which can be, for example, z<inf>n</inf>. At the second stage, we estimate the function value at the point obtained in the first stage using a rate optimal nonparametric procedure. We derive non-asymptotic upper bounds for the quadratic risk and optimization risk of z<inf>n</inf>, and for the risk of estimating f<sup>∗</sup>. We establish minimax lower bounds showing that, under certain choice of parameters, the proposed algorithms achieve the minimax optimal rates of convergence on the class of smooth and strongly convex functions.",Local polynomial estimator | Minimax optimality | Nonparametric regression | Passive design | Stochastic optimization,0,0.0,,,ANR,813999,Agence Nationale de la Recherche
2-s2.0-85137880629,10.24963/ijcai.2022/492,,,Estimation and Comparison of Linear Regions for ReLU Networks,cp,Conference Paper,Wang Y.,128568262,Garena,Singapore City,Singapore,1.0,"Wang, Yuan",57888300000,128568262,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3544-3550,"We study the relationship between the arrangement of neurons and the complexity of the ReLU-activated neural networks measured by the number of linear regions. More specifically, we provide both theoretical and empirical evidence for the point of view that shallow networks tend to have higher complexity than deep ones when the total number of neurons is fixed. In the theoretical part, we prove that this is the case for networks whose neurons in the hidden layers are arranged in the forms of 1 × 2n, 2 × n and n × 2; in the empirical part, we implement an algorithm that precisely tracks (hence counts) all the linear regions, and run it on networks with various structures. Although the complexity of the algorithm is quite high, we verify that the problem of calculating the number of linear regions of a ReLU network is itself NP-hard. So currently there is no surprisingly efficient way to solve it. Roughly speaking, in the algorithm we divide the linear regions into subregions called the “activation regions”, which are convex and easy to propagate through the network. The relationship between the number of the linear regions and that of the activation regions is also discussed.",,9,1.0,all publisherfree2read,All Open Access Bronze,,,
2-s2.0-85147660108,,,,Estimation and inference on high-dimensional individualized treatment rule in observational data using split-and-pooled de-correlated score,ar,Article,Liang M.,60032179;60013959;60279163;60003533;60278093,University of Wisconsin-Madison;University of Florida;Fred Hutchinson Cancer Center;Sookmyung Women's University;Cornell Ann S. Bowers College of Computing and Information Science,Madison;Gainesville;Seattle;Seoul;Ithaca,United States;United States;United States;South Korea;United States,5.0,"Liang, Muxuan;Choi, Young Geun;Ning, Yang;Smith, Maureen A.;Zhao, Ying Qi",56779426100;55790689000;55604476900;10142115300;55499191800,60013959;60003533;60278093;60032179;60279163,2022-07-01,1 July 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,A99,,"With the increasing adoption of electronic health records, there is an increasing interest in developing individualized treatment rules, which recommend treatments according to patients’ characteristics, from large observational data. However, there is a lack of valid inference procedures for such rules developed from this type of data in the presence of high-dimensional covariates. In this work, we develop a penalized doubly robust method to estimate the optimal individualized treatment rule from high-dimensional data. We propose a split-and-pooled de-correlated score to construct hypothesis tests and confidence intervals. Our proposal adopts the data splitting to conquer the slow convergence rate of nuisance parameter estimations, such as non-parametric methods for outcome regression or propensity models. We establish the limiting distributions of the split-and-pooled de-correlated score test and the corresponding one-step estimator in high-dimensional setting. Simulation and real data analysis are conducted to demonstrate the superiority of the proposed method.",double-robustness | high-dimensional inference | Individualized treatment rule | precision medicine | semiparametric inference,3,0.0,,,NIH,R01 DK108073,National Institutes of Health
2-s2.0-85163724712,,,,Euler-Lagrange Analysis of Generative Adversarial Networks,ar,Article,Asokan S.,60014097,Indian Institute of Science,Bengaluru,India,2.0,"Asokan, Siddarth;Seelamantula, Chandra Sekhar",57224863605;24779785600,60014097;60014097,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"We consider Generative Adversarial Networks (GANs) and address the underlying functional optimization problem ab initio within a variational setting. Strictly speaking, the optimization of the generator and discriminator functions must be carried out in accordance with the Euler-Lagrange conditions, which become particularly relevant in scenarios where the optimization cost involves regularizers comprising the derivatives of these functions. Considering Wasserstein GANs (WGAN) with a gradient-norm penalty, we show that the optimal discriminator is the solution to a Poisson differential equation. In principle, the optimal discriminator can be obtained in closed form without having to train a neural network. We illustrate this by employing a Fourier-series approximation to solve the Poisson differential equation. Experimental results based on synthesized Gaussian data demonstrate superior convergence behavior of the proposed approach in comparison with the baseline WGAN variants that employ weight-clipping, gradient or Lipschitz penalties on the discriminator on low-dimensional data. We also analyze the truncation error of the Fourier-series approximation and the estimation error of the Fourier coefficients in a high-dimensional setting. We demonstrate applications to real-world images considering latent-space prior matching in Wasserstein autoencoders and present performance comparisons on benchmark datasets such as MNIST, SVHN, CelebA, CIFAR-10, and Ukiyo-E. We demonstrate that the proposed approach achieves comparable reconstruction error and Fréchet inception distance with faster convergence and up to two-fold improvement in image sharpness.",Calculus of variations | Euler-Lagrange conditions | Fourier-series approximation | Generative adversarial networks | Wasserstein autoencoder,4,0.0,,,SERB,2021-2022,Science and Engineering Research Board
2-s2.0-85143719995,10.1609/aaai.v36i4.20342,,,Event-Aware Multimodal Mobility Nowcasting,cp,Conference Paper,Wang Z.,60025272;60011362,The University of Tokyo;RMIT University,Tokyo;Melbourne,Japan;Australia,6.0,"Wang, Zhaonan;Jiang, Renhe;Xue, Hao;Salim, Flora D.;Song, Xuan;Shibasaki, Ryosuke",57210640460;57202284492;57202435857;7801420000;7402269519;7003648498,60025272-60011362;60025272;60011362;60011362;60025272;60025272,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,4228-4236,"As a decisive part in the success of Mobility-as-a-Service (MaaS), spatio-temporal predictive modeling for crowd movements is a challenging task particularly considering scenarios where societal events drive mobility behavior deviated from the normality. While tremendous progress has been made to model high-level spatio-temporal regularities with deep learning, most, if not all of the existing methods are neither aware of the dynamic interactions among multiple transport modes nor adaptive to unprecedented volatility brought by potential societal events. In this paper, we are therefore motivated to improve the canonical spatio-temporal network (ST-Net) from two perspectives: (1) design a heterogeneous mobility information network (HMIN) to explicitly represent intermodality in multimodal mobility; (2) propose a memory-augmented dynamic filter generator (MDFG) to generate sequence-specific parameters in an on-the-fly fashion for various scenarios. The enhanced event-aware spatio-temporal network, namely EAST-Net, is evaluated on several real-world datasets with a wide variety and coverage of societal events. Both quantitative and qualitative experimental results verify the superiority of our approach compared with the state-of-the-art baselines. Code and data are published on https://github.com/underdoc-wang/EAST-Net.",,40,1.0,all publisherfullgold,All Open Access Gold,ARC,DP190101485,Australian Research Council
2-s2.0-85141669751,10.1609/aaai.v36i6.20575,,,Evidential Neighborhood Contrastive Learning for Universal Domain Adaptation,cp,Conference Paper,Chen L.,60014966;132138814,Peking University;Huawei Technologies,Beijing;,China;,5.0,"Chen, Liang;Lou, Yihang;He, Jianzhong;Bai, Tao;Deng, Minghua",57216623640;57194419125;57221175262;57232290700;7202079431,60014966;132138814;132138814;132138814;60014966,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,6258-6267,"Universal domain adaptation (UniDA) aims to transfer the knowledge learned from a labeled source domain to an unlabeled target domain without any constraints on the label sets. However, domain shift and category shift make UniDA extremely challenging, mainly attributed to the requirement of identifying both shared “known” samples and private “unknown” samples. Previous methods barely exploit the intrinsic manifold structure relationship between two domains for feature alignment, and they rely on the softmax-based scores with class competition nature to detect underlying “unknown” samples. Therefore, in this paper, we propose a novel evidenTial Neighborhood conTrastive learning framework called TNT to address these issues. Specifically, TNT first proposes a new domain alignment principle: semantically consistent samples should be geometrically adjacent to each other, whether within or across domains. From this criterion, a cross domain multi-sample contrastive loss based on mutual nearest neighbors is designed to achieve common category matching and private category separation. Second, toward accurate “unknown” sample detection, TNT introduces a class competition-free uncertainty score from the perspective of evidential deep learning. Instead of setting a single threshold, TNT learns a category-aware heterogeneous threshold vector to reject diverse “unknown” samples. Extensive experiments on three benchmarks demonstrate that TNT significantly outperforms previous state-of-the-art UniDA methods.",,45,1.0,all publisherfullgold,All Open Access Gold,NSFC,12126305,National Natural Science Foundation of China
2-s2.0-85187418147,10.1609/aaai.v38i17.29936,,,ExpeL: LLM Agents Are Experiential Learners,cp,Conference Paper,Zhao A.,60025278,Tsinghua University,Beijing,China,6.0,"Zhao, Andrew;Huang, Daniel;Xu, Quentin;Lin, Matthieu;Liu, Yong Jin;Huang, Gao",57749011000;58588570000;58587838500;57221840371;35933567100;7403425368,60025278;60025278;60025278;60025278;60025278;60025278,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,17.0,,19632-19642,"The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model’s generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.",,128,1.0,all publisherfullgold,All Open Access Gold,NKRDPC,2022ZD0114900,"Guoqiang Institute, Tsinghua University"
2-s2.0-85208282875,10.1613/jair.1.16299,,,Expected 1.x Makespan-Optimal Multi-Agent Path Finding on Grid Graphs in Low Polynomial Time,ar,Article,Guo T.,60119141,Rutgers University–New Brunswick,New Brunswick,United States,2.0,"Guo, Teng;Yu, Jingjin",57221800026;55745955900,60119141;60119141,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,443-479,"Multi-Agent Path Finding (MAPF) is NP-hard to solve optimally, even on graphs, suggesting no polynomial-time algorithms can compute exact optimal solutions for them. This raises a natural question: How optimal can polynomial-time algorithms reach? Whereas algorithms for computing constant-factor optimal solutions have been developed, the constant factor is generally very large, limiting their application potential. In this work, among other breakthroughs, we propose the first low-polynomial-time MAPF algorithms delivering 1-1.5 (resp., 1-1.67) asymptotic makespan optimality guarantees for 2D (resp., 3D) grids for random instances at a very high 1/3 agent density, with high probability. Moreover, when regularly distributed obstacles are introduced, our methods experience no performance degradation. These methods generalize to support 100% agent density. Regardless of the dimensionality and density, our high-quality methods are enabled by a unique hierarchical integration of two key building blocks. At the higher level, we apply the labeled Grid Rearrangement Algorithm (GRA), capable of performing efficient reconfiguration on grids through row/column shuffles. At the lower level, we devise novel methods that efficiently simulate row/column shuffles returned by GRA. Our implementations of GRA-based algorithms are highly effective in extensive numerical evaluations, demonstrating excellent scalability compared to other SOTA methods. For example, in 3D settings, GRA-based algorithms readily scale to grids with over 370, 000 vertices and over 120, 000 agents and consistently achieve conservative makespan optimality approaching 1.5, as predicted by our theoretical analysis.",,2,1.0,all publisherfullgold,All Open Access Gold,NSF,IIS-1845888,National Science Foundation
2-s2.0-85194225808,10.1613/jair.1.15642,,,Experimental Design of Extractive Question-Answering Systems: Influence of Error Scores and Answer Length,ar,Article,Farea A.,60011170;60072795,Tampere University;Taiz University,Tampere;Taiz,Finland;Yemen,2.0,"Farea, Amer;Emmert-Streib, Frank",57913000200;15057742200,60011170-60072795;60011170,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,87-125,"Question-answering (QA) systems are becoming more and more important because they enable human-computer communication in a natural language. In recent years, significant progress has been made with transformer-based models that leverage deep learning in combination with large amounts of text data. However, a significant challenge with QA systems lies in their complexity rooted in the ambiguity and flexibility of a natural language. This makes even their evaluation a formidable task. For this reason, in this study, we focus on the evaluation of extractive question-answering (EQA) systems by conducting a large-scale analysis of distilBERT using benchmark data provided by the Stanford Question Answering Dataset (SQuAD). Specifically, the main objectives of this paper are fourfold. First, we study the influence of the answer length on the performance and we demonstrate that there is an inverse correlation between both. Second, we study differences in exact match (EM) measures because there are different definitions commonly used in the literature. As a result, we find that despite the fact that all of those measures are named”exact match” these measures are actually different from each other. Third, we study the practical relevance of these different definitions because due to the ambivalent meaning of”exact match” in the literature, it is often unclear if reported improvements are genuine or only due to a change in the exact match measure. Importantly, our results show that differences between differently defined EM measures are in the same order of magnitude as reported differences found in the literature. This raises concerns about the robustness of reported results. Fourth, we provide guidelines to improve the experimental design of general EQA studies, aiming to enhance performance evaluation and minimize the potential for spurious results.",,5,1.0,all publisherfullgold,All Open Access Gold,IIE,,Institute of International Education
2-s2.0-85124994270,10.1613/JAIR.1.13200,,,Explainable Deep Learning: A Field Guide for the Uninitiated,ar,Article,Ras G.,60102054;60076757;101288518,"Donders Institute for Brain, Cognition and Behaviour;Amazon.com, Inc.;LLC",Nijmegen;Seattle;Dayton,Netherlands;United States;United States,4.0,"Ras, Gabriëlle;Xie, Ning;van Gerven, Marcel;Doran, Derek",57211804111;57194693905;8925642800;24801978800,60102054;60076757;60102054;101288518,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,329-396,"Deep neural networks (DNNs) are an indispensable machine learning tool despite the difficulty of diagnosing what aspects of a model’s input drive its decisions. In countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that DNN decisions are driven by aspects appropriate in the context of its use. The development of methods and studies enabling the explanation of a DNN’s decisions has thus blossomed into an active and broad area of research. The field’s complexity is exacerbated by competing definitions of what it means “to explain” the actions of a DNN and to evaluate an approach’s “ability to explain”. This article offers a field guide to explore the space of explainable deep learning for those in the AI/ML field who are uninitiated. The field guide: i) Introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) discusses user-oriented explanation design and future directions. We hope the guide is seen as a starting point for those embarking on this research field.",,260,1.0,all publisherfullgold,All Open Access Gold,DOD,17192,U.S. Department of Defense
2-s2.0-85170364519,10.24963/ijcai.2023/7,,,Explainable Multi-Agent Reinforcement Learning for Temporal Queries,cp,Conference Paper,Boggess K.,60021918;60002765,University of Virginia;Bar-Ilan University,Charlottesville;Ramat Gan,United States;Israel,3.0,"Boggess, Kayla;Kraus, Sarit;Feng, Lu",57219636864;7103086512;57200501105,60021918;60002765;60021918,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,55-63,"As multi-agent reinforcement learning (MARL) systems are increasingly deployed throughout society, it is imperative yet challenging for users to understand the emergent behaviors of MARL agents in complex environments. This work presents an approach for generating policy-level contrastive explanations for MARL to answer a temporal user query, which specifies a sequence of tasks completed by agents with possible cooperation. The proposed approach encodes the temporal query as a PCTL<sup>*</sup> logic formula and checks if the query is feasible under a given MARL policy via probabilistic model checking. Such explanations can help reconcile discrepancies between the actual and anticipated multi-agent behaviors. The proposed approach also generates correct and complete explanations to pinpoint reasons that make a user query infeasible. We have successfully applied the proposed approach to four benchmark MARL domains (up to 9 agents in one domain). Moreover, the results of a user study show that the generated explanations significantly improve user performance and satisfaction.",,11,1.0,all publisherfullgold,All Open Access Gold,EU,CCF-1942836,Emory University
2-s2.0-85161984456,10.1613/JAIR.1.14580,,,Exploiting Action Impact Regularity and Exogenous State Variables for Offline Reinforcement Learning,ar,Article,Liu V.,60030835,University of Alberta,Edmonton,Canada,3.0,"Liu, Vincent;Wright, James R.;White, Martha",57218925303;57206755702;55392368300,60030835;60030835;60030835,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,,77.0,,71-101,"Offline reinforcement learning—learning a policy from a batch of data—is known to be hard for general MDPs. These results motivate the need to look at specific classes of MDPs where offline reinforcement learning might be feasible. In this work, we explore a restricted class of MDPs to obtain guarantees for offline reinforcement learning. The key property, which we call Action Impact Regularity (AIR), is that actions primarily impact a part of the state (an endogenous component) and have limited impact on the remaining part of the state (an exogenous component). AIR is a strong assumption, but it nonetheless holds in a number of real-world domains including financial markets. We discuss algorithms that exploit the AIR property, and provide a theoretical analysis for an algorithm based on Fitted-Q Iteration. Finally, we demonstrate that the algorithm outperforms existing offline reinforcement learning algorithms across different data collection policies in simulated and real world environments where the regularity holds.",,1,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85196053620,10.1613/jair.1.14947,,,Exploiting Contextual Target Attributes for Target Sentiment Classification,ar,Article,Xing B.,60018273;60023932;60025710;60078616;60004678,"University of Science and Technology Beijing;University of Technology Sydney;Agency for Science, Technology and Research, Singapore;School of Computer Science and Engineering;A-Star, Institute of High Performance Computing",Beijing;Sydney;Singapore City;Singapore City;Singapore City,China;Australia;Singapore;Singapore;Singapore,2.0,"Xing, Bowen;Tsang, Ivor W.",57211753171;7004570429,60018273;60025710-60004678-60078616-60023932,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,419-439,"In the past few years, pre-trained language models (PTLMs) have brought significant improvements to target sentiment classification (TSC). Existing PTLM-based models can be categorized into two groups: 1) fine-tuning-based models that adopt PTLM as the context encoder; 2) prompting-based models that transfer the classification task to the text/word generation task. Despite the improvements achieved by these models, we argue that they have their respective limitations. For fine-tuning-based models, they cannot make the best use of the PTLMs’ strong language modeling ability because the pre-train task and downstream fine-tuning task are not consistent. For prompting-based models, although they can sufficiently leverage the language modeling ability, it is hard to explicitly model the target-context interactions, which are widely realized as a crucial point of this task. In this paper, we present a new perspective of leveraging PTLM for TSC: simultaneously leveraging the merits of both language modeling and explicit target-context interactions via contextual target attributes. Specifically, we design the domain- and target-constrained cloze test, which can leverage the PTLMs’ strong language modeling ability to generate the given target’s attributes pertaining to the review context. The attributes contain the background and property information of the target, which can help to enrich the semantics of the review context and the target. To exploit the attributes for tackling TSC, we first construct a heterogeneous information graph by treating the attributes as nodes and combining them with (1) the syntax graph automatically produced by the off-the-shelf dependency parser and (2) the semantics graph of the review context, which is derived from the self-attention mechanism. Then we propose a heterogeneous information gated graph convolutional network to model the interactions among the attribute information, the syntactic information, and the contextual information. The experimental results on three benchmark datasets demonstrate the superiority of our model, which achieves new state-of-the-art performance.",,1,1.0,all publisherfullgold repository repositoryvor,All Open Access Gold Green,ARC,2022YFC3502303,Australian Research Council
2-s2.0-85214400957,,,,Exploiting Discovered Regression Discontinuities to Debias Conditioned-on-observable Estimators,ar,Article,Jakubowski B.,60009982;60021784;60021508,Harvard University;New York University;University of Notre Dame,Cambridge;New York;Notre Dame,United States;United States;United States,4.0,"Jakubowski, Benjamin;Somanchi, Sriram;McFowland, Edward;Neill, Daniel B.",59505371000;57053149600;59837705600;16556096100,60021784;60021508;60009982;60021784,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"Regression discontinuity (RD) designs are widely used to estimate causal effects in the absence of a randomized experiment. However, standard approaches to RD analysis face two significant limitations. First, they require a priori knowledge of discontinuities in treatment. Second, they yield doubly-local treatment effect estimates, and fail to provide more general causal effect estimates away from the discontinuity. To address these limitations, we introduce a novel method for automatically detecting RDs at scale, integrating information from multiple discovered discontinuities with an observational estimator, and extrapolating away from discovered, local RDs. We demonstrate the performance of our method on two synthetic datasets, showing improved performance compared to direct use of an observational estimator, direct extrapolation of RD estimates, and existing methods for combining multiple causal effect estimates. Finally, we apply our novel method to estimate spatially heterogeneous treatment effects in the context of a recent economic development problem.",causal inference | Gaussian processes | heterogeneous treatment effects | natural experiments | regression discontinuity designs,2,0.0,,,,,
2-s2.0-85172422139,10.1613/jair.1.14714,,,Exploiting Functional Constraints in Automatic Dominance Breaking for Constraint Optimization,ar,Article,Lee J.H.M.,60019578;60002798,Monash University;Chinese University of Hong Kong,Melbourne;Hong Kong,Australia;Hong Kong,2.0,"Lee, Jimmy H.M.;Zhong, Allen Z.",57203144456;57203845114,60002798;60019578,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,,78.0,,1-35,"Dominance breaking is a powerful technique in improving the solving efficiency of Constraint Optimization Problems (COPs) by removing provably suboptimal solutions with additional constraints. While dominance breaking is effective in a range of practical problems, it is usually problem specific and requires human insights into problem structures to come up with correct dominance breaking constraints. Recently, a framework is proposed to generate nogood constraints automatically for dominance breaking, which formulates nogood generation as solving auxiliary Constraint Satisfaction Problems (CSPs). However, the framework uses a pattern matching approach to synthesize the auxiliary generation CSPs from the specific forms of objectives and constraints in target COPs, and is only applicable to a limited class of COPs. This paper proposes a novel rewriting system to derive constraints for the auxiliary generation CSPs automatically from COPs with nested function calls, significantly generalizing the original framework. In particular, the rewriting system exploits functional constraints flattened from nested functions in a high-level modeling language. To generate more effective dominance breaking nogoods and derive more relaxed constraints in generation CSPs, we further characterize how to extend the system with rewriting rules exploiting function properties, such as monotonicity, commutativity, and associativity, for specific functional constraints. Experimentation shows significant runtime speedup using the dominance breaking nogoods generated by our proposed method. Studying patterns of generated nogoods also demonstrates that our proposal can reveal dominance relations in the literature and discover new dominance relations on problems with ineffective or no known dominance breaking constraints.",,2,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,研究資助局,,"Research Grants Council, University Grants Committee"
2-s2.0-105000557326,,,,Exploiting LLM Quantization,cp,Conference Paper,Egashira K.,60025858,ETH Zürich,Zurich,Switzerland,5.0,"Egashira, Kazuki;Vero, Mark;Staab, Robin;He, Jingxuan;Vechev, Martin",59185447300;57701138500;57313536200;57204734521;8876227900,60025858;60025858;60025858;60025858;60025858,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Quantization leverages lower-precision weights to reduce the memory usage of large language models (LLMs) and is a key technique for enabling their deployment on commodity hardware. While LLM quantization's impact on utility has been extensively explored, this work for the first time studies its adverse effects from a security perspective. We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model. We demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii). This procedure results in an LLM that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i). We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack. In practice, the adversary could host the resulting full-precision model on an LLM community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices.",,4,0.0,,,EC,MB22.00088,European Commission
2-s2.0-105018577652,,,,"Exploration, Exploitation, and Engagement in Multi-Armed Bandits with Abandonment",ar,Article,Yang Z.,60155336;60105232,Michigan Engineering;ShanghaiTech University,Ann Arbor;Shanghai,United States;China,3.0,"Yang, Zixian;Liu, Xin;Ying, Lei",57733605000;57191657447;35239178600,60155336;60105232;60155336,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"The traditional multi-armed bandit (MAB) model for recommendation systems assumes the user stays in the system for the entire learning horizon. In new online education platforms such as ALEKS or new video recommendation systems such as TikTok, the amount of time a user spends on the app depends on how engaging the recommended contents are. Users may temporarily leave the system if the recommended items cannot engage the users. To understand the exploration, exploitation, and engagement in these systems, we propose a new model, called MAB-A where “A” stands for abandonment and the abandonment probability depends on the current recommended item and the user’s past experience (called state). We propose two algorithms, ULCB and KL-ULCB, both of which do more exploration (being optimistic) when the user likes the previous recommended item and less exploration (being pessimistic) when the user does not. We prove that both ULCB and KL-ULCB achieve logarithmic regret, Oplog Kq, where K is the number of visits (or episodes). Furthermore, the regret bound under KL-ULCB is asymptotically sharp. We also extend the proposed algorithms to the general-state setting. Simulation results show that the proposed algorithms have significantly lower regret than the traditional UCB and KL-UCB, and Q-learning-based algorithms.<sup>1</sup>",abandonment | exploitation | exploration | multi-armed bandit | regret bound,1,0.0,,,NSF,2002608,University of Michigan
2-s2.0-85204297307,,,,Explore Internal and External Similarity for Single Image Deraining with Graph Neural Networks,cp,Conference Paper,Wang C.,60021182;60008928;60001455;60016094,Sun Yat-Sen University;The Hong Kong Polytechnic University;Anhui University;Dongbei University of Finance and Economics,Guangzhou;Hong Kong;Hefei;Dalian,China;Hong Kong;China;China,4.0,"Wang, Cong;Wang, Wei;Yu, Chengjin;Mu, Jie",57209930195;57208538575;57197725625;57217013984,60021182-60008928;60021182;60001455;60016094,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1371-1379,"Patch-level non-local self-similarity is an important property of natural images. However, most existing methods do not consider this property into neural networks for image deraining, thus affecting recovery performance. Motivated by this property, we find that there exists significant patch recurrence property of a rainy image, that is, similar patches tend to recur many times in one image and its multi-scale images and external images. To better model this property for image detaining, we develop a multi-scale graph network with exemplars, called MSGNN, that contains two branches: 1) internal data-based supervised branch is used to model the internal relations of similar patches from the rainy image itself and its multi-scale images and 2) external data-participated unsupervised branch is used to model the external relations of the similar patches in the rainy image and exemplar. Specifically, we construct a graph model by searching the k-nearest neighboring patches from both the rainy images in a multi-scale framework and the exemplar. After obtaining the corresponding k neighboring patches from the multi-scale images and exemplar, we build a graph and aggregate them in an attentional manner so that the graph can provide more information from similar patches for image deraining. We embed the proposed graph in a deep neural network and train it in an end-to-end manner. Extensive experiments demonstrate that the proposed algorithm performs favorably against eight state-of-the-art methods on five public synthetic datasets and one real-world dataset. The source codes will be available at https://github.com/supersupercong/MSGNN.",,8,0.0,,,NSFC,62306343,"Science, Technology and Innovation Commission of Shenzhen Municipality"
2-s2.0-85174387653,,,,Exploring Chemical Space with Score-based Out-of-distribution Generation,cp,Conference Paper,Lee S.,60032144,Korea Advanced Institute of Science and Technology,Daejeon,South Korea,3.0,"Lee, Seul;Jo, Jaehyeong;Hwang, Sung Ju",57226337984;57207774545;57687927300,60032144;60032144;60032144,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,18852-18871,"A well-known limitation of existing molecular generative models is that the generated molecules highly resemble those in the training set. To generate truly novel molecules that may have even better properties for de novo drug discovery, more powerful exploration in the chemical space is necessary. To this end, we propose Molecular Out-Of-distribution Diffusion (MOOD), a score-based diffusion scheme that incorporates out-of-distribution (OOD) control in the generative stochastic differential equation (SDE) with simple control of a hyperparameter, thus requires no additional costs. Since some novel molecules may not meet the basic requirements of real-world drugs, MOOD performs conditional generation by utilizing the gradients from a property predictor that guides the reverse-time diffusion process to high-scoring regions according to target properties such as protein-ligand interactions, drug-likeness, and synthesizability. This allows MOOD to search for novel and meaningful molecules rather than generating unseen yet trivial ones. We experimentally validate that MOOD is able to explore the chemical space beyond the training distribution, generating molecules that outscore ones found with existing methods, and even the top 0.01% of the original training pool. Our code is available at https://github.com/SeulLee05/MOOD.",,6,0.0,,,MSIP,2021-0-02068,"Ministry of Science, ICT and Future Planning"
2-s2.0-85150330605,,,,Exploring Example Influence in Continual Learning,cp,Conference Paper,Sun Q.,60019533,Tianjin University,Tianjin,China,5.0,"Sun, Qing;Lyu, Fan;Shang, Fanhua;Feng, Wei;Wan, Liang",58416561000;57204292808;36716893400;56471162500;7202596019,60019533;60019533;60019533;60019533;60019533,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Continual Learning (CL) sequentially learns new tasks like human beings, with the goal to achieve better Stability (S, remembering past tasks) and Plasticity (P, adapting to new tasks). Due to the fact that past training data is not available, it is valuable to explore the influence difference on S and P among training examples, which may improve the learning pattern towards better SP. Inspired by Influence Function (IF), we first study example influence via adding perturbation to example weight and computing the influence derivation. To avoid the storage and calculation burden of Hessian inverse in neural networks, we propose a simple yet effective MetaSP algorithm to simulate the two key steps in the computation of IF and obtain the S- and P-aware example influence. Moreover, we propose to fuse two kinds of example influence by solving a dual-objective optimization problem, and obtain a fused influence towards SP Pareto optimality. The fused influence can be used to control the update of model and optimize the storage of rehearsal. Empirical results show that our algorithm significantly outperforms state-of-the-art methods on both task- and class-incremental benchmark CL datasets.",,36,0.0,,,NSFC,61876220,National Natural Science Foundation of China
2-s2.0-85139900342,,,,Expression might be enough: representing pressure and demand for reinforcement learning based traffic signal control,cp,Conference Paper,Zhang L.,60005465;60028265;60011664;60104225,University of Electronic Science and Technology of China;Lanzhou University;University of Wollongong;Jiangxi University of Science and Technology,Chengdu;Lanzhou;Wollongong;Ganzhou,China;China;Australia;China,6.0,"Zhang, Liang;Wu, Qiang;Shen, Jun;Lü, Linyuan;Du, Bo;Wu, Jianqing",56796979500;57207980608;55649570000;55474680600;55840648700;57204072539,60028265;60005465;60011664;60005465;60011664;60104225,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,26645-26654,"Many studies confirmed that a proper traffic state representation is more important than complex algorithms for the classical traffic signal control (TSC) problem. In this paper, we (1) present a novel, flexible and efficient method, namely advanced max pressure (Advanced-MP), taking both running and queuing vehicles into consideration to decide whether to change current signal phase; (2) inventively design the traffic movement representation with the efficient pressure and effective running vehicles from Advanced-MP, namely advanced traffic state (ATS); and (3) develop a reinforcement learning (RL) based algorithm template, called Advanced-XLight, by combining ATS with the latest RL approaches, and generate two RL algorithms, namely”Advanced-MPLight” and”Advanced-CoLight” from Advanced-XLight. Comprehensive experiments on multiple real-world datasets show that: (1) the Advanced-MP outperforms baseline methods, and it is also efficient and reliable for deployment; and (2) Advanced-MPLight and Advanced-CoLight can achieve the state-of-the-art.",advanced max pressure | Advanced-XLight | reinforcement learning | traffic signal control | traffic state representation,51,0.0,,,NSFC,11622538,National Natural Science Foundation of China
2-s2.0-85200550127,,,,FAITHFUL EXPLANATIONS OF BLACK-BOX NLP MODELS USING LLM-GENERATED COUNTERFACTUALS,cp,Conference Paper,Gat Y.O.,60030162;60021726;131534662,Columbia University;Microsoft Research;Technion,New York;Redmond;,United States;United States;China,6.0,"Gat, Yair Ori;Calderon, Nitay;Feder, Amir;Chapanin, Alexander;Sharma, Amit;Reichart, Roi",57733819800;57340422200;57219688007;58657073900;57214355703;51665948700,131534662;131534662;60030162;131534662;60021726;131534662,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.",,7,0.0,,,,,
2-s2.0-85192281462,,,,FANTASTIC GENERALIZATION MEASURES ARE NOWHERE TO BE FOUND,cp,Conference Paper,Gastpar M.,60022195;60002999;120233571,Massachusetts Institute of Technology;University of Haifa;EPFL,Cambridge;Haifa;Villigen,United States;Israel;Switzerland,4.0,"Gastpar, Michael;Nachum, Ido;Shafer, Jonathan;Weinberger, Thomas",7005231107;57218189837;57219457663;58638018300,120233571;60002999;60022195;120233571,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"We study the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, in their paper “Fantastic Generalization Measures and Where to Find Them,” Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them are uniformly tight. This raises the question of whether uniformly-tight generalization bounds are at all possible in the overparameterized setting. We consider two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis (e.g., margin bounds). We prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that may in addition also depend on the learning algorithm (e.g., stability bounds). For these bounds, we show a trade-off between the algorithm's performance and the bound's tightness. Namely, if the algorithm achieves good accuracy on certain distributions, then no generalization bound can be uniformly tight for it in the overparameterized setting. We explain how these formal results can, in our view, inform research on generalization bounds for neural networks, while stressing that other interpretations of these results are also possible.",,4,0.0,,,UCB,200364,University of California Berkeley
2-s2.0-85200603032,,,,FEDCDA: FEDERATED LEARNING WITH CROSS-ROUND DIVERGENCE-AWARE AGGREGATION,cp,Conference Paper,Wang H.,60003970;60025761;60005510,Zhejiang University;Huazhong University of Science and Technology;Nanyang Technological University,Hangzhou;Wuhan;Singapore City,China;China;Singapore,6.0,"Wang, Haozhao;Xu, Haoran;Li, Yichen;Xu, Yuan;Li, Ruixuan;Zhang, Tianwei",57203550518;59008636300;59454664200;58998970000;7404724385;55635885400,60005510;60003970;60025761;60005510;60025761;60005510,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"In Federated Learning (FL), model aggregation is pivotal. It involves a global server iteratively aggregating client local trained models in successive rounds without accessing private data. Traditional methods typically aggregate the local models from the current round alone. However, due to the statistical heterogeneity across clients, the local models from different clients may be greatly diverse, making the obtained global model incapable of maintaining the specific knowledge of each local model. In this paper, we introduce a novel method, FedCDA, which selectively aggregates cross-round local models, decreasing discrepancies between the global model and local models. The principle behind FedCDA is that due to the different global model parameters received in different rounds and the non-convexity of deep neural networks, the local models from each client may converge to different local optima across rounds. Therefore, for each client, we select a local model from its several recent local models obtained in multiple rounds, where the local model is selected by minimizing its divergence from the local models of other clients. This ensures the aggregated global model remains close to all selected local models to maintain their data knowledge. Extensive experiments conducted on various models and datasets reveal our approach outperforms state-of-the-art aggregation methods.",,35,0.0,,,NSFC,U1936108,National Natural Science Foundation of China
2-s2.0-85164380821,,,,FEDERATED NEURAL BANDITS,cp,Conference Paper,Dai Z.,60017161;60141072,National University of Singapore;MIT Department of Electrical Engineering and Computer Science,Singapore City;Cambridge,Singapore;United States,6.0,"Dai, Zhongxiang;Shu, Yao;Verma, Arun;Fan, Flint Xiaofeng;Low, Bryan Kian Hsiang;Jaillet, Patrick",57022883500;57219507600;57201339677;57327290000;7102180182;6701591623,60017161;60017161;60017161;60017161;60017161;60141072,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Recent works on neural contextual bandits have achieved compelling performances due to their ability to leverage the strong representation power of neural networks (NNs) for reward prediction. Many applications of contextual bandits involve multiple agents who collaborate without sharing raw observations, thus giving rise to the setting of federated contextual bandits. Existing works on federated contextual bandits rely on linear or kernelized bandits, which may fall short when modeling complex real-world reward functions. So, this paper introduces the federated neural-upper confidence bound (FN-UCB) algorithm. To better exploit the federated setting, FN-UCB adopts a weighted combination of two UCBs: UCB<sup>a</sup> allows every agent to additionally use the observations from the other agents to accelerate exploration (without sharing raw observations), while UCB<sup>b</sup> uses an NN with aggregated parameters for reward prediction in a similar way to federated averaging for supervised learning. Notably, the weight between the two UCBs required by our theoretical analysis is amenable to an interesting interpretation, which emphasizes UCB<sup>a</sup> initially for accelerated exploration and relies more on UCB<sup>b</sup> later after enough observations have been collected to train the NNs for accurate reward prediction (i.e., reliable exploitation). We prove sub-linear upper bounds on both the cumulative regret and the number of communication rounds of FN-UCB, and empirically demonstrate its competitive performance.",,17,0.0,,,,A19E4a0101,
2-s2.0-85196960093,,,,FEDERATED Q-LEARNING: LINEAR REGRET SPEEDUP WITH LOW COMMUNICATION COST,cp,Conference Paper,Zheng Z.,60001439,Pennsylvania State University,University Park,United States,4.0,"Zheng, Zhong;Gao, Fengyu;Xue, Lingzhou;Yang, Jing",57221155340;58797333500;38863321700;55574202986,60001439;60001439;60001439;60001439,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"In this paper, we consider federated reinforcement learning for tabular episodic Markov Decision Processes (MDP) where, under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data. While linear speedup in the number of agents has been achieved for some metrics, such as convergence rate and sample complexity, in similar settings, it is unclear whether it is possible to design a model-free algorithm to achieve linear regret speedup with low communication cost. We propose two federated Q-Learning algorithms termed as FedQ-Hoeffding and FedQ-Bernstein, respectively, and show that the corresponding total regrets achieve a linear speedup compared with their single-agent counterparts when the time horizon is sufficiently large, while the communication cost scales logarithmically in the total number of time steps T. Those results rely on an event-triggered synchronization mechanism between the agents and the server, a novel step size selection when the server aggregates the local estimates of the state-action values to form the global estimates, and a set of new concentration inequalities to bound the sum of non-martingale differences. This is the first work showing that linear regret speedup and logarithmic communication cost can be achieved by model-free algorithms in federated reinforcement learning.",,9,0.0,,,NSF,CNS-1956276,National Science Foundation
2-s2.0-85200574504,,,,FEDTRANS: CLIENT-TRANSPARENT UTILITY ESTIMATION FOR ROBUST FEDERATED LEARNING,cp,Conference Paper,Yang M.,60006288,Delft University of Technology,Delft,Netherlands,4.0,"Yang, Mingkun;Zhu, Ran;Wang, Qing;Yang, Jie",57209198273;57207736876;55850163900;56370016500,60006288;60006288;60006288;60006288,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Federated Learning (FL) is an important privacy-preserving learning paradigm that plays an important role in the Intelligent Internet of Things. Training a global model in FL, however, is vulnerable to the data noise across the clients. In this paper, we introduce FedTrans, a novel client-transparent client utility estimation method designed to guide client selection for noisy scenarios, mitigating performance degradation problems. To estimate the client utility, we propose a Bayesian framework that models client utility and its relationships with the weight parameters and the performance of local models. We then introduce a variational inference algorithm to effectively infer client utility at the FL server, given only a small amount of auxiliary data. Our evaluation results demonstrate that leveraging FedTrans to select the clients can improve the accuracy performance (up to 7.8%), ensuring the robustness of FL in noisy scenarios.",,1,0.0,,,NWO,HORIZONMSCA-2022-SE-01,Nederlandse Organisatie voor Wetenschappelijk Onderzoek
2-s2.0-85129604597,10.1613/jair.1.13167,,,FFCI: A Framework for Interpretable Automatic Evaluation of Summarization,ar,Article,Koto F.,60118847,School of Computing and Information Systems,Melbourne,Australia,3.0,"Koto, Fajri;Baldwin, Timothy;Lau, Jey Han",56572738100;8716547300;52163791800,60118847;60118847;60118847,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,1553-1607,"In this paper, we propose FFCI, a framework for fine-grained summarization evaluation that comprises four elements: faithfulness (degree of factual consistency with the source), focus (precision of summary content relative to the reference), coverage (recall of summary content relative to the reference), and inter-sentential coherence (document fluency between adjacent sentences). We construct a novel dataset for focus, coverage, and inter-sentential coherence, and develop automatic methods for evaluating each of the four dimensions of FFCI based on cross-comparison of evaluation metrics and model-based evaluation methods, including question answering (QA) approaches, semantic textual similarity (STS), next-sentence prediction (NSP), and scores derived from 19 pre-trained language models. We then apply the developed metrics in evaluating a broad range of summarization models across two datasets, with some surprising findings.",,15,1.0,all publisherfullgold,All Open Access Gold,DFAT,,"Department of Foreign Affairs and Trade, Australian Government"
2-s2.0-85170361945,10.24963/ijcai.2023/194,,,FGNet: Towards Filling the Intra-class and Inter-class Gaps for Few-shot Segmentation,cp,Conference Paper,Zhang Y.,60019118;60025345;127880299,University of Science and Technology of China;Guangzhou University;Hefei National Laboratory,Hefei;Guangzhou;Hefei,China;China;China,3.0,"Zhang, Yuxuan;Yang, Wei;Wang, Shaowei",58610957500;57001308200;57022077700,60019118;60019118-127880299;60025345,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,1749-1758,"Current few-shot segmentation (FSS) approaches have made tremendous achievements based on prototypical learning techniques. However, due to the scarcity of the support data provided, FSS methods still suffer from the intra-class and inter-class gaps. In this paper, we propose a uniform network to fill both the gaps, termed FGNet. It consists of the novel design of a Self-Adaptive Module (SAM) to emphasize the query feature to generate an enhanced prototype for self-alignment. Such a prototype caters to each query sample itself since it contains the underlying intra-instance information, which gets around the intra-class appearance gap. Moreover, we design an Inter-class Feature Separation Module (IFSM) to separate the feature space of the target class from other classes, which contributes to bridging the inter-class gap. In addition, we present several new losses and a method termed B-SLIC, which help to further enhance the separation performance of FGNet. Experimental results show that FGNet reduces both the gaps for FSS by SAM and IFSM respectively, and achieves state-of-the-art performances on both PASCAL-5<sup>i</sup> and COCO-20<sup>i</sup> datasets compared with previous top-performing approaches.",,6,1.0,all publisherfullgold,All Open Access Gold,NSFC,AHY150300,National Natural Science Foundation of China
2-s2.0-105000468297,,,,FINCON: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making,cp,Conference Paper,Yu Y.,60009982;60027392;132246222,Harvard University;Stevens Institute of Technology;The Fin AI,Cambridge;Hoboken;New Haven,United States;United States;United States,18.0,"Yu, Yangyang;Yao, Zhiyuan;Li, Haohang;Deng, Zhiyang;Jiang, Yuechen;Cao, Yupeng;Chen, Zhi;Suchow, Jordan W.;Cui, Zhenyu;Liu, Rong;Xu, Zhaozhuo;Zhang, Denghui;Subbalakshmi, Koduvayur;Xiong, Guojun;He, Yueru;Huang, Jimin;Li, Dong;Xie, Qianqian",57203202581;57788971000;57715056400;58916313600;58752052500;57344157600;58605681600;36717028800;56995511500;55739649000;57171068000;57218133426;6602342476;57222067315;58915728800;56726873100;58916461900;57190030285,60027392;60027392;60027392;60027392;60027392;60027392;60027392;60027392;60027392;60027392;60027392;60027392;60027392;60009982;132246222;132246222;132246222;132246222,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Large language models (LLMs) have shown potential in complex financial tasks, but sequential financial decision-making remains challenging due to the volatile environment and the need for intelligent risk management. While LLM-based agent systems have achieved impressive returns, optimizing multi-source information synthesis and decision-making through timely experience refinement is underexplored. We introduce FINCON, an LLM-based multi-agent framework with CONceptual verbal reinforcement for diverse FINancial tasks. Inspired by real-world investment firm structures, FINCON employs a manager-analyst hierarchy, enabling synchronized cross-functional agent collaboration towards unified goals via natural language interactions. Its dual-level risk-control component enhances decision-making by monitoring daily market risk and updating systematic investment beliefs through self-critique. These conceptualized beliefs provide verbal reinforcement for future decisions, selectively propagated to relevant agents, improving performance while reducing unnecessary peer-to-peer communication costs. FINCON generalizes well across tasks, including single stock trading and portfolio management.",,21,0.0,,,,,
2-s2.0-85196126379,,,,FINETUNING TEXT-TO-IMAGE DIFFUSION MODELS FOR FAIRNESS,cp,Conference Paper,Shen X.,60017161;60106371;126553437,National University of Singapore;NUS Graduate School;Sea AI Lab,Singapore City;Singapore City;Singapore City,Singapore;Singapore;Singapore,6.0,"Shen, Xudong;Du, Chao;Pang, Tianyu;Lin, Min;Wong, Yongkang;Kankanhalli, Mohan",57271429100;57194774618;57204799576;55926433700;35085446500;7003629165,60106371;126553437;126553437;126553437;60017161;60017161,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives of fairness beyond absolute equality, which is demonstrated by controlling age to a 75% young and 25% old distribution while simultaneously debiasing gender and race. Finally, our method is scalable: it can debias multiple concepts at once by simply including these prompts in the finetuning data. We share code and various fair diffusion model adaptors at https://sail-sg.github.io/finetune-fair-diffusion/.",,24,0.0,,,NRF,,National Research Foundation Singapore
2-s2.0-85137860245,10.24963/ijcai.2022/429,,,FLS: A New Local Search Algorithm for K-means with Smaller Search Space,cp,Conference Paper,Huang J.,60017060;60153573;60031188,Central South University;School of Engineering and Applied Sciences;Penn State Behrend,Changsha;Buffalo;Erie,China;United States;United States,5.0,"Huang, Junyu;Feng, Qilong;Huang, Ziyun;Xu, Jinhui;Wang, Jianxin",57219549184;24483019600;56025795600;35243841000;35197282200,60017060;60017060;60031188;60153573;60017060,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3092-3098,"The k-means problem is an extensively studied unsupervised learning problem with various applications in decision making and data mining. In this paper, we propose a fast and practical local search algorithm for the k-means problem. Our method reduces the search space of swap pairs from O(nk) to O(k<sup>2</sup>), and applies random mutations to find potentially better solutions when local search falls into poor local optimum. With the assumption of data distribution that each optimal cluster has”average” size of Ω(<sup>n</sup><inf>k</inf>), which is common in many datasets and k-means benchmarks, we prove that our proposed algorithm gives a (100 + ε)- approximate solution in expectation. Empirical experiments show that our algorithm achieves better performance compared to existing state-of-the-art local search methods on k-means benchmarks and large datasets.",,2,1.0,all publisherfree2read,All Open Access Bronze,NSFC,61872450,National Natural Science Foundation of China
2-s2.0-85189508260,10.1609/aaai.v38i4.28069,,,FRED: Towards a Full Rotation-Equivariance in Aerial Image Object Detection,cp,Conference Paper,Lee C.,60032144;60068689,Korea Advanced Institute of Science and Technology;Hanbat National University,Daejeon;Daejeon,South Korea;South Korea,5.0,"Lee, Chanho;Son, Jinsu;Shon, Hyounguk;Jeon, Yunho;Kim, Junmo",57202404894;58849878200;57223808587;57201325729;36015494900,60032144;60032144;60032144;60068689;60032144,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,4.0,,2883-2891,"Rotation-equivariance is an essential yet challenging property in oriented object detection. While general object detectors naturally leverage robustness to spatial shifts due to the translation-equivariance of the conventional CNNs, achieving rotation-equivariance remains an elusive goal. Current detectors deploy various alignment techniques to derive rotation-invariant features, but still rely on high capacity models and heavy data augmentation with all possible rotations. In this paper, we introduce a Fully Rotation-Equivariant Oriented Object Detector (FRED), whose entire process from the image to the bounding box prediction is strictly equivariant. Specifically, we decouple the invariant task (object classification) and the equivariant task (object localization) to achieve end-to-end equivariance. We represent the bounding box as a set of rotation-equivariant vectors to implement rotation-equivariant localization. Moreover, we utilized these rotation-equivariant vectors as offsets in the deformable convolution, thereby enhancing the existing advantages of spatial adaptation. Leveraging full rotation-equivariance, our FRED demonstrates higher robustness to image-level rotation compared to existing methods. Furthermore, we show that FRED is one step closer to non-axis aligned learning through our experiments. Compared to state-of-the-art methods, our proposed method delivers comparable performance on DOTAv1.0 and outperforms by 1.5 mAP on DOTA-v1.5, all while significantly reducing the model parameters to 16%.",,22,1.0,all publisherfullgold,All Open Access Gold,KHIDI,HI20C1234,Korea Health Industry Development Institute
2-s2.0-85167661806,10.1609/aaai.v37i1.25218,,,FSR: A General Frequency-Oriented Framework to Accelerate Image Super-resolution Networks,cp,Conference Paper,Li J.,60025278;60000937;60361251;60271961,Tsinghua University;Shenzhen University;Harbin Institute of Technology Shenzhen;Peng Cheng Laboratory,Beijing;Shenzhen;Shenzhen;Shenzhen,China;China;China;China,6.0,"Li, Jinmin;Dai, Tao;Zhu, Mingyan;Chen, Bin;Wang, Zhi;Xia, Shu Tao",58486103300;56940086700;57285584400;59981150600;55913248200;7202892509,60025278-60000937;60000937;60025278-60271961;60361251-60271961;60025278;60025278-60271961,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,1343-1350,"Deep neural networks (DNNs) have witnessed remarkable achievement in image super-resolution (SR), and plenty of DNN-based SR models with elaborated network designs have recently been proposed. However, existing methods usually require substantial computations by operating in spatial domain. To address this issue, we propose a general frequency-oriented framework (FSR) to accelerate SR networks by considering data characteristics in frequency domain. Our FSR mainly contains dual feature aggregation module (DFAM) to extract informative features in both spatial and transform domains, followed by a four-path SR-Module with different capacities to super-resolve in the frequency domain. Specifically, DFAM further consists of a transform attention block (TABlock) and a spatial context block (SCBlock) to extract global spectral information and local spatial information, respectively, while SR-Module is a parallel network container that contains four to-be-accelerated branches. Furthermore, we propose an adaptive weight strategy for a trade-off between image details recovery and visual quality. Extensive experiments show that our FSR can save FLOPs by almost 40% while reducing inference time by 50% for other SR methods (e.g., FSRCNN, CARN, SRResNet and RCAN). Code is available at https://github.com/THU-Kingmin/FSR.",,14,1.0,all publisherfullgold,All Open Access Gold,NSFC,62171248,National Natural Science Foundation of China
2-s2.0-105000550370,,,,FUSU: A Multi-temporal-source Land Use Change Segmentation Dataset for Fine-grained Urban Semantic Understanding,cp,Conference Paper,Yuan S.,60025278;60021182;60006541;126141874;130486237,Tsinghua University;Sun Yat-Sen University;The University of Hong Kong;PengCheng Laboratory;National Supercomputing Center,Beijing;Guangzhou;Hong Kong;Chengdu;Shenzhen,China;China;Hong Kong;China;China,9.0,"Yuan, Shuai;Lin, Guancong;Zhang, Lixian;Dong, Runmin;Zhang, Jinxiao;Chen, Shuang;Zheng, Juepeng;Wang, Jie;Fu, Haohuan",57213198049;59185586000;57207392945;57205415789;58521563200;58254361800;58630253200;58862690000;8713118400,60006541;60021182;130486237;60025278;60025278;60006541;60021182;126141874;60025278,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Fine urban change segmentation using multi-temporal remote sensing images is essential for understanding human-environment interactions in urban areas. Although there have been advances in high-quality land cover datasets that reveal the physical features of urban landscapes, the lack of fine-grained land use datasets hinders a deeper understanding of how human activities are distributed across the landscape and the impact of these activities on the environment, thus constraining proper technique development. To address this, we introduce FUSU, the first fine-grained land use change segmentation dataset for Fine-grained Urban Semantic Understanding. FUSU features the most detailed land use classification system to date, with 17 classes and 30 billion pixels of annotations. It includes bi-temporal high-resolution satellite images with 0.2-0.5 m ground sample distance and monthly optical and radar satellite time series, covering 847 km<sup>2</sup> across five urban areas in the southern and northern of China with different geographical features. The fine-grained land use pixel-wise annotations and high spatial-temporal resolution data provide a robust foundation for developing proper deep learning models to provide contextual insights on human activities and urbanization. To fully leverage FUSU, we propose a unified time-series architecture for both change detection and segmentation. We benchmark FUSU on various methods for several tasks. Dataset and code are available at: https://github.com/yuanshuai0914/FUSU.",,2,0.0,,,NKRDPC,2023YFB3002400,National Key Research and Development Program of China
2-s2.0-85160304536,10.1613/JAIR.1.14267,,,FactGen: Faithful Text Generation by Factuality-aware Pre-training and Contrastive Ranking Fine-tuning,ar,Article,Lan Z.,60018205;60280914;60112903,"Xiamen University;Shanghai Artificial Intelligence Laboratory;Baidu, Inc.",Xiamen;Shanghai;Beijing,China;China;China,7.0,"Lan, Zhibin;Li, Wei;Su, Jinsong;Xiao, Xinyan;Liu, Jiachen;Wu, Wenhao;Lyu, Yajuan",58290285000;57221638294;55157800300;57192307873;57207860075;57211802977;57192308367,60018205-60280914;60112903;60018205-60280914;60112903;60112903;60112903;60112903,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,1281-1303,"Conditional text generation is supposed to generate a fluent and coherent target text that is faithful to the source text. Although pre-trained models have achieved promising results, they still suffer from the crucial factuality problem. To deal with this issue, we propose a factuality-aware pretraining-finetuning framework named FactGen, which fully considers factuality during two training stages. Specifically, at the pre-training stage, we utilize a natural language inference model to construct target texts that are entailed by the source texts, resulting in a more factually consistent pre-training objective. Then, during the fine-tuning stage, we further introduce a contrastive ranking loss to encourage the model to generate factually consistent text with higher probability. Extensive experiments on three conditional text generation tasks demonstrate the effectiveness and generality of our training framework.",,4,1.0,all publisherfullgold,All Open Access Gold,NSFC,62276219,National Natural Science Foundation of China
2-s2.0-85167977370,10.1609/aaai.v37i11.26618,,,Factual and Informative Review Generation for Explainable Recommendation,cp,Conference Paper,Xie Z.,60030612;60007278,"University of California, San Diego;University of California, Irvine",La Jolla;Irvine,United States;United States,4.0,"Xie, Zhouhang;Singh, Sameer;McAuley, Julian;Majumder, Bodhisattwa Prasad",57238437200;58124003300;14822353500;56022323600,60030612;60007278;60030612;60030612,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,13816-13824,"Recent models can generate fluent and grammatical synthetic reviews while accurately predicting user ratings. The generated reviews, expressing users' estimated opinions towards related products, are often viewed as natural language 'rationales' for the jointly predicted rating. However, previous studies found that existing models often generate repetitive, universally applicable, and generic explanations, resulting in uninformative rationales. Further, our analysis shows that previous models' generated content often contain factual hallucinations. These issues call for novel solutions that could generate both informative and factually grounded explanations. Inspired by recent success in using retrieved content in addition to parametric knowledge for generation, we propose to augment the generator with a personalized retriever, where the retriever's output serves as external knowledge for enhancing the generator. Experiments on Yelp, TripAdvisor, and Amazon Movie Reviews dataset show our model could generate explanations that more reliably entail existing reviews, are more diverse, and are rated more informative by human evaluators.",,22,1.0,all publisherfullgold,All Open Access Gold,NSF,1750063,National Science Foundation
2-s2.0-85137916456,10.24963/ijcai.2022/14,,,Fair Equilibria in Sponsored Search Auctions: The Advertisers' Perspective,cp,Conference Paper,Birmpas G.,60032350;60021796;131486353,Sapienza Università di Roma;Università Bocconi;Meta,Rome;Milan;London,Italy;Italy;United Kingdom,4.0,"Birmpas, Georgios;Celli, Andrea;Colini-Baldeschi, Riccardo;Leonardi, Stefano",57126310800;57195953040;55604464700;56366507000,60032350;60021796;131486353;60032350,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,95-101,"In this work we introduce a new class of mechanisms composed of a traditional Generalized Second Price (GSP) auction and a fair division scheme, in order to achieve some desired level of fairness between groups of Bayesian strategic advertisers. We propose two mechanisms, β-Fair GSP and GSP-EFX, that compose GSP with, respectively, an envy-free up to one item, and an envy-free up to any item fair division scheme. The payments of GSP are adjusted in order to compensate advertisers that suffer a loss of efficiency due the fair division stage. We investigate the strategic learning implications of the deployment of sponsored search auction mechanisms that obey to such fairness criteria. We prove that, for both mechanisms, if bidders play so as to minimize their external regret they are guaranteed to reach an equilibrium with good social welfare. We also prove that the mechanisms are budget balanced, so that the payments charged by the traditional GSP mechanism are a good proxy of the total compensation offered to the advertisers. Finally, we evaluate the quality of the allocations through experiments on real-world data.",,2,1.0,all publisherfree2read,All Open Access Bronze,ERC,788893,European Research Council
2-s2.0-85162152708,10.1613/JAIR.1.12847,,,Fair and Efficient Allocation of Scarce Resources Based on Predicted Outcomes: Implications for Homeless Service Delivery,cp,Conference Paper,Kube A.R.,60010261;60279735;60073778,"Washington University in St. Louis;College of Engineering and Computing;Washington University in St. Louis, George Warren Brown School of Social Work",St. Louis;Fairfax;St. Louis,United States;United States;United States,3.0,"Kube, Amanda R.;Das, Sanmay;Fowler, Patrick J.",57205186792;55476999400;24463988400,60010261;60279735;60073778,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,1219-1245,"Artificial intelligence, machine learning, and algorithmic techniques in general, provide two crucial abilities with the potential to improve decision-making in the context of allocation of scarce societal resources. They have the ability to flexibly and accurately model treatment response at the individual level, potentially allowing us to better match available resources to individuals. In addition, they have the ability to reason simultaneously about the effects of matching sets of scarce resources to populations of individuals. In this work, we leverage these abilities to study algorithmic allocation of scarce societal resources in the context of homelessness. In communities throughout the United States, there is constant demand for an array of homeless services intended to address different levels of need. Allocations of housing services must match households to appropriate services that continuously fluctuate in availability, while inefficiencies in allocation could ""waste""scarce resources as households will remain in-need and reenter the homeless system, increasing the overall demand for homeless services. This complex allocation problem introduces novel technical and ethical challenges. Using administrative data from a regional homeless system, we formulate the problem of ""optimal""allocation of resources given data on households with need for homeless services. The optimization problem aims to allocate available resources such that predicted probabilities of household reentry are minimized. The key element of this work is its use of a counterfactual prediction approach that predicts household probabilities of reentry into homeless services if assigned to each service. Through these counterfactual predictions, we find that this approach has the potential to improve the efficiency of the homeless system by reducing re-entry, and, therefore, system-wide demand. However, efficiency comes with trade-offs - a significant fraction of households are assigned to services that increase probability of re-entry. To address this issue as well as the inherent fairness considerations present in any context where there are insufficient resources to meet demand, we discuss the efficiency, equity, and fairness issues that arise in our work and consider potential implications for homeless policies.",,12,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85147248689,10.1613/jair.1.13778,,,Fair in the Eyes of Others,ar,Article,Shams P.,60001422;60104653,Sorbonne Université;Université Grenoble Alpes,Paris;Saint Martin d'Heres,France;France,4.0,"Shams, Parham;Beynier, Aurélie;Bouveret, Sylvain;Maudet, Nicolas",57212529765;12768473300;12768107700;57210975587,60001422;60001422;60104653;60001422,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,913-951,"Envy-freeness is a widely studied notion in resource allocation, capturing some aspects of fairness. The notion of envy being inherently subjective though, it might be the case that an agent envies another agent, but that from the other agents’ point of view, she has no reason to do so. The difficulty here is to define the notion of objectivity, since no ground-truth can properly serve as a basis of this definition. A natural approach is to consider the judgement of the other agents as a proxy for objectivity. Building on previous work by Parijs (who introduced “unanimous envy”) we propose the notion of approval envy: an agent a<inf>i</inf> experiences approval envy towards a<inf>j</inf> if she is envious of a<inf>j</inf>, and sufficiently many agents agree that this should be the case, from their own perspectives. Another thoroughly studied notion in resource allocation is proportionality. The same variant can be studied, opening natural questions regarding the links between these two notions. We exhibit several properties of these notions. Computing the minimal threshold guaranteeing approval envy and approval non-proportionality clearly inherits well-known intractable results from envy-freeness and proportionality, but (i) we identify some tractable cases such as house allocation; and (ii) we provide a general method based on a mixed integer programming encoding of the problem, which proves to be efficient in practice. This allows us in particular to show experimentally that existence of such allocations, with a rather small threshold, is very often observed.",,4,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,
2-s2.0-85203802758,,,,FairProof: Confidential and Certifiable Fairness for Neural Networks,cp,Conference Paper,Yadav C.,60012708;60030612,"Stanford University;University of California, San Diego",Stanford;La Jolla,United States;United States,4.0,"Yadav, Chhavi;Chowdhury, Amrita Roy;Boneh, Dan;Chaudhuri, Kamalika",57215332382;57202020208;7003748305;8935564900,60030612;60030612;60012708;60030612,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,55682-55705,"Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential. Consequently, there is a growing distrust about the fairness properties of these models in the minds of consumers, who are often at the receiving end of model predictions. To this end, we propose FairProof- a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the fairness of a model, while maintaining confidentiality. We also propose a fairness certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system. We implement FairProof in Gnark and demonstrate empirically that our system is practically feasible. Code is available at https://github.com/infinite-pursuits/FairProof.",,1,0.0,,,DARPA,W911NF2110317,Defense Advanced Research Projects Agency
2-s2.0-85160287419,10.1613/JAIR.1.14050,,,Fairness in Forecasting of Observations of Linear Dynamical Systems,ar,Article,Zhou Q.,60015150;60005141;60013323,Imperial College London;University College Dublin;Czech Technical University in Prague,London;Dublin;Prague,United Kingdom;Ireland;Czech Republic,3.0,"Zhou, Quan;Mareček, Jakub;Shorten, Robert",56909174900;29767701700;7003386909,60015150-60005141;60013323;60015150-60005141,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,1247-1280,"In machine learning, training data often capture the behaviour of multiple subgroups of some underlying human population. This behaviour can often be modelled as observations of an unknown dynamical system with an unobserved state. When the training data for the subgroups are not controlled carefully, however, under-representation bias arises. To counter under-representation bias, we introduce two natural notions of fairness in time-series forecasting problems: subgroup fairness and instantaneous fairness. These notion extend predictive parity to the learning of dynamical systems. We also show globally convergent methods for the fairness-constrained learning problems using hierarchies of convexifications of non-commutative polynomial optimisation problems. We also show that by exploiting sparsity in the convexifications, we can reduce the run time of our methods considerably. Our empirical results on a biased data set motivated by insurance applications and the well-known COMPAS data set demonstrate the efficacy of our methods.",,4,1.0,all publisherfullgold,All Open Access Gold,UKRI,10040569,Innovate UK
2-s2.0-85170355946,10.24963/ijcai.2023/49,,,Fairness via Group Contribution Matching,cp,Conference Paper,Li T.,60005510;60013789;60025710;60103821;60022904;60273040;60004678,"Nanyang Technological University;Beihang University;Agency for Science, Technology and Research, Singapore;Zhejiang Sci-Tech University;New Jersey Institute of Technology;Institute of Information Engineering;A-Star, Institute of High Performance Computing",Singapore City;Beijing;Singapore City;Hangzhou;Newark;Beijing;Singapore City,Singapore;China;Singapore;China;United States;China;Singapore,8.0,"Li, Tianlin;Li, Zhiming;Li, Anran;Du, Mengnan;Liu, Aishan;Guo, Qing;Meng, Guozhu;Liu, Yang",57218764226;57226344159;57218704295;57203397578;57188724406;57191163500;56747189200;56911879800,60005510;60005510;60005510;60022904;60013789;60004678-60025710;60273040;60005510-60103821,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,436-445,"Fairness issues in Deep Learning models have recently received increasing attention due to their significant societal impact. Although methods for mitigating unfairness are constantly proposed, little research has been conducted to understand how discrimination and bias develop during the standard training process. In this study, we propose analyzing the contribution of each subgroup (i.e., a group of data with the same sensitive attribute) in the training process to understand the cause of such bias development process. We propose a gradient-based metric to assess training subgroup contribution disparity, showing that unequal contributions from different subgroups are one source of such unfairness. One way to balance the contribution of each subgroup is through oversampling, which ensures that an equal number of samples are drawn from each subgroup during each training iteration. However, we find that even with a balanced number of samples, the contribution of each group remains unequal, resulting in unfairness under such a strategy. To address the above issues, we propose an easy but effective group contribution matching (GCM) method to match the contribution of each subgroup. Our experiments show that our GCM effectively improves fairness and outperforms other methods significantly.",,12,1.0,all publisherfullgold,All Open Access Gold,NRF,AISG2-RP-2020-019,National Research Foundation Singapore
2-s2.0-85174390921,,,,Fast (1 + ε)-Approximation Algorithms for Binary Matrix Factorization,cp,Conference Paper,Velingker A.,60025988;60027950;60005286;60006191,Universität Wien;Carnegie Mellon University;Rice University;Google LLC,Vienna;Pittsburgh;Houston;Mountain View,Austria;United States;United States;United States,4.0,"Velingker, Ameya;Vötsch, Maximilian;Woodruff, David P.;Zhou, Samson",55647490500;58031331000;35407448600;57193615301,60006191;60025988;60027950-60006191;60005286,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,34952-34977,"We introduce efficient (1 + ε)-approximation algorithms for the binary matrix factorization (BMF) problem, where the inputs are a matrix A ∈ {0,1}<sup>n</sup>×<sup>d</sup>, a rank parameter k > 0, as well as an accuracy parameter ε > 0, and the goal is to approximate A as a product of low-rank factors U ∈ {0,1}<sup>n</sup>×<sup>k</sup> and V ∈ {0,1}<sup>k</sup>×<sup>d</sup>. Equivalently, we want to find U and V that minimize the Frobenius loss ∥UV − A∥<sup>2</sup><inf>F</inf>. Before this work, the state-of-the-art for this problem was the approximation algorithm of Kumar et al. [ICML 2019], which achieves a Capproximation for some constant C ≥ 576. We give the first (1 + ε)-approximation algorithm using running time singly exponential in k, where k is typically a small integer. Our techniques generalize to other common variants of the BMF problem, admitting bicriteria (1 + ε)approximation algorithms for L<inf>p</inf> loss functions and the setting where matrix operations are performed in F<inf>2</inf>. Our approach can be implemented in standard big data models, such as the streaming or distributed models.",,2,0.0,,,,,
2-s2.0-85163073753,,,,Fast Convex Optimization for Two-Layer ReLU Networks: Equivalent Model Classes and Cone Decompositions,cp,Conference Paper,Mishkin A.,60141508,Stanford Engineering,Stanford,United States,3.0,"Mishkin, Aaron;Sahiner, Arda;Pilanci, Mert",57198861935;57215195115;35076270900,60141508;60141508;60141508,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,15770-15816,"We develop fast algorithms and robust software for convex optimization of two-layer neural networks with ReLU activation functions. Our work leverages a convex reformulation of the standard weight-decay penalized training problem as a set of group-ℓ<inf>1</inf>-regularized data-local models, where locality is enforced by polyhedral cone constraints. In the special case of zero-regularization, we show that this problem is exactly equivalent to unconstrained optimization of a convex “gated ReLU” network. For problems with nonzero regularization, we show that convex gated ReLU models obtain data-dependent approximation bounds for the ReLU training problem. To optimize the convex reformulations, we develop an accelerated proximal gradient method and a practical augmented Lagrangian solver. We show that these approaches are faster than standard training heuristics for the non-convex problem, such as SGD, and outperform commercial interior-point solvers. Experimentally, we verify our theoretical results, explore the group-ℓ<inf>1</inf> regularization path, and scale convex optimization for neural networks to image classification on MNIST and CIFAR-10.",,14,0.0,,,NSF,DGE-1656518,National Science Foundation
2-s2.0-105018575240,,,,Fast Policy Extragradient Methods for Competitive Games with Entropy Regularization,ar,Article,Cen S.,60104842;60022452,College of Engineering;Wharton School of the University of Pennsylvania,Pittsburgh;Philadelphia,United States;United States,3.0,"Cen, Shicong;Wei, Yuting;Chi, Yuejie",57218338551;57191034702;26435030200,60104842;60022452;60104842,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"This paper investigates the problem of computing the equilibrium of competitive games in the form of two-player zero-sum games, which is often modeled as a constrained saddle-point optimization problem with probability simplex constraints. Despite recent efforts in understanding the last-iterate convergence of extragradient methods in the unconstrained setting, the theoretical underpinnings of these methods in the constrained settings, especially those using multiplicative updates, remain highly inadequate, even when the objective function is bilinear. Motivated by the algorithmic role of entropy regularization in single-agent reinforcement learning and game theory, we develop provably efficient extragradient methods to find the quantal response equilibrium (QRE)-which are solutions to zero-sum two-player matrix games with entropy regularization-at a linear rate. The proposed algorithms can be implemented in a decentralized manner, where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent’s actions directly. In addition, by controlling the knob of entropy regularization, the proposed algorithms can locate an approximate Nash equilibrium of the unregularized matrix game at a sublinear rate without assuming the Nash equilibrium to be unique. Our methods also lead to efficient policy extragradient algorithms for solving (entropy-regularized) zero-sum Markov games at similar rates. All of our convergence rates are nearly dimension-free, which are independent of the size of the state and action spaces up to logarithm factors, highlighting the positive role of entropy regularization for accelerating convergence.",entropy regularization | extragradient methods | global convergence | matrix game | multiplicative updates | no-regret learning | zero-sum Markov game,0,0.0,,,CMU,DMS-2143215,Google Research
2-s2.0-105000492651,,,,Fast Samplers for Inverse Problems in Iterative Refinement Models,cp,Conference Paper,Pandey K.,60142655,Department of Computer Science,Irvine,United States,3.0,"Pandey, Kushagra;Yang, Ruihan;Mandt, Stephan",57548051200;57224770203;35272748600,60142655;60142655;60142655,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Constructing fast samplers for unconditional diffusion and flow-matching models has received much attention recently; however, existing methods for solving inverse problems, such as super-resolution, inpainting, or deblurring, still require hundreds to thousands of iterative steps to obtain high-quality results. We propose a plug-and-play framework for constructing efficient samplers for inverse problems, requiring only pre-trained diffusion or flow-matching models. We present Conditional Conjugate Integrators, which leverage the specific form of the inverse problem to project the respective conditional diffusion/flow dynamics into a more amenable space for sampling. Our method complements popular posterior approximation methods for solving inverse problems using diffusion/flow models. We evaluate the proposed method's performance on various linear image restoration tasks across multiple datasets, employing diffusion and flow-matching models. Notably, on challenging inverse problems like 4× super-resolution on the ImageNet dataset, our method can generate high-quality samples in as few as 5 conditional sampling steps and outperforms competing baselines requiring 20-1000 steps. Our code will be publicly available at https://github.com/mandt-lab/c-pigdm.",,4,0.0,,,IARPA,IIS-2047418,Intelligence Advanced Research Projects Activity
2-s2.0-85191160832,,,,Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms,cp,Conference Paper,Zhang Q.,60025778,"University of Michigan, Ann Arbor",Ann Arbor,United States,2.0,"Zhang, Qining;Ying, Lei",57219165234;35239178600,60025778;60025778,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"This paper considers a stochastic Multi-Armed Bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of T consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces Regret Optimal Best Arm Identification (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present an algorithm called EOCP and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in Oplog Tq rounds with predetermined stopping time and Oplog<sup>2</sup> Tq rounds with adaptive stopping time. We further characterize lower bounds on the commitment time (equivalent to the sample complexity) of ROBAI, showing that EOCP and its variants are sample optimal with pre-determined stopping time, and almost sample optimal with adaptive stopping time. Numerical results confirm our theoretical analysis and reveal an interesting “over-exploration” phenomenon carried by classic UCB algorithms, such that EOCP has smaller regret even though it stops exploration much earlier than UCB, i.e., Oplog Tq versus OpTq, which suggests over-exploration is unnecessary and potentially harmful to system performance.",,3,0.0,,,NSF,2112471,National Science Foundation
2-s2.0-85168243660,10.1609/aaai.v37i7.26003,,,FastAMI — a Monte Carlo Approach to the Adjustment for Chance in Clustering Comparison Metrics,cp,Conference Paper,Klede K.,60000765,Friedrich-Alexander-Universität Erlangen-Nürnberg,Erlangen,Germany,4.0,"Klede, Kai;Schwinn, Leo;Zanca, Dario;Eskofier, Björn",58307112200;57219525142;57202056859;26428080900,60000765;60000765;60000765;60000765,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,8317-8324,"Clustering is at the very core of machine learning, and its applications proliferate with the increasing availability of data. However, as datasets grow, comparing clusterings with an adjustment for chance becomes computationally difficult, preventing unbiased ground-truth comparisons and solution selection. We propose FastAMI, a Monte Carlo-based method to efficiently approximate the Adjusted Mutual Information (AMI) and extend it to the Standardized Mutual Information (SMI). The approach is compared with the exact calculation and a recently developed variant of the AMI based on pairwise permutations, using both synthetic and real data. In contrast to the exact calculation our method is fast enough to enable these adjusted information-theoretic comparisons for large datasets while maintaining considerably more accurate results than the pairwise approach.",,2,1.0,all publisherfullgold,All Open Access Gold,StMWi,,"Bayerisches Staatsministerium für Wirtschaft, Infrastruktur, Verkehr und Technologie"
2-s2.0-85147246672,10.1613/jair.1.13878,,,"Favoring Eagerness for Remaining Items: Designing Efficient, Fair, and Strategyproof Mechanisms",ar,Article,Guo X.,60014966;60136396;60148232,Peking University;Thomas J. Watson College of Engineering and Applied Science;Department of Computer Science,Beijing;Binghamton;Troy,China;United States;United States,5.0,"Guo, Xiaoxi;Sikdar, Sujoy;Xia, Lirong;Cao, Yongzhi;Wang, Hanpin",57219623990;56028510100;23011050500;55470280700;22734029400,60014966;60136396;60148232;60014966;60014966,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,287-339,"In the assignment problem, the goal is to assign indivisible items to agents who have ordinal preferences, efficiently and fairly, in a strategyproof manner. In practice, first-choice maximality, i.e., assigning a maximal number of agents their top items, is often identified as an important efficiency criterion and measure of agents’ satisfaction. In this paper, we propose a natural and intuitive efficiency property, favoring-eagerness-for-remaining-items (FERI), which requires that each item is allocated to an agent who ranks it highest among remaining items, thereby implying first-choice maximality. Using FERI as a heuristic, we design mechanisms that satisfy ex-post or ex-ante variants of FERI together with combinations of other desirable properties of efficiency (Pareto-efficiency), fairness (strong equal treatment of equals and sd-weak-envy-freeness), and strategyproofness (sdweak-strategyproofness). We also explore the limits of FERI mechanisms in providing stronger efficiency, fairness, or strategyproofness guarantees through impossibility results.",,2,1.0,all publisherfullgold,All Open Access Gold,NSF,1453542,National Science Foundation
2-s2.0-85163101266,,,,FedNL: Making Newton-Type Methods Applicable to Federated Learning,cp,Conference Paper,Safaryan M.,60092945;60000308;60116488,King Abdullah University of Science and Technology;Moscow Institute of Physics and Technology;Institut Polytechnique de Paris,Thuwal;Dolgoprudny;Palaiseau,Saudi Arabia;Russian Federation;France,4.0,"Safaryan, Mher;Islamov, Rustem;Qian, Xun;Richtárik, Peter",56072616400;57222274259;57216159846;49862075900,60092945;60092945-60000308-60116488;60092945;60092945,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,18959-19010,"Inspired by recent work of Islamov et al (2021), we propose a family of Federated Newton Learn (FedNL) methods, which we believe is a marked step in the direction of making second-order methods applicable to FL. In contrast to the aforementioned work, FedNL employs a different Hessian learning technique which i) enhances privacy as it does not rely on the training data to be revealed to the coordinating server, ii) makes it applicable beyond generalized linear models, and iii) provably works with general contractive compression operators for compressing the local Hessians, such as Top-K or Rank-R, which are vastly superior in practice. Notably, we do not need to rely on error feedback for our methods to work with contractive compressors. Moreover, we develop FedNL-PP, FedNL-CR and FedNL-LS, which are variants of FedNL that support partial participation, and globalization via cubic regularization and line search, respectively, and FedNL-BC, which is a variant that can further benefit from bidirectional compression of gradients and models, i.e., smart uplink gradient and smart downlink model compression. We prove local convergence rates that are independent of the condition number, the number of training data points, and compression variance. Our communication efficient Hessian learning technique provably learns the Hessian at the optimum. Finally, we perform a variety of numerical experiments that show that our FedNL methods have state-of-the-art communication complexity when compared to key baselines.",,32,0.0,,,,,
2-s2.0-85137474443,10.1609/aaai.v36i7.20785,,,FedSoft: Soft Clustered Federated Learning with Proximal Local Updating,cp,Conference Paper,Ruan Y.,60027950,Carnegie Mellon University,Pittsburgh,United States,2.0,"Ruan, Yichen;Joe-Wong, Carlee",57205680816;47962256600,60027950;60027950,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,8124-8131,"Traditionally, clustered federated learning groups clients with the same data distribution into a cluster, so that every client is uniquely associated with one data distribution and helps train a model for this distribution. We relax this hard association assumption to soft clustered federated learning, which allows every local dataset to follow a mixture of multiple source distributions. We propose FedSoft, which trains both locally personalized models and high-quality cluster models in this setting. FedSoft limits client workload by using proximal updates to require the completion of only one optimization task from a subset of clients in every communication round. We show, analytically and empirically, that FedSoft effectively exploits similarities between the source distributions to learn personalized and cluster models that perform well.",,107,1.0,all publisherfullgold,All Open Access Gold,NSF,CNS-2106891,National Science Foundation
2-s2.0-85203828214,,,,Feel-Good Thompson Sampling for Contextual Dueling Bandits,cp,Conference Paper,Li X.,60153950,UCLA Samueli School of Engineering,Los Angeles,United States,3.0,"Li, Xuheng;Zhao, Heyang;Gu, Quanquan",58749875900;57323629100;56020748300,60153950;60153950;60153950,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,29406-29426,"Contextual dueling bandits, where a learner compares two options based on context and receives feedback indicating which was preferred, extends classic dueling bandits by incorporating contextual information for decision-making and preference learning. Several algorithms based on the upper confidence bound (UCB) have been proposed for linear contextual dueling bandits. However, no algorithm based on posterior sampling has been developed in this setting, despite the empirical success observed in traditional contextual bandits. In this paper, we propose a Thompson sampling algorithm, named FGTS.CDB, for linear contextual dueling bandits. At the core of our algorithm is a new Feel-Good exploration term specifically tailored for dueling bandits. This term leverages the independence of the two selected arms, thereby avoiding a cross term in the analysis. We show that our algorithm achieves nearly minimax-optimal regret, i.e., Õ(d√T), where d is the model dimension and T is the time horizon. Finally, we evaluate our algorithm on synthetic data and observe that FGTS.CDB outperforms existing algorithms by a large margin.",,1,0.0,,,NSF,DMS-2323113,National Science Foundation
2-s2.0-85137584583,10.1613/JAIR.1.13470,,,Finding and Recognizing Popular Coalition Structures,ar,Article,Brandt F.,60019722,Technische Universität München,Munich,Germany,2.0,"Brandt, Felix;Bullinger, Martin",35232736700;57220049673,60019722;60019722,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,569-626,"An important aspect of multi-agent systems concerns the formation of coalitions that are stable or optimal in some well-defined way. The notion of popularity has recently received a lot of attention in this context. A partition is popular if there is no other partition in which more agents are better off than worse off. In this paper, we study popularity, strong popularity, and mixed popularity (which is particularly attractive because existence is guaranteed by the Minimax Theorem) in a variety of coalition formation settings. Extending previous work on marriage games, we show that mixed popular partitions in roommate games can be found efficiently via linear programming and a separation oracle. This approach is quite universal, leading to efficient algorithms for verifying whether a given partition is popular and for finding strongly popular partitions (resolving an open problem). By contrast, we prove that both problems become computationally intractable when moving from coalitions of size 2 to coalitions of size 3, even when preferences are strict and globally ranked. Moreover, we show that finding popular, strongly popular, and mixed popular partitions in symmetric additively separable hedonic games and symmetric fractional hedonic games is NP-hard. Together, these results indicate strong boundaries to the tractability of popularity in both ordinal and cardinal models of hedonic games.",,18,1.0,all publisherfullgold,All Open Access Gold,DFG,BR 2312/12-1,Deutsche Forschungsgemeinschaft
2-s2.0-85126576959,10.1613/JAIR.1.13112,,,Fine-Grained Prediction of Political Leaning on Social Media with Unsupervised Deep Learning,ar,Article,Fagni T.,60021199,Consiglio Nazionale delle Ricerche,Rome,Italy,2.0,"Fagni, Tiziano;Cresci, Stefano",14017678600;56178304900,60021199;60021199,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,633-672,"Predicting the political leaning of social media users is an increasingly popular task, given its usefulness for electoral forecasts, opinion dynamics models and for studying the political dimension of polarization and disinformation. Here, we propose a novel unsupervised technique for learning fine-grained political leaning from the textual content of social media posts. Our technique leverages a deep neural network for learning latent political ideologies in a representation learning task. Then, users are projected in a low-dimensional ideology space where they are subsequently clustered. The political leaning of a user is automatically derived from the cluster to which the user is assigned. We evaluated our technique in two challenging classification tasks and we compared it to baselines and other state-of-the-art approaches. Our technique obtains the best results among all unsupervised techniques, with micro F1 = 0:426 in the 8-class task and micro F1 = 0:772 in the 3-class task. Other than being interesting on their own, our results also pave the way for the development of new and better unsupervised approaches for the detection of fine-grained political leaning.",,12,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,871042,
2-s2.0-85137884204,10.24963/ijcai.2022/247,,,Fine-grained Complexity of Partial Minimum Satisfiability,cp,Conference Paper,Bliznets I.,60018163;60020513;60020133,TU Wien;HSE University;St. Petersburg Department of V.A.Steklov Institute of Mathematics of the Russian Academy of Sciences,Vienna;Moscow;Saint Petersburg,Austria;Russian Federation;Russian Federation,3.0,"Bliznets, Ivan;Sagunov, Danil;Simonov, Kirill",55515753000;57210284912;57212864071,60020513-60020133;60020133;60018163,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1774-1780,"There is a well-known approach to cope with NP-hard problems in practice: reduce the given problem to SAT or MAX-SAT and run a SAT or a MAX-SAT solver. This method is very efficient since SAT/MAX-SAT solvers are extremely well-studied, as well as the complexity of these problems. At AAAI 2011, Li et al. proposed an alternative to this approach and suggested the Partial Minimum Satisfiability problem as a reduction target for NP-hard problems. They developed the MinSatz solver and showed that reducing to PARTIAL MIN-SAT and using MinSatz is in some cases more efficient than reductions to SAT or MAX-SAT. Since then many results connected to the PARTIAL MIN-SAT problem were published. However, to the best of our knowledge, the worst-case complexity of PARTIAL MIN-SAT has not been studied up until now. Our goal is to fix the issue and show a O<sup>∗</sup>((2 - ∊)<sup>m</sup>) lower bound under the SETH assumption (here m is the total number of clauses), as well as several other lower bounds and parameterized exact algorithms with better-than-trivial running time.",,2,1.0,all publisherfree2read,All Open Access Bronze,ВШЭ,Y 1329,National Research University Higher School of Economics
2-s2.0-85146197462,10.1609/aaai.v36i5.20534,,,First Order Rewritability in Ontology-Mediated Querying in Horn Description Logics,cp,Conference Paper,Toman D.,60014171,University of Waterloo,Waterloo,Canada,2.0,"Toman, David;Weddell, Grant",6701332309;6701401874,60014171;60014171,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,5897-5905,"We consider first-order (FO) rewritability for query answering in ontology-mediated querying (OMQ) in which ontologies are formulated in Horn fragments of description logics (DLs). In general, OMQ approaches for such logics rely on non-FO rewriting of the query and/or on non-FO completion of the data, called an ABox. Specifically, we consider the problem of FO rewritability in terms of Beth definability, and show how Craig interpolation can then be used to effectively construct the rewritings, when they exist, from the Clark's completion of Datalog-like programs encoding a given DL TBox and optionally a query. We show how this approach to FO rewritability can also be used to (a) capture integrity constraints commonly available in backend relational data sources, (b) capture constraints inherent in mapping such sources to an ABox, and (c) can be used as an alternative to deriving so-called perfect rewritings of queries in the case of DL-Lite ontologies.",,4,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85147323967,10.1613/jair.1.13511,,,First-Order Rewritability and Complexity of Two-Dimensional Temporal Ontology-Mediated Queries,ar,Article,Artale A.,60020661;60018353;60020513;60009016;60009914,"University of Liverpool;Technische Universität Dresden;HSE University;Birkbeck, University of London;Free University of Bozen-Bolzano",Liverpool;Dresden;Moscow;London;Bolzano,United Kingdom;Germany;Russian Federation;United Kingdom;Italy,6.0,"Artale, Alessandro;Kontchakov, Roman;Kovtunova, Alisa;Ryzhikov, Vladislav;Wolter, Frank;Zakharyaschev, Michael",6601971854;23008721700;57003556800;23467849100;7005739306;6602982360,60009914;60009016;60018353;60009016;60020661;60009016-60020513,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,1223-1291,"Aiming at ontology-based data access to temporal data, we design two-dimensional temporal ontology and query languages by combining logics from the (extended) DL-Lite family with linear temporal logic LTL over discrete time (Z, <). Our main concern is first-order rewritability of ontology-mediated queries (OMQs) that consist of a 2D ontology and a positive temporal instance query. Our target languages for FO-rewritings are two-sorted FO(<)—first-order logic with sorts for time instants ordered by the built-in precedence relation < and for the domain of individuals—its extension FO(<, ≡) with the standard congruence predicates t ≡ 0 (mod n), for any fixed n > 1, and FO(RPR) that admits relational primitive recursion. In terms of circuit complexity, FO(<, ≡)- and FO(RPR)rewritability guarantee answering OMQs in uniform AC<sup>0</sup> and NC<sup>1</sup>, respectively. We proceed in three steps. First, we define a hierarchy of 2D DL-Lite/LTL ontology languages and investigate the FO-rewritability of OMQs with atomic queries by constructing projections onto 1D LTL OMQs and employing recent results on the FO-rewritability of propositional LTL OMQs. As the projections involve deciding consistency of ontologies and data, we also consider the consistency problem for our languages. While the undecidability of consistency for 2D ontology languages with expressive Boolean role inclusions might be expected, we also show that, rather surprisingly, the restriction to Krom and Horn role inclusions leads to decidability (and ExpSpace-completeness), even if one admits full Booleans on concepts. As a final step, we lift some of the rewritability results for atomic OMQs to OMQs with expressive positive temporal instance queries. The lifting results are based on an in-depth study of the canonical models and only concern Horn ontologies.",,9,1.0,all publisherfullgold,All Open Access Gold,CPEC,EP/S032207/1,California Postsecondary Education Commission
2-s2.0-85161980367,10.1613/JAIR.1.13942,,,FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?,ar,Article,Tuli S.,60015150;60141284,Imperial College London;School of Engineering and Applied Science,London;Princeton,United Kingdom;United States,4.0,"Tuli, Shikhar;Dedhia, Bhishma;Tuli, Shreshth;Jha, Niraj K.",57208274340;58968468300;57208274339;7102310305,60141284;60141284;60015150;60141284,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,,77.0,,39-70,"The existence of a plethora of language models makes the problem of selecting the best one for a custom task challenging. Most state-of-the-art methods leverage transformer-based models (e.g., BERT) or their variants. However, training such models and exploring their hyperparameter space is computationally expensive. Prior work proposes several neural architecture search (NAS) methods that employ performance predictors (e.g., surrogate models) to address this issue; however, such works limit analysis to homogeneous models that use fixed dimensionality throughout the network. This leads to sub-optimal architectures. To address this limitation, we propose a suite of heterogeneous and flexible models, namely FlexiBERT, that have varied encoder layers with a diverse set of possible operations and different hidden dimensions. For better-posed surrogate modeling in this expanded design space, we propose a new graph-similarity-based embedding scheme. We also propose a novel NAS policy, called BOSHNAS, that leverages this new scheme, Bayesian modeling, and second-order optimization, to quickly train and use a neural surrogate model to converge to the optimal architecture. A comprehensive set of experiments shows that the proposed policy, when applied to the FlexiBERT design space, pushes the performance frontier upwards compared to traditional models. FlexiBERT-Mini, one of our proposed models, has 3% fewer parameters than BERT-Mini and achieves 8.9% higher GLUE score. A FlexiBERT model with equivalent performance as the best homogeneous model has 2.6× smaller size. FlexiBERT-Large, another proposed model, attains state-of-the-art results, outperforming the baseline models by at least 5.7% on the GLUE benchmark.",,7,1.0,all publisherfullgold,All Open Access Gold,NSF,CCF-2203399,National Science Foundation
2-s2.0-85165193600,10.1613/jair.1.14139,,,FlexiBO: A Decoupled Cost-Aware Multi-Objective Optimization Approach for Deep Neural Networks,ar,Article,Iqbal M.S.,60018179;60008827,University of South Carolina;University of Wyoming,Columbia;Laramie,United States;United States,4.0,"Iqbal, Md Shahriar;Su, Jianhai;Kotthoff, Lars;Jamshidi, Pooyan",57215352993;57217476574;36447466900;34880055700,60018179;60018179;60008827;60018179,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,645-682,"The design of machine learning systems often requires trading off different objectives, for example, prediction error and energy consumption for deep neural networks (DNNs). Typically, no single design performs well in all objectives; therefore, finding Pareto-optimal designs is of interest. The search for Pareto-optimal designs involves evaluating designs in an iterative process, and the measurements are used to evaluate an acquisition function that guides the search process. However, measuring different objectives incurs different costs. For example, the cost of measuring the prediction error of DNNs is orders of magnitude higher than that of measuring the energy consumption of a pre-trained DNN as it requires re-training the DNN. Current state-of-the-art methods do not consider this difference in objective evaluation cost, potentially incurring expensive evaluations of objective functions in the optimization process. In this paper, we develop a novel decoupled and cost-aware multi-objective optimization algorithm, which we call Flexible Multi-Objective Bayesian Optimization (FlexiBO) to address this issue. For evaluating each design, FlexiBO selects the objective with higher relative gain by weighting the improvement of the hypervolume of the Pareto region with the measurement cost of each objective. This strategy, therefore, balances the expense of collecting new information with the knowledge gained through objective evaluations, preventing FlexiBO from performing expensive measurements for little to no gain. We evaluate FlexiBO on seven state-of-the-art DNNs for image recognition, natural language processing (NLP), and speech-to-text translation. Our results indicate that, given the same total experimental budget, FlexiBO discovers designs with 4.8% to 12.4% lower hypervolume error than the best method in state-of-the-art multi-objective optimization.",,3,1.0,all publisherfullgold,All Open Access Gold,NSF,1813537,National Science Foundation
2-s2.0-85167691182,10.1609/aaai.v37i1.25146,,,Flexible 3D Lane Detection by Hierarchical Shape Matching,cp,Conference Paper,Guan Z.,60018308;60114181,Xi'an Jiaotong University;Tencent,Xi'an;Shenzhen,China;China,9.0,"Guan, Zhihao;Liu, Ruixin;Yuan, Zejian;Liu, Ao;Tang, Kun;Zhou, Tong;Li, Erlong;Zheng, Chao;Mei, Shuqi",57473960700;57219767287;7401477128;58119802500;58423661600;58037560800;58037944100;57816562800;57223909260,60018308;60018308;60018308;60114181;60114181;60114181;60114181;60114181;60114181,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,694-701,"As one of the basic while vital technologies for HD map construction, 3D lane detection is still an open problem due to varying visual conditions, complex typologies, and strict demands for precision. In this paper, an end-to-end flexible and hierarchical lane detector is proposed to precisely predict 3D lane lines from point clouds. Specifically, we design a hierarchical network predicting flexible representations of lane shapes at different levels, simultaneously collecting global instance semantics and avoiding local errors. In the global scope, we propose to regress parametric curves w.r.t adaptive axes that help to make more robust predictions towards complex scenes, while in the local vision the structure of lane segment is detected in each of the dynamic anchor cells sampled along the global predicted curves. Moreover, corresponding global and local shape matching losses and anchor cell generation strategies are designed. Experiments on two datasets show that we overwhelm current top methods under high precision standards, and full ablation studies also verify each part of our method. Our codes will be released at https://github.com/Doo-do/FHLD.",,3,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,NSFC,61976170,National Natural Science Foundation of China
2-s2.0-85203821244,,,,Flexible Residual Binarization for Image Super-Resolution,cp,Conference Paper,Zhang Y.,60025084;60025858;60013789,Shanghai Jiao Tong University;ETH Zürich;Beihang University,Shanghai;Zurich;Beijing,China;Switzerland;China,6.0,"Zhang, Yulun;Qin, Haotong;Zhao, Zixiang;Liu, Xianglong;Danelljan, Martin;Yu, Fisher",56928496400;57215219668;57218542866;36100195100;56422503300;57141404100,60025084;60025858;60025858;60013789;60025858;60025858,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,59731-59740,"Binarized image super-resolution (SR) has attracted much research attention due to its potential to drastically reduce parameters and operations.However, most binary SR works binarize network weights directly, which hinders high-frequency information extraction.Furthermore, as a pixel-wise reconstruction task, binarization often results in heavy representation content distortion.To address these issues, we propose a flexible residual binarization (FRB) method for image SR.We first propose a second-order residual binarization (SRB), to counter the information loss caused by binarization.In addition to the primary weight binarization, we also binarize the reconstruction error, which is added as a residual term in the prediction.Furthermore, to narrow the representation content gap between the binarized and full-precision networks, we propose Distillation-guided Binarization Training (DBT).We uniformly align the contents of different bit widths by constructing a normalized attention form.Finally, we generalize our method by applying our FRB to binarize convolution and Transformer-based SR networks, resulting in two binary baselines: FRBC and FRBT.We conduct extensive experiments and comparisons with recent leading binarization methods.Our proposed baselines, FRBC and FRBT, achieve superior performance both quantitatively and visually.",,2,0.0,,,STCSM,2021SHZDZX0102,Huawei Technologies
2-s2.0-85147708647,10.1609/aaai.v36i1.19966,,,Flow-Based Unconstrained Lip to Speech Generation,cp,Conference Paper,He J.,60003970;126235536,Zhejiang University;Huawei Cloud,Hangzhou;St Cloud,China;United States,6.0,"He, Jinzheng;Zhao, Zhou;Ren, Yi;Liu, Jinglin;Huai, Baoxing;Yuan, Nicholas",57219785062;55959624600;57214747414;57219735077;55959581300;55818206900,60003970;60003970;60003970;60003970;126235536;126235536,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,1166-1173,"Unconstrained lip-to-speech aims to generate corresponding speeches based on silent facial videos with no restriction to head pose or vocabulary. It is desirable to generate intelligible and natural speech with a fast speed in unconstrained settings. Currently, to handle the more complicated scenarios, most existing methods adopt the autoregressive architecture, which is optimized with the MSE loss. Although these methods have achieved promising performance, they are prone to bring issues including high inference latency and mel-spectrogram over-smoothness. To tackle these problems, we propose a novel flow-based non-autoregressive lip-to-speech model (GlowLTS) to break autoregressive constraints and achieve faster inference. Concretely, we adopt a flow-based decoder which is optimized by maximizing the likelihood of the training data and is capable of more natural and fast speech generation. Moreover, we devise a condition module to improve the intelligibility of generated speech. We demonstrate the superiority of our proposed method through objective and subjective evaluation on Lip2Wav-Chemistry-Lectures and Lip2Wav-Chess-Analysis datasets. Our demo video can be found at https://glowlts.github.io/.",,15,1.0,all publisherfullgold,All Open Access Gold,NSFC,62072397,National Natural Science Foundation of China
2-s2.0-85185581329,,,,ForecastPFN: Synthetically-Trained Zero-Shot Forecasting,cp,Conference Paper,Dooley S.,60031581;131185517,California Institute of Technology;Abacus.AI,Pasadena;,United States;,5.0,"Dooley, Samuel;Khurana, Gurnoor Singh;Mohapatra, Chirag;Naidu, Siddartha;White, Colin",57219586723;58722785200;58195070300;58570775100;56927360900,131185517;131185517;131185517;131185517;131185517-60031581,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"The vast majority of time-series forecasting approaches require a substantial training dataset. However, many real-life forecasting applications have very little initial observations, sometimes just 40 or fewer. Thus, the applicability of most forecasting methods is restricted in data-sparse commercial applications. While there is recent work in the setting of very limited initial data (so-called 'zero-shot' forecasting), its performance is inconsistent depending on the data used for pretraining. In this work, we take a different approach and devise ForecastPFN, the first zero-shot forecasting model trained purely on a novel synthetic data distribution. ForecastPFN is a prior-data fitted network, trained to approximate Bayesian inference, which can make predictions on a new time series dataset in a single forward pass. Through extensive experiments, we show that zero-shot predictions made by ForecastPFN are more accurate and faster compared to state-of-the-art forecasting methods, even when the other methods are allowed to train on hundreds of additional in-distribution data points.",,40,0.0,,,,,
2-s2.0-85174389855,,,,Fractional Denoising for 3D Molecular Pre-training,cp,Conference Paper,Feng S.,60019499;60025278,Chinese Academy of Sciences;Tsinghua University,Beijing;Beijing,China;China,5.0,"Feng, Shikun;Ni, Yuyan;Lan, Yanyan;Ma, Zhi Ming;Ma, Weiying",58530695700;58530726600;59301567800;55479145500;36071778600,60025278;60019499;60025278;60019499;60025278,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,9938-9961,"Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low sampling coverage and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of noise and design a novel fractional denoising method (Frad), which only denoises the latter coordinate part. In this way, Frad enjoys both the merits of sampling more low-energy structures and the force field equivalence. Extensive experiments show the effectiveness of Frad in molecular representation, with a new state-of-the-art on 9 out of 12 tasks of QM9 and on 7 out of 8 targets of MD17 The code is released publicly at https://github.com/fengshikun/Frad.",,26,0.0,,,THU,20221080053,Tsinghua University
2-s2.0-85159606824,,,,Framework for Evaluating Faithfulness of Local Explanations,cp,Conference Paper,Dasgupta S.,60030612;60005681,"University of California, San Diego;Tel Aviv University",La Jolla;Tel Aviv-Yafo,United States;Israel,3.0,"Dasgupta, Sanjoy;Frost, Nave;Moshkovitz, Michal",7202154680;57194513669;56337532100,60030612;60005681;60005681,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,4794-4815,"We study the faithfulness of an explanation system to the underlying prediction model. We show that this can be captured by two properties, consistency and sufficiency, and introduce quantitative measures of the extent to which these hold. Interestingly, these measures depend on the test-time data distribution. For a variety of existing explanation systems, such as anchors, we analytically study these quantities. We also provide estimators and sample complexity bounds for empirically determining the faithfulness of black-box explanation systems. Finally, we experimentally validate the new properties and estimators.",,33,0.0,,,NSF,993/17,National Science Foundation
2-s2.0-85163079076,,,,FriendlyCore: Practical Differentially Private Aggregation,cp,Conference Paper,Tsfadia E.,60005681;60006191,Tel Aviv University;Google LLC,Tel Aviv-Yafo;Mountain View,Israel;United States,5.0,"Tsfadia, Eliad;Cohen, Edith;Kaplan, Haim;Mansour, Yishay;Stemmer, Uri",56271626000;7403588889;7403208727;7004528828;55580450000,60006191-60005681;60006191-60005681;60006191-60005681;60006191-60005681;60006191-60005681,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,21828-21863,"Differentially private algorithms for common metric aggregation tasks, such as clustering or averaging, often have limited practicality due to their complexity or to the large number of data points that is required for accurate results. We propose a simple and practical tool, FriendlyCore, that takes a set of points D from an unrestricted (pseudo) metric space as input. When D has effective diameter r, FriendlyCore returns a “stable” subset C ⊆ D that includes all points, except possibly a few outliers, and is guaranteed to have diameter r. FriendlyCore can be used to preprocess the input before privately aggregating it, potentially simplifying the aggregation or boosting its accuracy. Surprisingly, FriendlyCore is light-weight with no dependence on the dimension. We empirically demonstrate its advantages in boosting the accuracy of mean estimation and clustering tasks such as k-means and k-GMM, outperforming tailored methods.",,24,0.0,,,,882396,
2-s2.0-85141328111,10.1609/aaai.v36i10.21293,,,From Good to Best: Two-Stage Training for Cross-Lingual Machine Reading Comprehension,cp,Conference Paper,Chen N.,60014966;60018491;60026532,Peking University;Simon Fraser University;Microsoft Corporation,Beijing;Burnaby;Redmond,China;Canada;United States,4.0,"Chen, Nuo;Shou, Linjun;Gong, Ming;Pei, Jian",57221907210;57211168243;57214933038;35273378100,60014966;60026532;60026532;60018491,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,10501-10508,"Cross-lingual Machine Reading Comprehension (xMRC) is challenging due to the lack of training data in low-resource languages. The recent approaches use training data only in a resource-rich language like English to fine-tune large-scale cross-lingual pre-trained language models. Due to the big difference between languages, a model fine-tuned only by a source language may not perform well for target languages. Interestingly, we observe that while the top-1 results predicted by the previous approaches may often fail to hit the ground-truth answers, the correct answers are often contained in the top-k predicted results. Based on this observation, we develop a two-stage approach to enhance the model performance. The first stage targets at recall: we design a hard-learning (HL) algorithm to maximize the likelihood that the top-k predictions contain the accurate answer. The second stage focuses on precision: an answer-aware contrastive learning (AA-CL) mechanism is developed to learn the fine difference between the accurate answer and other candidates. Our extensive experiments show that our model significantly outperforms a series of strong baselines on two cross-lingual MRC benchmark datasets.",,15,1.0,all publisherfullgold,All Open Access Gold,NSERC,,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85170382361,10.24963/ijcai.2023/695,,,Full Scaling Automation for Sustainable Development of Green Data Centers,cp,Conference Paper,Wang S.,60121285,Ant group,Hangzhou,China,8.0,"Wang, Shiyu;Sun, Yinbo;Shi, Xiaoming;Shiyi, Zhu;Ma, Lin Tao;Zhang, James;Zheng, Yang Fei;Jian, Liu",57286751200;57888091500;57285372300;57749002800;57211270347;57221657806;57747705700;58569289600,60121285;60121285;60121285;60121285;60121285;60121285;60121285;60121285,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,6264-6271,"The rapid rise in cloud computing has resulted in an alarming increase in data centers' carbon emissions, which now account for >3% of global greenhouse gas emissions, necessitating immediate steps to combat their mounting strain on the global climate. An important focus of this effort is to improve resource utilization in order to save electricity usage. Our proposed Full Scaling Automation (FSA) mechanism is an effective method of dynamically adapting resources to accommodate changing workloads in large-scale cloud computing clusters, enabling the clusters in data centers to maintain their desired CPU utilization target and thus improve energy efficiency. FSA harnesses the power of deep representation learning to accurately predict the future workload of each service and automatically stabilize the corresponding target CPU usage level, unlike the previous autoscaling methods, such as Autopilot or FIRM, that need to adjust computing resources with statistical models and expert knowledge. Our approach achieves significant performance improvement compared to the existing work in real-world datasets. We also deployed FSA on large-scale cloud computing clusters in industrial data centers, and according to the certification of the China Environmental United Certification Center (CEC), a reduction of 947 tons of carbon dioxide, equivalent to a saving of 1538,000 kWh of electricity, was achieved during the Double 11 shopping festival of 2022, marking a critical step for our company's strategic goal towards carbon neutrality by 2030.",,5,0.0,,,,,
2-s2.0-85163081248,,,,G-Mixup: Graph Data Augmentation for Graph Classification,cp,Conference Paper,Han X.,60020547;60148288;60150231,Texas A&M University;George R. Brown School of Engineering and Computing;School of Computing,College Station;Houston;Athens,United States;United States;United States,4.0,"Han, Xiaotian;Jiang, Zhimeng;Liu, Ninghao;Hu, Xia",8608353200;57201520949;57191072267;35114937200,60020547;60020547;60150231;60148288,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,8230-8248,"This work develops mixup for graph data. Mixup has shown superiority in improving the generalization and robustness of neural networks by interpolating features and labels between two random samples. Traditionally, Mixup can work on regular, grid-like, and Euclidean data such as image or tabular data. However, it is challenging to directly adopt Mixup to augment graph data because different graphs typically: 1) have different numbers of nodes; 2) are not readily aligned; and 3) have unique typologies in non-Euclidean space. To this end, we propose G-Mixup to augment graphs for graph classification by interpolating the generator (i.e., graphon) of different classes of graphs. Specifically, we first use graphs within the same class to estimate a graphon. Then, instead of directly manipulating graphs, we interpolate graphons of different classes in the Euclidean space to get mixed graphons, where the synthetic graphs are generated through sampling based on the mixed graphons. Extensive experiments show that G-Mixup substantially improves the generalization and robustness of GNNs.",,171,0.0,,,NSF,IIS-1750074,National Science Foundation
2-s2.0-85189562187,10.1609/aaai.v38i6.28441,,,G2P-DDM: Generating Sign Pose Sequence from Gloss Sequence with Discrete Diffusion Model,cp,Conference Paper,Xie P.,60013789;60027950,Beihang University;Carnegie Mellon University,Beijing;Pittsburgh,China;United States,6.0,"Xie, Pan;Zhang, Qipeng;Taiying, Peng;Tang, Hao;Du, Yao;Li, Zexian",57226606342;57219132123;57573392600;57208238003;57226855865;57215198341,60013789;60013789;60013789;60027950;60013789;60013789,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,6.0,,6234-6242,"The Sign Language Production (SLP) project aims to automatically translate spoken languages into sign sequences. Our approach focuses on the transformation of sign gloss sequences into their corresponding sign pose sequences (G2P). In this paper, we present a novel solution for this task by converting the continuous pose space generation problem into a discrete sequence generation problem. We introduce the Pose-VQVAE framework, which combines Variational Autoencoders (VAEs) with vector quantization to produce a discrete latent representation for continuous pose sequences. Additionally, we propose the G2P-DDM model, a discrete denoising diffusion architecture for length-varied discrete sequence data, to model the latent prior. To further enhance the quality of pose sequence generation in the discrete space, we present the CodeUnet model to leverage spatial-temporal information. Lastly, we develop a heuristic sequential clustering method to predict variable lengths of pose sequences for corresponding gloss sequences. Our results show that our model outperforms state-of-the-art G2P models on the public SLP evaluation benchmark. For more generated results, please visit our project page: https://slpdiffusier.github.io/g2p-ddm.",,14,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85147539123,10.1609/aaai.v36i10.21320,,,GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-supervised Learning and Explicit Policy Injection,cp,Conference Paper,He W.,60027363;60102083;60118460,University of Chinese Academy of Sciences;Shenzhen Institutes of Advanced Technology;Alibaba Group Holding Limited,Beijing;Shenzhen;Hangzhou,China;China;China,12.0,"He, Wanwei;Dai, Yinpei;Zheng, Yinhe;Wu, Yuchuan;Cao, Zheng;Liu, Dermot;Jiang, Peng;Yang, Min;Huang, Fei;Si, Luo;Sun, Jian;Li, Yongbin",57221048519;57204044197;55879250200;57362554400;58834530600;57362557000;59794004000;56349712700;57210150087;7006717974;57214928826;57216693726,60102083-60027363;60118460;60118460;60118460;60118460;60118460;60118460;60102083;60118460;60118460;60118460;60118460,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,10749-10757,"Pre-trained models have proved to be powerful in enhancing task-oriented dialog systems. However, current pre-training methods mainly focus on enhancing dialog understanding and generation tasks while neglecting the exploitation of dialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog model that explicitly learns dialog policy from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised learning. Specifically, we introduce a dialog act prediction task for policy optimization during pre-training and employ a consistency regularization term to refine the learned representation with the help of unlabeled dialogs. We also implement a gating mechanism to weigh suitable unlabeled dialog samples. Empirical results show that GALAXY substantially improves the performance of task-oriented dialog systems, and achieves new state-of-the-art results on benchmark datasets: In-Car, MultiWOZ2.0 and Multi- WOZ2.1, improving their end-to-end combined scores by 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a stronger few-shot ability than existing models under various low-resource settings. For reproducibility, we release the code and data at https: //github.com/siat-nlp/GALAXY.",,126,1.0,all publisherfullgold,All Open Access Gold,NSFC,61906185,National Natural Science Foundation of China
2-s2.0-85160817683,,,,GENEFACE: GENERALIZED AND HIGH-FIDELITY AUDIO-DRIVEN 3D TALKING FACE SYNTHESIS,cp,Conference Paper,Ye Z.,60117751;60159665,"College of Computer Science and Technology, Zhejiang University;ByteDance Ltd.",Hangzhou;Beijing,China;China,6.0,"Ye, Zhenhui;Jiang, Ziyue;Ren, Yi;Liu, Jinglin;He, Jin Zheng;Zhao, Zhou",57219796529;57221080015;57214747414;57219735077;57219785062;55959624600,60117751;60117751;60159665;60117751;60117751;60117751,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Generating photo-realistic video portraits with arbitrary speech audio is a crucial problem in film-making and virtual reality. Recently, several works explore the usage of neural radiance field (NeRF) in this task to improve 3D realness and image fidelity. However, the generalizability of previous NeRF-based methods to out-of-domain audio is limited by the small scale of training data. In this work, we propose GeneFace, a generalized and high-fidelity NeRF-based talking face generation method, which can generate natural results corresponding to various out-of-domain audio. Specifically, we learn a variational motion generator on a large lip-reading corpus, and introduce a domain adaptative post-net to calibrate the result. Moreover, we learn a NeRF-based renderer conditioned on the predicted facial motion. A head-aware torso-NeRF is proposed to eliminate the head-torso separation problem. Extensive experiments show that our method achieves more generalized and high-fidelity talking face generation compared to previous methods.",,32,0.0,,,NSFC,62222211,National Natural Science Foundation of China
2-s2.0-85192957748,,,,GENERALIZATION IN DIFFUSION MODELS ARISES FROM GEOMETRY-ADAPTIVE HARMONIC REPRESENTATIONS,cp,Conference Paper,Kadkhodaie Z.,60021784;60010578,New York University;Collège de France,New York;Paris,United States;France,4.0,"Kadkhodaie, Zahra;Guth, Florentin;Simoncelli, Eero P.;Mallat, Stéphane",57219688414;57219462219;7004044833;7006576695,60021784;60021784;60021784;60010578,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Deep neural networks (DNNs) trained for image denoising are able to generate high-quality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the question of whether these networks are learning the “true” continuous density of the data. Here, we show that two DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, when the number of training images is large enough. In this regime of strong generalization, diffusion-generated images are distinct from the training set, and are of high visual quality, suggesting that the inductive biases of the DNNs are well-aligned with the data density. We analyze the learned denoising functions and show that the inductive biases give rise to a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous regions. We demonstrate that trained denoisers are inductively biased towards these geometry-adaptive harmonic bases since they arise not only when the network is trained on photographic images, but also when it is trained on image classes supported on low-dimensional manifolds for which the harmonic basis is suboptimal. Finally, we show that when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic, the denoising performance of the networks is near-optimal.",,38,0.0,,,SF,1922658,Simons Foundation
2-s2.0-85140827418,,,,GNN-LM: LANGUAGE MODELING BASED ON GLOBAL CONTEXTS VIA GNN,cp,Conference Paper,Meng Y.,60003970;60005510;60033100;125575937,Zhejiang University;Nanyang Technological University;Nanjing University;Shannon.AI,Hangzhou;Singapore City;Nanjing;Shannon,China;Singapore;China;Ireland,7.0,"Meng, Yuxian;Zong, Shi;Li, Xiaoya;Sun, Xiaofei;Zhang, Tianwei;Wu, Fei;Li, Jiwei",57216617236;57318351700;57216621036;57216611212;55635885400;55533360000;56350059800,125575937;60033100;125575937;125575937-60003970;60005510;60003970;125575937-60003970,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Inspired by the notion that “to copy is easier than to memorize”, in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM.",,18,0.0,,,MOST,2020YFC0832500,Ministry of Science and Technology of the People's Republic of China
2-s2.0-85137900458,10.24963/ijcai.2022/326,,,GOCPT: Generalized Online Canonical Polyadic Tensor Factorization and Completion,cp,Conference Paper,Yang C.,60282642;60112142,Siebel School of Computing and Data Science;IQVIA Inc.,Urbana;Durham,United States;United States,3.0,"Yang, Chaoqi;Qian, Cheng;Sun, Jimeng",57219371735;57102624100;9737233900,60282642;60112142;60282642,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2348-2354,"Low-rank tensor factorization or completion is well-studied and applied in various online settings, such as online tensor factorization (where the temporal mode grows) and online tensor completion (where incomplete slices arrive gradually). However, in many real-world settings, tensors may have more complex evolving patterns: (i) one or more modes can grow; (ii) missing entries may be filled; (iii) existing tensor elements can change. Existing methods cannot support such complex scenarios. To fill the gap, this paper proposes a Generalized Online Canonical Polyadic (CP) Tensor factorization and completion framework (named GOCPT) for this general setting, where we maintain the CP structure of such dynamic tensors during the evolution. We show that existing online tensor factorization and completion setups can be unified under the GOCPT framework. Furthermore, we propose a variant, named GOCPT<inf>E</inf>, to deal with cases where historical tensor elements are unavailable (e.g., privacy protection), which achieves similar fitness as GOCPT but with much less computational cost. Experimental results demonstrate that our GOCPT can improve fitness by up to 2.8% on the JHU Covid data and 9.2% on a proprietary patient claim dataset over baselines. Our variant GOCPT<inf>E</inf> shows up to 1.2% and 5.5% fitness improvement on two datasets with about 20% speedup compared to the best model.",,6,1.0,all publisherfree2read,All Open Access Bronze,NSF,PPoSS 2028839,National Science Foundation
2-s2.0-85168249355,10.1609/aaai.v37i9.26298,,,GOHSP: A Unified Framework of Graph and Optimization-Based Heterogeneous Structured Pruning for Vision Transformer,cp,Conference Paper,Yin M.,60119448;123289278,Samsung Research America;Rutgers University,Mountain View;Kearneysville,United States;United States,5.0,"Yin, Miao;Uzkent, Burak;Shen, Yilin;Jin, Hongxia;Yuan, Bo",57201486559;36969362600;35180368000;57202307406;57203738678,123289278;60119448;60119448;60119448;123289278,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,10954-10962,"The recently proposed Vision transformers (ViTs) have shown very impressive empirical performance in various computer vision tasks, and they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale sizes, which then severely hinder their potential deployment in many practical resources-constrained applications. To mitigate this challenging problem, structured pruning is a promising solution to compress model size and enable practical efficiency. However, unlike its current popularity for CNNs and RNNs, structured pruning for ViT models is little explored. In this paper, we propose GOHSP, a unified framework of Graph and Optimization-based Structured Pruning for ViT models. We first develop a graph-based ranking for measuring the importance of attention heads, and the extracted importance information is further integrated to an optimization-based procedure to impose the heterogeneous structured sparsity patterns on the ViT models. Experimental results show that our proposed GOHSP demonstrates excellent compression performance. On CIFAR-10 dataset, our approach can bring 40% parameters reduction with no accuracy loss for ViT-Small model. On ImageNet dataset, with 30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models, our approach achieves 1.65% and 0.76% accuracy increase over the existing structured pruning methods, respectively.",,12,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85127047478,,,,GRADIENT IMPORTANCE LEARNING FOR INCOMPLETE OBSERVATIONS,cp,Conference Paper,Gao Q.,60008724;60092945,Duke University;King Abdullah University of Science and Technology,Durham;Thuwal,United States;Saudi Arabia,9.0,"Gao, Qitong;Wang, Dong;Amason, Joshua D.;Yuan, Siyang;Tao, Chenyang;Henao, Ricardo;Hadziahmetovic, Majda;Carin, Lawrence;Pajic, Miroslav",57209134235;59054243000;57210020685;57216616709;55872455600;15051946200;24467949300;7004561693;35194126200,60008724;60008724;60008724;60008724;60008724;60008724;60008724;60008724-60092945;60008724,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Though recent works have developed methods that can generate estimates (or imputations) of the missing entries in a dataset to facilitate downstream analysis, most depend on assumptions that may not align with real-world applications and could suffer from poor performance in subsequent tasks such as classification. This is particularly true if the data have large missingness rates or a small sample size. More importantly, the imputation error could be propagated into the prediction step that follows, which may constrain the capabilities of the prediction model. In this work, we introduce the gradient importance learning (GIL) method to train multilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly perform inference from inputs containing missing values without imputation. Specifically, we employ reinforcement learning (RL) to adjust the gradients used to train these models via back-propagation. This allows the model to exploit the underlying information behind missingness patterns. We test the approach on real-world time-series (i.e., MIMIC-III), tabular data obtained from an eye clinic, and a standard dataset (i.e., MNIST), where our imputation-free predictions outperform the traditional two-step imputation-based predictions using state-of-the-art imputation methods.",,9,0.0,,,NSF,CNS-1652544,National Science Foundation
2-s2.0-85204300742,,,,GS2P: A Generative Pre-trained Learning to Rank Model with Over-parameterization for Web-Scale Search,cp,Conference Paper,Li Y.,60025084;60112903,"Shanghai Jiao Tong University;Baidu, Inc.",Shanghai;Beijing,China;China,7.0,"Li, Yuchen;Xiong, Haoyi;Kong, Linghe;Bian, Jiang;Wang, Shuaiqiang;Chen, Guihai;Yin, Dawei",56819149900;55362625600;35345242100;57206770577;22636318500;7406537386;35759826200,60025084-60112903;60112903;60025084;60112903;60112903;60025084;60112903,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,8433-8438,"While learning to rank (LTR) is widely employed in web searches to prioritize pertinent webpages from the retrieved contents based on input queries, traditional LTR models stumble over two principal stumbling blocks leading to subpar performance: 1) the lack of well-annotated query-webpage pairs with ranking scores to cover search queries of various popularity, debilitating their coverage of search queries across the popularity spectrum, and 2) ill-trained models that are incapable of inducing generalized representations for LTR, culminating in over-fitting. To tackle above challenges, we proposed a Generative Semi-Supervised Pre-trained (GS<sup>2</sup>P) LTR model. We conduct extensive offline experiments on a publicly available dataset and a real-world dataset collected from a large-scale search engine. We also deploy GS<sup>2</sup>P at a large-scale web search engine with realistic traffic, where we can observe significant improvement in real-world applications.",,0,0.0,,,NSFC,62172276,Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning
2-s2.0-85213812777,,,,Gap Minimization for Knowledge Sharing and Transfer,ar,Article,Wang B.,60002494;60013789;60010884;60032619;60102562;60006320,Université McGill;Beihang University;Western University;Université Laval;School of Engineering and Applied Science;MIT Computer Science & Artificial Intelligence Laboratory,Montreal;Beijing;London;Quebec;Philadelphia;Cambridge,Canada;China;Canada;Canada;United States;United States,8.0,"Wang, Boyu;Mendez, Jorge A.;Shui, Changjian;Zhou, Fan;Wu, Di;Xu, Gezheng;Gagné, Christian;Eaton, Eric",34979599200;57208439336;57211752755;57220917575;57226552924;57755004200;12140356400;12445029500,60010884;60006320;60032619;60013789;60002494;60010884;60032619;60102562,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"Learning from multiple related tasks by knowledge sharing and transfer has become increasingly relevant over the last two decades. In order to successfully transfer information from one task to another, it is critical to understand the similarities and differences between the domains. In this paper, we introduce the notion of performance gap, an intuitive and novel measure of the distance between learning tasks. Unlike existing measures which are used as tools to bound the difference of expected risks between tasks (e.g., H-divergence or discrepancy distance), we theoretically show that the performance gap can be viewed as a data- and algorithm-dependent regularizer, which controls the model complexity and leads to finer guarantees. More importantly, it also provides new insights and motivates a novel principle for designing strategies for knowledge sharing and transfer: gap minimization. We instantiate this principle with two algorithms: 1. gapBoost, a novel and principled boosting algorithm that explicitly minimizes the performance gap between source and target domains for transfer learning; and 2. gapMTNN, a representation learning algorithm that reformulates gap minimization as semantic conditional matching for multitask learning. Our extensive evaluation on both transfer learning and multitask learning benchmark data sets shows that our methods outperform existing baselines.",Algorithmic Stability | Multitask Learning | Performance Gap | Regularization | Transfer Learning,12,0.0,,,CRSNG,FA8750-18-2-0117,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85139011572,,,,Gaussian Mixture Variational Autoencoder with Contrastive Learning for Multi-Label Classification,cp,Conference Paper,Bai J.,60278093,Cornell Ann S. Bowers College of Computing and Information Science,Ithaca,United States,3.0,"Bai, Junwen;Kong, Shufeng;Gomes, Carla",57193751374;57208498281;7101707114,60278093;60278093;60278093,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,1383-1398,"Multi-label classification (MLC) is a prediction task where each sample can have more than one label. We propose a novel contrastive learning boosted multi-label prediction model based on a Gaussian mixture variational autoencoder (C-GMVAE), which learns a multimodal prior space and employs a contrastive loss. Many existing methods introduce extra complex neural modules like graph neural networks to capture the label correlations, in addition to the prediction modules. We find that by using contrastive learning in the supervised setting, we can exploit label information effectively in a data-driven manner, and learn meaningful feature and label embeddings which capture the label correlations and enhance the predictive power. Our method also adopts the idea of learning and aligning latent spaces for both features and labels. In contrast to previous works based on a unimodal prior, C-GMVAE imposes a Gaussian mixture structure on the latent space, to alleviate the posterior collapse and over-regularization issues. C-GMVAE outperforms existing methods on multiple public datasets and can often match other models' full performance with only 50% of the training data. Furthermore, we show that the learnt embeddings provide insights into the interpretation of label-label interactions.",,49,0.0,,,NSF,W911NF-17-1-0187,National Science Foundation
2-s2.0-105000494944,,,,GenRec: Unifying Video Generation and Recognition with Diffusion Models,cp,Conference Paper,Weng Z.,60009860;60020304;126798013,"Fudan University;University of Maryland, College Park;Shanghai Collaborative Innovation Center of Intelligent Visual Computing",Shanghai;College Park;Shanghai,China;United States;China,5.0,"Weng, Zejia;Yang, Xitong;Xing, Zhen;Wu, Zuxuan;Jiang, Yu Gang",57218107226;57188768295;57537600900;56377225900;14054081900,60009860-126798013;60020304;60009860-126798013;60009860-126798013;60009860-126798013,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Video diffusion models are able to generate high-quality videos by learning strong spatial-temporal priors on large-scale datasets. In this paper, we aim to investigate whether such priors derived from a generative process are suitable for video recognition, and eventually joint optimization of generation and recognition. Building upon Stable Video Diffusion, we introduce GenRec, the first unified framework trained with a random-frame conditioning process so as to learn generalized spatial-temporal representations. The resulting framework can naturally supports generation and recognition, and more importantly is robust even when visual inputs contain limited information. Extensive experiments demonstrate the efficacy of GenRec for both recognition and generation. In particular, GenRec achieves competitive recognition performance, offering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. GenRec also performs the best on class-conditioned image-to-video generation, achieving 46.5 and 49.3 FVD scores on SSV2 and EK-100 datasets. Furthermore, GenRec demonstrates extraordinary robustness in scenarios that only limited frames can be observed. Code will be available at https://github.com/wengzejia1/GenRec.",,1,0.0,,,NSFC,62032006,National Natural Science Foundation of China
2-s2.0-85147651422,10.1609/aaai.v36i1.19963,,,Generalizable Person Re-identification via Self-Supervised Batch Norm Test-Time Adaption,cp,Conference Paper,Han K.,60019499;60027363;60018486;60278199,Chinese Academy of Sciences;University of Chinese Academy of Sciences;Institute of Automation Chinese Academy of Sciences;Center for Excellence in Brain Science and Intelligence Technology,Beijing;Beijing;Beijing;Shanghai,China;China;China;China,5.0,"Han, Ke;Si, Chenyang;Huang, Yan;Wang, Liang;Tan, Tieniu",57220093699;57204287726;59677233300;57218666547;7402022125,60018486-60027363;60018486;60018486-60027363;60018486-60027363-60278199-60019499;60018486-60027363-60278199,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,1140-1147,"In this paper, we investigate the generalization problem of person re-identification (re-id), whose major challenge is the distribution shift on an unseen domain. As an important tool of regularizing the distribution, batch normalization (BN) has been widely used in existing methods. However, they neglect that BN is severely biased to the training domain and inevitably suffers the performance drop if directly generalized without being updated. To tackle this issue, we propose Batch Norm Test-time Adaption (BNTA), a novel re-id framework that applies the self-supervised strategy to update BN parameters adaptively. Specifically, BNTA quickly explores the domain-aware information within unlabeled target data before inference, and accordingly modulates the feature distribution normalized by BN to adapt to the target domain. This is accomplished by two designed self-supervised auxiliary tasks, namely part positioning and part nearest neighbor matching, which help the model mine the domain-aware information with respect to the structure and identity of body parts, respectively. To demonstrate the effectiveness of our method, we conduct extensive experiments on three re-id datasets and confirm the superior performance to the state-of-the-art methods.",,17,1.0,all publisherfullgold,All Open Access Gold,NSFC,2019JZZY010119,Key Technology Research and Development Program of Shandong
2-s2.0-85163053963,,,,Generalization Bounds using Lower Tail Exponents in Stochastic Optimizers,cp,Conference Paper,Hodgkinson L.,60025038;60009254;60105786,"University of California, Berkeley;Purdue University;Département d'Informatique de l'ENS",Berkeley;West Lafayette;Paris,United States;United States;France,4.0,"Hodgkinson, Liam;Şimşekli, Umut;Khanna, Rajiv;Mahoney, Michael W.",57217157902;36622388800;35317824600;7202006961,60025038;60105786;60009254;60025038,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,8774-8795,"Despite the ubiquitous use of stochastic optimization algorithms in machine learning, the precise impact of these algorithms and their dynamics on generalization performance in realistic non-convex settings is still poorly understood. While recent work has revealed connections between generalization and heavy-tailed behavior in stochastic optimization, this work mainly relied on continuous-time approximations; and a rigorous treatment for the original discrete-time iterations is yet to be performed. To bridge this gap, we present novel bounds linking generalization to the lower tail exponent of the transition kernel associated with the optimizer around a local minimum, in both discrete- and continuous-time settings. To achieve this, we first prove a data- and algorithm-dependent generalization bound in terms of the celebrated Fernique-Talagrand functional applied to the trajectory of the optimizer. Then, we specialize this result by exploiting the Markovian structure of stochastic optimizers, and derive bounds in terms of their (data-dependent) transition kernels. We support our theory with empirical results from a variety of neural networks, showing correlations between generalization error and lower tail exponents.",,19,0.0,,,NSF,,National Science Foundation
2-s2.0-85170365861,10.24963/ijcai.2023/420,,,Generalization Guarantees of Self-Training of Halfspaces under Label Noise Corruption,cp,Conference Paper,Hadjadj L.,60104653,Université Grenoble Alpes,Saint Martin d'Heres,France,3.0,"Hadjadj, Lies;Amini, Massih Reza;Louhichi, Sana",57364060500;7005942550;6602218817,60104653;60104653;60104653,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,3777-3785,"We investigate the generalization properties of a self-training algorithm with halfspaces. The approach learns a list of halfspaces iteratively from labeled and unlabeled training data, in which each iteration consists of two steps: exploration and pruning. In the exploration phase, the halfspace is found sequentially by maximizing the unsigned-margin among unlabeled examples and then assigning pseudo-labels to those that have a distance higher than the current threshold. These pseudo-labels are allegedly corrupted by noise. The training set is then augmented with noisy pseudo-labeled examples, and a new classifier is trained. This process is repeated until no more unlabeled examples remain for pseudo-labeling. In the pruning phase, pseudo-labeled samples that have a distance to the last halfspace greater than the associated unsigned-margin are then discarded. We prove that the misclassification error of the resulting sequence of classifiers is bounded and show that the resulting semi-supervised approach never degrades performance compared to the classifier learned using only the initial labeled training set. Experiments carried out on a variety of benchmarks demonstrate the efficiency of the proposed approach compared to state-of-the-art methods.",,3,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,
2-s2.0-85174390110,,,,"Generalization on the Unseen, Logic Reasoning and Degree Curriculum",cp,Conference Paper,Abbe E.,126394243;121099459,EPFL;Apple,Neuchatel;Sunnyvale,Switzerland;United States,4.0,"Abbe, Emmanuel;Bengio, Samy;Lotfi, Aryo;Rizk, Kevin",16237841500;57203254475;57222185156;58095452100,126394243-121099459;121099459;126394243;126394243,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,31-60,"This paper considers the learning of logical (Boolean) functions with focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an 'extrapolating' or 'reasoning' learner. We then study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for a class of network models including instances of Transformers, random features models, and diagonal linear networks, a min-degree-interpolator is learned on the unseen. We also provide evidence that other instances with larger learning rates or mean-field networks reach leaky min-degree solutions. These findings lead to two implications: (1) we provide an explanation to the length generalization problem (e.g., Anil et al. 2022); (2) we introduce a curriculum learning algorithm called Degree-Curriculum that learns monomials more efficiently by incrementing supports.",,26,0.0,,,,,
2-s2.0-85189363397,10.1609/aaai.v38i1.27797,,,Generalize for Future: Slow and Fast Trajectory Learning for CTR Prediction,cp,Conference Paper,Zhu J.,60170584,"JD.com, Inc.",Beijing,China,6.0,"Zhu, Jian;Liu, Congcong;Jiang, Xue;Peng, Changping;Lin, Zhangang;Shao, Jingping",57348842200;57194613111;58546549900;57219435453;57348887100;57226469579,60170584;60170584;60170584;60170584;60170584;60170584,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,1.0,,428-436,"Deep neural networks (DNNs) have achieved significant advancements in click-through rate (CTR) prediction by demonstrating strong generalization on training data. However, in real-world scenarios, the assumption of independent and identically distributed (i.i.d.) conditions, which is fundamental to this problem, is often violated due to temporal distribution shifts. This violation can lead to suboptimal model performance when optimizing empirical risk without access to future data, resulting in overfitting on the training data and convergence to a single sharp minimum. To address this challenge, we propose a novel model updating framework called Slow and Fast Trajectory Learning (SFTL) network. SFTL aims to mitigate the discrepancy between past and future domains while quickly adapting to recent changes in small temporal drifts. This mechanism entails two interactions among three complementary learners: (i) the Working Learner, which updates model parameters using modern optimizers (e.g., Adam, Adagrad) and serves as the primary learner in the recommendation system, (ii) the Slow Learner, which is updated in each temporal domain by directly assigning the model weights of the working learner, and (iii) the Fast Learner, which is updated in each iteration by assigning exponentially moving average weights of the working learner. Additionally, we propose a novel rank-based trajectory loss to facilitate interaction between the working learner and trajectory learner, aiming to adapt to temporal drift and enhance performance in the current domain compared to the past. We provide theoretical understanding and conduct extensive experiments on real-world CTR prediction datasets to validate the effectiveness and efficiency of SFTL in terms of both convergence speed and model performance. The results demonstrate the superiority of SFTL over existing approaches.",,2,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85130402256,,,,Generalized Ambiguity Decomposition for Ranking Ensemble Learning,ar,Article,Liu H.,60014966;60124584,Peking University;National Engineering Research Center for Software Engineering,Beijing;Beijing,China;China,3.0,"Liu, Hongzhi;Du, Yingpeng;Wu, Zhonghai",57304569900;57203116944;36186428900,60014966;60014966;60124584,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Error decomposition analysis is a key problem for ensemble learning, which indicates that proper combination of multiple models can achieve better performance than any individual one. Existing theoretical research of ensemble learning focuses on regression or classification tasks. There is limited theoretical research for ranking ensemble. In this paper, we first generalize the ambiguity decomposition theory from regression ensemble to ranking ensemble, which proves the effectiveness of ranking ensemble with consideration of list-wise ranking information. According to the generalized theory, we propose an explicit diversity measure for ranking ensemble, which can be used to enhance the diversity of ensemble and improve the performance of ensemble model. Furthermore, we adopt an adaptive learning scheme to learn query-dependent ensemble weights, which can fit into the generalized theory and help to further improve the performance of ensemble model. Extensive experiments on recommendation and information retrieval tasks demonstrate the effectiveness and theoretical advantages of the proposed method compared with several state-of-the-art methods.",adaptive learning | ambiguity decomposition theory | diversity measure | ensemble learning | ranking ensemble,7,0.0,,,NSFC,61232005,National Natural Science Foundation of China
2-s2.0-85179136504,10.1613/JAIR.1.14238,,,Generalizing Group Fairness in Machine Learning via Utilities,ar,Article,Blandin J.,60137961,College of Engineering,Chicago,United States,2.0,"Blandin, Jack;Kash, Ian A.",57231286300;14622661200,60137961;60137961,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,747-780,"Group fairness definitions such as Demographic Parity and Equal Opportunity make assumptions about the underlying decision-problem that restrict them to classification problems. Prior work has translated these definitions to other machine learning environments, such as unsupervised learning and reinforcement learning, by implementing their closest mathematical equivalent. As a result, there are numerous bespoke interpretations of these definitions. This work aims to unify the shared aspects of each of these bespoke definitions, and to this end we provide a group fairness framework that generalizes beyond just classification problems. We leverage two fairness principles that enable this generalization. First, our framework measures outcomes in terms of utilities, rather than predictions, and does so for both the decision-maker and the individual. Second, our framework can consider counterfactual outcomes, rather than just observed outcomes, thus preventing loopholes where fairness criteria are satisfied through self-fulfilling prophecies. We provide concrete examples of how our utility fairness framework avoids these assumptions and thus naturally integrates with classification, clustering, and reinforcement learning fairness problems. We also show that many of the bespoke interpretations of Demographic Parity and Equal Opportunity fit nicely as special cases of our framework.",,3,1.0,all publisherfullgold,All Open Access Gold,NSF,1939743,National Science Foundation
2-s2.0-85148649452,10.1613/jair.1.13909,,,Generating Random SAT Instances: Multiple Solutions could be Predefined and Deeply Hidden,ar,Article,Zhao D.,60019118;60022414;60001455;60130479,"University of Science and Technology of China;Wuhan University of Technology;Anhui University;School of Computer Science and Technology, Harbin Institute of Technology",Hefei;Wuhan;Hefei;Harbin,China;China;China;China,6.0,"Zhao, Dongdong;Liao, Lei;Luo, Wenjian;Xiang, Jianwen;Jiang, Hao;Hu, Xiaoyi",55445442200;58113023500;7202199213;7201547065;57195606127;57195492668,60022414-60019118;60022414;60130479;60022414;60001455;60022414,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,435-470,"The generation of SAT instances is an important issue in computer science, and it is useful for researchers to verify the effectiveness of SAT solvers. Addressing this issue could inspire researchers to propose new search strategies. SAT problems exist in various real-world applications, some of which have more than one solution. However, although several algorithms for generating random SAT instances have been proposed, few can be used to generate hard instances that have multiple predefined solutions. In this paper, we propose the KHidden-M algorithm to generate SAT instances with multiple predefined solutions that could be hard to solve by the local search strategy when the number of predefined solutions is small enough and the Hamming distance between them is not less than half of the solution length. Specifically, first, we generate an SAT instance that is satisfied by all of the predefined solutions. Next, if the generated SAT instance does not satisfy the hardness condition, then a strategy will be conducted to adjust clauses through multiple iterations to improve the hardness of the whole instance. We propose three strategies to generate the SAT instance in the first part. The first strategy is called the random strategy, which randomly generates clauses that are satisfied by all of the predefined solutions. The other two strategies are called the estimating strategy and greedy strategy, and using them, we attempt to generate an instance that directly satisfies or is closer to the hardness condition for the local search strategy. We employ two SAT solvers (i.e., WalkSAT and Kissat) to investigate the hardness of the SAT instances generated by our algorithm in the experiments. The experimental results show the effectiveness of the random, estimating and greedy strategies. Compared to the state-of-the-art algorithm for generating SAT instances with predefined solutions, namely, M-hidden, our algorithm could be more effective in generating hard SAT instances.",,3,1.0,all publisherfullgold,All Open Access Gold,NSFC,61175045,National Natural Science Foundation of China
2-s2.0-85189606760,10.1609/aaai.v38i17.29837,,,Generative Multi-Modal Knowledge Retrieval with Large Language Models,cp,Conference Paper,Long X.,60025278;60114181,Tsinghua University;Tencent,Beijing;Shenzhen,China;China,7.0,"Long, Xinwei;Zeng, Jiali;Meng, Fandong;Ma, Zhiyuan;Zhang, Kaiyan;Zhou, Bowen;Zhou, Jie",57226480335;57225679604;55847567500;57710902200;57224929194;7401906756;57211746430,60025278;60114181;60114181;60025278;60025278;60025278;60114181,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,17.0,,18733-18741,"Knowledge retrieval with multi-modal queries plays a crucial role in supporting knowledge-intensive multi-modal applications. However, existing methods face challenges in terms of their effectiveness and training efficiency, especially when it comes to training and integrating multiple retrievers to handle multi-modal queries. In this paper, we propose an innovative end-to-end generative framework for multi-modal knowledge retrieval. Our framework takes advantage of the fact that large language models (LLMs) can effectively serve as virtual knowledge bases, even when trained with limited data. We retrieve knowledge via a two-step process: 1) generating knowledge clues related to the queries, and 2) obtaining the relevant document by searching databases using the knowledge clue. In particular, we first introduce an object-aware prefix-tuning technique to guide multi-grained visual learning. Then, we align multi-grained visual features into the textual feature space of the LLM, employing the LLM to capture cross-modal interactions. Subsequently, we construct instruction data with a unified format for model training. Finally, we propose the knowledge-guided generation strategy to impose prior constraints in the decoding steps, thereby promoting the generation of distinctive knowledge clues. Through experiments conducted on three benchmarks, we demonstrate significant improvements ranging from 3.0% to 14.6% across all evaluation metrics when compared to strong baselines.",,25,1.0,all publisherfullgold,All Open Access Gold,NKRDPC,2022ZD0160603,National Key Research and Development Program of China
2-s2.0-85213834539,,,,Generic Unsupervised Optimization for a Latent Variable Model With Exponential Family Observables,ar,Article,Mousavi H.,60020306,Universität Oldenburg,Oldenburg,Germany,4.0,"Mousavi, Hamid;Drefs, Jakob;Hirschberger, Florian;Lücke, Jörg",57213356463;57193878502;57219502224;13104958300,60020306;60020306;60020306;60020306,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"Latent variable models (LVMs) represent observed variables by parameterized functions of latent variables. Prominent examples of LVMs for unsupervised learning are probabilistic PCA or probabilistic sparse coding which both assume a weighted linear summation of the latents to determine the mean of a Gaussian distribution for the observables. In many cases, however, observables do not follow a Gaussian distribution. For unsupervised learning, LVMs which assume specific non-Gaussian observables (e.g., Bernoulli or Poisson) have therefore been considered. Already for specific choices of distributions, parameter optimization is challenging and only a few previous contributions considered LVMs with more generally defined observable distributions. In this contribution, we do consider LVMs that are defined for a range of different distributions, i.e., observables can follow any (regular) distribution of the exponential family. Furthermore, the novel class of LVMs presented here is defined for binary latents, and it uses maximization in place of summation to link the latents to observables. In order to derive an optimization procedure, we follow an expectation maximization approach for maximum likelihood parameter estimation. We then show, as our main result, that a set of very concise parameter update equations can be derived which feature the same functional form for all exponential family distributions. The derived generic optimization can consequently be applied (without further derivations) to different types of metric data (Gaussian and non-Gaussian) as well as to different types of discrete data. Moreover, the derived optimization equations can be combined with a recently suggested variational acceleration which is likewise generically applicable to the LVMs considered here. Thus, the combination maintains generic and direct applicability of the derived optimization procedure, but, crucially, enables efficient scalability. We numerically verify our analytical results using different observable distributions, and, furthermore, discuss some potential applications such as learning of variance structure, noise type estimation and denoising.",expectation maximization | exponential family distributions | Latent variable models | unsupervised learning | variational optimization,1,0.0,,,DFG,464104047,Deutsche Forschungsgemeinschaft
2-s2.0-85203837522,,,,Genie: Generative Interactive Environments,cp,Conference Paper,Bruce J.,60010365;60111161,The University of British Columbia;DeepMind Technologies Limited,Vancouver;London,Canada;United Kingdom,25.0,"Bruce, Jake;Dennis, Michael;Edwards, Ashley;Parker-Holder, Jack;Shi, Yuge;Hughes, Edward;Lai, Matthew;Mavalankar, Aditi;Steigerwald, Richie;Apps, Chris;Aytar, Yusuf;Bechtle, Sarah;Behbahani, Feryal;Chan, Stephanie;Heess, Nicolas;Gonzalez, Lucy;Osindero, Simon;Ozair, Sherjil;Reed, Scott;Zhang, Jingwei;Zolna, Konrad;Clune, Jeff;de Freitas, Nando;Singh, Satinder;Rocktäschel, Tim",57204318795;57217155227;58834589000;57218717508;58897954300;57208443684;57196122202;57188998007;57954899400;57211645935;24450005200;57188670394;56809520400;55455872400;36450421000;58078008800;12779018500;56354673800;57210562801;57200627009;57039137600;23388416900;6602751682;55548164600;55899274800,60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161;60111161-60010365;60111161;60111161;60111161,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,4603-4623,"We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.",,16,0.0,,,,,
2-s2.0-85129577533,10.1613/jair.1.13550,,,Get out of the BAG! Silos in AI Ethics Education: Unsupervised Topic Modeling Analysis of Global AI Curricula,ar,Article,Javed R.T.,60007493;60002014;60031040;60021255;60197139;60105219,"Universität Bonn;The Royal Institute of Technology (KTH);Umeå Universitet;UiT Norges Arktiske Universitet;Computer Science and Engineering Department, College of Engineering, Qatar University;Information Technology University",Bonn;Stockholm;Umea;Tromso;Doha;Lahore,Germany;Sweden;Sweden;Norway;Qatar;Pakistan,8.0,"Javed, Rana T.;Nasir, Osama;Borit, Melania;Vanhée, Loïs;Zea, Elias;Gupta, Shivam;Vinuesa, Ricardo;Qadir, Junaid",57211800983;57226728345;37076777600;53985383600;55566069700;57209786210;55553485100;15058218600,60105219;60105219;60021255;60031040;60002014;60007493;60002014;60197139,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,933-965,"The domain of Artificial Intelligence (AI) ethics is not new, with discussions going back at least 40 years. Teaching the principles and requirements of ethical AI to students is considered an essential part of this domain, with an increasing number of technical AI courses taught at several higher-education institutions around the globe including content related to ethics. By using Latent Dirichlet Allocation (LDA), a generative probabilistic topic model, this study uncovers topics in teaching ethics in AI courses and their trends related to where the courses are taught, by whom, and at what level of cognitive complexity and specificity according to Bloom's taxonomy. In this exploratory study based on unsupervised machine learning, we analyzed a total of 166 courses: 116 from North American universities, 11 from Asia, 36 from Europe, and 10 from other regions. Based on this analysis, we were able to synthesize a model of teaching approaches, which we call BAG (Build, Assess, and Govern), that combines specific cognitive levels, course content topics, and disciplines affiliated with the department(s) in charge of the course. We critically assess the implications of this teaching paradigm and provide suggestions about how to move away from these practices. We challenge teaching practitioners and program coordinators to reflect on their usual procedures so that they may expand their methodology beyond the confines of stereotypical thought and traditional biases regarding what disciplines should teach and how.",,31,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,UiT,952026,Universitetet i Tromsø
2-s2.0-85163184392,,,,Globally Gated Deep Linear Networks,cp,Conference Paper,Li Q.,60009982;60007903,Harvard University;Hebrew University of Jerusalem,Cambridge;Jerusalem,United States;Israel,2.0,"Li, Qianyi;Sompolinsky, Haim",57211231516;57203612551,60009982;60009982-60007903,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Recently proposed Gated Linear Networks (GLNs) present a tractable nonlinear network architecture, and exhibit interesting capabilities such as learning with local error signals and reduced forgetting in sequential learning. In this work, we introduce a novel gating architecture, named Globally Gated Deep Linear Networks (GGDLNs) where gating units are shared among all processing units in each layer, thereby decoupling the architectures of the nonlinear but unlearned gating and the learned linear processing motifs. We derive exact equations for the generalization properties of Bayesian Learning in these networks in the finite-width thermodynamic limit, defined by N, P ! 1 while P/N = O(1) where N and P are the hidden layers⧠width and size of training data sets respectfully. We find that the statistics of the network predictor can be expressed in terms of kernels that undergo shape renormalization through a data-dependent order parameter matrix compared to the infinite-width Gaussian Process (GP) kernels. Our theory accurately captures the behavior of finite width GGDLNs trained with gradient descent (GD) dynamics. We show that kernel shape renormalization gives rise to rich generalization properties w.r.t. network width, depth and L<inf>2</inf> regularization amplitude. Interestingly, networks with a large number of gating units behave similarly to standard ReLU architectures. Although gating units in the model do not participate in supervised learning, we show the utility of unsupervised learning of the gating parameters. Additionally, our theory allows the evaluation of the network* ability for learning multiple tasks by incorporating task-relevant information into the gating units. In summary, our work is the first exact theoretical solution of learning in a family of nonlinear networks with finite width. The rich and diverse behavior of the GGDLNs suggests that they are helpful analytically tractable models of learning single and multiple tasks, in finite-width nonlinear deep networks.",,10,0.0,,,NIH,1U19NS104653,National Institutes of Health
2-s2.0-85146847172,,,,Gradient Descent: The Ultimate Optimizer,cp,Conference Paper,Chandra K.,60006320;60271648,MIT Computer Science & Artificial Intelligence Laboratory;Meta,Cambridge;Menlo Park,United States;United States,4.0,"Chandra, Kartik;Xie, Audrey;Ragan-Kelley, Jonathan;Meijer, Erik",57194273846;57958881100;18435174100;8951368300,60006320;60006320;60006320;60271648,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer* hyperparameters, such as its step size. Recent work has shown how the step size can itself be optimized alongside the model parameters by manually deriving expressions for “hypergradients” ahead of time. We show how to automatically compute hypergradients with a simple and elegant modification to backpropagation. This allows us to easily apply the method to other optimizers and hyperparameters (e.g. momentum coefficients). We can even recursively apply the method to its own hyper-hyperparameters, and so on ad infinitum. As these towers of optimizers grow taller, they become less sensitive to the initial choice of hyperparameters. We present experiments validating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch implementation of this algorithm (see people.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer).",,40,0.0,,,NSF,-1231216,National Science Foundation
2-s2.0-85126715483,10.1609/aaai.v36i6.20611,,,Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win,cp,Conference Paper,Evci U.,60002306;60271648;132138808,University of Calgary;Meta;Google,Calgary;Menlo Park;,Canada;United States;,4.0,"Evci, Utku;Ioannou, Yani;Keskin, Cem;Dauphin, Yann",57191074676;55548420000;24477101000;50760997300,132138808;60002306;60271648;132138808,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,6577-6586,"Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). Through our analysis of gradient flow during training we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and demonstrate the importance of using sparsity-aware initialization. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from — however, this comes at the cost of learning novel solutions.",,43,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85167977954,10.1609/aaai.v37i3.25499,,,Gradient-Based Graph Attention for Scene Text Image Super-resolution,cp,Conference Paper,Zhu X.,60017060;60000891,Central South University;Loughborough University,Changsha;Loughborough,China;United Kingdom,6.0,"Zhu, Xiangyuan;Guo, Kehua;Fang, Hui;Ding, Rui;Wu, Zheng;Schaefer, Gerald",57212582580;35317515700;55470721600;58761638400;57745803700;7102506029,60017060;60017060;60000891;60017060;60017060;60000891,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,3861-3869,"Scene text image super-resolution (STISR) in the wild has been shown to be beneficial to support improved vision-based text recognition from low-resolution imagery. An intuitive way to enhance STISR performance is to explore the wellstructured and repetitive layout characteristics of text and exploit these as prior knowledge to guide model convergence. In this paper, we propose a novel gradient-based graph attention method to embed patch-wise text layout contexts into image feature representations for high-resolution text image reconstruction in an implicit and elegant manner. We introduce a non-local group-wise attention module to extract text features which are then enhanced by a cascaded channel attention module and a novel gradient-based graph attention module in order to obtain more effective representations by exploring correlations of regional and local patch-wise text layout properties. Extensive experiments on the benchmark TextZoom dataset convincingly demonstrate that our method supports excellent text recognition and outperforms the current state-of-the-art in STISR. The source code is available at https://github.com/xyzhu1/TSAN.",,17,1.0,all publisherfullgold,All Open Access Gold,NSFC,2020SK2059,National Natural Science Foundation of China
2-s2.0-85137935657,10.24963/ijcai.2022/631,,,Grape: Grammar-Preserving Rule Embedding,cp,Conference Paper,Zhu Q.,60001604,Ministry of Education of the People's Republic of China,Beijing,China,5.0,"Zhu, Qihao;Sun, Zeyu;Zhang, Wenjie;Xiong, Yingfei;Zhang, Lu",59718923000;57211525326;57204395928;35744243000;56275778800,60001604;60001604;60001604;60001604;60001604,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4545-4551,"Word embedding has been widely used in various areas to boost the performance of the neural models. However, when processing context-free languages, embedding grammar rules with word embedding loses two types of information. One is the structural relationship between the grammar rules, and the other one is the content information of the rule definition. In this paper, we make the first attempt to learn a grammar-preserving rule embedding. We first introduce a novel graph structure to represent the context-free grammar. Then, we apply a Graph Neural Network (GNN) to extract the structural information and use a gating layer to integrate content information. We conducted experiments on six widely-used benchmarks containing four context-free languages. The results show that our approach improves the accuracy of the base model by 0.8 to 6.4 percentage points. Furthermore, Grape also achieves 1.6 F1 score improvement on the method naming task which shows the generality of our approach.",,5,1.0,all publisherfree2read,All Open Access Bronze,NSFC,61922003,National Natural Science Foundation of China
2-s2.0-85167995049,10.1609/aaai.v37i6.25907,,,Graph Anomaly Detection via Multi-Scale Contrastive Learning Networks with Augmented View,cp,Conference Paper,Duan J.,60024350,National University of Defense Technology China,Changsha,China,8.0,"Duan, Jingcan;Wang, Siwei;Zhang, Pei;Zhu, En;Hu, Jingtao;Jin, Hu;Liu, Yue;Dong, Zhibin",57995033500;57209196796;57221726957;55363876800;57209201545;57688261400;57204189368;58023234700,60024350;60024350;60024350;60024350;60024350;60024350;60024350;60024350,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,7459-7467,"Graph anomaly detection (GAD) is a vital task in graph-based machine learning and has been widely applied in many real-world applications. The primary goal of GAD is to capture anomalous nodes from graph datasets, which evidently deviate from the majority of nodes. Recent methods have paid attention to various scales of contrastive strategies for GAD, i.e., node-subgraph and node-node contrasts. However, they neglect the subgraph-subgraph comparison information which the normal and abnormal subgraph pairs behave differently in terms of embeddings and structures in GAD, resulting in sub-optimal task performance. In this paper, we fulfill the above idea in the proposed multi-view multi-scale contrastive learning framework with subgraph-subgraph contrast for the first practice. To be specific, we regard the original input graph as the first view and generate the second view by graph augmentation with edge modifications. With the guidance of maximizing the similarity of the subgraph pairs, the proposed subgraph-subgraph contrast contributes to more robust subgraph embeddings despite of the structure variation. Moreover, the introduced subgraph-subgraph contrast cooperates well with the widely-adopted node-subgraph and node-node contrastive counterparts for mutual GAD performance promotions. Besides, we also conduct sufficient experiments to investigate the impact of different graph augmentation approaches on detection performance. The comprehensive experimental results well demonstrate the superiority of our method compared with the state-of-the-art approaches and the effectiveness of the multi-view subgraph pair contrastive strategy for the GAD task. The source code is released at https://github.com/FelixDJC/GRADATE.",,136,1.0,all publisherfullgold,All Open Access Gold,NSFC,61872371,National Natural Science Foundation of China
2-s2.0-85204288450,,,,Graph Collaborative Expert Finding with Contrastive Learning,cp,Conference Paper,Peng Q.,60019533;60104243;131699624,Tianjin University;Hainan Tropical Ocean University;Du Xiaoman Finicial Technology,Tianjin;Sanya;Beijing,China;China;China,5.0,"Peng, Qiyao;Wang, Wenjun;Liu, Hongtao;Huo, Cuiying;Shao, Minglai",57211217483;55714121400;57200280821;57216287644;57203383934,60019533;60019533-60104243;131699624;60019533;60019533,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2288-2296,"In Community Question Answering (CQA) websites, most current expert finding methods often model expert embeddings from textual features and optimize them with expert-question first-order interactions, i.e., this expert has answered this question.In this paper, we try to address the limitation of current models that typically neglect the intrinsic high-order connectivity within expert-question interactions, which is pivotal for collaborative effects.We introduce an innovative and simple approach: by conceptualizing expert-question interactions as a bipartite graph, and then we propose a novel graph-based expert finding method based on contrastive learning to effectively capture both first-order and intricate high-order connectivity, named CGEF.Specifically, we employ a question encoder to model questions from titles and employ the graph attention network to recursively propagate embeddings.Besides, to alleviate the problem of sparse interactions, we devise two auxiliary tasks to enhance expert modeling.First, we generate multiple views of one expert, including: 1) behavior-level augmentation drops interaction edges randomly in the graph; 2) interest-level augmentation randomly replaces question titles with tags in the graph.Then we maximize the agreement between one expert and the corresponding augmented expert on a specific view.In this way, the model can effectively inject collaborative signals into expert modeling.Extensive experiments on six CQA datasets demonstrate significant improvements compared with recent methods.",,1,0.0,,,NSFC,62272338,National Natural Science Foundation of China
2-s2.0-85204286712,,,,Graph Contrastive Learning with Reinforcement Augmentation,cp,Conference Paper,Liu Z.,60104026,Beijing National Research Center for Information Science and Technology,Beijing,China,3.0,"Liu, Ziyang;Wang, Chaokun;Wu, Cheng",59285783200;22036854300;59890166300,60104026;60104026;60104026,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2225-2233,"Graph contrastive learning (GCL), designing contrastive objectives to learn embeddings from augmented graphs, has become a prevailing method for extracting embeddings from graphs in an unsupervised manner.As an important procedure in GCL, graph data augmentation (GDA) directly affects the model performance on downstream tasks.Currently, the GCL methods typically treat GDA as independent events, neglecting its continuity.In this paper, we regard the GDA in GCL as a Markov decision process and propose a novel graph reinforcement augmentation framework for GCL.Based on this framework, we design a Graph Advantage Actor-Critic (GA2C) model.We conduct extensive experiments to evaluate GA2C on unsupervised learning, transfer learning, and semi-supervised learning.The experimental results demonstrate the performance superiority of GA2C over the state-of-the-art GCL models.Furthermore, we verify that GA2C is more efficient than the other GCL methods with learnable GDA and provide two examples of chemical molecular graphs from ZINC-2M to demonstrate that GA2C generates meaningful augmented views, where the edge weights reflect the importance of chemical bonds in the molecule.",,5,0.0,,,NSFC,62372264,National Natural Science Foundation of China
2-s2.0-105000506190,,,,Graph Diffusion Policy Optimization,cp,Conference Paper,Liu Y.,60014402;60117933;126553437,Renmin University of China;National Key Laboratory of Computer-Aided Design and Graphics Systems;Sea AI Lab,Beijing;Hangzhou;Singapore City,China;China;Singapore,6.0,"Liu, Yijing;Du, Chao;Pang, Tianyu;Li, Chongxuan;Lin, Min;Chen, Wei",57203685046;57194774618;57204799576;57189095239;55926433700;55613230656,60117933;126553437;126553437;60014402;126553437;60117933,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Recent research has made significant progress in optimizing diffusion models for downstream objectives, which is an important pursuit in fields such as graph generation for drug design. However, directly applying these models to graph presents challenges, resulting in suboptimal performance. This paper introduces graph diffusion policy optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning. GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives. Code is available at https://github.com/sail-sg/GDPO.",,3,0.0,,,ZJNSF,LD24F020011,Natural Science Foundation of Zhejiang Province
2-s2.0-85189608705,10.1609/aaai.v38i8.28723,,,Graph Disentangled Contrastive Learning with Personalized Transfer for Cross-Domain Recommendation,cp,Conference Paper,Liu J.,60019533,Tianjin University,Tianjin,China,5.0,"Liu, Jing;Sun, Lele;Nie, Weizhi;Jing, Peiguang;Su, Yuting",57192545575;58943098400;54080080700;56971244300;55797981700,60019533;60019533;60019533;60019533;60019533,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,8.0,,8769-8777,"Cross-Domain Recommendation (CDR) has been proven to effectively alleviate the data sparsity problem in Recommender System (RS). Recent CDR methods often disentangle user features into domain-invariant and domain-specific features for efficient cross-domain knowledge transfer. Despite showcasing robust performance, three crucial aspects remain unexplored for existing disentangled CDR approaches: i) The significance nuances of the interaction behaviors are ignored in generating disentangled features; ii) The user features are disentangled irrelevant to the individual items to be recommended; iii) The general knowledge transfer overlooks the user's personality when interacting with diverse items. To this end, we propose a Graph Disentangled Contrastive framework for CDR (GDCCDR) with personalized transfer by meta-networks. An adaptive parameter-free filter is proposed to gauge the significance of diverse interactions, thereby facilitating more refined disentangled representations. In sight of the success of Contrastive Learning (CL) in RS, we propose two CL-based constraints for item-aware disentanglement. Proximate CL ensures the coherence of domain-invariant features between domains, while eliminatory CL strives to disentangle features within each domains using mutual information between users and items. Finally, for domain-invariant features, we adopt meta-networks to achieve personalized transfer. Experimental results on four real-world datasets demonstrate the superiority of GDCCDR over state-of-the-art methods.",,31,1.0,all publisherfullgold,All Open Access Gold,NKRDPC,2021YFF0901603,National Key Research and Development Program of China
2-s2.0-85148986796,,,,Graph Neural Networks are Dynamic Programmers,cp,Conference Paper,Dudzik A.,60111161,DeepMind Technologies Limited,London,United Kingdom,2.0,"Dudzik, Andrew;Veličković, Petar",57211634361;57190809820,60111161;60111161,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Recent advances in neural algorithmic reasoning with graph neural networks (GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural network will be better at learning to execute a reasoning task (in terms of sample complexity) if its individual components align well with the target algorithm. Specifically, GNNs are claimed to align with dynamic programming (DP), a general problem-solving strategy which expresses many polynomial-time algorithms. However, has this alignment truly been demonstrated and theoretically quantified? Here we show, using methods from category theory and abstract algebra, that there exists an intricate connection between GNNs and DP, going well beyond the initial observations over individual algorithms such as Bellman-Ford. Exposing this connection, we easily verify several prior findings in the literature, produce better-grounded GNN architectures for edge-centric tasks, and demonstrate empirical results on the CLRS algorithmic reasoning benchmark. We hope our exposition will serve as a foundation for building stronger algorithmically aligned GNNs.",,31,0.0,,,,,
2-s2.0-85179132521,10.1613/JAIR.1.15280,,,Graphmax for Text Generation,ar,Article,Liu B.,60006541;130544088,The University of Hong Kong;Soithwestern University of Finance and Economics,Hong Kong;,Hong Kong;China,2.0,"Liu, Bin;Yin, Guosheng",58842475500;8725807500,130544088;60006541,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,823-848,"In text generation, a large language model (LM) makes a choice of each new word based only on the former selection of its context using the softmax function. Nevertheless, the link statistics information of concurrent words based on a scene-specific corpus is valuable in choosing the next word, which can help to ensure the topic of the generated text to be aligned with the current task. To fully explore the co-occurrence information, we propose a graphmax function for task-specific text generation. Using the graph-based regularization, graphmax enables the final word choice to be determined by both the global knowledge from the LM and the local knowledge from the scene-specific corpus. The traditional softmax function is regularized with a graph total variation (GTV) term, which incorporates the local knowledge into the LM and encourages the model to consider the statistical relationships between words in a scene-specific corpus. The proposed graphmax is versatile and can be readily plugged into any large pre-trained LM for text generation and machine translation. Through extensive experiments, we demonstrate that the new GTV-based regularization can improve performances in various natural language processing (NLP) tasks in comparison with existing methods. Moreover, through human experiments, we observe that participants can easily distinguish the text generated by graphmax or softmax.",,1,1.0,all publisherfullgold,All Open Access Gold,研究資助局,T45-401/22-N,"Research Grants Council, University Grants Committee"
2-s2.0-105000486831,,,,Great Minds Think Alike: The Universal Convergence Trend of Input Salience,cp,Conference Paper,Wang Y.,60148444,College of Engineering,West Lafayette,United States,3.0,"Wang, Yipei;Siskind, Jeffrey Mark;Wang, Xiaoqian",57344408100;6701590426;56355253400,60148444;60148444;60148444,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Uncertainty is introduced in optimized DNNs through stochastic algorithms, forming specific distributions. Training models can be seen as random sampling from this distribution of optimized models. In this work, we study the distribution of optimized DNNs as a family of functions by leveraging a pointwise approach. We focus on the input saliency maps, as the input gradient field is decisive to the models' mathematical essence. Our investigation of saliency maps reveals a counter-intuitive trend: two stochastically optimized models tend to resemble each other more as either of their capacities increases. Therefore, we hypothesize several properties of these distributions, suggesting that (1) Within the same model architecture (e.g., CNNs, ResNets), different family variants (e.g., varying capacities) tend to align in terms of their population mean directions of the input salience. And (2) the distributions of optimized models follow a convergence trend to their shared population mean as the capacity increases. Furthermore, we also propose semi-parametric distributions based on the Saw distribution to model the convergence trend, satisfying all the counter-intuitive observations. Our experiments shed light on the significant implications of our hypotheses in various application domains, including black-box attacks, deep ensembles, etc. These findings not only enhance our understanding of DNN behaviors but also offer valuable insights for their practical application in diverse areas of deep learning.",,0,0.0,,,DARPA,2103299-01,Defense Advanced Research Projects Agency
2-s2.0-85144586841,,,,Greedification Operators for Policy Optimization: Investigating Forward and Reverse KL Divergences,ar,Article,Chan A.,60193824,Alberta Machine Intelligence Institute,Edmonton,Canada,6.0,"Chan, Alan;Silva, Hugo;Lim, Sungsu;Kozuno, Tadashi;Rupam Mahmood, A.;White, Martha",59579903000;57219633416;57219754235;42961642900;55413282000;55392368300,60193824;60193824;60193824;60193824;60193824;60193824,2022-08-01,1 August 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,A90,,"Approximate Policy Iteration (API) algorithms alternate between (approximate) policy evaluation and (approximate) greedification. Many different approaches have been explored for approximate policy evaluation, but less is understood about approximate greedification and what choices guarantee policy improvement. In this work, we investigate approximate greedification when reducing the KL divergence between the parameterized policy and the Boltzmann distribution over action values. In particular, we investigate the difference between the forward and reverse KL divergences, with varying degrees of entropy regularization; these are chosen because they underlie many existing policy optimization approaches, as we highlight in this work. We show that the reverse KL has stronger policy improvement guarantees, and that reducing the forward KL can result in a worse policy. We also demonstrate, however, that a large enough reduction of the forward KL can induce improvement under additional assumptions. Empirically, we show on simple continuous-action environments that the forward KL can induce more exploration, but at the cost of a more suboptimal policy. No significant differences were observed in the discrete-action setting or on a suite of benchmark problems. This work provides novel theoretical and empirical insights about the forward KL and reverse KL for greedification, and clear next steps for understanding and improving our policy optimization algorithms.",kl divergence | policy gradient | policy iteration | reinforcement learning,22,0.0,,,AMII,,Alberta Machine Intelligence Institute
2-s2.0-105000490695,,,,Guiding a Diffusion Model with a Bad Version of Itself,cp,Conference Paper,Karras T.,60103653;60076695,Aalto University;NVIDIA,Espoo;Santa Clara,Finland;United States,6.0,"Karras, Tero;Aittala, Miika;Kynkäänniemi, Tuomas;Lehtinen, Jaakko;Aila, Timo;Laine, Samuli",36022486900;35104427100;57194596988;7005279290;56007617800;13008308700,60076695;60076695;60103653;60103653;60076695;60076695,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64×64 and 1.25 for 512×512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.",,20,0.0,,,,,
2-s2.0-85189551450,10.1609/aaai.v38i12.29248,,,GxVAEs: Two Joint VAEs Generate Hit Molecules from Gene Expression Profiles,cp,Conference Paper,Li C.,60000264,Nagoya University,Nagoya,Japan,2.0,"Li, Chen;Yamanishi, Yoshihiro",57221582813;7006213985,60000264;60000264,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,12.0,,13455-13463,"The de novo generation of hit-like molecules that show bioactivity and drug-likeness is an important task in computer-aided drug discovery. Although artificial intelligence can generate molecules with desired chemical properties, most previous studies have ignored the influence of disease-related cellular environments. This study proposes a novel deep generative model called GxVAEs to generate hit-like molecules from gene expression profiles by leveraging two joint variational autoencoders (VAEs). The first VAE, ProfileVAE, extracts latent features from gene expression profiles. The extracted features serve as the conditions that guide the second VAE, which is called MolVAE, in generating hit-like molecules. GxVAEs bridge the gap between molecular generation and the cellular environment in a biological system, and produce molecules that are biologically meaningful in the context of specific diseases. Experiments and case studies on the generation of therapeutic molecules show that GxVAEs outperforms current state-of-the-art baselines and yield hit-like molecules with potential bioactivity and drug-like properties. We were able to successfully generate the potential molecular structures with therapeutic effects for various diseases from patients’ disease profiles.",,11,1.0,all publisherfullgold,All Open Access Gold,JSPS,23KF0063,Japan Society for the Promotion of Science
2-s2.0-85203846082,,,,H-Consistency Guarantees for Regression,cp,Conference Paper,Mao A.,60006191;60003261,Google LLC;Courant Institute of Mathematical Sciences,Mountain View;New York,United States;United States,3.0,"Mao, Anqi;Mohri, Mehryar;Zhong, Yutao",57215212129;7101967852;57223767183,60003261;60003261-60006191;60003261,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,34712-34737,"We present a detailed study of H-consistency bounds for regression. We first present new theorems that generalize the tools previously given to establish H-consistency bounds. This generalization proves essential for analyzing H-consistency bounds specific to regression. Next, we prove a series of novel H-consistency bounds for surrogate loss functions of the squared loss, under the assumption of a symmetric distribution and a bounded hypothesis set. This includes positive results for the Huber loss, all `<inf>p</inf> losses, p ≥ 1, the squared є-insensitive loss, as well as a negative result for the є-insensitive loss used in Support Vector Regression (SVR). We further leverage our analysis of H-consistency for regression and derive principled surrogate losses for adversarial regression (Section 5). This readily establishes novel algorithms for adversarial regression, for which we report favorable experimental results in Section 6.",,10,0.0,,,,,
2-s2.0-85139470689,10.1613/JAIR.1.13643,,,HEBO: Pushing The Limits of Sample-Efficient Hyperparameter Optimisation,ar,Article,Cowen-Rivers A.I.,60022148;60031101;60011226;60092530,"University College London;University of Cambridge;Technische Universität Darmstadt;Huawei Technologies Co., Ltd.",London;Cambridge;Darmstadt;Shenzhen,United Kingdom;United Kingdom;Germany;China,11.0,"Cowen-Rivers, Alexander I.;Lyu, Wenlong;Tutunov, Rasul;Wang, Zhi;Grosnit, Antoine;Rhys, Ryan;Maravel, Alexandre Max;Jianye, Hao;Wang, Jun;Peters, Jan;Bou-Ammar, Haitham",57203816687;57197866641;55846475300;57191582204;57221840446;57213268757;57222087091;36809586800;55902731900;35248912800;55080007000,60092530;60092530;60092530;60092530;60092530;60031101;60092530;60092530;60092530-60022148;60011226;60092530-60022148,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,1269-1349,"In this work we rigorously analyse assumptions inherent to black-box optimisation hyper-parameter tuning tasks. Our results on the Bayesmark benchmark indicate that heteroscedasticity and non-stationarity pose significant challenges for black-box optimisers. Based on these findings, we propose a Heteroscedastic and Evolutionary Bayesian Optimisation solver (HEBO). HEBO performs non-linear input and output warping, admits exact marginal log-likelihood optimisation and is robust to the values of learned parameters. We demonstrate HEBO's empirical efficacy on the NeurIPS 2020 Black-Box Optimisation challenge, where HEBO placed first. Upon further analysis, we observe that HEBO significantly outperforms existing black-box optimisers on 108 machine learning hyperparameter tuning tasks comprising the Bayesmark benchmark. Our findings indicate that the majority of hyper-parameter tuning tasks exhibit heteroscedasticity and non-stationarity, multiobjective acquisition ensembles with Pareto front solutions improve queried configurations, and robust acquisition maximisers afford empirical advantages relative to their non-robust counterparts. We hope these findings may serve as guiding principles for practitioners of Bayesian optimisation.",,100,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85141767435,,,,HOW DOES SIMSIAM AVOID COLLAPSE WITHOUT NEGATIVE SAMPLES? A UNIFIED UNDERSTANDING WITH SELF-SUPERVISED CONTRASTIVE LEARNING,cp,Conference Paper,Zhang C.,60032144,Korea Advanced Institute of Science and Technology,Daejeon,South Korea,6.0,"Zhang, Chaoning;Zhang, Kang;Zhang, Chenshuang;Pham, Trung X.;Yoo, Chang D.;Kweon, In So",55898462700;57462231900;57462141800;57206465717;7201746384;7003450602,60032144;60032144;60032144;60032144;60032144;60032144,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"To avoid collapse in self-supervised learning (SSL), a contrastive loss is widely used but often requires a large number of negative samples. Without negative samples yet achieving competitive performance, a recent work (Chen & He, 2021) has attracted significant attention for providing a minimalist simple Siamese (SimSiam) method to avoid collapse. However, the reason for how it avoids collapse without negative samples remains not fully clear and our investigation starts by revisiting the explanatory claims in the original SimSiam. After refuting their claims, we introduce vector decomposition for analyzing the collapse based on the gradient analysis of the l<inf>2</inf>-normalized representation vector. This yields a unified perspective on how negative samples and SimSiam alleviate collapse. Such a unified perspective comes timely for understanding the recent progress in SSL.",,53,0.0,,,MSIP,,"Ministry of Science, ICT and Future Planning"
2-s2.0-85199855823,,,,HOW TO EXPLOIT HYPERSPHERICAL EMBEDDINGS FOR OUT-OF-DISTRIBUTION DETECTION?,cp,Conference Paper,Ming Y.,60153202;119273346,"School of Computer, Data & Information Sciences;Meta",Madison;Meta,United States;United States,4.0,"Ming, Yifei;Sun, Yiyou;Dia, Ousmane;Li, Yixuan",57221840069;57195558086;36600145200;57188823744,60153202;60153202;119273346;60153202,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Out-of-distribution (OOD) detection is a critical task for reliable machine learning. Recent advances in representation learning give rise to distance-based OOD detection, where testing samples are detected as OOD if they are relatively far away from the centroids or prototypes of in-distribution (ID) classes. However, prior methods directly take off-the-shelf contrastive losses that suffice for classifying ID samples, but are not optimally designed when test inputs contain OOD samples. In this work, we propose CIDER, a novel representation learning framework that exploits hyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses to promote strong ID-OOD separability: a dispersion loss that promotes large angular distances among different class prototypes, and a compactness loss that encourages samples to be close to their class prototypes. We analyze and establish the unexplored relationship between OOD detection performance and the embedding properties in the hyperspherical space, and demonstrate the importance of dispersion and compactness. CIDER establishes superior performance, outperforming the latest rival by 13.33% in FPR95. Code is available at https://github.com/deeplearning-wisc/cider.",,90,0.0,,,,FA9550-23-1-0184,
2-s2.0-85150358504,,,,HOW TO INJECT BACKDOORS WITH BETTER CONSISTENCY: LOGIT ANCHORING ON CLEAN DATA,cp,Conference Paper,Zhang Z.,60014966;60000060;60021178;60121285,Peking University;Lehigh University;Sony Corporation;Ant group,Beijing;Bethlehem;Tokyo;Hangzhou,China;United States;Japan;China,5.0,"Zhang, Zhiyuan;Lyu, Lingjuan;Wang, Weiqiang;Sun, Lichao;Sun, Xu",57205407081;57189234207;57225974160;57193994946;55744667900,60014966;60021178;60121285;60000060;60014966,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Since training a large-scale backdoored model from scratch requires a large training dataset, several recent attacks have considered to inject backdoors into a trained clean model without altering model behaviors on the clean data. Previous work finds that backdoors can be injected into a trained clean model with Adversarial Weight Perturbation (AWP), which means the variation of parameters are small in backdoor learning. In this work, we observe an interesting phenomenon that the variations of parameters are always AWPs when tuning the trained clean model to inject backdoors. We further provide theoretical analysis to explain this phenomenon. We are the first to formulate the behavior of maintaining accuracy on clean data as the consistency of backdoored models, which includes both global consistency and instance-wise consistency. We extensively analyze the effects of AWPs on the consistency of backdoored models. In order to achieve better consistency, we propose a novel anchoring loss to anchor or freeze the model behaviors on the clean data, with a theoretical guarantee. Both the analytical and empirical results validate the effectiveness of our anchoring loss in improving the consistency, especially the instance-wise consistency.",,14,0.0,,,,,
2-s2.0-85204312225,,,,HVOFusion: Incremental Mesh Reconstruction Using Hybrid Voxel Octree,cp,Conference Paper,Liu S.,60003970;131699496,Zhejiang University;Udeer.ai,Hangzhou;,China;,3.0,"Liu, Shaofan;Chen, Junbo;Zhu, Jianke",58497623000;58507377400;14036690500,60003970;131699496;60003970,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,6850-6858,"Incremental scene reconstruction is essential to the navigation in robotics. Most of the conventional methods typically make use of either TSDF (truncated signed distance functions) volume or neural networks to implicitly represent the surface. Due to the voxel representation or involving with time-consuming sampling, they have difficulty in balancing speed, memory storage, and surface quality. In this paper, we propose a novel hybrid voxel-octree approach to effectively fuse octree with voxel structures so that we can take advantage of both implicit surface and explicit triangular mesh representation. Such sparse structure preserves triangular faces in the leaf nodes and produces partial meshes sequentially for incremental reconstruction. This storage scheme allows us to naturally optimize the mesh in explicit 3D space to achieve higher surface quality. We iteratively deform the mesh towards the target and recovers vertex colors by optimizing a shading model. Experimental results on several datasets show that our proposed approach is capable of quickly and accurately reconstructing a scene with realistic colors. Code is available at https://github.com/Frankuzi/HVOFusion.",,1,0.0,,,ZJU,62376244,Zhejiang University
2-s2.0-85150344024,,,,HYPERPARAMETER TUNING WITH RENYI DIFFERENTIAL PRIVACY,cp,Conference Paper,Papernot N.,60006191,Google LLC,Mountain View,United States,2.0,"Papernot, Nicolas;Steinke, Thomas",56732917800;56354113000,60006191;60006191,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm's hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.",,43,0.0,,,,,
2-s2.0-85180350552,,,,HeadSculpt: Crafting 3D Head Avatars with Text,cp,Conference Paper,Han X.,60015150;60006541;60021097;126235503;129164841,Imperial College London;The University of Hong Kong;University of Surrey;iFlyTek-Surrey Joint Research Centre on Artificial Intelligence;Surrey Institute for People-Centred AI,London;Hong Kong;Guildford;Surrey;Surrey,United Kingdom;Hong Kong;United Kingdom;United Kingdom;United Kingdom,8.0,"Han, Xiao;Cao, Yukang;Han, Kai;Zhu, Xiatian;Deng, Jiankang;Song, Yi Zhe;Xiang, Tao;Wong, Kwan Yee K.",57226625525;57667416500;57197058785;56050744800;56514437400;18038601200;58947347600;57216110773,60021097-126235503;60006541;60006541;60021097-129164841;60015150;60021097-126235503;60021097-126235503;60006541,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Recently, text-guided 3D generative methods have made remarkable advancements in producing high-quality textures and geometry, capitalizing on the proliferation of large vision-language and image diffusion models. However, existing methods still struggle to create high-fidelity 3D head avatars in two aspects: (1) They rely mostly on a pre-trained text-to-image diffusion model whilst missing the necessary 3D awareness and head priors. This makes them prone to inconsistency and geometric distortions in the generated avatars. (2) They fall short in fine-grained editing. This is primarily due to the inherited limitations from the pre-trained 2D image diffusion models, which become more pronounced when it comes to 3D head avatars. In this work, we address these challenges by introducing a versatile coarse-to-fine pipeline dubbed HeadSculpt for crafting (i.e., generating and editing) 3D head avatars from textual prompts. Specifically, we first equip the diffusion model with 3D awareness by leveraging landmark-based control and a learned textual embedding representing the back view appearance of heads, enabling 3D-consistent head avatar generations. We further propose a novel identity-aware editing score distillation strategy to optimize a textured mesh with a high-resolution differentiable rendering technique. This enables identity preservation while following the editing instruction. We showcase HeadSculpt's superior fidelity and editing capabilities through comprehensive experiments and comparisons with existing methods.",,18,0.0,,,研究資助局,27208022,"Research Grants Council, University Grants Committee"
2-s2.0-85137933796,10.24963/ijcai.2022/399,,,Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning,cp,Conference Paper,Cho Y.J.,60027950;60021726,Carnegie Mellon University;Microsoft Research,Pittsburgh;Redmond,United States;United States,5.0,"Cho, Yae Jee;Manoel, Andre;Joshi, Gauri;Sim, Robert;Dimitriadis, Dimitrios",57189729778;55855395500;7103298819;57204295607;6603343444,60021726-60027950;60021726;60027950;60021726;60021726,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2881-2887,"Federated learning (FL) enables edge-devices to collaboratively learn a model without disclosing their private data to a central aggregating server. Most existing FL algorithms require models of identical architecture to be deployed across the clients and server, making it infeasible to train large models due to clients' limited system resources. In this work, we propose a novel ensemble knowledge transfer method named Fed-ET in which small models (different in architecture) are trained on clients, and used to train a larger model at the server. Unlike in conventional ensemble learning, in FL the ensemble can be trained on clients' highly heterogeneous data. Cognizant of this property, Fed-ET uses a weighted consensus distillation scheme with diversity regularization that efficiently extracts reliable consensus from the ensemble while improving generalization by exploiting the diversity within the ensemble. We show the generalization bound for the ensemble of weighted models trained on heterogeneous datasets that supports the intuition of Fed-ET. Our experiments on image and language tasks show that Fed-ET significantly outperforms other state-of-the-art FL algorithms with fewer communicated parameters, and is also robust against high data-heterogeneity.",,73,1.0,all publisherfree2read,All Open Access Bronze,,,
2-s2.0-85148270437,10.1609/aaai.v37i8.26192,,,Heterogeneous Graph Masked Autoencoders,cp,Conference Paper,Tian Y.,60021508;60016247;60155914,University of Notre Dame;Brandeis University;College of Engineering,Notre Dame;Waltham;Notre Dame,United States;United States;United States,5.0,"Tian, Yijun;Dong, Kaiwen;Zhang, Chunhui;Zhang, Chuxu;Chawla, Nitesh V.",57340427000;58588835400;57477020400;55879440900;35077581400,60155914-60021508;60155914-60021508;60016247;60016247;60155914-60021508,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,9997-10005,"Generative self-supervised learning (SSL), especially masked autoencoders, has become one of the most exciting learning paradigms and has shown great potential in handling graph data. However, real-world graphs are always heterogeneous, which poses three critical challenges that existing methods ignore: 1) how to capture complex graph structure? 2) how to incorporate various node attributes? and 3) how to encode different node positions? In light of this, we study the problem of generative SSL on heterogeneous graphs and propose HGMAE, a novel heterogeneous graph masked autoencoder model to address these challenges. HGMAE captures comprehensive graph information via two innovative masking techniques and three unique training strategies. In particular, we first develop metapath masking and adaptive attribute masking with dynamic mask rate to enable effective and stable learning on heterogeneous graphs. We then design several training strategies including metapath-based edge reconstruction to adopt complex structural information, target attribute restoration to incorporate various node attributes, and positional feature prediction to encode node positional information. Extensive experiments demonstrate that HGMAE outperforms both contrastive and generative state-of-the-art baselines on several tasks across multiple datasets. Codes are available at https://github.com/meettyj/HGMAE.",,111,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-105018462533,,,,Heterogeneous-Agent Reinforcement Learning,ar,Article,Zhong Y.,60026851;60022148;60014966;60023932;128486264,University of Oxford;University College London;Peking University;University of Technology Sydney;Beijing Institute for General Artificial Intelligence,Oxford;London;Beijing;Sydney;Beijing,United Kingdom;United Kingdom;China;Australia;China,6.0,"Zhong, Yifan;Kuba, Jakub Grudzien;Feng, Xidong;Hu, Siyi;Ji, Jiaming;Yang, Yaodong",57958966400;57238555400;57243848400;57219757552;57462254600;56167927700,60014966-128486264;60026851;60022148;60023932;60014966;60014966,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL), and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPPO and provides a general template for cooperative MARL algorithmic designs. We prove that all algorithms derived from HAML inherently enjoy monotonic improvement of joint return and convergence to Nash Equilibrium. As its natural outcome, HAML validates more novel algorithms in addition to HATRPO and HAPPO, including HAA2C, HADDPG, and HATD3, which generally outperform their existing MA-counterparts. We comprehensively test HARL algorithms on six challenging benchmarks and demonstrate their superior effectiveness and stability for coordinating heterogeneous agents compared to strong baselines such as MAPPO and QMIX.<sup>1</sup>",cooperative multi-agent reinforcement learning | heterogeneous-agent mirror learning | heterogeneous-agent reinforcement learning algorithms | heterogeneous-agent trust region learning | sequential update scheme,85,0.0,,,NKPs,2022QNRC002,Tencent
2-s2.0-85174414966,,,,Hiding Data Helps: On the Benefits of Masking for Sparse Coding,cp,Conference Paper,Chidambaram M.,60140145;60280409,Department of Computer Science;Department of Computer Science,Durham;Providence,United States;United States,4.0,"Chidambaram, Muthu;Wu, Chenwei;Cheng, Yu;Ge, Rong",57313800700;57219749075;59179931700;59816734400,60140145;60140145;60280409;60140145,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,5600-5615,"Sparse coding, which refers to modeling a signal as sparse linear combinations of the elements of a learned dictionary, has proven to be a successful (and interpretable) approach in applications such as signal processing, computer vision, and medical imaging. While this success has spurred much work on provable guarantees for dictionary recovery when the learned dictionary is the same size as the ground-truth dictionary, work on the setting where the learned dictionary is larger (or over-realized) with respect to the ground truth is comparatively nascent. Existing theoretical results in this setting have been constrained to the case of noise-less data. We show in this work that, in the presence of noise, minimizing the standard dictionary learning objective can fail to recover the elements of the ground-truth dictionary in the over-realized regime, regardless of the magnitude of the signal in the data-generating process. Furthermore, drawing from the growing body of work on self-supervised learning, we propose a novel masking objective for which recovering the ground-truth dictionary is in fact optimal as the signal increases for a large class of data-generating processes. We corroborate our theoretical results with experiments across several parameter regimes showing that our proposed objective also enjoys better empirical performance than the standard reconstruction objective.",,0,0.0,,,NSF,CCF-1845171,National Science Foundation
2-s2.0-85169666242,10.1613/JAIR.1.14185,,,Hierarchical Decompositions and Termination Analysis for Generalized Planning,ar,Article,Srivastava S.,60093807,School of Computing and Augmented Intelligence,Tempe,United States,1.0,"Srivastava, Siddharth",16228415800,60093807,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,1203-1236,"This paper presents new methods for analyzing and evaluating generalized plans that can solve broad classes of related planning problems. Although synthesis and learning of generalized plans has been a longstanding goal in AI, it remains challenging due to fundamental gaps in methods for analyzing the scope and utility of a given generalized plan. This paper addresses these gaps by developing a new conceptual framework along with proof techniques and algorithmic processes for assessing termination and goal-reachability related properties of generalized plans. We build upon classic results from graph theory to decompose generalized plans into smaller components that are then used to derive hierarchical termination arguments. These methods can be used to determine the utility of a given generalized plan, as well as to guide the synthesis and learning processes for generalized plans. We present theoretical as well as empirical results illustrating the scope of this new approach. Our analysis shows that this approach significantly extends the class of generalized plans that can be assessed automatically, thereby reducing barriers in the synthesis and learning of reliable generalized plans.",,5,1.0,all publisherfullgold,All Open Access Gold,NSF,IIS 1942856,National Science Foundation
2-s2.0-85170373785,10.24963/ijcai.2023/163,,,Hierarchical Prompt Learning for Compositional Zero-Shot Recognition,cp,Conference Paper,Wang H.,60025578,Xidian University,Xi'an,China,4.0,"Wang, Henan;Yang, Muli;Wei, Kun;Deng, Cheng",58282815900;57226091622;57215780888;57208019993,60025578;60025578;60025578;60025578,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,1470-1478,"Compositional Zero-Shot Learning (CZSL) aims to imitate the powerful generalization ability of human beings to recognize novel compositions of known primitive concepts that correspond to a state and an object, e.g., “purple apple”. To fully capture the intra- and inter-class correlations between compositional concepts, in this paper, we propose to learn them in a hierarchical manner. Specifically, we set up three hierarchical embedding spaces that respectively model the states, the objects, and their compositions, which serve as three “experts” that can be combined in inference for more accurate predictions. We achieve this based on the recent success of large-scale pretrained Vision-Language Models, e.g., CLIP, which provides a strong initial knowledge of image-text relationships. To better adapt this knowledge to CZSL, we propose to learn three hierarchical prompts by explicitly fixing the unrelated word tokens in the three embedding spaces. Despite its simplicity, our proposed method consistently yields superior performance over current state-of-the-art approaches on three widely-used CZSL benchmarks.",,24,1.0,all publisherfullgold,All Open Access Gold,NSFC,62071361,National Natural Science Foundation of China
2-s2.0-85188871186,10.1609/aaai.v38i12.29253,,,High-Dimensional Analysis for Generalized Nonlinear Regression: From Asymptotics to Algorithm,cp,Conference Paper,Li J.,60014402;60273040,Renmin University of China;Institute of Information Engineering,Beijing;Beijing,China;China,3.0,"Li, Jian;Liu, Yong;Wang, Weiping",57202722471;55954392300;57272010000,60273040;60014402;60273040,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,12.0,,13500-13508,"Overparameterization often leads to benign overfitting, where deep neural networks can be trained to overfit the training data but still generalize well on unseen data. However, it lacks a generalized asymptotic framework for nonlinear regressions and connections to conventional complexity notions. In this paper, we propose a generalized high-dimensional analysis for nonlinear regression models, including various nonlinear feature mapping methods and subsampling. Specifically, we first provide an implicit regularization parameter and asymptotic equivalents related to a classical complexity notion, i.e., effective dimension. We then present a high-dimensional analysis for nonlinear ridge regression and extend it to ridgeless regression in the under-parameterized and over-parameterized regimes, respectively. We find that the limiting risks decrease with the effective dimension. Motivated by these theoretical findings, we propose an algorithm, namely RFRed, to improve generalization ability. Finally, we validate our theoretical findings and the proposed algorithm through several experiments.",,3,1.0,all publisherfullgold,All Open Access Gold,NSFC,62106257,National Natural Science Foundation of China
2-s2.0-85189499176,10.1609/aaai.v38i5.28256,,,High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field,cp,Conference Paper,Qin M.,60025278,Tsinghua University,Beijing,China,6.0,"Qin, Minghan;Liu, Yifan;Xu, Yuelang;Zhao, Xiaochen;Liu, Yebin;Wang, Haoqian",58664550900;57214948424;57997330100;57219788848;57209055753;59853120900,60025278;60025278;60025278;60025278;60025278;60025278,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,5.0,,4569-4577,"One crucial aspect of 3D head avatar reconstruction lies in the details of facial expressions. Although recent NeRF-based photo-realistic 3D head avatar methods achieve high-quality avatar rendering, they still encounter challenges retaining intricate facial expression details because they overlook the potential of specific expression variations at different spatial positions when conditioning the radiance field. Motivated by this observation, we introduce a novel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained by a simple MLP-based generation network, encompassing both spatial positional features and global expression information. Benefiting from rich and diverse information of the SVE at different positions, the proposed SVE-conditioned NeRF can deal with intricate facial expressions and achieve realistic rendering and geometry details of high-fidelity 3D head avatars. Additionally, to further elevate the geometric and rendering quality, we introduce a new coarse-to-fine training strategy, including a geometry initialization strategy at the coarse stage and an adaptive importance sampling strategy at the fine stage. Extensive experiments indicate that our method outperforms other state-of-the-art (SOTA) methods in rendering and geometry quality on mobile phonecollected and public datasets.",,3,1.0,all publisherfullgold,All Open Access Gold,NKRDPC,2022YFB36066,National Key Research and Development Program of China
2-s2.0-85174390075,,,,Hindsight Learning for MDPs with Exogenous Inputs,cp,Conference Paper,Sinclair S.R.,60020304;60104946;60021726,"University of Maryland, College Park;Cornell University College of Engineering;Microsoft Research",College Park;Ithaca;Redmond,United States;United States;United States,9.0,"Sinclair, Sean R.;Frujeri, Felipe;Cheng, Ching An;Marshall, Luke;Barbalho, Hugo;Li, Jingling;Neville, Jennifer;Menache, Ishai;Swaminathan, Adith",57217281089;57219698427;55201972800;57195917361;55053388100;57201620706;7006145328;8353757900;55523007700,60104946;60021726;60021726;60021726;60021726;60020304;60021726;60021726;60021726,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,31877-31914,"Many resource management problems require sequential decision-making under uncertainty, where the only uncertainty affecting the decision outcomes are exogenous variables outside the control of the decision-maker. We model these problems as Exo-MDPs (Markov Decision Processes with Exogenous Inputs) and design a class of data-efficient algorithms for them termed Hindsight Learning (HL). Our HL algorithms achieve data efficiency by leveraging a key insight: having samples of the exogenous variables, past decisions can be revisited in hindsight to infer counterfactual consequences that can accelerate policy improvements. We compare HL against classic baselines in the multi-secretary and airline revenue management problems. We also scale our algorithms to a business-critical cloud resource management problem - allocating Virtual Machines (VMs) to physical machines, and simulate their performance with real datasets from a large public cloud provider. We find that HL algorithms outperform domain-specific heuristics, as well as state-of-the-art reinforcement learning methods.",,11,0.0,,,NSF,CCF-1948256,National Science Foundation
2-s2.0-85174406472,,,,How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control,cp,Conference Paper,Teneggi J.,60145911;60145989,Whiting School of Engineering;Johns Hopkins Department of Biomedical Engineering,Baltimore;Baltimore,United States;United States,4.0,"Teneggi, Jacopo;Tivnan, Matthew;Stayman, J. Webster;Sulam, Jeremias",57223004516;56441668900;6603798106;56497201300,60145911;60145989;60145989;60145911-60145989,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,33940-33960,"Score-based generative modeling, informally referred to as diffusion models, continue to grow in popularity across several important domains and tasks. While they provide high-quality and diverse samples from empirical distributions, important questions remain on the reliability and trustworthiness of these sampling procedures for their responsible use in critical scenarios. Conformal prediction is a modern tool to construct finite-sample, distribution-free uncertainty guarantees for any black-box predictor. In this work, we focus on image-to-image regression tasks and we present a generalization of the Risk-Controlling Prediction Sets (RCPS) procedure, that we term K-RCPS, which allows to (i) provide entrywise calibrated intervals for future samples of any diffusion model, and (ii) control a certain notion of risk with respect to a ground truth image with minimal mean interval length. Differently from existing conformal risk control procedures, ours relies on a novel convex optimization approach that allows for multidimensional risk control while provably minimizing the mean interval length. We illustrate our approach on two real-world image denoising problems: on natural images of faces as well as on computed tomography (CT) scans of the abdomen, demonstrating state of the art performance.",,20,0.0,,,,,
2-s2.0-85137895107,10.24963/ijcai.2022/383,,,Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention,cp,Conference Paper,Xu Y.,60026532,Microsoft Corporation,Redmond,United States,10.0,"Xu, Yichong;Zhu, Chenguang;Wang, Shuohang;Sun, Siqi;Cheng, Hao;Liu, Xiaodong;Gao, Jianfeng;He, Pengcheng;Zeng, Michael;Huang, Xuedong",57195953104;57210636804;57191852679;57216688909;56900231300;57207781831;55702627000;57205508983;57211638200;7410247202,60026532;60026532;60026532;60026532;60026532;60026532;60026532;60026532;60026532;60026532,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2762-2768,"Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4% in comparison to the human accuracy of 88.9%.",,20,1.0,all publisherfree2read repository repositoryam,All Open Access Bronze Green,,,
2-s2.0-85204283796,,,,HyDiscGAN: A Hybrid Distributed cGAN for Audio-Visual Privacy Preservation in Multimodal Sentiment Analysis,cp,Conference Paper,Wu Z.,60026851;60073652;60016835;124058305,University of Oxford;Tongji University;Beijing Institute of Technology;DeepBlue Academy of Sciences,Oxford;Shanghai;Beijing;Shanghai,United Kingdom;China;China;China,6.0,"Wu, Zhuojia;Zhang, Qi;Miao, Duoqian;Yi, Kun;Fan, Wei;Hu, Liang",58652645400;57199111398;7006323434;57885446800;57221024776;57188765841,60073652;60073652-124058305;60073652;60016835;60026851;60073652-124058305,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,6550-6558,"Multimodal Sentiment Analysis (MSA) aims to identify speakers' sentiment tendencies in multimodal video content, raising serious concerns about privacy risks associated with multimodal data, such as voiceprints and facial images. Recent distributed collaborative learning has been verified as an effective paradigm for privacy preservation in multimodal tasks. However, they often overlook the privacy distinctions among different modalities, struggling to strike a balance between performance and privacy preservation. Consequently, it poses an intriguing question of maximizing multimodal utilization to improve performance while simultaneously protecting necessary modalities. This paper forms the first attempt at modality-specified (i.e., audio and visual) privacy preservation in MSA tasks. We propose a novel Hybrid Distributed cross-modality cGAN framework (HyDiscGAN), which learns multimodality alignment to generate fake audio and visual features conditioned on shareable de-identified textual data. The objective is to leverage the fake features to approximate real audio and visual content to guarantee privacy preservation while effectively enhancing performance. Extensive experiments show that compared with the state-of-the-art MSA model, HyDiscGAN can achieve superior or competitive performance while preserving privacy.",,13,0.0,,,NSFC,62276190,National Natural Science Foundation of China
2-s2.0-85151277861,10.1609/aaai.v37i1.25111,,,Hybrid CNN-Transformer Feature Fusion for Single Image Deraining,cp,Conference Paper,Chen X.,60010080;60018326,Nanjing University of Science and Technology;Shenyang Aerospace University,Nanjing;Shenyang,China;China,5.0,"Chen, Xiang;Pan, Jinshan;Lu, Jiyang;Fan, Zhentao;Li, Hao",57247002600;55258321100;57886979900;57553339500;57218714440,60010080;60010080;60018326;60018326;60010080,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,378-386,"Since rain streaks exhibit diverse geometric appearances and irregular overlapped phenomena, these complex characteristics challenge the design of an effective single image deraining model. To this end, rich local-global information representations are increasingly indispensable for better satisfying rain removal. In this paper, we propose a lightweight Hybrid CNN-Transformer Feature Fusion Network (dubbed as HCT-FFN) in a stage-by-stage progressive manner, which can harmonize these two architectures to help image restoration by leveraging their individual learning strengths. Specifically, we stack a sequence of the degradation-aware mixture of experts (DaMoE) modules in the CNN-based stage, where appropriate local experts adaptively enable the model to emphasize spatially-varying rain distribution features. As for the Transformer-based stage, a background-aware vision Transformer (BaViT) module is employed to complement spatially-long feature dependencies of images, so as to achieve global texture recovery while preserving the required structure. Considering the indeterminate knowledge discrepancy among CNN features and Transformer features, we introduce an interactive fusion branch at adjacent stages to further facilitate the reconstruction of high-quality deraining results. Extensive evaluations show the effectiveness and extensibility of our developed HCT-FFN. The source code is available at https://github.com/cschenxiang/HCT-FFN.",,69,1.0,all publisherfullgold,All Open Access Gold,NSFC,61872421,National Natural Science Foundation of China
2-s2.0-85203802546,,,,Hybrid Inverse Reinforcement Learning,cp,Conference Paper,Ren J.,60007776;60027950;115827813,Cornell University;Carnegie Mellon University;Aurora Innovation,Ithaca;Pittsburgh;Aurora,United States;United States;United States,5.0,"Ren, Juntao;Swamy, Gokul;Wu, Zhiwei Steven;Bagnell, J. Andrew;Choudhury, Sanjiban",58119647200;57208159882;56272530400;59157684500;55842232500,60007776;60027950;60027950;60027950-115827813;60007776,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,42428-42448,"The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert's. In this work, we propose using hybrid RL - training on a mixture of online and expert data - to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formally, we derive a reduction from inverse RL to expert-competitive RL (rather than globally optimal RL) that allows us to dramatically reduce interaction during the inner policy search loop while maintaining the benefits of the IRL approach. This allows us to derive both model-free and model-based hybrid inverse RL algorithms with strong policy performance guarantees. Empirically, we find that our approaches are significantly more sample efficient than standard inverse RL and several other baselines on a suite of continuous control tasks.",,1,0.0,,,NSF,2312956,Google
2-s2.0-85174390543,,,,Hyena Hierarchy: Towards Larger Convolutional Language Models,cp,Conference Paper,Poli M.,60012708;60009507,Stanford University;University of Montreal,Stanford;Montreal,United States;Canada,9.0,"Poli, Michael;Massaroli, Stefano;Nguyen, Eric;Fu, Daniel Y.;Dao, Tri;Baccus, Stephen;Bengio, Yoshua;Ermon, Stefano;Ré, Christopher",7005349668;57204718476;57894907400;57219508129;57202056886;6507775699;7003958245;35791579200;10739281400,60012708;60009507;60012708;60012708;60012708;60012708;60009507;60012708;60012708,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,28043-28078,"Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WIKITEXT103 and THE PILE), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100× faster at sequence length 64K.",,127,0.0,,,,,
2-s2.0-85204300352,,,,Hyperparameter Optimization Can Even Be Harmful in Off-Policy Learning and How to Deal with It,cp,Conference Paper,Saito Y.,60007776;60272367,"Cornell University;CyberAgent, Inc.",Ithaca;Tokyo,United States;Japan,2.0,"Saito, Yuta;Nomura, Masahiro",57208902755;57219498307,60007776;60272367,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4860-4867,"There has been a growing interest in off-policy evaluation in the literature such as recommender systems and personalized medicine. We have so far seen significant progress in developing estimators aimed at accurately estimating the effectiveness of counterfactual policies based on biased logged data. However, there are many cases where those estimators are used not only to evaluate the value of decision making policies but also to search for the best hyperparameters from a large candidate space. This work explores the latter hyperparameter optimization (HPO) task for off-policy learning. We empirically show that naively applying an unbiased estimator of the generalization performance as a surrogate objective in HPO can cause an unexpected failure, merely pursuing hyperparameters whose generalization performance is greatly overestimated. We then propose simple and computationally efficient corrections to the typical HPO procedure to deal with the aforementioned issues simultaneously. Empirical investigations demonstrate the effectiveness of our proposed HPO algorithm in situations where the typical procedure fails severely.",,0,0.0,,,,,
2-s2.0-85196879564,,,,IDEMPOTENT GENERATIVE NETWORK,cp,Conference Paper,Shocher A.,60025038;60006191,"University of California, Berkeley;Google LLC",Berkeley;Mountain View,United States;United States,6.0,"Shocher, Assaf;Dravid, Amil;Gandelsman, Yossi;Mosseri, Inbar;Rubinstein, Michael;Efros, Alexei A.",56829538600;57219466758;57214469458;55811234800;59829547900;7005029942,60025038-60006191;60025038;60025038;60006191;60006191;60025038,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"We propose a new approach for generative modeling based on training a neural network to be idempotent. An idempotent operator is one that can be applied sequentially without changing the result beyond the initial application, namely f(f(z)) = f(z). The proposed model f is trained to map a source distribution (e.g, Gaussian noise) to a target distribution (e.g. realistic images) using the following objectives: (1) Instances from the target distribution should map to themselves, namely f(x) = x. We define the target manifold as the set of all instances that f maps to themselves. (2) Instances that form the source distribution should map onto the defined target manifold. This is achieved by optimizing the idempotence term, f(f(z)) = f(z) which encourages the range of f(z) to be on the target manifold. Under ideal assumptions such a process provably converges to the target distribution. This strategy results in a model capable of generating an output in one step, maintaining a consistent latent space, while also allowing sequential applications for refinement. Additionally, we find that by processing inputs from both target and source distributions, the model adeptly projects corrupted or modified data back to the target manifold. This work is a first step towards a “global projector” that enables projecting any input into a target data distribution.",,5,0.0,,,DOS,,U.S. Department of State
2-s2.0-85137851987,10.24963/ijcai.2022/489,,,IMO3: Interactive Multi-Objective Off-Policy Optimization,cp,Conference Paper,Wang N.,60021918;60006191;132110499,University of Virginia;Google LLC;Amazon,Charlottesville;Mountain View;,United States;United States;,5.0,"Wang, Nan;Wang, Hongning;Karimzadehgan, Maryam;Kveton, Branislav;Boutilier, Craig",57198399552;48762142200;26421243400;55837807300;7003804813,60021918;60021918;60006191;132110499;60006191,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3523-3529,"Most real-world optimization problems have multiple objectives. A system designer needs to find a policy that trades off these objectives to reach a desired operating point. This problem has been studied extensively in the setting of known objective functions. However, we consider a more practical but challenging setting of unknown objective functions. In industry, optimization under this setting is mostly approached with online A/B testing, which is often costly and inefficient. As an alternative, we propose Interactive Multi-Objective Off-policy Optimization (IMO<sup>3</sup>). The key idea of IMO<sup>3</sup> is to interact with a system designer using policies evaluated in an off-policy fashion to uncover which policy maximizes her unknown utility function. We theoretically show that IMO<sup>3</sup> identifies a near-optimal policy with high probability, depending on the amount of designer's feedback and training data for off-policy estimation. We demonstrate its effectiveness empirically on several multi-objective optimization problems.",,5,1.0,all publisherfree2read,All Open Access Bronze,NSF,2007492,National Science Foundation
2-s2.0-85168638341,,,,IMPROVED SAMPLE COMPLEXITY FOR REWARD-FREE REINFORCEMENT LEARNING UNDER LOW-RANK MDPS,cp,Conference Paper,Cheng Y.,60003500;60001439;60019118,The Ohio State University;Pennsylvania State University;University of Science and Technology of China,Columbus;University Park;Hefei,United States;United States;China,4.0,"Cheng, Yuan;Huang, Ruiquan;Yang, Jing;Liang, Yingbin",57761781900;57327431700;55574202986;16068942200,60019118;60001439;60001439;60003500,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"In reward-free reinforcement learning (RL), an agent explores the environment first without any reward information, in order to achieve certain learning goals afterwards for any given reward. In this paper we focus on reward-free RL under low-rank MDP models, in which both the representation and linear weight vectors are unknown. Although various algorithms have been proposed for reward-free low-rank MDPs, the corresponding sample complexity is still far from being satisfactory. In this work, we first provide the first known sample complexity lower bound that holds for any algorithm under low-rank MDPs. This lower bound implies it is strictly harder to find a near-optimal policy under low-rank MDPs than under linear MDPs. We then propose a novel model-based algorithm, coined RAFFLE, and show it can both find an ϵ-optimal policy and achieve an ϵ-accurate system identification via reward-free exploration, with a sample complexity significantly improving the previous results. Such a sample complexity matches our lower bound in the dependence on ϵ, as well as on K in the large d regime, where d and K respectively denote the representation dimension and action space cardinality. Finally, we provide a planning algorithm (without further interaction with true environment) for RAFFLE to learn a near-accurate representation, which is the first known representation learning guarantee under the same setting.",,12,0.0,,,,,
2-s2.0-85144585079,,,,INFORMATION-THEORETIC ONLINE MEMORY SELECTION FOR CONTINUAL LEARNING,cp,Conference Paper,Sun S.,60016849;60006191;129320562;129320564;129321021,University of Toronto;Google LLC;DeepMind;Vector Institute;Baidu Apollo,Toronto;Mountain View;;;,Canada;United States;;;,5.0,"Sun, Shengyang;Calandriello, Daniele;Hu, Huiyi;Li, Ang;Titsias, Michalis K.",57204800262;55737371500;55575406000;58335326600;6603225248,60016849-129320564-129320562;129320562;60006191-129320562;129321021-129320562;129320562,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"A challenging problem in task-free continual learning is the online selection of a representative replay memory from data streams. In this work, we investigate the online memory selection problem from an information-theoretic perspective. To gather the most information, we propose the surprise and the learnability criteria to pick informative points and to avoid outliers. We present a Bayesian model to compute the criteria efficiently by exploiting rank-one matrix structures. We demonstrate that these criteria encourage selecting informative points in a greedy algorithm for online memory selection. Furthermore, by identifying the importance of the timing to update the memory, we introduce a stochastic information-theoretic reservoir sampler (InfoRS), which conducts sampling among selective points with high information. Compared to reservoir sampling, InfoRS demonstrates improved robustness against data imbalance. Finally, empirical performances over continual learning benchmarks manifest its efficiency and efficacy.",,36,0.0,,,,,
2-s2.0-85149711996,,,,"INSNET: An Efficient, Flexible, and Performant Insertion-based Text Generation Model",cp,Conference Paper,Lu S.,60027550,"University of California, Los Angeles",Los Angeles,United States,3.0,"Lu, Sidi;Meng, Tao;Peng, Nanyun",57214817481;57216692617;57204466260,60027550;60027550;60027550,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"We propose INSNET, an expressive insertion-based text generator with efficient training and flexible decoding (parallel or sequential). Unlike most existing insertion-based text generation works that require re-encoding of the context after each insertion operation and thus are inefficient to train, INSNET only requires one pass of context encoding for the entire sequence during training by introducing a novel insertion-oriented position encoding and a light-weighted slot representation strategy to enable computation sharing. Furthermore, we propose an algorithm INSNET-Dinic to better determine the parallelization of insertion operations that provides a controllable switch between parallel and sequential decoding, making it flexible to handle more parallelizable tasks such as machine translation with efficient decoding, or less parallelizable tasks such as open-domain text generation to guarantee high-quality outputs. Experiments on two lexically constrained text generation datasets and three machine translation datasets demonstrate INSNET's advantages over previous insertion-based methods in terms of training speed, inference efficiency, and generation quality.",,10,0.0,,,,,
2-s2.0-85200567304,,,,INTERNAL CROSS-LAYER GRADIENTS FOR EXTENDING HOMOGENEITY TO HETEROGENEITY IN FEDERATED LEARNING,cp,Conference Paper,Chan Y.H.,60006541,The University of Hong Kong,Hong Kong,Hong Kong,5.0,"Chan, Yun Hin;Zhou, Rui;Zhao, Running;Jiang, Zhihan;Ngai, Edith C.H.",57339111700;59884794200;57209454099;57212679315;7003298971,60006541;60006541;60006541;60006541;60006541,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradient distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverages internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods can be tailored to accommodate model-homogeneous FL methods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their capabilities to handle the system heterogeneity. Copious experimental results validate the effectiveness of InCo Aggregation, spotlighting internal cross-layer gradients as a promising avenue to enhance the performance in heterogeneous FL.",,9,0.0,,,,17203320,
2-s2.0-85124964445,10.1613/JAIR.1.13113,,,Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge,ar,Article,Dognin P.,60017366;60108014,"IBM Thomas J. Watson Research Center;IBM, South Africa",Yorktown Heights;Johannesburg,United States;South Africa,9.0,"Dognin, Pierre;Melnyk, Igor;Mroueh, Youssef;Padhi, Inkit;Rigotti, Mattia;Ross, Jarret;Schiff, Yair;Young, Richard A.;Belgodere, Brian",55967126700;57057398500;55699828500;57205673185;15731884000;57201311992;57221860490;57215354299;55994815600,60017366;60017366;60017366;60017366;60017366;60017366;60017366;60108014;60017366,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,437-459,"Image captioning has recently demonstrated impressive progress largely owing to the introduction of neural network algorithms trained on curated dataset like MS-COCO. Often work in this field is motivated by the promise of deployment of captioning systems in practical applications. However, the scarcity of data and contexts in many competition datasets renders the utility of systems trained on these datasets limited as an assistive technology in real-world settings, such as helping visually impaired people navigate and accomplish everyday tasks. This gap motivated the introduction of the novel VizWiz dataset, which consists of images taken by the visually impaired and captions that have useful, task-oriented information. In an attempt to help the machine learning computer vision field realize its promise of producing technologies that have positive social impact, the curators of the VizWiz dataset host several competitions, including one for image captioning. This work details the theory and engineering from our winning submission to the 2020 captioning competition. Our work provides a step towards improved assistive image captioning systems.",,31,1.0,all publisherfullgold,All Open Access Gold,IBM,,International Business Machines Corporation
2-s2.0-85189607726,10.1609/aaai.v38i21.30355,,,ImageSTEAM: Teacher Professional Development for Integrating Visual Computing into Middle School Lessons,cp,Conference Paper,Jayasuriya S.,60003892;60029747,Arizona State University;University of Georgia,Tempe;Athens,United States;United States,9.0,"Jayasuriya, Suren;Swisher, Kimberlee;Rego, Joshua D.;Chandran, Sreenithy;Mativo, John;Kurz, Terri;Collins, Cerenity E.;Robinson, Dawn T.;Pidaparti, Ramana",56258873100;57218240762;57221263755;57193614884;8529129800;26032110800;58973453400;7404645127;7005705093,60003892;60003892;60003892;60003892;60029747;60003892;60029747;60029747;60029747,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,21.0,,23101-23109,"Artificial intelligence (AI) and its teaching in the K-12 grades has been championed as a vital need for the United States due to the technology's future prominence in the 21st century. However, there remain several barriers to effective AI lessons at these age groups including the broad range of interdisciplinary knowledge needed and the lack of formal training or preparation for teachers to implement these lessons. In this experience report, we present ImageSTEAM, a teacher professional development for creating lessons surrounding computer vision, machine learning, and computational photography/cameras targeted for middle school grades 6-8 classes. Teacher professional development workshops were conducted in the states of Arizona and Georgia from 2021-2023 where lessons were co-created with teachers to introduce various specific visual computing concepts while aligning to state and national standards. In addition, the use of a variety of computer vision and image processing software including custom designed Python notebooks were created as technology activities and demonstrations to be used in the classroom. Educational research showed that teachers improved their self-efficacy and outcomes for concepts in computer vision, machine learning, and artificial intelligence when participating in the program. Results from the professional development workshops highlight key opportunities and challenges in integrating this content into the standard curriculum, the benefits of a co-creation pedagogy, and the positive impact on teacher and student's learning experiences. The open-source program curriculum is available at www.imagesteam.org.",,3,1.0,all publisherfullgold,All Open Access Gold,NSF,DRL-1949384,National Science Foundation
2-s2.0-85191155931,,,,Imitation Learning from Imperfection: Theoretical Justifications and Algorithms,cp,Conference Paper,Li Z.,60033100;60008592;60108865;60128916;131185514,"Nanjing University;Hong Kong University of Science and Technology;The Chinese University of Hong Kong, Shenzhen;Shenzhen Research Institute of Big Data;Polixir.ai",Nanjing;Hong Kong;Shenzhen;Shenzhen;,China;Hong Kong;China;China;,5.0,"Li, Ziniu;Xu, Tian;Qin, Zeyu;Yu, Yang;Luo, Zhi Quan",57219590356;57219586626;58107211400;58308453100;58654018800,60108865-60128916;60033100-131185514;60008592;60033100-131185514;60108865-60128916,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Imitation learning (IL) algorithms excel in acquiring high-quality policies from expert data for sequential decision-making tasks. But, their effectiveness is hampered when faced with limited expert data. To tackle this challenge, a novel framework called (offline) IL with supplementary data has been proposed [25, 61], which enhances learning by incorporating an additional yet imperfect dataset obtained inexpensively from sub-optimal policies. Nonetheless, learning becomes challenging due to the potential inclusion of out-of-expert-distribution samples. In this work, we propose a mathematical formalization of this framework, uncovering its limitations. Our theoretical analysis reveals that a naive approach-applying the behavioral cloning (BC) algorithm concept to the combined set of expert and supplementary data-may fall short of vanilla BC, which solely relies on expert data. This deficiency arises due to the distribution shift between the two data sources. To address this issue, we propose a new importance-sampling-based technique for selecting data within the expert distribution. We prove that the proposed method eliminates the gap of the naive approach, highlighting its efficacy when handling imperfect data. Empirical studies demonstrate that our method outperforms previous state-of-the-art methods in tasks including robotic locomotion control, Atari video games, and image classification. Overall, our work underscores the potential of improving IL by leveraging diverse data sources through effective data selection.",,13,0.0,,,NKRDPC,2020AAA0107200,National Key Research and Development Program of China
2-s2.0-85137864680,10.24963/ijcai.2022/242,,,Imperceptible Backdoor Attack: From Input Space to Feature Representation,cp,Conference Paper,Zhong N.,60009860,Fudan University,Shanghai,China,3.0,"Zhong, Nan;Qian, Zhenxing;Zhang, Xinpeng",57210103991;35280781200;57218701409,60009860;60009860;60009860,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1736-1742,"Backdoor attacks are rapidly emerging threats to deep neural networks (DNNs). In the backdoor attack scenario, attackers usually implant the backdoor into the target model by manipulating the training dataset or training process. Then, the compromised model behaves normally for benign input yet makes mistakes when the pre-defined trigger appears. In this paper, we analyze the drawbacks of existing attack approaches and propose a novel imperceptible backdoor attack. We treat the trigger pattern as a special kind of noise following a multinomial distribution. A U-net-based network is employed to generate concrete parameters of multinomial distribution for each benign input. This elaborated trigger ensures that our approach is invisible to both humans and statistical detection. Besides the design of the trigger, we also consider the robustness of our approach against model diagnose-based defences. We force the feature representation of malicious input stamped with the trigger to be entangled with the benign one. We demonstrate the effectiveness and robustness against multiple state-of-the-art defences through extensive datasets and networks. Our trigger only modifies less than 1% pixels of a benign image while the modification magnitude is 1. Our source code is available at https://github.com/Ekko-zn/IJCAI2022-Backdoor.",,33,1.0,all publisherfree2read,All Open Access Bronze,NSFC,U1936214,National Natural Science Foundation of China
2-s2.0-85174400491,,,,Implicit Graph Neural Networks: A Monotone Operator Viewpoint,cp,Conference Paper,Baker J.,60025488;60024266,The University of Utah;Oak Ridge National Laboratory,Salt Lake City;Oak Ridge,United States;United States,4.0,"Baker, Justin;Wang, Qingsong;Hauck, Cory;Wang, Bao",57675085200;57218872596;25637072500;57808014700,60025488;60025488;60024266;60025488,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,1521-1548,"Implicit graph neural networks (IGNNs) - that solve a fixed-point equilibrium equation using Picard iteration for representation learning - have shown remarkable performance in learning long-range dependencies (LRD) in the underlying graphs. However, IGNNs suffer from several issues, including 1) their expressivity is limited by their parameterizations for the well-posedness guarantee, 2) IGNNs are unstable in learning LRD, and 3) IGNNs become computationally inefficient when learning LRD. In this paper, we provide a new well-posedness characterization for IGNNs leveraging monotone operator theory, resulting in a much more expressive parameterization than the existing one. We also propose an orthogonal parameterization for IGNN based on Cayley transform to stabilize learning LRD. Furthermore, we leverage Anderson-accelerated operator splitting schemes to efficiently solve for the fixed point of the equilibrium equation of IGNN with monotone or orthogonal parameterization. We verify the computational efficiency and accuracy of the new models over existing IGNNs on various graph learning tasks at both graph and node levels. Code is available at https://github.com/Utah-Math-Data-Science/MIGNN.",,5,0.0,,,NSF,DMS-1952339,National Science Foundation
2-s2.0-85214017205,,,,Implicit Regularization and Entrywise Convergence of Riemannian Optimization for Low Tucker-Rank Tensor Completion,ar,Article,Wang H.,60009860;60011069;130612447,Fudan University;East China University of Science and Technology;China Mobile (Zhejiang) Research & Innovation Institute,Shanghai;Shanghai;Hangzhou,China;China;China,3.0,"Wang, Haifeng;Chen, Jinchi;Wei, Ke",57219636980;57197709185;55911095900,60009860-130612447;60011069;60009860,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,347,,"This paper is concerned with the low Tucker-rank tensor completion problem, which is about reconstructing a tensor T ∈ R<sup>n</sup>×n×<sup>n</sup> of low multilinear rank from partially observed entries. Riemannian optimization algorithms are a class of efficient methods for this problem, but the theoretical convergence analysis is still lacking. In this manuscript, we establish the entrywise convergence of the vanilla Riemannian gradient method for low Tucker-rank tensor completion under the nearly optimal sampling complexity O(n<sup>3</sup>/<sup>2</sup>). Meanwhile, the implicit regularization phenomenon of the algorithm has also been revealed. As far as we know, this is the first work that has shown the entrywise convergence and implicit regularization property of a non-convex method for low Tucker-rank tensor completion. The analysis relies on the leave-one-out technique, and some of the technical results developed in the paper might be of broader interest in investigating the properties of other non-convex methods for this problem.",entrywise convergence | implicit regularization | leave-one-out | low rank tensor completion | Riemannian gradient | Tucker decomposition,5,0.0,,,NKRDPC,2021YFA1003300,National Key Research and Development Program of China
2-s2.0-85146941964,,,,Improved Generalization Bounds for Adversarially Robust Learning,ar,Article,Attias I.,60005681;60027161,Tel Aviv University;Ben-Gurion University of the Negev,Tel Aviv-Yafo;Beer-Sheva,Israel;Israel,3.0,"Attias, Idan;Kontorovich, Aryeh;Mansour, Yishay",57219502457;37028335400;7004528828,60027161;60027161;60005681,2022-06-01,1 June 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"We consider a model of robust learning in an adversarial environment. The learner gets uncorrupted training data with access to possible corruptions that may be affected by the adversary during testing. The learner’s goal is to build a robust classifier, which will be tested on future adversarial examples. The adversary is limited to k possible corruptions for each input. We model the learner-adversary interaction as a zero-sum game. This model is closely related to the adversarial examples model of Schmidt et al. (2018); Madry et al. (2017). Our main results consist of generalization bounds for the binary and multiclass classification, as well as the real-valued case (regression). For the binary classification setting, we both tighten the generalization bound of Feige et al. (2015), and are also able to handle infinite hypothesis classes. The sample complexity is improved from O( <sup>1</sup><inf>4</inf> log(<sup>|H|</sup><inf>δ</inf> )) to O( <sup>1</sup><inf>2</inf> (k VC(H) log <sup>3 2 +α</sup>(k VC(H)) + log(<sup>1</sup><inf>δ</inf> )) for any α > 0. Additionally, we extend the algorithm and generalization bound from the binary to the multiclass and real-valued cases. Along the way, we obtain results on fat-shattering dimension and Rademacher complexity of k-fold maxima over function classes; these may be of independent interest. For binary classification, the algorithm of Feige et al. (2015) uses a regret minimization algorithm and an ERM oracle as a black box; we adapt it for the multiclass and regression settings. The algorithm provides us with near-optimal policies for the players on a given training sample.",Adversarial Robustness | PAC Learning | Sample Complexity | Zero-Sum Game,17,0.0,,,ISF,,Israel Science Foundation
2-s2.0-85203823498,,,,Improving Antibody Humanness Prediction using Patent Data,cp,Conference Paper,Uçar T.,60031101;60004219,University of Cambridge;AstraZeneca,Cambridge;Cambridge,United Kingdom;United Kingdom,6.0,"Uçar, Talip;Ramon, Aubin;Oglic, Dino;Croasdale-Wood, Rebecca;Diethe, Tom;Sormanni, Pietro",57219502507;58298170900;56358144100;58848726300;23388087700;55159736800,60004219;60031101;60004219;60004219;60004219;60031101,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,48878-48891,"We investigate the potential of patent data for improving the antibody humanness prediction using a multi-stage, multi-loss training process. Humanness serves as a proxy for the immunogenic response to antibody therapeutics, one of the major causes of attrition in drug discovery and a challenging obstacle for their use in clinical settings. We pose the initial learning stage as a weakly-supervised contrastive-learning problem, where each antibody sequence is associated with possibly multiple identifiers of function and the objective is to learn an encoder that groups them according to their patented properties. We then freeze a part of the contrastive encoder and continue training it on the patent data using the cross-entropy loss to predict the humanness score of a given antibody sequence. We illustrate the utility of the patent data and our approach by performing inference on three different immunogenicity datasets, unseen during training. Our empirical results demonstrate that the learned model consistently outperforms the alternative baselines and establishes new state-of-the-art on five out of six inference tasks, irrespective of the used metric.",,1,0.0,,,,,AstraZeneca
2-s2.0-85137685102,10.1609/aaai.v36i10.21359,,,Improving Neural Cross-Lingual Abstractive Summarization via Employing Optimal Transport Distance for Knowledge Distillation,cp,Conference Paper,Nguyen T.T.,60005510;124356029,Nanyang Technological University;VinAI Research,Singapore City;Hanoi,Singapore;Viet Nam,2.0,"Nguyen, Thong Thanh;Luu, Anh Tuan",57217776052;55490655100,124356029;60005510,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,11103-11111,"Current state-of-the-art cross-lingual summarization models employ multi-task learning paradigm, which works on a shared vocabulary module and relies on the self-attention mechanism to attend among tokens in two languages. However, correlation learned by self-attention is often loose and implicit, inefficient in capturing crucial cross-lingual representations between languages. The matter worsens when performing on languages with separate morphological or structural features, making the cross-lingual alignment more challenging, resulting in the performance drop. To overcome this problem, we propose a novel Knowledge-Distillation-based framework for Cross-Lingual Summarization, seeking to explicitly construct cross-lingual correlation by distilling the knowledge of the monolingual summarization teacher into the cross-lingual summarization student. Since the representations of the teacher and the student lie on two different vector spaces, we further propose a Knowledge Distillation loss using Sinkhorn Divergence, an Optimal-Transport distance, to estimate the discrepancy between those teacher and student representations. Due to the intuitively geometric nature of Sinkhorn Divergence, the student model can productively learn to align its produced cross-lingual hidden states with monolingual hidden states, hence leading to a strong correlation between distant languages. Experiments on cross-lingual summarization datasets in pairs of distant languages demonstrate that our method outperforms state-of-the-art models under both high and low-resourced settings.",,47,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85148739753,,,,Improving Policy Optimization with Generalist-Specialist Learning,cp,Conference Paper,Jia Z.,60030612,"University of California, San Diego",La Jolla,United States,6.0,"Jia, Zhiwei;Li, Xuanlin;Ling, Zhan;Liu, Shuang;Wu, Yiran;Su, Hao",57207818469;57231753600;57219687879;57202056002;57788564100;55208624800,60030612;60030612;60030612;60030612;60030612;60030612,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,10104-10119,"Generalization in deep reinforcement learning over unseen environment variations usually requires policy learning over a large set of diverse training variations. We empirically observe that an agent trained on many variations (a generalist) tends to learn faster at the beginning, yet its performance plateaus at a less optimal level for a long time. In contrast, an agent trained only on a few variations (a specialist) can often achieve high returns under a limited computational budget. To have the best of both worlds, we propose a novel generalist-specialist training framework. Specifically, we first train a generalist on all environment variations; when it fails to improve, we launch a large population of specialists with weights cloned from the generalist, each trained to master a selected small subset of variations. We finally resume the training of the generalist with auxiliary rewards induced by demonstrations of all specialists. In particular, we investigate the timing to start specialist training and compare strategies to learn generalists with assistance from specialists. We show that this framework pushes the envelope of policy learning on several challenging and popular benchmarks including Procgen, Meta-World and ManiSkill.",,15,0.0,,,,,
2-s2.0-85182143955,10.1613/JAIR.1.15001,,,Improving Resource Allocations by Sharing in Pairs,ar,Article,Bredereck R.,60000762;60011604;60022381;60017351;60021841,Humboldt-Universität zu Berlin;Technische Universität Berlin;Beijing Jiaotong University;AGH University of Krakow;Technische Universität Clausthal,Berlin;Berlin;Beijing;Krakow;Clausthal-Zellerfeld,Germany;Germany;China;Poland;Germany,5.0,"Bredereck, Robert;Kaczmarczyk, Andrzej;Luo, Junjie;Niedermeier, Rolf;Sachse, Florian",36720017100;57193490864;57212538094;7004137881;57382723200,60000762-60021841;60017351-60011604;60022381;60011604;60011604,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,1069-1109,"Given an initial resource allocation, where some agents may envy others or where a different distribution of resources might lead to a higher social welfare, our goal is to improve the allocation without reassigning resources. We consider a sharing concept allowing resources being shared with social network neighbors of the resource owners. More precisely, our model allows agents to form pairs which then may share a limited number of resources. Sharing a resource can come at some costs or loss in utility. To this end, we introduce a formal model that allows a central authority to compute an optimal sharing between neighbors based on an initial allocation. Advocating this point of view, we focus on the most basic scenario where each agent can participate in a bounded number of sharings. We present algorithms for optimizing utilitarian and egalitarian social welfare of allocations and for reducing the number of envious agents. In particular, we examine the computational complexity with respect to several natural parameters. Furthermore, we study cases with restricted social network structures and, among others, devise polynomial-time algorithms in path- and tree-like (hierarchical) social networks.",,3,1.0,all publisherfullgold,All Open Access Gold,H2020,2023XKRC007,Horizon 2020 Framework Programme
2-s2.0-85185117183,,,,Improving multiple-try Metropolis with local balancing,ar,Article,Gagnon P.,60009507;60021796,University of Montreal;Università Bocconi,Montreal;Milan,Canada;Italy,3.0,"Gagnon, Philippe;Maire, Florian;Zanella, Giacomo",57202989038;55221050200;56956669800,60009507;60009507;60021796,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"Multiple-try Metropolis (MTM) is a popular Markov chain Monte Carlo method with the appealing feature of being amenable to parallel computing. At each iteration, it samples several candidates for the next state of the Markov chain and randomly selects one of them based on a weight function. The canonical weight function is proportional to the target density. We show both theoretically and empirically that this weight function induces pathological behaviours in high dimensions, especially during the convergence phase. We propose to instead use weight functions akin to the locally-balanced proposal distributions of Zanella (2020), thus yielding MTM algorithms that do not exhibit those pathological behaviours. To theoretically analyse these algorithms, we study the high-dimensional performance of ideal schemes that can be thought of as MTM algorithms which sample an infinite number of candidates at each iteration, as well as the discrepancy between such schemes and the MTM algorithms which sample a finite number of candidates. Our analysis unveils a strong distinction between the convergence and stationary phases: in the former, local balancing is crucial and effective to achieve fast convergence, while in the latter, the canonical and novel weight functions yield similar performance. Numerical experiments include an application in precision medicine involving a computationally-expensive forward model, which makes the use of parallel computing within MTM iterations beneficial.",Bayesian statistics | Markov chain Monte Carlo | parallel computing | random-walk Metropolis | scaling limit | weak convergence,2,0.0,,,NSERC,101076564,Natural Sciences and Engineering Research Council of Canada
2-s2.0-105018663947,,,,Improving physics-informed neural networks with meta-learned optimization,ar,Article,Bihlo A.,60019000,Memorial University of Newfoundland,St John's,Canada,1.0,"Bihlo, Alex",24921225500,60019000,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"We show that the error achievable using physics-informed neural networks for solving differential equations can be substantially reduced when these networks are trained using meta-learned optimization methods rather than using fixed, hand-crafted optimizers as traditionally done. We choose a learnable optimization method based on a shallow multilayer perceptron that is meta-trained for specific classes of differential equations. We illustrate meta-trained optimizers for several equations of practical relevance in mathematical physics, including the linear advection equation, Poisson’s equation, the Korteweg–de Vries equation and Burgers’ equation. We also illustrate that meta-learned optimizers exhibit transfer learning abilities, in that a meta-trained optimizer on one differential equation can also be successfully deployed on another differential equation.",Learnable optimization | Meta-learning | Physics-informed neural networks | Scientific machine learning | Transfer learning,15,0.0,,,CRC,,Canada Research Chairs
2-s2.0-85174405837,,,,In Search for a Generalizable Method for Source Free Domain Adaptation,cp,Conference Paper,Boudiaf M.,60006191;60026786,Google LLC;École de Technologie Supérieure,Mountain View;Montreal,United States;Canada,5.0,"Boudiaf, Malik;Denton, Tom;van Merriënboer, Bart;Dumoulin, Vincent;Triantafillou, Eleni",57219688639;57222288880;57188495900;55237034500;57191043967,60026786;60006191;60006191;60006191;60006191,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,2914-2931,"Source-free domain adaptation (SFDA) is compelling because it allows adapting an off-the-shelf model to a new domain using only unlabelled data. In this work, we apply existing SFDA techniques to a challenging set of naturally-occurring distribution shifts in bioacoustics, which are very different from the ones commonly studied in computer vision. We find existing methods perform differently relative to each other than observed in vision benchmarks, and sometimes perform worse than no adaptation at all. We propose a new simple method which outperforms the existing methods on our new shifts while exhibiting strong performance on a range of vision datasets. Our findings suggest that existing SFDA methods are not as generalizable as previously thought and that considering diverse modalities can be a useful avenue for designing more robust models.",,10,0.0,,,,,
2-s2.0-85163184881,,,,Increasing Confidence in Adversarial Robustness Evaluations,cp,Conference Paper,Zimmermann R.S.,60017246;129867739,Eberhard Karls Universität Tübingen;Google,Tubingen;,Germany;,4.0,"Zimmermann, Roland S.;Brendel, Wieland;Tramèr, Florian;Carlini, Nicholas",57210751249;57207550476;56878876400;57194977162,60017246;60017246;129867739;129867739,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Hundreds of defenses have been proposed to make deep neural networks robust against minimal (adversarial) input perturbations. However, only a handful of these defenses held up their claims because correctly evaluating robustness is extremely challenging: Weak attacks often fail to find adversarial examples even if they unknowingly exist, thereby making a vulnerable network look robust. In this paper, we propose a test to identify weak attacks and, thus, weak defense evaluations. Our test slightly modifies a neural network to guarantee the existence of an adversarial example for every sample. Consequentially, any correct attack must succeed in breaking this modified network. For eleven out of thirteen previously-published defenses, the original evaluation of the defense fails our test, while stronger attacks that break these defenses pass it. We hope that attack unit tests - such as ours - will be a major component in future robustness evaluations and increase confidence in an empirical field that is currently riddled with skepticism. Online version & code: zimmerrol.github.io/active-tests/.",,11,0.0,,,DFG,2064/1,Good Ventures Foundation
2-s2.0-85129852863,10.1613/JAIR.1.12695,,,Incremental Event Calculus for Run-Time Reasoning,ar,Article,Tsilionis E.,60028900;60001104,"National and Kapodistrian University of Athens;National Centre for Scientific Research ""DEMOKRITOS""",Athens;Athens,Greece;Greece,3.0,"Tsilionis, Efthimis;Artikis, Alexander;Paliouras, Georgios",57090956500;6602748562;6602657411,60028900;60001104;60001104,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,967-1023,"We present a system for online, incremental composite event recognition. In streaming environments, the usual case is for data to arrive with a (variable) delay from, and to be revised by, the underlying sources. We propose RTEC<inf>inc</inf>, an incremental version of RTEC, a composite event recognition engine with formal, declarative semantics, that has been shown to scale to several real-world data streams. RTEC deals with delayed arrival and revision of events by computing all queries from scratch. This is often inefficient since it results in redundant computations. Instead, RTEC<inf>inc</inf> deals with delays and revisions in a more efficient way, by updating only the affected queries. We examine RTEC<inf>inc</inf> theoretically, presenting a complexity analysis, and show the conditions in which it outperforms RTEC. Moreover, we compare RTEC<inf>inc</inf> and RTEC experimentally using real-world and synthetic datasets. The results are compatible with our theoretical analysis and show that RTEC<inf>inc</inf> outperforms RTEC in many practical cases.",,12,1.0,all publisherfullgold,All Open Access Gold,H2020,825070,Horizon 2020 Framework Programme
2-s2.0-85168854072,,,,Incremental Learning in Diagonal Linear Networks,ar,Article,Berthier R.,60028186,École Polytechnique Fédérale de Lausanne,Lausanne,Switzerland,1.0,"Berthier, Raphaël",57195297984,60028186,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,171,,"Diagonal linear networks (DLNs) are a toy simplification of artificial neural networks; they consist in a quadratic reparametrization of linear regression inducing a sparse implicit regularization. In this paper, we describe the trajectory of the gradient flow of DLNs in the limit of small initialization. We show that incremental learning is effectively performed in the limit: coordinates are successively activated, while the iterate is the minimizer of the loss constrained to have support on the active coordinates only. This shows that the sparse implicit regularization of DLNs decreases with time. This work is restricted to the underparametrized regime with anti-correlated features for technical reasons.",diagonal linear networks | implicit bias | incremental learning | Lotka-Volterra | saddle-to-saddle dynamics,8,0.0,,,,,
2-s2.0-85197054022,10.1613/jair.1.15236,,,"Individual Fairness, Base Rate Tracking and the Lipschitz Condition",ar,Article,Eva B.,60008724,Duke University,Durham,United States,1.0,"Eva, Benjamin",57113453000,60008724,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,859-873,"In recent years, there has been a proliferation of competing conceptions of what it means for a predictive algorithm to treat its subjects fairly. Most approaches focus on explicating a notion of group fairness, i.e. of what it means for an algorithm to treat one group unfairly in comparison to another. In contrast, Dwork et al. (2012) attempt to carve out a formalised conception of individual fairness, i.e. of what it means for an algorithm to treat an individual fairly or unfairly. In this paper, I demonstrate that the conception of individual fairness advocated by Dwork et al. is closely related to a criterion of group fairness, called ‘base rate tracking’, introduced in Eva (2022). I subsequently show that base rate tracking solves some fundamental conceptual problems associated with the Lipschitz criterion, before arguing that group level fairness criteria are at least as powerful as their individual level counterparts when it comes to diagnosing algorithmic bias.",,0,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-105000489230,,,,Induced Model Matching: Restricted Models Help Train Full-Featured Models,cp,Conference Paper,Muneeb U.,60027561,University of Illinois at Chicago,Chicago,United States,2.0,"Muneeb, Usama;Ohannessian, Mesrob I.",57218453137;12243450700,60027561;60027561,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"We consider scenarios where a very accurate (often small) predictive model using restricted features is available when training a full-featured (often larger) model. This restricted model may be thought of as “side-information”, and can come either from an auxiliary dataset or from the same dataset by forcing the restriction. How can the restricted model be useful to the full model? To answer this, we introduce a methodology called Induced Model Matching (IMM). IMM aligns the context-restricted, or induced, version of the large model with the restricted model. We relate IMM to approaches such as noising, which is implicit in addressing the problem, and reverse knowledge distillation from weak teachers, which is explicit but does not exploit restriction being the nature of the weakness. We show that these prior methods can be thought of as approximations to IMM and can be problematic in terms of consistency. Experimentally, we first motivate IMM using logistic regression as a toy example. We then explore it in language modeling, the application that initially inspired it, and demonstrate it on both LSTM and transformer full models, using bigrams as restricted models. We lastly give a simple RL example, which shows that POMDP policies can help learn better MDP policies. The IMM principle is thus generally applicable in common scenarios where restricted data is cheaper to collect or restricted models are easier to learn.",,0,0.0,,,NSF,CCF-2146334,National Science Foundation
2-s2.0-85203794808,,,,"Information Complexity of Stochastic Convex Optimization: Applications to Generalization, Memorization, and Tracing",cp,Conference Paper,Attias I.,60016849;60005681;60027161;60111161;60141178;60278837,University of Toronto;Tel Aviv University;Ben-Gurion University of the Negev;DeepMind Technologies Limited;Khoury College of Computer Sciences;Vector Institute,Toronto;Tel Aviv-Yafo;Beer-Sheva;London;Boston;Toronto,Canada;Israel;Israel;United Kingdom;United States;Canada,5.0,"Attias, Idan;Dziugaite, Gintare Karolina;Haghifam, Mahdi;Livni, Roi;Roy, Daniel M.",57219502457;56896118000;57214610120;56096564800;8608757400,60027161-60278837;60111161;60141178;60005681;60278837-60016849,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,2035-2068,"In this work, we investigate the interplay between memorization and learning in the context of stochastic convex optimization (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou [SZ20]. Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni [Liv23]. We show that, in the L<sup>2</sup> Lipschitz-bounded setting and under strong convexity, every learner with an excess error ε has CMI bounded below by Ω(1/ε<sup>2</sup>) and Ω(1/ε), respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems. Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems.",,1,0.0,,,NSERC,,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85124195823,,,,Inherent Tradeoffs in Learning Fair Representations,ar,Article,Zhao H.,60000745;60027950,University of Illinois Urbana-Champaign;Carnegie Mellon University,Urbana;Pittsburgh,United States;United States,2.0,"Zhao, Han;Gordon, Geoffrey J.",57001574800;57203070987,60000745;60027950,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Real-world applications of machine learning tools in high-stakes domains are often regulated to be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with respect to a protected attribute. However, the exact tradeoff between fairness and accuracy is not entirely clear, even for the basic paradigm of classification problems. In this paper, we characterize an inherent tradeoff between statistical parity and accuracy in the classification setting by providing a lower bound on the sum of group-wise errors of any fair classifiers. Our impossibility theorem could be interpreted as a certain uncertainty principle in fairness: if the base rates differ among groups, then any fair classifier satisfying statistical parity has to incur a large error on at least one of the groups. We further extend this result to give a lower bound on the joint error of any (approximately) fair classifiers, from the perspective of learning fair representations. To show that our lower bound is tight, assuming oracle access to Bayes (potentially unfair) classifiers, we also construct an algorithm that returns a randomized classifier which is both optimal (in terms of accuracy) and fair. Interestingly, when the protected attribute can take more than two values, an extension of this lower bound does not admit an analytic solution. Nevertheless, in this case, we show that the lower bound can be efficiently computed by solving a linear program, which we term as the TV-Barycenter problem, a barycenter problem under the TV-distance. On the upside, we prove that if the group-wise Bayes optimal classifiers are close, then learning fair representations leads to an alternative notion of fairness, known as the accuracy parity, which states that the error rates are close between groups. Finally, we also conduct experiments on real-world datasets to confirm our theoretical findings.",Algorithmic fairness | Information theory | Representation learning,67,0.0,,,DARPA,87501720152,Defense Advanced Research Projects Agency
2-s2.0-85147256952,10.1613/jair.1.14015,,,Initialization of Feature Selection Search for Classification,ar,Article,Luque-Rodriguez M.,60003138;129141268,Universidad de Córdoba;Area of Project Engineering,Cordoba;Cordoba,Spain;Spain,4.0,"Luque-Rodriguez, Maria;Molina-Baena, Jose;Jimenez-Vilchez, Alfonso;Arauzo-Azofra, Antonio",57196628165;57207908695;57209025642;24176540800,60003138;60003138;60003138;129141268,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,953-983,"Selecting the best features in a data set improves accuracy and efficiency of classifiers in a learning process. Data sets generally have more features than necessary, some of them being irrelevant or redundant to others. For this reason, numerous feature selection methods have been developed, in which different evaluation functions and measures are applied. This paper proposes the systematic application of individual feature evaluation methods to initialize search-based feature subset selection methods. An exhaustive review of the starting methods used by genetic algorithms from 2014 to 2020 has been carried out. Subsequently, an in-depth empirical study has been carried out evaluating the proposal for different search-based feature selection methods (Sequential forward and backward selection, Las Vegas filter and wrapper, Simulated Annealing and Genetic Algorithms). Since the computation time is reduced and the classification accuracy with the selected features is improved, the initialization of feature selection proposed in this work is proved to be worth considering while designing any feature selection algorithms.",,6,1.0,all publisherfullgold,All Open Access Gold,MICINN,PID2020-115832GB-I00,Ministerio de Ciencia e Innovación
2-s2.0-85147696158,10.1609/aaai.v36i1.19930,,,InsCLR: Improving Instance Retrieval with Self-Supervision,cp,Conference Paper,Deng Z.,60118460;60121285;132139412;132139285,Alibaba Group Holding Limited;Ant group;Dmall;Meituan Inc,Hangzhou;Hangzhou;;,China;China;;,4.0,"Deng, Zelu;Zhong, Yujie;Guo, Sheng;Huang, Weilin",59615921700;57220844730;56416860100;56119269400,132139412;132139285;60121285;60118460,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,516-524,"This work aims at improving instance retrieval with self-supervision. We find that fine-tuning using the recently developed self-supervised learning (SSL) methods, such as SimCLR and MoCo, fails to improve the performance of instance retrieval. In this work, we identify that the learnt representations for instance retrieval should be invariant to large variations in viewpoint and background etc., whereas self-augmented positives applied by the current SSL methods can not provide strong enough signals for learning robust instance-level representations. To overcome this problem, we propose InsCLR, a new SSL method that builds on the instance-level contrast, to learn the intra-class invariance by dynamically mining meaningful pseudo positive samples from both mini-batches and a memory bank during training. Extensive experiments demonstrate that InsCLR achieves similar or even better performance than the state-ofthe-art SSL methods on instance retrieval. Code is available at https://github.com/zeludeng/insclr.",,13,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85174425181,,,,Instant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery Tickets from Large Models,cp,Conference Paper,Jaiswal A.,60013372,The University of Texas at Austin,Austin,United States,5.0,"Jaiswal, Ajay;Liu, Shiwei;Chen, Tianlong;Ding, Ying;Wang, Zhangyang",57221254179;57217824780;57221072108;35229200000;56288839400,60013372;60013372;60013372;60013372;60013372,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,14691-14701,"Large pre-trained transformers have been receiving explosive attention in the past few years, due to their wide adaptability for numerous downstream applications via fine-tuning, but their exponentially increasing parameter counts are becoming a primary hurdle to even just fine-tune them without industry-standard hardware. Recently, Lottery Ticket Hypothesis (LTH) and its variants, have been exploited to prune these large pre-trained models generating subnetworks that can achieve similar performance as their dense counterparts, but LTH pragmatism is enormously inhibited by repetitive full training and pruning routine of iterative magnitude pruning (IMP) which worsens with increasing model size. Motivated by the recent observations of model soups, which suggest that fine-tuned weights of multiple models can be merged to a better minima, we propose Instant Soup Pruning (ISP) to generate lottery ticket quality subnetworks, using a fraction of the original IMP cost by replacing the expensive intermediate pruning stages of IMP with computationally efficient weak mask generation and aggregation routine. More specifically, during the mask generation stage, ISP takes a small handful of iterations using varying training protocols and data subsets to generate many weak and noisy subnetworks, and superpose them to average out the noise creating a high-quality denoised subnetwork. Our extensive experiments and ablation on two popular large-scale pre-trained models: CLIP (unexplored in pruning till date) and BERT across multiple benchmark vision {MNIST, SVHN, Cars, GTSRB, CIFAR-10, CIFAR-100} and language datasets {MNLI, QNLI, QQP, SST,...} validate the effectiveness of ISP compared to several state-of-the-art pruning methods. Additionally, we show that ISP can be easily modified with minimal overhead to produce benefits comparable to model soups, without the prerequisite to generate multiple candidates fine-tuned models. Codes are available at: https://github. com/VITA-Group/instant_soup.",,12,0.0,,,UT,2022-21102100004,University of Texas at Austin
2-s2.0-85204281541,,,,Intention Progression with Temporally Extended Goals,cp,Conference Paper,Yao Y.,60007989;60015875;60104720;60024360,Universiteit Utrecht;University of Aberdeen;University of Nottingham Ningbo China;Open Universiteit,Utrecht;Aberdeen;Ningbo;Heerlen,Netherlands;United Kingdom;China;Netherlands,3.0,"Yao, Yuan;Alechina, Natasha;Logan, Brian",56420897200;6603450996;56092567900,60104720;60024360-60007989;60007989-60015875,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,292-301,"The Belief-Desire-Intention (BDI) approach to agent development has formed the basis for much of the research on architectures for autonomous agents. A key advantage of the BDI approach is that agents may pursue multiple intentions in parallel. However, previous approaches to managing possible interactions between concurrently executing intentions are limited to interactions between simple achievement goals (and in some cases maintenance goals). In this paper, we present a new approach to intention progression for agents with temporally extended goals which allow mixing reachability and invariant properties, e.g., “travel to location A while not exceeding a gradient of 5%”. Temporally extended goals may be specified at run-time (top-level goals), and as subgoals in plans. In addition, our approach allows human-authored plans and plans implemented as reinforcement learning policies to be freely mixed in an agent program, allowing the development of agents with 'neuro-symbolic' architectures.",,0,0.0,,,,2022A-234-G,
2-s2.0-85147255951,10.1613/jair.1.14019,,,Interpretable Local Concept-based Explanation with Human Feedback to Predict All-cause Mortality,ar,Article,Elshawi R.,60068856;60008981,Tartu Ülikool;Houston Methodist,Tartu;Houston,Estonia;United States,2.0,"Elshawi, Radwa;Al-Mallah, Mouaz H.",56507337800;8570397900,60068856;60008981,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,833-855,"Machine learning models are incorporated in different fields and disciplines in which some of them require a high level of accountability and transparency, for example, the healthcare sector. With the General Data Protection Regulation (GDPR), the importance for plausibility and verifiability of the predictions made by machine learning models has become essential. A widely used category of explanation techniques attempts to explain models’ predictions by quantifying the importance score of each input feature. However, summarizing such scores to provide human-interpretable explanations is challenging. Another category of explanation techniques focuses on learning a domain representation in terms of high-level human-understandable concepts and then utilizing them to explain predictions. These explanations are hampered by how concepts are constructed, which is not intrinsically interpretable. To this end, we propose Concept-based Local Explanations with Feedback (CLEF), a novel local model agnostic explanation framework for learning a set of high-level transparent concept definitions in high-dimensional tabular data that uses clinician-labeled concepts rather than raw features. CLEF maps the raw input features to high-level intuitive concepts and then decompose the evidence of prediction of the instance being explained into concepts. In addition, the proposed framework generates counterfactual explanations, suggesting the minimum changes in the instance’s concept-based explanation that will lead to a different prediction. We demonstrate with simulated user feedback on predicting the risk of mortality. Such direct feedback is more effective than other techniques, that rely on hand-labelled or automatically extracted concepts, in learning concepts that align with ground truth concept definitions.",,12,1.0,all publisherfullgold,All Open Access Gold,ERDF,MOBTT75,European Regional Development Fund
2-s2.0-85195856583,,,,Interpretable and Fair Boolean Rule Sets via Column Generation,ar,Article,Lawless C.,60104946;60011048,Cornell University College of Engineering;IBM Research,Ithaca;Yorktown Heights,United States;United States,4.0,"Lawless, Connor;Dash, Sanjeeb;Günlük, Oktay;Wei, Dennis",57226332130;9637416600;6507068853;35273532700,60104946;60011048;60104946;60011048,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"This paper considers the learning of Boolean rules in disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. We also consider the fairness setting and extend the formulation to include explicit constraints on two different measures of classification parity: equality of opportunity and equalized odds. Column generation (CG) is used to efficiently search over an exponential number of candidate rules without the need for heuristic rule mining. To handle large data sets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 data sets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurate. Compared to other fair and interpretable classifiers, our method is able to find rule sets that meet stricter notions of fairness with a modest trade-off in accuracy.",Classification | Fair Machine Learning | Integer Programming | Interpretability | Rule Sets,10,0.0,,,ONR,N00014-21-1-2575,Office of Naval Research
2-s2.0-85137576783,10.1609/aaai.v36i11.21442,,,Interpreting Gender Bias in Neural Machine Translation: Multilingual Architecture Matters,cp,Conference Paper,Costa-Jussà M.R.,60007592;60109361,Universitat Politècnica de Catalunya;Institute of Graduate Studies and Research,Barcelona;Alexandria,Spain;Egypt,6.0,"Costa-Jussà, Marta R.;Escolano, Carlos;Basta, Christine;Ferrando, Javier;Batlle, Roser;Kharitonova, Ksenia",15519053500;57197861020;57218283579;57218194743;57221907329;57221904366,60007592;60007592;60007592-60109361;60007592;60007592;60007592,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,11855-11863,"Multilingual neural machine translation architectures mainly differ in the number of sharing modules and parameters applied among languages. In this paper, and from an algorithmic perspective, we explore whether the chosen architecture, when trained with the same data, influences the level of gender bias. Experiments conducted in three language pairs show that language-specific encoder-decoders exhibit less bias than the shared architecture. We propose two methods for interpreting and studying gender bias in machine translation based on source embeddings and attention. Our analysis shows that, in the language-specific case, the embeddings encode more gender information, and their attention is more diverted. Both behaviors help in mitigating gender bias.",,17,1.0,all publisherfullgold,All Open Access Gold,H2020,947657,Horizon 2020 Framework Programme
2-s2.0-85138661364,,,,Interventional Contrastive Learning with Meta Semantic Regularizer,cp,Conference Paper,Qiang W.,60027363;60008592;60014402;60025256;60272288;60276981;119194081,University of Chinese Academy of Sciences;Hong Kong University of Science and Technology;Renmin University of China;Institute of Software Chinese Academy of Sciences;Southern Marine Science and Engineering Guangdong Laboratory;The Hong Kong University of Science and Technology (Guangzhou);Beijing Key Laboratory of Big Data Management and Analysis Methods,Beijing;Hong Kong;Beijing;Beijing;Guangzhou;Guangzhou;Beijing,China;Hong Kong;China;China;China;China;China,5.0,"Qiang, Wenwen;Li, Jiangmeng;Zheng, Changwen;Su, Bing;Xiong, Hui",57202680422;57223355548;7401934931;37121023200;7201935465,60025256-60027363-60272288;60025256-60027363-60272288;60025256-60272288;60014402-119194081;60276981-60008592,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,18018-18030,"Contrastive learning (CL)-based self-supervised learning models learn visual representations in a pairwise manner. Although the prevailing CL model has achieved great progress, in this paper, we uncover an ever-overlooked phenomenon: When the CL model is trained with full images, the performance tested in full images is better than that in foreground areas; when the CL model is trained with foreground areas, the performance tested in full images is worse than that in foreground areas. This observation reveals that backgrounds in images may interfere with the model learning semantic information and their influence has not been fully eliminated. To tackle this issue, we build a Structural Causal Model (SCM) to model the background as a confounder. We propose a backdoor adjustment-based regularization method, namely Interventional Contrastive Learning with Meta Semantic Regularizer (ICL-MSR), to perform causal intervention towards the proposed SCM. ICL-MSR can be incorporated into any existing CL methods to alleviate background distractions from representation learning. Theoretically, we prove that ICL-MSR achieves a tighter error bound. Empirically, our experiments on multiple benchmark datasets demonstrate that ICL-MSR is able to improve the performances of different state-of-the-art CL methods.",,30,0.0,,,NSFC,BJJWZYJH012019100020098,National Natural Science Foundation of China
2-s2.0-85150990204,10.1609/aaai.v37i9.26322,,,Interventional SHAP Values and Interaction Values for Piecewise Linear Regression Trees,cp,Conference Paper,Zern A.,60017246;60075737,Eberhard Karls Universität Tübingen;SCHUFA Holding AG,Tubingen;Hannover,Germany;Germany,3.0,"Zern, Artjom;Broelemann, Klaus;Kasneci, Gjergji",57201435067;35087925700;22834642800,60075737;60075737;60017246,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,11164-11173,"In recent years, game-theoretic Shapley values have gained increasing attention with respect to local model explanation by feature attributions. While the approach using Shapley values is model-independent, their (exact) computation is usually intractable, so efficient model-specific algorithms have been devised including approaches for decision trees or their ensembles in general. Our work goes further in this direction by extending the interventional TreeSHAP algorithm to piecewise linear regression trees, which gained more attention in the past few years. To this end, we introduce a decomposition of the contribution function based on decision paths, which allows a more comprehensible formulation of SHAP algorithms for tree-based models. Our algorithm can also be readily applied to computing SHAP interaction values for these models. In particular, as the main contribution of this paper, we provide a more efficient approach of interventional SHAP for tree-based models by precomputing statistics of the background data based on the tree structure.",,11,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85191198049,,,,Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP,cp,Conference Paper,Qian Q.,60118460;60006602;60118459,"Alibaba Group Holding Limited;University of Washington, Tacoma;Alibaba Group, USA",Hangzhou;Tacoma;San Mateo,China;United States;United States,3.0,"Qian, Qi;Xu, Yuanhong;Hu, Juhua",58847098600;57219692657;57209485366,60118459;60118460;60006602,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Vision-language pre-training methods, e.g., CLIP, demonstrate an impressive zero-shot performance on visual categorizations with the class proxy from the text embedding of the class name. However, the modality gap between the text and vision space can result in a sub-optimal performance. We theoretically show that the gap cannot be reduced sufficiently by minimizing the contrastive loss in CLIP and the optimal proxy for vision tasks may reside only in the vision space. Therefore, given unlabeled target vision data, we propose to learn the vision proxy directly with the help from the text proxy for zero-shot transfer. Moreover, according to our theoretical analysis, strategies are developed to further refine the pseudo label obtained by the text proxy to facilitate the intra-modal proxy learning (InMaP) for vision. Experiments on extensive downstream tasks confirm the effectiveness and efficiency of our proposal. Concretely, InMaP can obtain the vision proxy within one minute on a single GPU while improving the zero-shot accuracy from 77.02% to 80.21% on ImageNet with ViT-L/14@336 pre-trained by CLIP.",,15,0.0,,,,,
2-s2.0-85131347938,,,,Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning,ar,Article,Forestier S.,60013373,INRIA Institut National de Recherche en Informatique et en Automatique,Le Chesnay,France,5.0,"Forestier, Sebastien;Portelas, Remy;Sud-Ouest, I. B.;Oudeyer, Pierre Yves;Mollard, Yoan",56293782800;57219585851;57730078200;6507418132;57118116500,60013373;60013373;60013373;60013373;60013373,2022-04-01,1 April 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Intrinsically motivated spontaneous exploration is a key enabler of autonomous developmental learning in human children. It enables the discovery of skill repertoires through autotelic learning, i.e. the self-generation, self-selection, self-ordering and self-experimentation of learning goals. We present an algorithmic approach called Intrinsically Motivated Goal Exploration Processes (IMGEP) to enable similar properties of autonomous learning in machines. The IMGEP architecture relies on several principles: 1) self-generation of goals, generalized as parameterized fitness functions; 2) selection of goals based on intrinsic rewards; 3) exploration with incremental goal-parameterized policy search and exploitation with a batch learning algorithm; 4) systematic reuse of information acquired when targeting a goal for improving towards other goals. We present a particularly efficient form of IMGEP, called AMB, that uses a population-based policy and an object-centered spatiotemporal modularity. We provide several implementations of this architecture and demonstrate their ability to automatically generate a learning curriculum within several experimental setups. One of these experiments includes a real humanoid robot exploring multiple spaces of goals with several hundred continuous dimensions and with distractors. While no particular target goal is provided to these autotelic agents, this curriculum allows the discovery of diverse skills that act as stepping stones for learning more complex skills, e.g. nested tool use",automatic curriculum learning | autotelic agents | curiosity-driven learning | developmental AI | Developmental learning | goal exploration | intrinsic motivations | modularity | open-ended learning | population-based IMGEP | robotics,41,0.0,,,,,
2-s2.0-85163208281,,,,Invariance Learning based on Label Hierarchy,cp,Conference Paper,Toyota S.,60021523,The Graduate University for Advanced Studies,Hayama,Japan,2.0,"Toyota, Shoji;Fukumizu, Kenji",57219749737;6602093536,60021523;60021523,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Deep Neural Networks inherit biased correlations embedded in training data and hence may fail to predict desired labels on unseen domains (or environments), which have different distributions from the domain to provide training data. Invariance Learning (IL) has been developed recently to overcome this shortcoming; using training data in many domains, IL estimates such a predictor that is invariant to a change of domain. However, the requirement of training data in multiple domains is a strong restriction of using IL, since it demands expensive annotation. We propose a novel IL framework to overcome this problem. Assuming the availability of data from multiple domains for a classification task at a higher level, for which the labeling cost is lower, we estimate an invariant predictor for the target classification task with training data gathered in a single domain. Additionally, we propose two cross-validation methods for selecting hyperparameters of invariance regularization, which has not been addressed properly in existing IL methods. The effectiveness of the proposed framework, including the cross-validation, is demonstrated empirically. Theoretical analysis reveals that our framework can estimate the desirable invariant predictor with a hyperparameter fixed correctly, and that such a preferable hyperparameter is chosen by the proposed CV methods under some conditions.",,1,0.0,,,KAKEN,20J21396,Japan Society for the Promotion of Science
2-s2.0-105018461513,,,,Invariant and Equivariant Reynolds Networks,ar,Article,Sannai A.,60025272;60011001,The University of Tokyo;Kyoto University,Tokyo;Kyoto,Japan;Japan,3.0,"Sannai, Akiyoshi;Kawano, Makoto;Kumagai, Wataru",24535598300;57191037821;15829402000,60011001;60025272;60025272,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Various data exhibit symmetry, including permutations in graphs and point clouds. Machine learning methods that utilize this symmetry have achieved considerable success. In this study, we explore learning models for data exhibiting group symmetry. Our focus is on transforming deep neural networks using Reynolds operators, which average over the group to convert a function into an invariant or equivariant form. While learning methods based on Reynolds operators are well-established, they often face computational complexity challenges. To address this, we introduce two new methods that reduce the computational burden associated with the Reynolds operator: (i) Although the Reynolds operator traditionally averages over the entire group, we demonstrate that it can be effectively approximated by averaging over specific subsets of the group, termed the Reynolds design. (ii) We reveal that the pre-model does not require all input variables. Instead, using a select number of partial inputs (Reynolds dimension) is sufficient to achieve a universally applicable model. Employing these methods, which hinge on the Reynolds design and Reynolds dimension concepts, allows us to construct universally applicable models with manageable computational complexity. Our experiments on benchmark data indicate that our approach is more efficient than existing methods.",equivariance | graph neural networks | invariant representations | Reynolds operator | symmetry,1,0.0,,,PRESTO,23H04974,Precursory Research for Embryonic Science and Technology
2-s2.0-85208289787,10.1613/jair.1.15244,,,Inverting Cryptographic Hash Functions via Cube-and-Conquer,ar,Article,Zaikin O.,60002049;60010545,Novosibirsk State University;Matrosov Institute for System Dynamics and Control Theory of Siberian Branch of Russian Academy of Sciences,Novosibirsk;Irkutsk,Russian Federation;Russian Federation,1.0,"Zaikin, Oleg",56786079600,60002049-60010545,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,359-399,"MD4 and MD5 are fundamental cryptographic hash functions proposed in the early 1990s. MD4 consists of 48 steps and produces a 128-bit hash given a message of arbitrary finite size. MD5 is a more secure 64-step extension of MD4. Both MD4 and MD5 are vulnerable to practical collision attacks, yet it is still not realistic to invert them, i.e., to find a message given a hash. In 2007, the 39-step version of MD4 was inverted by reducing to SAT and applying a CDCL solver along with the so-called Dobbertin’s constraints. As for MD5, in 2012 its 28-step version was inverted via a CDCL solver for one specified hash without adding any extra constraints. In this study, Cube-and-Conquer (a combination of CDCL and lookahead) is applied to invert step-reduced versions of MD4 and MD5. For this purpose, two algorithms are proposed. The first one generates inverse problems for MD4 by gradually modifying the Dobbertin’s constraints. The second algorithm tries the cubing phase of Cube-and-Conquer with different cutoff thresholds to find the one with the minimum runtime estimate of the conquer phase. This algorithm operates in two modes: (i) estimating the hardness of a given propositional Boolean formula; (ii) incomplete SAT solving of a given satisfiable propositional Boolean formula. While the first algorithm is focused on inverting step-reduced MD4, the second one is not area-specific and is therefore applicable to a variety of classes of hard SAT instances. In this study, 40-, 41-, 42-, and 43-step MD4 are inverted for the first time via the first algorithm and the estimating mode of the second algorithm. Also, 28-step MD5 is inverted for four hashes via the incomplete SAT solving mode of the second algorithm. For three hashes out of them, it is done for the first time.",,4,1.0,all publisherfullgold,All Open Access Gold,Minobrnauka,075-15-2022-282,Ministry of Education and Science of the Russian Federation
2-s2.0-85163186180,,,,Is Out-of-Distribution Detection Learnable?,cp,Conference Paper,Fang Z.,60026553;60025858;60023932;60014347;60021474;60153202,"University of Melbourne;ETH Zürich;University of Technology Sydney;Hong Kong Baptist University;Shenyang Institute of Automation Chinese Academy of Sciences;School of Computer, Data & Information Sciences",Melbourne;Zurich;Sydney;Hong Kong;Shenyang;Madison,Australia;Switzerland;Australia;Hong Kong;China;United States,6.0,"Fang, Zhen;Li, Yixuan;Lu, Jie;Dong, Jiahua;Han, Bo;Liu, Feng",57211269319;57188823744;7601559842;57215778780;57191281044;57118471000,60023932;60153202;60023932;60021474-60025858;60014347;60023932-60026553,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Supervised learning aims to train a classifier under the assumption that training and test data are from the same distribution. To ease the above assumption, researchers have studied a more realistic setting: out-of-distribution (OOD) detection, where test data may come from classes that are unknown during training (i.e., OOD data). Due to the unavailability and diversity of OOD data, good generalization ability is crucial for effective OOD detection algorithms. To study the generalization of OOD detection, in this paper, we investigate the probably approximately correct (PAC) learning theory of OOD detection, which is proposed by researchers as an open problem. First, we find a necessary condition for the learnability of OOD detection. Then, using this condition, we prove several impossibility theorems for the learnability of OOD detection under some scenarios. Although the impossibility theorems are frustrating, we find that some conditions of these impossibility theorems may not hold in some practical scenarios. Based on this observation, we next give several necessary and sufficient conditions to characterize the learnability of OOD detection in some practical scenarios. Lastly, we also offer theoretical supports for several representative OOD detection works based on our OOD theory.",,104,0.0,,,AFOSR,22200720,Air Force Office of Scientific Research
2-s2.0-105018671909,,,,Iterate Averaging in the Quest for Best Test Error,ar,Article,Granziol D.,60026851;60020650;60120016,University of Oxford;University of Bristol;Department of Engineering,Oxford;Bristol;Cambridge,United Kingdom;United Kingdom;United Kingdom,5.0,"Granziol, Diego;Baskerville, Nicholas P.;Wan, Xingchen;Albanie, Samuel;Roberts, Stephen",57200213654;57219636322;57219526422;57202293265;57203276441,60026851;60020650;60026851;60120016;60026851,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"We analyse and explain the increased generalisation performance of iterate averaging using a Gaussian process perturbation model between the true and batch risk surface on the high dimensional quadratic. We derive three phenomena from our theoretical results: (1) The importance of combining iterate averaging (IA) with large learning rates and regularisation for improved generalisation. (2) Justification for less frequent averaging. (3) That we expect adaptive gradient methods to work equally well, or better, with iterate averaging than their non-adaptive counterparts. Inspired by these results, together with empirical investigations of the importance of appropriate regularisation for the solution diversity of the iterates, we propose two adaptive algorithms with iterate averaging. These give significantly better results compared to stochastic gradient descent (SGD), require less tuning and do not require early stopping or validation set monitoring. We showcase the efficacy of our approach on the CIFAR-10/100, ImageNet and Penn Treebank datasets on a variety of modern and classical network architectures.",adaptive gradient methods | deep learning limit | deep learning theory | generalisation | iterate averaging,0,0.0,,,,EP/T028572/1,
2-s2.0-85189554020,10.1613/jair.1.14924,,,Iterative Train Scheduling under Disruption with Maximum Satisfiability,ar,Article,Lemos A.,60004956,Instituto Superior Técnico,Lisbon,Portugal,4.0,"Lemos, Alexandre;Gouveia, Filipe;Monteiro, Pedro T.;Lynce, Ines",57200338817;57195585102;35362263400;6603143167,60004956;60004956;60004956;60004956,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,1047-1090,"This paper proposes an iterative Maximum Satisfiability (MaxSAT) approach designed to solve train scheduling optimization problems. The generation of railway timetables is known to be intractable for a single track. We consider hundreds of trains on interconnected multi-track railway networks with complex connections between trains. Furthermore, the proposed algorithm is incremental to reduce the impact of time discretization. The performance of our approach is evaluated with the real-world Swiss Federal Railway (SBB) Crowd Sourcing Challenge benchmark and Periodic Event Scheduling Problems benchmark (PESPLib). The execution time of the proposed approach is shown to be, on average, twice as fast as the best existing solution for the SBB instances. In addition, we achieve a significant improvement over SAT-based solutions for solving the PESPLib instances. We also analyzed real schedule data from Switzerland and the Netherlands to create a disruption generator based on probability distributions. The novel incremental algorithm allows solving the train scheduling problem under disruptions with better performance than traditional algorithms.",,3,1.0,all publisherfullgold,All Open Access Gold,FCT,SFRH/BD/143212/2019,Fundação para a Ciência e a Tecnologia
2-s2.0-85148092730,,,,Joint Continuous and Discrete Model Selection via Submodularity,ar,Article,Bunton J.,60153950,UCLA Samueli School of Engineering,Los Angeles,United States,2.0,"Bunton, Jonathan;Tabuada, Paulo",57219547383;6603754297,60153950;60153950,2022-11-01,1 November 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,329,,"In model selection problems for machine learning, the desire for a well-performing model with meaningful structure is typically expressed through a regularized optimization problem. In many scenarios, however, the meaningful structure is specified in some discrete space, leading to difficult nonconvex optimization problems. In this paper, we connect the model selection problem with structure-promoting regularizers to submodular function minimization with continuous and discrete arguments. In particular, we leverage the theory of submodular functions to identify a class of these problems that can be solved exactly and efficiently with an agnostic combination of discrete and continuous optimization routines. We show how simple continuous or discrete constraints can also be handled for certain problem classes, and extend these ideas to a robust optimization framework. We also show how some problems outside of this class can be embedded into the class, further extending the class of problems our framework can accommodate. Finally, we numerically validate our theoretical results with several proof-of-concept examples with synthetic and real-world data, comparing against state-of-the-art algorithms.",convex optimization | mixed continuous discrete optimization | sparsity | submodular function minimization | Submodularity,0,0.0,,,ARL,W911NF-17-2-0196,Army Research Laboratory
2-s2.0-85144747569,10.1609/aaai.v36i1.19880,,,Joint Human Pose Estimation and Instance Segmentation with PosePlusSeg,cp,Conference Paper,Ahmad N.,60211585,Hanyang University ERICA Campus,Ansan,South Korea,4.0,"Ahmad, Niaz;Khan, Jawad;Kim, Jeremy Yuhyun;Lee, Youngmoon",57226101655;57194026018;57790020800;57193209234,60211585;60211585;60211585;60211585,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,69-76,"Despite the advances in multi-person pose estimation, state-of-the-art techniques only deliver the human pose structure. Yet, they do not leverage the keypoints of human pose to deliver whole-body shape information for human instance segmentation. This paper presents PosePlusSeg, a joint model designed for both human pose estimation and instance segmentation. For pose estimation, PosePlusSeg first takes a bottom-up approach to detect the soft and hard keypoints of individuals by producing a strong keypoint heat map, then improves the keypoint detection confidence score by producing a body heat map. For instance segmentation, PosePlusSeg generates a mask offset where keypoint is defined as a centroid for the pixels in the embedding space, enabling instance-level segmentation for the human class. Finally, we propose a new pose and instance segmentation algorithm that enables PosePlusSeg to determine the joint structure of the human pose and instance segmentation. Experiments using the COCO challenging dataset demonstrate that PosePlusSeg copes better with challenging scenarios, like occlusions, entangled limbs, and overlapped people. PosePlusSeg outperforms state-of-the-art detection-based approaches achieving a 0.728 mAP for human pose estimation and a 0.445 mAP for instance segmentation. Code has been made available at https://github.com/RaiseLab/PosePlusSeg.",,7,1.0,all publisherfullgold,All Open Access Gold,MSIP,IITP-2020-0-101741,"Ministry of Science, ICT and Future Planning"
2-s2.0-85138117911,10.1613/JAIR.1.13981,,,Joint Optimization of Concave Scalarized Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm,ar,Article,Bai Q.,60009254,Purdue University,West Lafayette,United States,3.0,"Bai, Qinbo;Agarwal, Mridul;Aggarwal, Vaneet",57215744230;57215275559;24066547400,60009254;60009254;60009254,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,1565-1597,"Many engineering problems have multiple objectives, and the overall aim is to optimize a non-linear function of these objectives. In this paper, we formulate the problem of maximizing a non-linear concave function of multiple long-term objectives. A policy-gradient based model-free algorithm is proposed for the problem. To compute an estimate of the gradient, an asymptotically biased estimator is proposed. The proposed algorithm is shown to achieve convergence to within an ∊ of the global optima after sampling (Formula Presented) trajectories where γ is the discount factor and M is the number of the agents, thus achieving the same dependence on ∊ as the policy gradient algorithm for the standard reinforcement learning.",,3,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85128954521,10.1613/JAIR.1.13350,,,Jointly Learning Environments and Control Policies with Projected Stochastic Gradient Ascent,ar,Article,Bolland A.,60000964,Université de Liège,Liege,Belgium,4.0,"Bolland, Adrien;Boukas, Ioannis;Berger, Mathias;Ernst, Damien",57219630268;57210467374;57207916399;7103071727,60000964;60000964;60000964;60000964,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,117-171,"We consider the joint design and control of discrete-time stochastic dynamical systems over a finite time horizon. We formulate the problem as a multi-step optimization problem under uncertainty seeking to identify a system design and a control policy that jointly maximize the expected sum of rewards collected over the time horizon considered. The transition function, the reward function and the policy are all parametrized, assumed known and differentiable with respect to their parameters. We then introduce a deep reinforcement learning algorithm combining policy gradient methods with model-based optimization techniques to solve this problem. In essence, our algorithm iteratively approximates the gradient of the expected return via Monte-Carlo sampling and automatic differentiation and takes projected gradient ascent steps in the space of environment and policy parameters. This algorithm is referred to as Direct Environment and Policy Search (DEPS). We assess the performance of our algorithm in three environments concerned with the design and control of a mass-spring-damper system, a small-scale off-grid power system and a drone, respectively. In addition, our algorithm is benchmarked against a state-of-the-art deep reinforcement learning algorithm used to tackle joint design and control problems. We show that DEPS performs at least as well or better in all three environments, consistently yielding solutions with higher returns in fewer iterations. Finally, solutions produced by our algorithm are also compared with solutions produced by an algorithm that does not jointly optimize environment and policy parameters, highlighting the fact that higher returns can be achieved when joint optimization is performed.",,4,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,FNRS,,Fonds De La Recherche Scientifique - FNRS
2-s2.0-85148099476,,,,JsonGrinder.jl: automated differentiable neural architecture for embedding arbitrary JSON data,ar,Article,Mandlík Š.,60013323;129183985,Czech Technical University in Prague;Avast Software s.r.o.,Prague;,Czech Republic;,4.0,"Mandlík, Šimon;Račinský, Matěj;Lisý, Viliam;Pevný, Tomáš",57223742301;57224541301;27567791200;23486243000,60013323-129183985;129183985;60013323-129183985;60013323-129183985,2022-09-01,1 September 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,298,,"Standard machine learning (ML) problems are formulated on data converted into a suitable tensor representation. However, there are data sources, for example in cybersecurity, that are naturally represented in a unifying hierarchical structure, such as XML, JSON, and Protocol Buffers. Converting this data to a tensor representation is usually done by manual feature engineering, which is laborious, lossy, and prone to bias originating from the human inability to correctly judge the importance of particular features. JsonGrinder.jl is a library automating various ML tasks on these difficult sources. Starting with an arbitrary set of JSON samples, it automatically creates a differentiable ML model (called HMILnet), which embeds raw JSON samples into a fixed-size tensor representation. This embedding network can be naturally extended by an arbitrary ML model expecting tensor inputs in order to perform classification, regression, or clustering.",,3,0.0,,,,,
2-s2.0-85148012676,,,,Jump Gaussian Process Model for Estimating Piecewise Continuous Regression Functions,ar,Article,Park C.,60002092,Florida State University,Tallahassee,United States,1.0,"Park, Chiwoo",14038000100,60002092,2022-09-01,1 September 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,278,,"This paper presents a Gaussian process (GP) model for estimating piecewise continuous regression functions. In many scientific and engineering applications of regression analysis, the underlying regression functions are often piecewise continuous in that data follow different continuous regression models for different input regions with discontinuities across regions. However, many conventional GP regression approaches are not designed for piecewise regression analysis. There are piecewise GP models to use explicit domain partitioning and pose independent GP models over partitioned regions. They are not flexible enough to model real datasets where data domains are divided by complex and curvy jump boundaries. We propose a new GP modeling approach to estimate an unknown piecewise continuous regression function. The new GP model seeks a local GP estimate of an unknown regression function at each test location, using local data neighboring the test location. Considering the possibilities of the local data being from different regions, the proposed approach partitions the local data into pieces by a local data partitioning function. It uses only the local data likely from the same region as the test location for the regression estimate. Since we do not know which local data points come from the relevant region, we propose a data-driven approach to split and subset local data by a local partitioning function. We discuss several modeling choices of the local data partitioning function, including a locally linear function and a locally polynomial function. We also investigate an optimization problem to jointly optimize the partitioning function and other covariance parameters using a likelihood maximization criterion. Several advantages of using the proposed approach over the conventional GP and piecewise GP modeling approaches are shown by various simulated experiments and real data studies.",Gaussian Process Regression | Jump Regression | Local Data Partitioning | Local Data Selection | Piecewise Regression,5,0.0,,,NSF,NSF-2152655/NSF-2152679,National Science Foundation
2-s2.0-85185654594,,,,KWIKBUCKS: CORRELATION CLUSTERING WITH CHEAP-WEAK AND EXPENSIVE-STRONG SIGNALS,cp,Conference Paper,Silwal S.,60022195;60006191,Massachusetts Institute of Technology;Google LLC,Cambridge;Mountain View,United States;United States,6.0,"Silwal, Sandeep;Ahmadian, Sara;Nystrom, Andrew;McCallum, Andrew;Ramachandran, Deepak;Kazemi, Mehran",57213834457;55754599800;57525299700;7003773569;56247655000;56394110200,60022195;60006191;60006191;60006191;60006191;60006191,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"The unprecedented rate at which machine learning (ML) models are growing in size necessitates novel approaches to enable efficient and scalable solutions. We contribute to this line of work by studying a novel version of the Budgeted Correlation Clustering problem (BCC) where along with a limited number of queries to an expensive oracle for node similarities (e.g. a large ML model), we have unlimited access to a cheaper but less accurate second oracle. Our formulation is inspired by many practical scenarios where coarse approximations of the expensive similarity metric can be efficiently obtained via weaker models. We develop a theoretically motivated algorithm that leverages the cheap oracle to judiciously query the strong oracle while maintaining high clustering quality. We empirically demonstrate gains in query minimization and clustering metrics on a variety of datasets with diverse strong and cheap oracles. Most notably, we demonstrate a practical application in text clustering based on expensive cross-attention language models by showing that cheaper (but weaker) embedding-based models can be leveraged to substantially reduce the number of inference calls to the former.",,12,0.0,,,,,
2-s2.0-85146927533,,,,Kernel Autocovariance Operators of Stationary Processes: Estimation and Convergence,ar,Article,Mollenhauer M.,60030718;60019656;60032824,Freie Universität Berlin;Heriot-Watt University;Zuse Institute Berlin,Berlin;Edinburgh;Berlin,Germany;United Kingdom;Germany,4.0,"Mollenhauer, Mattes;Klus, Stefan;Schütte, Christof;Koltai, Péter",57212409560;8948161100;7005968052;36600896300,60030718;60019656;60030718-60032824;60030718,2022-10-01,1 October 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,327,,"We consider autocovariance operators of a stationary stochastic process on a Polish space that is embedded into a reproducing kernel Hilbert space. We investigate how empirical estimates of these operators converge along realizations of the process under various conditions. In particular, we examine ergodic and strongly mixing processes and obtain several asymptotic results as well as finite sample error bounds. We provide applications of our theory in terms of consistency results for kernel PCA with dependent data and the conditional mean embedding of transition probabilities. Finally, we use our approach to examine the nonparametric estimation of Markov transition operators and highlight how our theory can give a consistency analysis for a large family of spectral analysis methods including kernel-based dynamic mode decomposition.",autocovariance operator | ergodic process | kernel mean embedding | mixing | stationary time series,6,0.0,,,DFG,A01,Deutsche Forschungsgemeinschaft
2-s2.0-85180791958,,,,Kernel-based estimation for partially functional linear model: Minimax rates and randomized sketches,ar,Article,Lv S.,60002798;60032744;60089945,Chinese University of Hong Kong;Shanghai University of Finance and Economics;Nanjing Audit University,Hong Kong;Shanghai;Nanjing,Hong Kong;China;China,3.0,"Lv, Shaogao;He, Xin;Wang, Junhui",36961359100;57194595481;14044437100,60089945;60032744;60002798,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"This paper considers the partially functional linear model (PFLM) where all predictive features consist of a functional covariate and a high dimensional scalar vector. Over an infinite dimensional reproducing kernel Hilbert space, the proposed estimation for PFLM is a least square approach with two mixed regularizations of a function-norm and an `<inf>1</inf>-norm. Our main task in this paper is to establish the minimax rates for PFLM under high dimensional setting, and the optimal minimax rates of estimation are established by using various techniques in empirical process theory for analyzing kernel classes. In addition, we propose an efficient numerical algorithm based on randomized sketches of the kernel matrix. Several numerical experiments are implemented to support our method and optimization strategy.",Functional linear models | minimax rates | randomized sketches | reproducing kernel Hilbert space | sparsity,2,0.0,,,SUFE,GRF-11304520,Shanghai University of Finance and Economics
2-s2.0-85189324000,10.1609/aaai.v38i9.28924,,,Knowledge Enhanced Representation Learning for Drug Discovery,cp,Conference Paper,Hoang T.L.,60005141;60029158;60108035,"University College Dublin;IBM Research - Zurich;IBM Research Europe, Ireland",Dublin;Ruschlikon;Dublin,Ireland;Switzerland;Ireland,9.0,"Hoang, Thanh Lam;Sbodio, Marco Luca;Galindo, Marcos Martinez;Zayats, Mykhaylo;Fernandez-Diaz, Raul;Valls, Victor;Picco, Gabriele;Berrospi, Cesar;Lopez, Vanessa",19639216700;55886127400;58514804300;56423541200;58483569100;55481418800;57217259095;18036727900;8928407100,60108035;60108035;60108035;60108035;60108035-60005141;60108035;60108035;60029158;60108035,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,9.0,,10544-10552,"Recent research on predicting the binding affinity between drug molecules and proteins use representations learned, through unsupervised learning techniques, from large databases of molecule SMILES and protein sequences. While these representations have significantly enhanced the predictions, they are usually based on a limited set of modalities, and they do not exploit available knowledge about existing relations among molecules and proteins. Our study reveals that enhanced representations, derived from multimodal knowledge graphs describing relations among molecules and proteins, lead to state-of-the-art results in well-established benchmarks (first place in the leaderboard for Therapeutics Data Commons benchmark “Drug-Target Interaction Domain Generalization Benchmark”, with an improvement of 8 points with respect to previous best result). Moreover, our results significantly surpass those achieved in standard benchmarks by using conventional pre-trained representations that rely only on sequence or SMILES data. We release our multimodal knowledge graphs, integrating data from seven public data sources, and which contain over 30 million triples. Pretrained models from our proposed graphs and benchmark task source code are also released.",,7,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85180974449,,,,Knowledge Hypergraph Embedding Meets Relational Algebra,ar,Article,Fatemi B.,60010365;127186796,The University of British Columbia;ServiceNow Research,Vancouver;Montreal,Canada;Canada,4.0,"Fatemi, Bahare;Taslakian, Perouz;Vazquez, David;Poole, David",57218202258;6504765315;36444850100;8380392800,60010365;127186796;127186796;60010365,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,105,,"Relational databases are a successful model for data storage, and rely on query languages for information retrieval. Most of these query languages are based on relational algebra, a mathematical formalization at the core of relational models. Knowledge graphs are flexible data storage structures that allow for knowledge completion using machine learning techniques. Knowledge hypergraphs generalize knowledge graphs by allowing multi-argument relations. This work studies knowledge hypergraph completion through the lens of relational algebra and its core operations. We explore the space between relational algebra foundations and machine learning techniques for knowledge completion. We investigate whether such methods can capture high-level abstractions in terms of relational algebra operations. We propose a simple embedding-based model called Relational Algebra Embedding (ReAlE) that performs link prediction in knowledge hypergraphs. We show theoretically that ReAlE is fully expressive and can represent the relational algebra operations of renaming, projection, set union, selection, and set difference. We verify experimentally that ReAlE outperforms state-of-the-art models in knowledge hypergraph completion, and in representing each of these primitive relational algebra operations. For the latter experiment, we generate a synthetic knowledge hypergraph, for which we design an algorithm based on the Erdős-Rényi model for generating random graphs.",Knowledge Hypergraph Completion | Knowledge Hypergraphs | Relational Algebra,11,0.0,,,,,
2-s2.0-85143745675,,,,LANGUAGE MODELING VIA STOCHASTIC PROCESSES,cp,Conference Paper,Wang R.E.,60012708,Stanford University,Stanford,United States,4.0,"Wang, Rose E.;Durmus, Esin;Goodman, Noah;Hashimoto, Tatsunori B.",57310835000;57209205698;12767629400;57202060550,60012708;60012708;60012708;60012708,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better). Human evaluators also prefer TC's output 28.6% more than the baselines.",,11,0.0,,,NSF,,National Science Foundation
2-s2.0-85130482665,,,,LEARNING AUDIO-VISUAL SPEECH REPRESENTATION BY MASKED MULTIMODAL CLUSTER PREDICTION,cp,Conference Paper,Shi B.,60074721;60355330,Toyota Technological Institute at Chicago;Meta Ai,Chicago;Menlo Park,United States;United States,4.0,"Shi, Bowen;Hsu, Wei Ning;Lakhotia, Kushal;Mohamed, Abdelrahman",57208008653;57191867824;57219749119;56215658600,60074721;60355330;60355330;60355330,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours) (Makino et al., 2019). The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%). Our code and models are available at https://github.com/facebookresearch/av_hubert.",,287,0.0,,,,,
2-s2.0-85149821337,,,,LEARNING BY DIRECTIONAL GRADIENT DESCENT,cp,Conference Paper,Silver D.,60022148;60111161;60113142,University College London;DeepMind Technologies Limited;Montreal Institute for Learning Algorithms,London;London;Montreal,United Kingdom;United Kingdom;Canada,5.0,"Silver, David;Goyal, Anirudh;Danihelka, Ivo;Hessel, Matteo;van Hasselt, Hado",7202151417;57194150283;56461353100;56594798000;21744168500,60111161-60022148;60113142;60111161-60022148;60111161;60111161-60022148,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"How should state be constructed from a sequence of observations, so as to best achieve some objective? Most deep learning methods update the parameters of the state representation by gradient descent. However, no prior method for computing the gradient is fully satisfactory, for example consuming too much memory, introducing too much variance, or adding too much bias. In this work, we propose a new learning algorithm that addresses these limitations. The basic idea is to update the parameters of the representation by using the directional derivative along a candidate direction, a quantity that may be computed online with the same computational cost as the representation itself. We consider several different choices of candidate direction, including random selection and approximations to the true gradient, and investigate their performance on several synthetic tasks.",,11,0.0,,,,,
2-s2.0-85182557952,,,,LEARNING HETEROGENEOUS INTERACTION STRENGTHS BY TRAJECTORY PREDICTION WITH GRAPH NEURAL NETWORK,cp,Conference Paper,Ha S.,60032144,Korea Advanced Institute of Science and Technology,Daejeon,South Korea,2.0,"Ha, Seungwoong;Jeong, Hawoong",57219794100;7401620063,60032144;60032144,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Dynamical systems with interacting agents are universal in nature, commonly modeled by a graph of relationships between their constituents. Recently, various works have been presented to tackle the problem of inferring those relationships from the system trajectories via deep neural networks, but most of the studies assume binary or discrete types of interactions for simplicity. In the real world, the interaction kernels often involve continuous interaction strengths, which cannot be accurately approximated by discrete relations. In this work, we propose the relational attentive inference network (RAIN) to infer continuously weighted interaction graphs without any ground-truth interaction strengths. Our model employs a novel pairwise attention (PA) mechanism to refine the trajectory representations and a graph transformer to extract heterogeneous interaction weights for each pair of agents. We show that our RAIN model with the PA mechanism accurately infers continuous interaction strengths for simulated physical systems in an unsupervised manner. Further, RAIN with PA successfully predicts trajectories from motion capture data with an interpretable interaction graph, demonstrating the virtue of modeling unknown dynamics with continuous weights.",,4,0.0,,,NRF,NRF-2022R1A2B5B02001752,National Research Foundation of Korea
2-s2.0-85191743672,,,,LEARNING HUMAN-COMPATIBLE REPRESENTATIONS FOR CASE-BASED DECISION SUPPORT,cp,Conference Paper,Liu H.,60117717,"Department of Computer Science, The University of Chicago",Chicago,United States,6.0,"Liu, Han;Tian, Yizhou;Chen, Chacha;Feng, Shi;Chen, Yuxin;Tan, Chenhao",57219116636;58150546100;58106170500;57193243996;36730725300;36483489900,60117717;60117717;60117717;60117717;60117717;60117717,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Algorithmic case-based decision support provides examples to aid people in decision making tasks by providing contexts for a test case. Despite the promising performance of supervised learning, representations learned by supervised models may not align well with human intuitions: what models consider similar examples can be perceived as distinct by humans. As a result, they have limited effectiveness in case-based decision support. In this work, we incorporate ideas from metric learning with supervised learning to examine the importance of alignment for effective decision support. In addition to instance-level labels, we use human-provided triplet judgments to learn human-compatible decision-focused representations. Using both synthetic data and human subject experiments in multiple classification tasks, we demonstrate that such representation is better aligned with human perception than representation solely optimized for classification. Human-compatible representations identify nearest neighbors that are perceived as more similar by humans and allow humans to make more accurate predictions, leading to substantial improvements in human decision accuracies (17.8% in butterfly vs. moth classification and 13.2% in pneumonia classification).",,5,0.0,,,U of C,IIS-2040989,University of Chicago
2-s2.0-85186368404,,,,LEARNING INTERACTIVE REAL-WORLD SIMULATORS,cp,Conference Paper,Yang S.,60025038;60022195;60030835;60111161,"University of California, Berkeley;Massachusetts Institute of Technology;University of Alberta;DeepMind Technologies Limited",Berkeley;Cambridge;Edmonton;London,United States;United States;Canada;United Kingdom,7.0,"Yang, Sherry;Du, Yilun;Ghasemipour, Kamyar;Tompson, Jonathan;Kaelbling, Leslie;Schuurmans, Dale;Abbeel, Pieter",58151303400;57194623792;58662520600;56374097400;6701501293;57204335408;8269962600,60025038-60111161;60022195;60111161;60111161;60022195;60111161-60030835;60025038,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different dimensions (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, we can simulate the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move by ∆<inf>x</inf>, ∆<inf>y</inf>” from otherwise static scenes and objects. We use the simulator to train both high-level vision-language policies and low-level reinforcement learning policies, each of which can be deployed in the real world in zero shot after training purely in simulation. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience, opening up even wider applications. Video demos can be found at https://universal-simulator.github.io.",,47,0.0,,,,,
2-s2.0-85173149125,,,,LEARNING KERNELIZED CONTEXTUAL BANDITS IN A DISTRIBUTED AND ASYNCHRONOUS ENVIRONMENT,cp,Conference Paper,Li C.,60003269;60021918;60013402,Princeton University;University of Virginia;Oregon State University,Princeton;Charlottesville;Corvallis,United States;United States;United States,4.0,"Li, Chuanhao;Wang, Huazheng;Wang, Mengdi;Wang, Hongning",57221147753;57190498323;57215322066;48762142200,60021918;60013402;60003269;60021918,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Despite the recent advances in communication-efficient distributed bandit learning, most existing solutions are restricted to parametric models, e.g., linear bandits and generalized linear bandits (GLB). In comparison, kernel bandits, which search for non-parametric functions in a reproducing kernel Hilbert space (RKHS), offer higher modeling capacity. But the only existing work in distributed kernel bandits adopts a synchronous communication protocol, which greatly limits its practical use (e.g., every synchronization step requires all clients to participate and wait for data exchange). In this paper, in order to improve the robustness against delays and unavailability of clients that are common in practice, we propose the first asynchronous solution based on approximated kernel regression for distributed kernel bandit learning. A set of effective treatments are developed to ensure approximation quality and communication efficiency. Rigorous theoretical analysis about the regret and communication cost is provided; and extensive empirical evaluations demonstrate the effectiveness of our solution.",,8,0.0,,,,1006977,
2-s2.0-85143440167,,,,LEARNING LONG-TERM REWARD REDISTRIBUTION VIA RANDOMIZED RETURN DECOMPOSITION,cp,Conference Paper,Ren Z.,60025278;60025084;60000745;129320752;129321296,Tsinghua University;Shanghai Jiao Tong University;University of Illinois Urbana-Champaign;HeliXon Limited;BIMSA,Beijing;Shanghai;Urbana;;,China;China;United States;;,4.0,"Ren, Zhizhou;Guo, Ruihan;Zhou, Yuan;Peng, Jian",57218718504;57365612400;56972686000;27267770800,60000745;60025084;129321296;60000745-60025278-129320752,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Many practical applications of reinforcement learning require agents to learn from sparse and delayed rewards. It challenges the ability of agents to attribute their actions to future outcomes. In this paper, we consider the problem formulation of episodic reinforcement learning with trajectory feedback. It refers to an extreme delay of reward signals, in which the agent can only obtain one reward signal at the end of each trajectory. A popular paradigm for this problem setting is learning with a designed auxiliary dense reward function, namely proxy reward, instead of sparse environmental signals. Based on this framework, this paper proposes a novel reward redistribution algorithm, randomized return decomposition (RRD), to learn a proxy reward function for episodic reinforcement learning. We establish a surrogate problem by Monte-Carlo sampling that scales up least-squares-based reward redistribution to long-horizon problems. We analyze our surrogate loss function by connection with existing methods in the literature, which illustrates the algorithmic properties of our approach. In experiments, we extensively evaluate our proposed method on a variety of benchmark tasks with episodic rewards and demonstrate substantial improvement over baseline algorithms.",,17,0.0,,,NSF,CCF-2006526,National Science Foundation
2-s2.0-85146510933,,,,LEARNING STRIDES IN CONVOLUTIONAL NEURAL NETWORKS,cp,Conference Paper,Riad R.,60108665;60006191,Université PSL;Google LLC,Paris;Mountain View,France;United States,4.0,"Riad, Rachid;Teboul, Olivier;Grangier, David;Zeghidour, Neil",57222515052;57218718460;14034229500;57189595748,60108665;60006191;60006191;60006191,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires cross-validation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet.",,38,0.0,,,,,
2-s2.0-85174389080,,,,LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework,cp,Conference Paper,Kim W.,60032144,Korea Advanced Institute of Science and Technology,Daejeon,South Korea,3.0,"Kim, Woojun;Kim, Jeonghye;Sung, Youngchul",57219495971;58653610400;8055148900,60032144;60032144;60032144,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,16619-16638,"In this paper, a unified framework for exploration in reinforcement learning (RL) is proposed based on an option-critic model. The proposed framework learns to integrate a set of diverse exploration strategies so that the agent can adaptively select the most effective exploration strategy over time to realize a relevant exploration-exploitation trade-off for each given task. The effectiveness of the proposed exploration framework is demonstrated by various experiments in the MiniGrid and Atari environments.",,3,0.0,,,DAPA,UD230017TD,Defense Acquisition Program Administration
2-s2.0-85188177557,,,,LET MODELS SPEAK CIPHERS: MULTIAGENT DEBATE THROUGH EMBEDDINGS,cp,Conference Paper,Pham C.,60007363;60019674;60159665,Northwestern University;Boston University;ByteDance Ltd.,Evanston;Boston;Beijing,United States;United States;China,9.0,"Pham, Chau;Liu, Boyi;Yang, Yingxiang;Chen, Zhengyu;Liu, Tianyi;Yuan, Jianbo;Plummer, Bryan A.;Wang, Zhaoran;Yang, Hongxia",58713820400;57112793700;58743039000;59844238700;58736922100;55918243700;57189663903;56393977400;57054215300,60019674;60007363;60159665;60159665;60159665;60159665;60019674;60007363;60159665,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights, outperforming the state-of-the-art LLM debate methods using natural language by 0.5 − 5.0% across five reasoning tasks and multiple open-source LLMs of varying sizes. This showcases the superiority and robustness of embeddings as an alternative “language” for communication among LLMs. We anticipate that CIPHER will inspire further exploration for the design of interactions within LLM agent systems, offering a new direction that could significantly influence future developments in the field.",,6,0.0,,,,,
2-s2.0-85200604975,,,,LET'S VERIFY STEP BY STEP,cp,Conference Paper,Lightman H.,60121131,"OpenAI, Inc.",San Francisco,United States,10.0,"Lightman, Hunter;Kosaraju, Vineet;Burda, Yura;Edwards, Harri;Baker, Bowen;Lee, Teddy;Leike, Jan;Schulman, John;Sutskever, Ilya;Cobbe, Karl",58416361400;57126407300;59874991700;57219748054;57210411174;58158854900;55925224300;55921038800;24831264500;57213835747,60121131;60121131;60121131;60121131;60121131;60121131;60121131;60121131;60121131;60121131,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",,145,0.0,,,,,
2-s2.0-85196754713,,,,LIGHTHGNN: DISTILLING HYPERGRAPH NEURAL NETWORKS INTO MLPS FOR 100× FASTER INFERENCE,cp,Conference Paper,Feng Y.,60025278;60023813,Tsinghua University;Shanghai University,Beijing;Shanghai,China;China,4.0,"Feng, Yifan;Luo, Yihe;Ying, Shihui;Gao, Yue",57207868951;58895692200;24451523300;57198714223,60025278;60025278;60023813;60025278,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment. In practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference. In this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed. Specifically, we introduce LightHGNN and LightHGNN<sup>+</sup> for fast inference with low complexity. LightHGNN directly distills the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN<sup>+</sup> further explicitly injects reliable high-order correlations into the student MLPs to achieve topology-aware distillation and resistance to over-smoothing. Experiments on eight hypergraph datasets demonstrate that even without hypergraph dependency, the proposed LightHGNNs can still achieve competitive or even better performance than HGNNs and outperform vanilla MLPs by 16.3 on average. Extensive experiments on three graph datasets further show the average best performance of our LightHGNNs compared with all other methods. Experiments on synthetic hypergraphs with 5.5w vertices indicate LightHGNNs can run 100× faster than HGNNs, showcasing their ability for latency-sensitive deployments.",,8,0.0,,,NSFC,62088102,National Natural Science Foundation of China
2-s2.0-105000540338,,,,LLM Circuit Analyses Are Consistent Across Training and Scale,cp,Conference Paper,Tigges C.,60002483;60011460;132385916,Universiteit van Amsterdam;Brown University;EleutherAI,Amsterdam;Providence;,Netherlands;United States;,4.0,"Tigges, Curt;Hanna, Michael;Yu, Qinan;Biderman, Stella",58316457500;58282671300;58079778800;57222033152,132385916;60002483;60011460;132385916,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Most currently deployed LLMs undergo continuous training or additional finetuning. By contrast, most research into LLMs' internal mechanisms focuses on models at one snapshot in time (the end of pre-training), raising the question of whether their results generalize to real-world settings. Existing studies of mechanisms over time focus on encoder-only or toy models, which differ significantly from most deployed models. In this study, we track how model mechanisms, operationalized as circuits, emerge and evolve across 300 billion tokens of training in decoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters. We find that task abilities and the functional components that support them emerge consistently at similar token counts across scale. Moreover, although such components may be implemented by different attention heads over time, the overarching algorithm that they implement remains. Surprisingly, both these algorithms and the types of components involved therein tend to replicate across model scale. Finally, we find that circuit size correlates with model size and can fluctuate considerably over time even when the same algorithm is implemented. These results suggest that circuit analyses conducted on small models at the end of pre-training can provide insights that still apply after additional training and over model scale.",,5,0.0,,,,,
2-s2.0-85200545235,,,,LMSYS-CHAT-1M: A LARGE-SCALE REAL-WORLD LLM CONVERSATION DATASET,cp,Conference Paper,Zheng L.,60025038;60030612;60027950;60195969;123976447,"University of California, Berkeley;University of California, San Diego;Carnegie Mellon University;Mohamed Bin Zayed University of Artificial Intelligence;Stanford",Berkeley;La Jolla;Pittsburgh;Abu Dhabi;Stanford,United States;United States;United States;United Arab Emirates;United States,13.0,"Zheng, Lianmin;Chiang, Wei Lin;Sheng, Ying;Li, Tianle;Zhuang, Siyuan;Wu, Zhanghao;Zhuang, Yonghao;Li, Zhuohan;Lin, Zi;Xing, Eric P.;Gonzalez, Joseph E.;Stoica, Ion;Zhang, Hao",57203129159;57188807660;57218197286;58631327700;57219586350;57211645201;57441653500;57218308817;57215718354;57685890100;57200981709;7007009125;56414397000,60025038;60025038;60025038-123976447;60025038;60025038;60025038;60027950;60025038;60030612;60027950-60195969;60025038;60025038;60030612,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.",,53,0.0,,,,,
2-s2.0-85191147781,,,,Label Correction of Crowdsourced Noisy Annotations with an Instance-Dependent Noise Transition Model,cp,Conference Paper,Guo H.,60010884,Western University,London,Canada,3.0,"Guo, Hui;Wang, Boyu;Yi, Grace Y.",57217752242;34979599200;7101660701,60010884;60010884;60010884,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"The predictive ability of supervised learning algorithms hinges on the quality of annotated examples, whose labels often come from multiple crowdsourced annotators with diverse expertise. To aggregate noisy crowdsourced annotations, many existing methods employ an annotator-specific instance-independent noise transition matrix to characterize the labeling skills of each annotator. Learning an instance-dependent noise transition model, however, is challenging and remains relatively less explored. To address this problem, in this paper, we formulate the noise transition model in a Bayesian framework and subsequently design a new label correction algorithm. Specifically, we approximate the instance-dependent noise transition matrices using a Bayesian network with a hierarchical spike and slab prior. To theoretically characterize the distance between the noise transition model and the true instance-dependent noise transition matrix, we provide a posterior-concentration theorem that ensures the posterior consistency in terms of the Hellinger distance. We further formulate the label correction process as a hypothesis testing problem and propose a novel algorithm to infer the true label from the noisy annotations based on the pairwise likelihood ratio test. Moreover, we establish an information-theoretic bound on the Bayes error for the proposed method. We validate the effectiveness of our approach through experiments on benchmark and real-world datasets.",,11,0.0,,,NSERC,,Canada Research Chairs
2-s2.0-85213812000,,,,"Labels, Information, and Computation: Efficient Learning Using Sufficient Labels",ar,Article,Duan S.,60154244,Herbert Wertheim College of Engineering,Gainesville,United States,3.0,"Duan, Shiyu;Chang, Spencer;Príncipe, José C.",57212505639;58836299900;36045958600,60154244;60154244;60154244,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"In supervised learning, obtaining a large set of fully-labeled training data is expensive. We show that we do not always need full label information on every single training example to train a competent classifier. Specifically, inspired by the principle of sufficiency in statistics, we present a statistic (a summary) of the fully-labeled training set that captures almost all the relevant information for classification but at the same time is easier to obtain directly. We call this statistic “sufficiently-labeled data” and prove its sufficiency and efficiency for finding the optimal hidden representations, on which competent classifier heads can be trained using as few as a single randomly-chosen fully-labeled example per class. Sufficiently-labeled data can be obtained from annotators directly without collecting the fully-labeled data first. And we prove that it is easier to directly obtain sufficiently-labeled data than obtaining fully-labeled data. Furthermore, sufficiently-labeled data is naturally more secure since it stores relative, instead of absolute, information. Extensive experimental results are provided to support our theory.",Classification | Data Efficiency | Data Security | Deep Learning | Privacy-Preserving Learning,5,0.0,,,ARPA,FA9453-18-1-0039,Defense Advanced Research Projects Agency
2-s2.0-85137869169,10.24963/ijcai.2022/647,,,Landmark Heuristics for Lifted Classical Planning,cp,Conference Paper,Wichlacz J.,60033241,Universität des Saarlandes,Saarbrucken,Germany,3.0,"Wichlacz, Julia;Höller, Daniel;Hoffmann, Jörg",57221325205;56077435300;23392307600,60033241;60033241;60033241,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4665-4671,"While state-of-the-art planning systems need a grounded (propositional) task representation, the input model is provided “lifted”, specifying predicates and action schemas with variables over a finite object universe. The size of the grounded model is exponential in predicate/action-schema arity, limiting applicability to cases where it is small enough. Recent work has taken up this challenge, devising an effective lifted forward search planner as basis for lifted heuristic search, as well as a variety of lifted heuristic functions based on the delete relaxation. Here we add a novel family of lifted heuristic functions, based on landmarks. We design two methods for landmark extraction in the lifted setting. The resulting heuristics exhibit performance advantages over previous heuristics in several benchmark domains. Especially the combination with lifted delete relaxation heuristics to a LAMA-style planner yields good results, beating the previous state of the art in lifted planning.",,9,1.0,all publisherfree2read,All Open Access Bronze,DFG,232722074 – SFB 1102,Deutsche Forschungsgemeinschaft
2-s2.0-85204397976,10.1613/jair.1.15865,,,Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges,ar,Article,La Malfa E.,60026851;60012070;60018163;60175816;60111768,University of Oxford;University of Leeds;TU Wien;Oxford Social Sciences Division;The Alan Turing Institute,Oxford;Leeds;Vienna;Oxford;London,United Kingdom;United Kingdom;Austria;United Kingdom;United Kingdom,9.0,"La Malfa, Emanuele;Petrov, Aleksandar;Frieder, Simon;Weinhuber, Christoph;Burnell, Ryan;Nazar, Raza;Cohn, Anthony G.;Shadbolt, Nigel;Wooldridge, Michael",57222070218;57943448600;57274381800;57222086631;57213626719;57224769895;57225810681;56867428600;56800190200,60026851-60111768;60026851;60026851-60018163;60026851;60111768;60175816;60111768-60012070;60026851-60111768;60026851-60111768,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,1497-1523,"Some of the most powerful language models currently are proprietary systems, accessible only via (typically restrictive) web or software programming interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. In contrast with scenarios where full model access is available, as in the case of open-source models, such closed-off language models present specific challenges for evaluating, benchmarking, and testing them. This paper has two goals: on the one hand, we delineate how the aforementioned challenges act as impediments to the accessibility, reproducibility, reliability, and trustworthiness of LMaaS. We systematically examine the issues that arise from a lack of information about language models for each of these four aspects. We conduct a detailed analysis of existing solutions, put forth a number of recommendations, and highlight directions for future advancements. On the other hand, it serves as a synthesized overview of the licences and capabilities of the most popular LMaaS.",,7,1.0,all publisherfullgold,All Open Access Gold,ATI,EP/S024050/1,Alan Turing Institute
2-s2.0-85198098829,,,,Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation,cp,Conference Paper,Wu X.,60014966;60008928;60069717,Peking University;The Hong Kong Polytechnic University;Hangzhou Normal University,Beijing;Hong Kong;Hangzhou,China;Hong Kong;China,5.0,"Wu, Xingyu;Zhong, Yan;Wu, Jibin;Jiang, Bingbing;Tan, Kay Chen",57218925248;57219876921;57204645027;57195605541;25655559700,60008928;60014966;60008928;60069717;60008928,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,5235-5244,"Algorithm selection, a critical process of automated machine learning, aims to identify the most suitable algorithm for solving a specific problem pri- or to execution. Mainstream algorithm selection techniques heavily rely on problem features, while the role of algorithm features remains largely unexplored. Due to the intrinsic complexity of algorithms, effective methods for universally extracting algorithm information are lacking. This paper takes a significant step towards bridging this gap by introducing Large Language Models (LLMs) into algorithm selection for the first time. By comprehending the code text, LLM not only captures the structural and semantic aspects of the algorithm, but also demonstrates contextual awareness and library function understanding. The high-dimensional algorithm representation extracted by LLM, after undergoing a feature selection module, is combined with the problem representation and passed to the similarity calculation module. The selected algorithm is determined by the matching degree between a given problem and different algorithms. Extensive experiments validate the performance superiority of the proposed model and the efficacy of each key module. Furthermore, we present a theoretical upper bound on model complexity, showcasing the influence of algorithm representation and feature selection modules. This provides valuable theoretical guidance for the practical implementation of our method.",,12,0.0,,,研究資助局,PolyU15215623,"Research Grants Council, University Grants Committee"
2-s2.0-85181890123,,,,Large data limit of the MBO scheme for data clustering: convergence of the dynamics,ar,Article,Laux T.,60007493,Universität Bonn,Bonn,Germany,2.0,"Laux, Tim;Lelmi, Jona",57191484971;57219755780,60007493;60007493,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,344,,"We prove that the dynamics of the MBO scheme for data clustering converge to a viscosity solution to mean curvature flow. The main ingredients are (i) a new abstract convergence result based on quantitative estimates for heat operators and (ii) the derivation of these estimates in the setting of random geometric graphs. To implement the scheme in practice, two important parameters are the number of eigenvalues for computing the heat operator and the step size of the scheme. The results of the current paper give a theoretical justification for the choice of these parameters in relation to sample size and interaction width.",clustering | continuum limits | Graph MBO | semi-supervised learning | viscosity solutions,5,0.0,,,DFG,EXC-2047/1 – 390685813,Deutsche Forschungsgemeinschaft
2-s2.0-85168255922,10.1609/aaai.v37i7.26023,,,Layer-Wise Adaptive Model Aggregation for Scalable Federated Learning,cp,Conference Paper,Lee S.,60029311;60028876,University of Southern California;Inha University,Los Angeles;Incheon,United States;South Korea,3.0,"Lee, Sunwoo;Zhang, Tuo;Avestimehr, Salman",57190129718;57225936209;16202171800,60029311-60028876;60029311;60029311,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,8491-8499,"In Federated Learning (FL), a common approach for aggregating local solutions across clients is periodic full model averaging. It is, however, known that different layers of neural networks can have a different degree of model discrepancy across the clients. The conventional full aggregation scheme does not consider such a difference and synchronizes the whole model parameters at once, resulting in inefficient network bandwidth consumption. Aggregating the parameters that are similar across the clients does not make meaningful training progress while increasing the communication cost. We propose FedLAMA, a layer-wise adaptive model aggregation scheme for scalable FL. FedLAMA adjusts the aggregation interval in a layer-wise manner, jointly considering the model discrepancy and the communication cost. This fine-grained aggregation strategy enables to reduce the communication cost without significantly harming the model accuracy. Our extensive empirical study shows that, as the aggregation interval increases, FedLAMA shows a remarkably smaller accuracy drop than the periodic full aggregation, while achieving comparable communication efficiency.",,59,1.0,all publisherfullgold,All Open Access Gold,DARPA,HR001120C0088,Defense Advanced Research Projects Agency
2-s2.0-85174384208,,,,Lazy Agents: A New Perspective on Solving Sparse Reward Problem in Multi-agent Reinforcement Learning,cp,Conference Paper,Liu B.,60027363;60018486;60072902;130354080,University of Chinese Academy of Sciences;Institute of Automation Chinese Academy of Sciences;Macau University of Science and Technology;Nanjing AIRIA,Beijing;Beijing;Taipa;Nanjing,China;China;Macao;China,6.0,"Liu, Boyin;Pu, Zhiqiang;Pan, Yi;Yi, Jianqiang;Liang, Yanyan;Zhang, Du",57362691500;54389776500;58359987900;14016793800;17435256900;57193483289,60027363-60018486;60027363-60018486-130354080;60018486;60027363-60018486;60072902;60072902,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,21404-21425,"Sparse reward remains a valuable and challenging problem in multi-agent reinforcement learning (MARL). This paper addresses this issue from a new perspective, i.e., lazy agents. We empirically illustrate how lazy agents damage learning from both exploration and exploitation. Then, we propose a novel MARL framework called Lazy Agents Avoidance through Influencing External States (LAIES). Firstly, we examine the causes and types of lazy agents in MARL using a causal graph of the interaction between agents and their environment. Then, we mathematically define the concept of fully lazy agents and teams by calculating the causal effect of their actions on external states using the do-calculus process. Based on definitions, we provide two intrinsic rewards to motivate agents, i.e., individual diligence intrinsic motivation (IDI) and collaborative diligence intrinsic motivation (CDI). IDI and CDI employ counterfactual reasoning based on the external states transition model (ESTM) we developed. Empirical results demonstrate that our proposed method achieves state-of-the-art performance on various tasks, including the sparse-reward version of StarCraft multi-agent challenge (SMAC) and Google Research Football (GRF). Our code is open-source and available at https://github.com/liuboyin/LAIES.",,2,0.0,,,CAS,173211KYSB20200002,Chinese Academy of Sciences
2-s2.0-85202061739,,,,Leaky Hockey Stick Loss: The First Negatively Divergent Margin-based Loss Function for Classification,ar,Article,Kwon O.R.,60029445,University of Minnesota Twin Cities,Minneapolis,United States,2.0,"Kwon, Oh Ran;Zou, Hui",58071295200;7202373047,60029445;60029445,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,239,,"Many modern classification algorithms are formulated through the regularized empirical risk minimization (ERM) framework, where the risk is defined based on a loss function. We point out that although the loss function in decision theory is non-negative by definition, the non-negativity of the loss function in ERM is not necessary to be classification-calibrated and to produce a Bayes consistent classifier. We introduce the leaky hockey stick loss (LHS loss), the first negatively divergent margin-based loss function. We prove that the LHS loss is classification-calibrated. When the hinge loss is replaced with the LHS loss in the ERM approach for deriving the kernel support vector machine (SVM), the corresponding optimization problem has a well-defined solution named the kernel leaky hockey stick classifier (LHS classifier). Under mild regularity conditions, we prove that the kernel LHS classifier is Bayes risk consistent. In our theoretical analysis, we overcome multiple challenges caused by the negative divergence of the LHS loss that does not exist in the analysis of the usual kernel machines. For a numerical demonstration, we provide a computationally efficient algorithm to solve the kernel LHS classifier and compare it to the kernel SVM on simulated data and fifteen benchmark data sets. To conclude this work, we further present a class of negatively divergent margin-based loss functions that have similar theoretical properties to those of the LHS loss. Interestingly, the LHS loss can be viewed as a limiting case of this family of negatively divergent margin-based loss functions.",bayes risk consistency | classification-calibrated | loss function | majorization minimization principle | margin maximizing,1,0.0,,,NSF,1915842,National Science Foundation
2-s2.0-85147735102,10.1609/aaai.v36i6.20562,,,Leaping through Time with Gradient-Based Adaptation for Recommendation,cp,Conference Paper,Chairatanakul N.,60283164;60024621,Institute of Science Tokyo;National Institute of Advanced Industrial Science and Technology,Tokyo;Tsukuba,Japan;Japan,4.0,"Chairatanakul, Nuttapong;Hoang, N. T.;Liu, Xin;Murata, Tsuyoshi",57213686428;57219498569;56948501900;7402737180,60283164-60024621;60283164;60024621;60283164-60024621,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,6141-6149,"Modern recommender systems are required to adapt to the change in user preferences and item popularity. Such a problem is known as the temporal dynamics problem, and it is one of the main challenges in recommender system modeling. Different from the popular recurrent modeling approach, we propose a new solution named LeapRec to the temporal dynamic problem by using trajectory-based meta-learning to model time dependencies. LeapRec characterizes temporal dynamics by two complement components named global time leap (GTL) and ordered time leap (OTL). By design, GTL learns long-term patterns by finding the shortest learning path across unordered temporal data. Cooperatively, OTL learns short-term patterns by considering the sequential nature of the temporal data. Our experimental results show that LeapRec consistently outperforms the state-of-the-art methods on several datasets and recommendation metrics. Furthermore, we provide an empirical study of the interaction between GTL and OTL, showing the effects of long- and short-term modeling.",,0,1.0,all publisherfullgold,All Open Access Gold,KAKEN,17H01785,Japan Society for the Promotion of Science
2-s2.0-85163167314,,,,Learnable Polyphase Sampling for Shift Invariant and Equivariant Convolutional Networks,cp,Conference Paper,Rojas-Gomez R.A.,60009254;60158506,Purdue University;The Grainger College of Engineering,West Lafayette;Urbana,United States;United States,5.0,"Rojas-Gomez, Renan A.;Lim, Teck Yian;Schwing, Alexander G.;Do, Minh N.;Yeh, Raymond A.",57221147528;57204050603;23478625400;34869347800;57189598022,60158506;60158506;60158506;60158506;60009254,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"We propose learnable polyphase sampling (LPS), a pair of learnable down/upsampling layers that enable truly shift-invariant and equivariant convolutional networks. LPS can be trained end-to-end from data and generalizes existing handcrafted downsampling layers. It is widely applicable as it can be integrated into any convolutional network by replacing down/upsampling layers. We evaluate LPS on image classification and semantic segmentation. Experiments show that LPS is on-par with or outperforms existing methods in both performance and shift consistency. For the first time, we achieve true shift-equivariance on semantic segmentation (PASCAL VOC), i.e., 100% shift consistency, outperforming baselines by an absolute 3.3%. Our project page and code are available at https://raymondyeh07.github.io/learnable_polyphase_sampling/.",,10,0.0,,,NSF,1718221,National Science Foundation
2-s2.0-85204292247,,,,Learning A Spiking Neural Network for Efficient Image Deraining,cp,Conference Paper,Song T.,60019616;60010080;60069704,Harbin Institute of Technology;Nanjing University of Science and Technology;Dalian Polytechnic University,Harbin;Nanjing;Dalian,China;China;China,6.0,"Song, Tianyu;Jin, Guiyue;Li, Pengpeng;Jiang, Kui;Chen, Xiang;Jin, Jiyu",58385944500;55091283400;57552178600;57203871718;57247002600;55013208900,60069704;60069704;60010080;60019616;60010080;60069704,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1254-1262,"Recently, spiking neural networks (SNNs) have demonstrated substantial potential in computer vision tasks. In this paper, we present an Efficient Spiking Deraining Network, called ESDNet. Our work is motivated by the observation that rain pixel values will lead to a more pronounced intensity of spike signals in SNNs. However, directly applying deep SNNs to image deraining task still remains a significant challenge. This is attributed to the information loss and training difficulties that arise from discrete binary activation and complex spatio-temporal dynamics. To this end, we develop a spiking residual block to convert the input into spike signals, then adaptively optimize the membrane potential by introducing attention weights to adjust spike responses in a data-driven manner, alleviating information loss caused by discrete binary activation. By this way, our ESDNet can effectively detect and analyze the characteristics of rain streaks by learning their fluctuations. This also enables better guidance for the deraining process and facilitates high-quality image reconstruction. Instead of relying on the ANN-SNN conversion strategy, we introduce a gradient proxy strategy to directly train the model for overcoming the challenge of training. Experimental results show that our approach gains comparable performance against ANN-based methods while reducing energy consumption by 54%. The code source is available at https://github.com/MingTian99/ESDNet.",,5,0.0,,,,,
2-s2.0-85168237134,10.1609/aaai.v37i7.26027,,,Learning Adversarially Robust Sparse Networks via Weight Reparameterization,cp,Conference Paper,Li C.,60027363;60030904,University of Chinese Academy of Sciences;Institute of Computing Technology Chinese Academy of Sciences,Beijing;Beijing,China;China,5.0,"Li, Chenhao;Qiu, Qiang;Zhang, Zhibin;Guo, Jiafeng;Cheng, Xueqi",58604401600;56022959700;55721674200;24174196100;55855927900,60030904-60027363;60030904;60030904;60030904;60030904,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,8527-8535,"Although increasing model size can enhance the adversarial robustness of deep neural networks, in resource-constrained environments, there exist critical sparsity constraints. While the recent robust pruning technologies show promising direction to obtain adversarially robust sparse networks, they perform poorly with high sparsity. In this work, we bridge this performance gap by reparameterizing network parameters to simultaneously learn the sparse structure and the robustness. Specifically, we introduce Twin-Rep, which reparameterizes original weights into the product of two factors during training and performs pruning on the reparameterized weights to satisfy the target sparsity constraint. Twin-Rep implicitly adds the sparsity constraint without changing the robust training objective, thus can enhance robustness under high sparsity. We also introduce another variant of weight reparameterization for better channel pruning. When inferring, we restore the original weight structure to obtain compact and robust networks. Extensive experiments on diverse datasets demonstrate that our method achieves state-of-the-art results, outperforming the current sparse robust training method and robustness-aware pruning method. Our code is available at https://github.com/UCAS-LCH/Twin-Rep.",,7,1.0,all publisherfullgold,All Open Access Gold,CAS,,Chinese Academy of Sciences
2-s2.0-85147603004,10.1609/aaai.v36i2.20074,,,Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection,cp,Conference Paper,Liu X.,60029306;60279548,Wuhan University;NC State College of Engineering,Wuhan;Raleigh,China;United States,3.0,"Liu, Xianpeng;Xue, Nan;Wu, Tianfu",57216368107;56421772400;55476641200,60279548;60029306;60279548,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,1810-1818,"Monocular 3D object detection aims to localize 3D bounding boxes in an input single 2D image. It is a highly challenging problem and remains open, especially when no extra information (e.g., depth, lidar and/or multi-frames) can be leveraged in training and/or inference. This paper proposes a simple yet effective formulation for monocular 3D object detection without exploiting any extra information. It presents the MonoCon method which learns Monocular Contexts, as auxiliary tasks in training, to help monocular 3D object detection. The key idea is that with the annotated 3D bounding boxes of objects in an image, there is a rich set of well-posed projected 2D supervision signals available in training, such as the projected corner keypoints and their associated offset vectors with respect to the center of 2D bounding box, which should be exploited as auxiliary tasks in training. The proposed MonoCon is motivated by the Cramèr-Wold theorem in measure theory at a high level. In implementation, it utilizes a very simple end-to-end design to justify the effectiveness of learning auxiliary monocular contexts, which consists of three components: a Deep Neural Network (DNN) based feature backbone, a number of regression head branches for learning the essential parameters used in the 3D bounding box prediction, and a number of regression head branches for learning auxiliary contexts. After training, the auxiliary context regression branches are discarded for better inference efficiency. In experiments, the proposed MonoCon is tested in the KITTI benchmark (car, pedestrian and cyclist). It outperforms all prior arts in the leaderboard on the car category and obtains comparable performance on pedestrian and cyclist in terms of accuracy. Thanks to the simple design, the proposed MonoCon method obtains the fastest inference speed with 38.7 fps in comparisons. Our code is released at https://git.io/MonoCon.",,136,1.0,all publisherfullgold,All Open Access Gold,NSFC,90IFDV0017-01-00,National Natural Science Foundation of China
2-s2.0-85204288402,,,,Learning Fair Representations for Recommendation via Information Bottleneck Principle,cp,Conference Paper,Xie J.,60002836;124340022,Hefei University of Technology;Hefei Comprehensive National Science Center,Hefei;Hefei,China;China,4.0,"Xie, Junsong;Yang, Yonghui;Wang, Zihan;Wu, Le",59333091900;57211757215;59194002200;55522678400,60002836;60002836;60002836;60002836-124340022,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2469-2477,"User-oriented recommender systems (RS) characterize users' preferences based on observed behaviors and are widely deployed in personalized services.However, RS may unintentionally capture biases related to sensitive attributes (e.g., gender) from behavioral data, leading to unfair issues and discrimination against particular groups (e.g., females).Adversarial training is a popular technique for fairness-aware RS, when filtering sensitive information in user modeling.Despite advancements in fairness, achieving a good accuracy-fairness trade-off remains a challenge in adversarial training.In this paper, we investigate fair representation learning from a novel information theory perspective.Specifically, we propose a model-agnostic Fair recommendation method via the Information Bottleneck principle (FairIB).The learning objective of FairIB is to maximize the mutual information between user representations and observed interactions, while simultaneously minimizing it between user representations and sensitive attributes.This approach facilitates the capturing of essential collaborative signals in user representations while mitigating the inclusion of unnecessary sensitive information.Empirical studies on two real-world datasets demonstrate the effectiveness of the proposed FairIB, which significantly improves fairness while maintaining competitive recommendation accuracy, either in single or multiple sensitive scenarios.The code is available at https://github.com/jsxie9/IJCAI FairIB.",,7,0.0,,,NKRDPC,2021ZD0111802,National Key Research and Development Program of China
2-s2.0-105000486976,,,,Learning Formal Mathematics From Intrinsic Motivation,cp,Conference Paper,Poesia G.,60012708;60002014;60141508,Stanford University;The Royal Institute of Technology (KTH);Stanford Engineering,Stanford;Stockholm;Stanford,United States;Sweden;United States,4.0,"Poesia, Gabriel;Broman, David;Haber, Nick;Goodman, Noah D.",56357998000;18036903200;56353182800;12767629400,60141508;60002014;60141508-60012708;60141508-60012708,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"How did humanity coax mathematics from the æther? We explore the Platonic view that mathematics can be discovered from its axioms-a game of conjecture and proof. We describe MINIMO (Mathematics from Intrinsic Motivation): an agent that jointly learns to pose challenging problems for itself (conjecturing) and solve them (theorem proving). Given a mathematical domain axiomatized in dependent type theory, we first combine methods for constrained decoding and type-directed synthesis to sample valid conjectures from a language model. Our method guarantees well-formed conjectures by construction, even as we start with a randomly initialized model. We use the same model to represent a policy and value function for guiding proof search. Our agent targets generating hard but provable conjectures-a moving target, since its own theorem proving ability also improves as it trains. We propose novel methods for hindsight relabeling on proof search trees to significantly improve the agent's sample efficiency in both tasks. Experiments on 3 axiomatic domains (propositional logic, arithmetic and group theory) demonstrate that our agent can bootstrap from only the axioms, self-improving in generating true and challenging conjectures and in finding proofs.",,1,0.0,,,NSF,1918771,Knut och Alice Wallenbergs Stiftelse
2-s2.0-85163066051,,,,Learning Iterative Reasoning through Energy Minimization,cp,Conference Paper,Du Y.,60006191;60006320,Google LLC;MIT Computer Science & Artificial Intelligence Laboratory,Mountain View;Cambridge,United States;United States,4.0,"Du, Yilun;Li, Shuang;Tenenbaum, Joshua;Mordatch, Igor",57194623792;57221639782;7006818404;35776942500,60006320;60006320;60006320;60006191,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,5570-5582,"Deep learning has excelled on complex pattern recognition tasks such as image classification and object recognition. However, it struggles with tasks requiring nontrivial reasoning, such as algorithmic computation. Humans are able to solve such tasks through iterative reasoning - spending more time thinking about harder tasks. Most existing neural networks, however, exhibit a fixed computational budget controlled by the neural network architecture, preventing additional computational processing on harder tasks. In this work, we present a new framework for iterative reasoning with neural networks. We train a neural network to parameterize an energy landscape over all outputs, and implement each step of the iterative reasoning as an energy minimization step to find a minimal energy solution. By formulating reasoning as an energy minimization problem, for harder problems that lead to more complex energy landscapes, we may then adjust our underlying computational budget by running a more complex optimization procedure. We empirically illustrate that our iterative reasoning approach can solve more accurate and generalizable algorithmic reasoning tasks in both graph and continuous domains. Finally, we illustrate that our approach can recursively solve algorithmic problems requiring nested reasoning. Code and additional information is available at https://energy-based-model.github.io/iterativereasoning-as-energy-minimization/.",,17,0.0,,,NSF,,National Science Foundation
2-s2.0-85163137388,,,,Learning Mixtures of Linear Dynamical Systems,cp,Conference Paper,Chen Y.,60141284,School of Engineering and Applied Science,Princeton,United States,2.0,"Chen, Yanxi;Poor, H. Vincent",57195552368;55665272100,60141284;60141284,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,3507-3557,"We study the problem of learning a mixture of multiple linear dynamical systems (LDSs) from unlabeled short sample trajectories, each generated by one of the LDS models. Despite the wide applicability of mixture models for time-series data, learning algorithms that come with end-to-end performance guarantees are largely absent from existing literature. There are multiple sources of technical challenges, including but not limited to (1) the presence of latent variables (i.e. the unknown labels of trajectories); (2) the possibility that the sample trajectories might have lengths much smaller than the dimension d of the LDS models; and (3) the complicated temporal dependence inherent to time-series data. To tackle these challenges, we develop a two-stage meta-algorithm, which is guaranteed to efficiently recover each ground-truth LDS model up to error O<sup>e</sup>(pd/T), where T is the total sample size. We validate our theoretical studies with numerical experiments, confirming the efficacy of the proposed algorithm.",,15,0.0,,,NSF,CCF-1907661,National Science Foundation
2-s2.0-85163162369,,,,Learning Neural Set Functions Under the Optimal Subset Oracle,cp,Conference Paper,Ou Z.,60015150;60021182;60114181,Imperial College London;Sun Yat-Sen University;Tencent,London;Guangzhou;Shenzhen,United Kingdom;China;China,6.0,"Ou, Zijing;Xu, Tingyang;Su, Qinliang;Li, Yingzhen;Zhao, Peilin;Bian, Yatao",59639065900;56515877000;56337181800;57189093229;36445134800;57219636616,60114181-60015150;60114181;60021182;60015150;60114181;60114181,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Learning neural set functions becomes increasingly important in many applications like product recommendation and compound selection in AI-aided drug discovery. The majority of existing works study methodologies of set function learning under the function value oracle, which, however, requires expensive supervision signals. This renders it impractical for applications with only weak supervisions under the Optimal Subset (OS) oracle, the study of which is surprisingly overlooked. In this work, we present a principled yet practical maximum likelihood learning framework, termed as EquiVSet, that simultaneously meets the following desiderata of learning neural set functions under the OS oracle: i) permutation invariance of the set mass function being modeled; ii) permission of varying ground set; iii) minimum prior; and iv) scalability. The main components of our framework involve: an energy-based treatment of the set mass function, DeepSet-style architectures to handle permutation invariance, mean-field variational inference, and its amortized variants. Thanks to the elegant combination of these advanced architectures, empirical studies on three real-world applications (including Amazon product recommendation, set anomaly detection and compound selection for virtual screening) demonstrate that EquiVSet outperforms the baselines by a large margin.",,6,0.0,,,,,
2-s2.0-85174883180,,,,Learning Optimal Group-structured Individualized Treatment Rules with Many Treatments,ar,Article,Ma H.,60016639;60142023,UNC Gillings School of Global Public Health;College of Arts & Sciences,Chapel Hill;Chapel Hill,United States;United States,3.0,"Ma, Haixu;Zeng, Donglin;Liu, Yufeng",58370123700;8725807700;36604897500,60142023;60016639;60142023,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,102,,"Data driven individualized decision making problems have received a lot of attentions in recent years. In particular, decision makers aim to determine the optimal Individualized Treatment Rule (ITR) so that the expected specified outcome averaging over heterogeneous patient-specific characteristics is maximized. Many existing methods deal with binary or a moderate number of treatment arms and may not take potential treatment effect structure into account. However, the effectiveness of these methods may deteriorate when the number of treatment arms becomes large. In this article, we propose GRoup Outcome Weighted Learning (GROWL) to estimate the latent structure in the treatment space and the optimal group-structured ITRs through a single optimization. In particular, for estimating group-structured ITRs, we utilize the Reinforced Angle based Multicategory Support Vector Machines (RAMSVM) to learn group-based decision rules under the weighted angle based multi-class classification framework. Fisher consistency, the excess risk bound, and the convergence rate of the value function are established to provide a theoretical guarantee for GROWL. Extensive empirical results in simulation studies and real data analysis demonstrate that GROWL enjoys better performance than several other existing methods.",Angle-based multicategory classification | Group structure | Individualized treatment rules | Precision medicine | Support Vector Machine,12,0.0,,,USNIH,GM126550,National Institutes of Health
2-s2.0-85189611364,10.1609/aaai.v38i8.28706,,,Learning Persistent Community Structures in Dynamic Networks via Topological Data Analysis,cp,Conference Paper,Kong D.,60025278,Tsinghua University,Beijing,China,3.0,"Kong, Dexu;Zhang, Anping;Li, Yang",58825518000;57577111400;55979172900,60025278;60025278;60025278,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,8.0,,8617-8626,"Dynamic community detection methods often lack effective mechanisms to ensure temporal consistency, hindering the analysis of network evolution. In this paper, we propose a novel deep graph clustering framework with temporal consistency regularization on inter-community structures, inspired by the concept of minimal network topological changes within short intervals. Specifically, to address the representation collapse problem, we first introduce MFC, a matrix factorization-based deep graph clustering algorithm that preserves node embedding. Based on static clustering results, we construct probabilistic community networks and compute their persistence homology, a robust topological measure, to assess structural similarity between them. Moreover, a novel neural network regularization TopoReg is introduced to ensure the preservation of topological similarity between inter-community structures over time intervals. Our approach enhances temporal consistency and clustering accuracy on real-world datasets with both fixed and varying numbers of communities. It is also a pioneer application of TDA in temporally persistent community detection, offering an insightful contribution to field of network analysis. Code and data are available at the public git repository: https://github.com/kundtx/MFC-TopoReg.",,6,1.0,all publisherfullgold,All Open Access Gold,NSFC,62371270,National Natural Science Foundation of China
2-s2.0-85167983597,10.1609/aaai.v37i6.25852,,,Learning Pessimism for Reinforcement Learning,cp,Conference Paper,Cetin E.,60011520,King's College London,London,United Kingdom,2.0,"Cetin, Edoardo;Celiktutan, Oya",57222485529;16229290400,60011520;60011520,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,6971-6979,"Off-policy deep reinforcement learning algorithms commonly compensate for overestimation bias during temporal-difference learning by utilizing pessimistic estimates of the expected target returns. In this work, we propose Generalized Pessimism Learning (GPL), a strategy employing a novel learnable penalty to enact such pessimism. In particular, we propose to learn this penalty alongside the critic with dual TD-learning, a new procedure to estimate and minimize the magnitude of the target returns bias with trivial computational cost. GPL enables us to accurately counteract overestimation bias throughout training without incurring the downsides of overly pessimistic targets. By integrating GPL with popular off-policy algorithms, we achieve state-of-the-art results in both competitive proprioceptive and pixel-based benchmarks.",,17,1.0,all publisherfullgold,All Open Access Gold,EPSRC,EP/R513064/1,Engineering and Physical Sciences Research Council
2-s2.0-85137856620,10.24963/ijcai.2022/697,,,Learning Pollution Maps from Mobile Phone Images,cp,Conference Paper,Bhardwaj A.,60003261,Courant Institute of Mathematical Sciences,New York,United States,4.0,"Bhardwaj, Ankit;Iyer, Shiva;Jalan, Yash;Subramanian, Lakshminarayanan",59575510600;57204429396;57888525100;7005384316,60003261;60003261;60003261;60003261,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,5024-5030,"Air pollution monitoring and management is one of the key challenges for urban sectors, especially in developing countries. Measuring pollution levels requires significant investment in reliable and durable instrumentation and subsequent maintenance. On the other hand, there have been many attempts by researchers to develop image-based pollution measurement models, which have shown significant results and established the feasibility of the idea. But, taking image-level models to a city-level system presents new challenges, which include scarcity of high-quality annotated data and a high amount of label noise. In this paper, we present a low-cost, end-to-end system for learning pollution maps using images captured through a mobile phone. We demonstrate our system for parts of New Delhi and Ghaziabad. We use transfer learning to overcome the problem of data scarcity. We investigate the effects of label noise in detail and introduce the metric of in-interval accuracy to evaluate our models in presence of noise. We use distributed averaging to learn pollution maps and mitigate the effects of noise to some extent. We also develop haze-based interpretable models which have comparable performance to mainstream models. With only 382 images from Delhi and Ghaziabad and single-scene dataset from Beijing and Shanghai, we are able to achieve a mean absolute error of 44 µg/m<sup>3</sup>in PM<inf>2.5</inf> concentration on a test set of 267 images and an in-interval accuracy of 67% on predictions. Going further, we learn pollution maps with a mean absolute error as low as 35 µg/m<sup>3</sup>and in-interval accuracy as high as 74% significantly mitigating the image models' error. We also show that the noise in pollution labels emerging from unreliable sensing instrumentation forms a significant barrier to the realization of an ideal air pollution monitoring system. Our code-base can be found at https://github.com/ankitbha/pollution with images.",,1,1.0,all publisherfree2read,All Open Access Bronze,NSF,OAC-2004572,National Science Foundation
2-s2.0-85168719596,10.24963/ijcai.2023/421,,,Learning Preference Models with Sparse Interactions of Criteria,cp,Conference Paper,Herin M.,60001422,Sorbonne Université,Paris,France,3.0,"Herin, Margot;Perny, Patrice;Sokolovska, Nataliya",57953881000;6603291973;25652236200,60001422;60001422;60001422,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,3786-3794,"Multicriteria decision making requires defining the result of conflicting and possibly interacting criteria. Allowing criteria interactions in a decision model increases the complexity of the preference learning task due to the combinatorial nature of the possible interactions. In this paper, we propose an approach to learn a decision model in which the interaction pattern is revealed from preference data and kept as simple as possible. We consider weighted aggregation functions like multilinear utilities or Choquet integrals, admitting representations including non-linear terms measuring the joint benefit or penalty attached to some combinations of criteria. The weighting coefficients known as Möbius masses model positive or negative synergies among criteria. We propose an approach to learn the Möbius masses, based on iterative reweighted least square for sparse recovery, and dualization to improve scalability. This approach is applied to learn sparse representations of the multilinear utility model and conjunctive/disjunctive forms of the discrete Choquet integral from preferences examples, in aggregation problems possibly involving more than 20 criteria.",,7,1.0,all publisherfullgold,All Open Access Gold,ANR,,Agence Nationale de la Recherche
2-s2.0-85181394143,,,,Learning Provably Robust Estimators for Inverse Problems via Jittering,cp,Conference Paper,Krainovic A.,60019722;60029311,Technische Universität München;University of Southern California,Munich;Los Angeles,Germany;United States,3.0,"Krainovic, Anselm;Soltanolkotabi, Mahdi;Heckel, Reinhard",57208619603;35100874400;36082546300,60019722;60029311;60019722,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Deep neural networks provide excellent performance for inverse problems such as denoising. However, neural networks can be sensitive to adversarial or worst-case perturbations. This raises the question of whether such networks can be trained efficiently to be worst-case robust. In this paper, we investigate whether jittering, a simple regularization technique that adds isotropic Gaussian noise during training, is effective for learning worst-case robust estimators for inverse problems. While well studied for prediction in classification tasks, the effectiveness of jittering for inverse problems has not been systematically investigated. In this paper, we present a novel analytical characterization of the optimal ℓ<inf>2</inf>-worst-case robust estimator for linear denoising and show that jittering yields optimal robust denoisers. Furthermore, we examine jittering empirically via training deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI). The results show that jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. Moreover, our results imply that training on real data which often contains slight noise is somewhat robustness enhancing.",,7,0.0,,,DARPA,1846369,Defense Advanced Research Projects Agency
2-s2.0-85191161643,,,,Learning Rule-Induced Subgraph Representations for Inductive Relation Prediction,cp,Conference Paper,Liu T.,60019118;127774694,University of Science and Technology of China;Hefei Comprehensive National Science Center,Hefei;Hefei,China;China,5.0,"Liu, Tianyu;Lv, Qitan;Wang, Jie;Yang, Shuling;Chen, Hanzhu",57200413781;58645186600;56122228300;58311990100;58311990200,60019118;60019118;60019118-127774694;60019118;60019118,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Inductive relation prediction (IRP)-where entities can be different during training and inference-has shown great power for completing evolving knowledge graphs. Existing works mainly focus on using graph neural networks (GNNs) to learn the representation of the subgraph induced from the target link, which can be seen as an implicit rule-mining process to measure the plausibility of the target link. However, these methods cannot differentiate the target link and other links during message passing, hence the final subgraph representation will contain irrelevant rule information to the target link, which reduces the reasoning performance and severely hinders the applications for real-world scenarios. To tackle this problem, we propose a novel single-source edge-wise GNN model to learn the Rule-inducEd Subgraph represenTations (REST), which encodes relevant rules and eliminates irrelevant rules within the subgraph. Specifically, we propose a single-source initialization approach to initialize edge features only for the target link, which guarantees the relevance of mined rules and target link. Then we propose several RNN-based functions for edge-wise message passing to model the sequential property of mined rules. REST is a simple and effective approach with theoretical support to learn the rule-induced subgraph representation. Moreover, REST does not need node labeling, which significantly accelerates the subgraph preprocessing time by up to 11.66×. Experiments on inductive relation prediction benchmarks demonstrate the effectiveness of our REST.",,9,0.0,,,NSFC,U19B2026,National Natural Science Foundation of China
2-s2.0-85137862024,10.24963/ijcai.2022/657,,,Learning and Exploiting Progress States in Greedy Best-First Search,cp,Conference Paper,Ferber P.,60012614;60023588;60009358;60033241,Universität Zürich;Universität Basel;Linköpings Universitet;Universität des Saarlandes,Zurich;Basel;Linkoping;Saarbrucken,Switzerland;Switzerland;Sweden;Germany,4.0,"Ferber, Patrick;Cohen, Liat;Seipp, Jendrik;Keller, Thomas",57218925392;57002044000;55361565700;57197101016,60023588-60033241;60023588;60009358;60023588-60012614,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4740-4746,"Previous work introduced the concept of progress states. After expanding a progress state, a greedy best-first search (GBFS) will only expand states with lower heuristic values. Current methods can identify progress states only for a single task and only after a solution for the task has been found. We introduce a novel approach that learns a description logic formula characterizing all progress states in a classical planning domain. Using the learned formulas in a GBFS to break ties in favor of progress states often significantly reduces the search effort.",,2,1.0,all publisherfree2read repository repositoryam,All Open Access Bronze Green,CHE,952215,Council for Higher Education
2-s2.0-85120506771,10.1145/3488560.3498443,,,Learning discrete representations via constrained clustering for effective and efficient dense retrieval,cp,Conference Paper,Zhan J.,60014402;60104026;60030904,Renmin University of China;Beijing National Research Center for Information Science and Technology;Institute of Computing Technology Chinese Academy of Sciences,Beijing;Beijing;Beijing,China;China;China,6.0,"Zhan, Jingtao;Mao, Jiaxin;Liu, Yiqun;Guo, Jiafeng;Zhang, Min;Ma, Shaoping",57217163895;56125702100;35327597400;24174196100;57043667600;7403725163,60104026;60014402;60104026;60030904;60104026;60104026,2022-02-11,11 February 2022,Wsdm 2022 Proceedings of the 15th ACM International Conference on Web Search and Data Mining,,21101079204.0,,Conference Proceeding,,,,1328-1336,"Dense Retrieval (DR) has achieved state-of-the-art first-stage ranking effectiveness. However, the efficiency of most existing DR models is limited by the large memory cost of storing dense vectors and the time-consuming nearest neighbor search (NNS) in vector space. Therefore, we present RepCONC, a novel retrieval model that learns discrete Representations via CONstrained Clustering. RepCONC jointly trains dual-encoders and the Product Quantization (PQ) method to learn discrete document representations and enables fast approximate NNS with compact indexes. It models quantization as a constrained clustering process, which requires the document embeddings to be uniformly clustered around the quantization centroids and supports end-to-end optimization of the quantization method and dual-encoders. We theoretically demonstrate the importance of the uniform clustering constraint in RepCONC and derive an efficient approximate solution for constrained clustering by reducing it to an instance of the optimal transport problem. Besides constrained clustering, RepCONC further adopts a vector-based inverted file system (IVF) to support highly efficient vector search on CPUs. Extensive experiments on two popular ad-hoc retrieval benchmarks show that RepCONC achieves better ranking effectiveness than competitive vector quantization baselines under different compression ratio settings. It also substantially outperforms a wide range of existing retrieval models in terms of retrieval effectiveness, memory efficiency, and time efficiency.",Dense retrieval | Index compression | Neural ranking,56,1.0,all publisherfree2read,All Open Access Bronze,NSFC,61532011,National Natural Science Foundation of China
2-s2.0-105000502018,,,,"Learning in Markov Games with Adaptive Adversaries: Policy Regret, Fundamental Barriers, and Efficient Algorithms",cp,Conference Paper,Nguyen-Tang T.,60145911,Whiting School of Engineering,Baltimore,United States,2.0,"Nguyen-Tang, Thanh;Arora, Raman",57222873355;35092083100,60145911;60145911,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"We study learning in a dynamically evolving environment modeled as a Markov game between a learner and a strategic opponent that can adapt to the learner's strategies. While most existing works in Markov games focus on external regret as the learning objective, external regret becomes inadequate when the adversaries are adaptive. In this work, we focus on policy regret - a counterfactual notion that aims to compete with the return that would have been attained if the learner had followed the best fixed sequence of policy, in hindsight. We show that if the opponent has unbounded memory or if it is non-stationary, then sample-efficient learning is not possible. For memory-bounded and stationary, we show that learning is still statistically hard if the set of feasible strategies for the learner is exponentially large. To guarantee learnability, we introduce a new notion of consistent adaptive adversaries, wherein, the adversary responds similarly to similar strategies of the learner. We provide algorithms that achieve √T policy regret against memory-bounded, stationary, and consistent adversaries.",,1,0.0,,,JHU,HR00112020004,Johns Hopkins University
2-s2.0-85148052235,,,,Learning linear non-Gaussian directed acyclic graph with diverging number of nodes,ar,Article,Zhao R.,60002798;60013983;60032744,Chinese University of Hong Kong;City University of Hong Kong;Shanghai University of Finance and Economics,Hong Kong;Hong Kong;Shanghai,Hong Kong;Hong Kong;China,3.0,"Zhao, Ruixuan;He, Xin;Wang, Junhui",57350233300;57194595481;14044437100,60013983;60032744;60002798,2022-09-01,1 September 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,269,,"An acyclic model, often depicted as a directed acyclic graph (DAG), has been widely employed to represent directional causal relations among collected nodes. In this article, we propose an efficient method to learn linear non-Gaussian DAG in high dimensional cases, where the noises can be of any continuous non-Gaussian distribution. The proposed method leverages the concept of topological layer to facilitate the DAG learning, and its theoretical justification in terms of exact DAG recovery is also established under mild conditions. Particularly, we show that the topological layers can be exactly reconstructed in a bottom-up fashion, and the parent-child relations among nodes can also be consistently established. The established asymptotic DAG recovery is in sharp contrast to that of many existing learning methods assuming parental faithfulness or ordered noise variances. The advantage of the proposed method is also supported by the numerical comparison against some popular competitors in various simulated examples as well as a real application on the global spread of COVID-19.",Causal inference | DAG | non-Gaussian noise | structural equation model | topological layer,6,0.0,,,,2019PJC051,
2-s2.0-85147254641,10.1613/jair.1.13734,,,Learning to Design Fair and Private Voting Rules,ar,Article,Mohsin F.,60025534;60011048,Rensselaer Polytechnic Institute;IBM Research,Troy;Yorktown Heights,United States;United States,5.0,"Mohsin, Farhad;Liu, Ao;Chen, Pin Yu;Rossi, Francesca;Xia, Lirong",57034633400;57210118926;36930105800;56066648900;23011050500,60025534;60025534;60011048;60011048;60025534,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,1139-1176,"Voting is used widely to identify a collective decision for a group of agents, based on their preferences. In this paper, we focus on evaluating and designing voting rules that support both the privacy of the voting agents and a notion of fairness over such agents. To do this, we introduce a novel notion of group fairness and adopt the existing notion of local differential privacy. We then evaluate the level of group fairness in several existing voting rules, as well as the trade-offs between fairness and privacy, showing that it is not possible to always obtain maximal economic efficiency with high fairness or high privacy levels. Then, we present both a machine learning and a constrained optimization approach to design new voting rules that are fair while maintaining a high level of economic efficiency. Finally, we empirically examine the effect of adding noise to create local differentially private voting rules and discuss the three-way trade-off between economic efficiency, fairness, and privacy.",,6,1.0,all publisherfullgold,All Open Access Gold,NSF,1453542,National Science Foundation
2-s2.0-85174424763,,,,Learning to Suggest Breaks: Sustainable Optimization of Long-Term User Engagement,cp,Conference Paper,Saig E.,60022403,Technion - Israel Institute of Technology,Haifa,Israel,2.0,"Saig, Eden;Rosenfeld, Nir",57202917888;57076232100,60022403;60022403,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,29671-29696,"Optimizing user engagement is a key goal for modern recommendation systems, but blindly pushing users towards increased consumption risks burn-out, churn, or even addictive habits. To promote digital well-being, most platforms now offer a service that periodically prompts users to take breaks. These, however, must be set up manually, and so may be suboptimal for both users and the system. In this paper, we study the role of breaks in recommendation, and propose a framework for learning optimal breaking policies that promote and sustain long-term engagement. Based on the notion that recommendation dynamics are susceptible to both positive and negative feedback, we cast recommendation as a Lotka-Volterra dynamical system, where breaking reduces to a problem of optimal control. We then give an efficient learning algorithm, provide theoretical guarantees, and empirically demonstrate the utility of our approach on semi-synthetic data.",,2,0.0,,,ISF,278/22,Israel Science Foundation
2-s2.0-85174426016,,,,Learning-Rate-Free Learning by D-Adaptation,cp,Conference Paper,Defazio A.,60008134;60355330,CNRS Centre National de la Recherche Scientifique;Meta Ai,Paris;Menlo Park,France;United States,2.0,"Defazio, Aaron;Mishchenko, Konstantin",6701745412;57204803503,60355330;60008134,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,7449-7479,"D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems. An open-source implementation is available.",,32,0.0,,,,,
2-s2.0-85152137164,,,,Learning-augmented count-min sketches via Bayesian nonparametrics,ar,Article,Dolera E.,60012259;60015197;126723690,Università degli Studi di Torino;Università degli Studi di Pavia;Cogent Labs,Turin;Pavia;Tokyo,Italy;Italy;Japan,3.0,"Dolera, Emanuele;Favaro, Stefano;Peluchetti, Stefano",26431297700;24172349900;57219583836,60015197;60012259;126723690,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"The count-min sketch (CMS) is a time and memory efficient randomized data structure that provides estimates of tokens’ frequencies in a data stream of tokens, i.e. point queries, based on random hashed data. A learning-augmented version of the CMS, referred to as CMS-DP, has been proposed by Cai, Mitzenmacher and Adams (NeurIPS 2018), and it relies on Bayesian nonparametric (BNP) modeling of the data stream of tokens via a Dirichlet process (DP) prior, with estimates of a point query being that are obtained as mean functionals of the posterior distribution of the point query, given the hashed data. While the CMS-DP has proved to improve on some aspects of CMS, it has the major drawback of arising from a “constructive” proof that builds upon arguments that are tailored to the DP prior, namely arguments that are not usable for other nonparametric priors. In this paper, we present a “Bayesian” proof of the CMS-DP that has the main advantage of building upon arguments that are usable under the popular Pitman-Yor process (PYP) prior, which generalizes the DP prior by allowing for a more flexible tail behaviour, ranging from geometric tails to heavy power-law tails. This result leads to develop a novel learning-augmented CMS under power-law data streams, referred to as CMS-PYP, which relies on BNP modeling of the data stream of tokens via a PYP prior. Under this more general framework, we apply the arguments of the “Bayesian” proof of the CMS-DP, suitably adapted to the PYP prior, in order to compute the posterior distribution of a point query, given the hashed data. Applications to synthetic data and real textual data show that the CMS-PYP outperforms the CMS and the CMS-DP in estimating low-frequency tokens, which are known to be of critical interest in textual data, and it is competitive with respect to a variation of the CMS designed to deal with the estimation of low-frequency tokens. An extension of our BNP approach to more general queries, such as range queries, is also discussed.",Bayesian nonparametrics | count-min sketch | Dirichlet process prior | likelihood-free estimation | Pitman-Yor process prior | point query | power-law data stream | random hashing,3,0.0,,,ERC,817257,European Research Council
2-s2.0-85213838835,,,,Least Squares Model Averaging for Distributed Data,ar,Article,Zhang H.,60000937;60020256;60007807,Shenzhen University;Capital Normal University;Shenzhen Polytechnic University,Shenzhen;Beijing;Shenzhen,China;China;China,3.0,"Zhang, Haili;Liu, Zhaobo;Zou, Guohua",58280604000;57210426818;7102849277,60007807;60000937;60020256,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,1-59,"Divide and conquer algorithm is a common strategy applied in big data. Model averaging has the natural divide-and-conquer feature, but its theory has not been developed in big data scenarios. The goal of this paper is to fill this gap. We propose two divide-and-conquer-type model averaging estimators for linear models with distributed data. Under some regularity conditions, we show that the weights from Mallows model averaging criterion converge in L<inf>2</inf> to the theoretically optimal weights minimizing the risk of the model averaging estimator. We also give the bounds of the in-sample and out-of-sample mean squared errors and prove the asymptotic optimality for the proposed model averaging estimators. Our conclusions hold even when the dimensions and the number of candidate models are divergent. Simulation results and a real airline data analysis illustrate that the proposed model averaging methods perform better than the commonly used model selection and model averaging methods in distributed data cases. Our approaches contribute to model averaging theory in distributed data and parallel computations, and can be applied in big data analysis to save time and reduce the computational burden.",consistency | distributed data | divide and conquer algorithm | Mallows’ criterion | model averaging | optimality,5,0.0,,,SZPT,6023312034K,Shenzhen Polytechnic
2-s2.0-85189287389,10.1609/aaai.v38i3.27948,,,Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection,cp,Conference Paper,Gao H.,60014966;60019118,Peking University;University of Science and Technology of China,Beijing;Hefei,China;China,7.0,"Gao, Hongzhi;Chen, Zheng;Chen, Zehui;Chen, Lin;Liu, Jiaming;Zhang, Shanghang;Zhao, Feng",58967583900;57206982683;57226331687;57770581800;57200321067;57054535400;57225866170,60019118;60019118;60019118;60019118;60014966;60014966;60019118,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,3.0,,1797-1805,"Training high-accuracy 3D detectors necessitates massive labeled 3D annotations with 7 degree-of-freedom, which is laborious and time-consuming. Therefore, the form of point annotations is proposed to offer significant prospects for practical applications in 3D detection, which is not only more accessible and less expensive but also provides strong spatial information for object localization. In this paper, we empirically discover that it is non-trivial to merely adapt Point-DETR to its 3D form, encountering two main bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it generates low-quality pseudo labels in distant regions due to the extreme sparsity of LiDAR points. To overcome these challenges, we introduce Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D detection, designed to fully capitalize on point-wise supervision within a constrained instance-wise annotation budget. Different from Point-DETR which encodes 3D positional information solely through a point encoder, we propose an explicit positional query initialization strategy to enhance the positional prior. Considering the low quality of pseudo labels at distant regions produced by the teacher model, we enhance the detector's perception by incorporating dense imagery data through a novel Cross-Modal Deformable RoI Fusion (D-RoI). Moreover, an innovative point-guided self-supervised learning technique is proposed to allow for fully exploiting point priors, even in student models. Extensive experiments on representative nuScenes dataset demonstrate our PointDETR3D obtains significant improvements compared to previous works. Notably, with only 5% of labeled data, PointDETR3D achieves over 90% performance of its fully supervised counterpart.",,0,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85170372986,10.24963/ijcai.2023/624,,,Levin Tree Search with Context Models,cp,Conference Paper,Orseau L.,60030835;60111161;60193824,University of Alberta;DeepMind Technologies Limited;Alberta Machine Intelligence Institute,Edmonton;London;Edmonton,Canada;United Kingdom;Canada,3.0,"Orseau, Laurent;Hutter, Marcus;Lelis, Levi H.S.",36146416600;11042473800;35812244500,60111161;60111161;60030835-60193824,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,5622-5630,"Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a probability distribution over actions) and comes with a theoretical guarantee on the number of expansions before reaching a goal node, depending on the quality of the policy. This guarantee can be used as a loss function, which we call the LTS loss, to optimize neural networks representing the policy (LTS+NN). In this work we show that the neural network can be substituted with parameterized context models originating from the online compression literature (LTS+CM). We show that the LTS loss is convex under this new model, which allows for using standard convex optimization tools, and obtain convergence guarantees to the optimal parameters in an online setting for a given set of solution trajectories - guarantees that cannot be provided for neural networks. The new LTS+CM algorithm compares favorably against LTS+NN on several benchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle (STP). The difference is particularly large on STP, where LTS+NN fails to solve most of the test instances while LTS+CM solves each test instance in a fraction of a second. Furthermore, we show that LTS+CM is able to learn a policy that solves the Rubik's cube in only a few hundred expansions, which considerably improves upon previous machine learning techniques.",,2,1.0,all publisherfullgold,All Open Access Gold,NSERC,,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85153675271,10.1613/JAIR.1.14565,,,Liability Regimes in the Age of AI: a Use-Case Driven Analysis of the Burden of Proof,ar,Article,Llorca D.F.,60027800;60103695,Universidad de Alcalá;European Commission Joint Research Centre,Alcala de Henares;Brussels,Spain;Belgium,5.0,"Llorca, David Fernández;Charisi, Vicky;Hamon, Ronan;Sánchez, Ignacio;Gómez, Emilia",16426189200;56766259400;57222400770;14819833800;14015483200,60103695-60027800;60103695;60103695;60103695;60103695,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,613-644,"New emerging technologies powered by Artificial Intelligence (AI) have the potential to disruptively transform our societies for the better. In particular, data-driven learning approaches (i.e., Machine Learning (ML)) have been a true revolution in the advancement of multiple technologies in various application domains. But at the same time there is growing concern about certain intrinsic characteristics of these methodologies that carry potential risks to both safety and fundamental rights. Although there are mechanisms in the adoption process to minimize these risks (e.g., safety regulations), these do not exclude the possibility of harm occurring, and if this happens, victims should be able to seek compensation. Liability regimes will therefore play a key role in ensuring basic protection for victims using or interacting with these systems. However, the same characteristics that make AI systems inherently risky, such as lack of causality, opacity, unpredictability or their self and continuous learning capabilities, may lead to considerable difficulties when it comes to proving causation. This paper presents three case studies, as well as the methodology to reach them, that illustrate these difficulties. Specifically, we address the cases of cleaning robots, delivery drones and robots in education. The outcome of the proposed analysis suggests the need to revise liability regimes to alleviate the burden of proof on victims in cases involving AI technologies.",,9,1.0,all publisherfullgold,All Open Access Gold,EC,,European Commission
2-s2.0-85167438921,10.1609/aaai.v37i4.25504,,,Lifting (D)QBF Preprocessing and Solving Techniques to (D)SSAT,cp,Conference Paper,Cheng C.,60005429,National Taiwan University,Taipei,Taiwan,2.0,"Cheng, Che;Jiang, Jie Hong R.",58529268000;55731830700,60005429;60005429,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,3906-3914,"Dependency stochastic Boolean satisfiability (DSSAT) generalizes stochastic Boolean satisfiability (SSAT) in existential variables being Henkinized allowing their dependencies on randomized variables to be explicitly specified. It allows NEXPTIME problems of reasoning under uncertainty and partial information to be compactly encoded. To date, no decision procedure has been implemented for solving DSSAT formulas. This work provides the first such tool by converting DSSAT into SSAT with dependency elimination, similar to converting dependency quantified Boolean formula (DQBF) to quantified Boolean formula (QBF). Moreover, we extend (D)QBF preprocessing techniques and implement the first standalone (D)SSAT preprocessor. Experimental results show that solving DSSAT via dependency elimination is highly applicable and that existing SSAT solvers may benefit from preprocessing.",,5,1.0,all publisherfullgold,All Open Access Gold,NSTC,111-2221-E-002-182,National Science and Technology Council
2-s2.0-85134756824,10.24963/ijcai.2022/128,,,Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer,cp,Conference Paper,Gao G.,60002798;60009400;60028928,Chinese University of Hong Kong;Nanjing University of Post and TeleCommunications;Research Organization of Information and Systems National Institute of Informatics,Hong Kong;Nanjing;Tokyo,Hong Kong;China;Japan,6.0,"Gao, Guangwei;Wang, Zhengxue;Li, Juncheng;Li, Wenjie;Yu, Yi;Zeng, Tieyong",55352183600;57222816019;57203988667;57827135000;57193005361;25423412800,60009400-60028928;60009400;60002798;60009400;60028928;60002798,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,913-919,"Single-image super-resolution (SISR) has achieved significant breakthroughs with the development of deep learning. However, these methods are difficult to be applied in real-world scenarios since they are inevitably accompanied by the problems of computational and memory costs caused by the complex operations. To solve this issue, we propose a Lightweight Bimodal Network (LBNet) for SISR. Specifically, an effective Symmetric CNN is designed for local feature extraction and coarse image reconstruction. Meanwhile, we propose a Recursive Transformer to fully learn the long-term dependence of images thus the global information can be fully used to further refine texture details. Studies show that the hybrid of CNN and Transformer can build a more efficient model. Extensive experiments have proved that our LBNet achieves more prominent performance than other state-of-the-art methods with a relatively low computational cost and memory consumption. The code is available at https://github.com/IVIPLab/LBNet.",,72,1.0,all publisherfree2read,All Open Access Bronze,CRF,8730063,Cancer Research Foundation
2-s2.0-105000484707,,,,Linking In-context Learning in Transformers to Human Episodic Memory,cp,Conference Paper,Ji-An L.,60030612;60021784;60121539,"University of California, San Diego;New York University;Department of Cognitive Science",La Jolla;New York;La Jolla,United States;United States;United States,4.0,"Ji-An, Li;Zhou, Corey Y.;Benna, Marcus K.;Mattar, Marcelo G.",57219788893;57215870996;57191410407;47161310500,60030612;60121539;60030612;60021784,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Understanding connections between artificial and biological intelligent systems can reveal fundamental principles of general intelligence. While many artificial intelligence models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between interacting attention heads and human episodic memory. We focus on induction heads, which contribute to in-context learning in Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate and late layers, qualitatively mirroring human memory biases. The ablation of CMR-like heads suggests their causal role in in-context learning. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields.",,3,0.0,,,NIH,R01NS125298,National Institutes of Health
2-s2.0-85137943175,10.24963/ijcai.2022/547,,,Local Differential Privacy Meets Computational Social Choice - Resilience under Voter Deletion,cp,Conference Paper,Tao L.,60003970;60005837;60021285;60029653,Zhejiang University;University of Houston;Texas Tech University;Kent State University,Hangzhou;Houston;Lubbock;Kent,China;United States;United States;United States,4.0,"Tao, Liangde;Chen, Lin;Xu, Lei;Shi, Weidong",57226330763;57190972669;57202564568;36176359500,60003970;60021285;60029653;60005837,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3940-3946,"The resilience of a voting system has been a central topic in computational social choice. Many voting rules, like plurality, are shown to be vulnerable as the attacker can target specific voters to manipulate the result. What if a local differential privacy (LDP) mechanism is adopted such that the true preference of a voter is never revealed in pre-election polls? In this case, the attacker can only infer stochastic information about a voter's true preference, and this may cause the manipulation of the electoral result significantly harder. The goal of this paper is to provide a quantitative study on the effect of adopting LDP mechanisms on a voting system. We introduce the metric PoLDP (power of LDP) that quantitatively measures the difference between the attacker's manipulation cost under LDP mechanisms and that without LDP mechanisms. The larger PoLDP is, the more robustness LDP mechanisms can add to a voting system. We give a full characterization of PoLDP for the voting system with plurality rule and provide general guidance towards the application of LDP mechanisms.",,2,1.0,all publisherfree2read,All Open Access Bronze,NSF,2004096,National Science Foundation
2-s2.0-85167869122,10.1609/aaai.v37i5.25737,,,Local Justice and Machine Learning: Modeling and Inferring Dynamic Ethical Preferences toward Allocations,cp,Conference Paper,Chen V.,60027950;60027392,Carnegie Mellon University;Stevens Institute of Technology,Pittsburgh;Hoboken,United States;United States,4.0,"Chen, Violet;Williams, Joshua;Leben, Derek;Heidari, Hoda",57215871720;57219630542;36192092900;56394228400,60027392;60027950;60027950;60027950,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,5956-5964,"We consider a setting in which a social planner has to make a sequence of decisions to allocate scarce resources in a high-stakes domain. Our goal is to understand stakeholders' dynamic moral preferences toward such allocational policies. In particular, we evaluate the sensitivity of moral preferences to the history of allocations and their perceived future impact on various socially salient groups. We propose a mathematical model to capture and infer such dynamic moral preferences. We illustrate our model through small-scale human-subject experiments focused on the allocation of scarce medical resource distributions during a hypothetical viral epidemic. We observe that participants' preferences are indeed history- and impact-dependent. Additionally, our preliminary experimental results reveal intriguing patterns specific to medical resources-a topic that is particularly salient against the backdrop of the global covid-19 pandemic.",,2,1.0,all publisherfullgold,All Open Access Gold,NSF,2040929,National Science Foundation
2-s2.0-85163067037,,,,Local Linear Convergence of Douglas-Rachford for Linear Programming: a Probabilistic Analysis,cp,Conference Paper,Faust O.,60031101;60119937,University of Cambridge;Faculty of Mathematics,Cambridge;Cambridge,United Kingdom;United Kingdom,2.0,"Faust, Oisín;Fawzi, Hamza",57222047676;54912653100,60119937-60031101;60119937-60031101,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,6358-6372,"Douglas-Rachford splitting/ADMM (henceforth DRS) is a very popular algorithm for solving convex optimisation problems to low or moderate accuracy, and in particular for solving large-scale linear programs. Despite recent progress, obtaining highly accurate solutions to linear programs with DRS remains elusive. In this paper we analyze the local linear convergence rate r of the DRS method for random linear programs, and give explicit and tight bounds on r. We show that 1−r<sup>2</sup> is typically of the order of m<sup>−1</sup>(n−m)<sup>−1</sup>, where n is the number of variables and m is the number of constraints. This provides a quantitative explanation for the very slow convergence of DRS/ADMM on random LPs. The proof of our result relies on an established characterisation of the linear rate of convergence as the cosine of the Friedrichs angle between two subspaces associated to the problem. We also show that the cosecant of this angle can be interpreted as a condition number for the LP.",,0,0.0,,,,,
2-s2.0-85145479937,10.1609/aaai.v36i3.20255,,,Local Surface Descriptor for Geometry and Feature Preserved Mesh Denoising,cp,Conference Paper,Zhao W.,60014966;60104026;60271961;60361344,"Peking University;Beijing National Research Center for Information Science and Technology;Peng Cheng Laboratory;Faculty of Computing, Harbin Institute of Technology",Beijing;Beijing;Shenzhen;Harbin,China;China;China;China,6.0,"Zhao, Wenbo;Liu, Xianming;Jiang, Junjun;Zhao, Debin;Li, Ge;Ji, Xiangyang",57190300870;35230216700;54902306100;7403490261;59622770600;7402837796,60361344-60271961;60361344-60271961;60361344-60271961;60361344-60271961;60014966;60104026,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,3446-3453,"3D meshes are widely employed to represent geometry structure of 3D shapes. Due to limitation of scanning sensor precision and other issues, meshes are inevitably affected by noise, which hampers the subsequent applications. Convolultional neural networks (CNNs) achieve great success in image processing tasks, including 2D image denoising, and have been proven to own the capacity of modeling complex features at different scales, which is also particularly useful for mesh denoising. However, due to the nature of irregular structure, CNNs-based denosing strategies cannot be trivially applied for meshes. To circumvent this limitation, in the paper, we propose the local surface descriptor (LSD), which is able to transform the local deformable surface around a face into 2D grid representation and thus facilitates the deployment of CNNs to generate denoised face normals. To verify the superiority of LSD, we directly feed LSD into the classical Resnet without any complicated network design. The extensive experimental results show that, compared to the state-of-the-arts, our method achieves encouraging performance with respect to both objective and subjective evaluations.",,10,1.0,all publisherfullgold,All Open Access Gold,NSFC,61922027,National Natural Science Foundation of China
2-s2.0-105018672518,,,,Localized Debiased Machine Learning: Efficient Inference on Quantile Treatment Effects and Beyond,ar,Article,Kallus N.,60122853;60104837,Tsinghua University School of Economics and Management;Cornell Tech,Beijing;New York,China;United States,3.0,"Kallus, Nathan;Mao, Xiaojie;Uehara, Masatoshi",56305894700;57206664374;57218718361,60104837;60122853;60104837,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"We consider estimating a low-dimensional parameter in an estimating equation involving high-dimensional nuisance functions that depend on the target parameter as an input. A central example is the efficient estimating equation for the (local) quantile treatment effect ((L)QTE) in causal inference, which involves the covariate-conditional cumulative distribution function evaluated at the quantile to be estimated. Existing approaches based on flexibly estimating the nuisances and plugging in the estimates, such as debiased machine learning (DML), require we learn the nuisance at all possible inputs. For (L)QTE, DML requires we learn the whole covariate-conditional cumulative distribution function. We instead propose localized debiased machine learning (LDML), which avoids this burdensome step and needs only estimate nuisances at a single initial rough guess for the target parameter. For (L)QTE, LDML involves learning just two regression functions, a standard task for machine learning methods. We prove that under lax rate conditions our estimator has the same favorable asymptotic behavior as the infeasible estimator that uses the unknown true nuisances. Thus, LDML notably enables practically-feasible and theoretically-grounded efficient estimation of important quantities in causal inference such as (L)QTEs when we must control for many covariates and/or flexible relationships, as we demonstrate in empirical studies.",Causal Inference | Conditional Value at Risk | Cross-fitting | Expectiles | Instrumental Variables | Neyman Orthogonality,11,0.0,,,NKPs,2022ZD0116700,National Key Research and Development Program of China
2-s2.0-85163145653,,,,Locally Hierarchical Auto-Regressive Modeling for Image Generation,cp,Conference Paper,You T.,60013682;60032330;60121136;129867838;129868364,Seoul National University;Pohang University of Science and Technology;Kakao Corp.;ECE;IPAI,Seoul;Pohang;Jeju;;,South Korea;South Korea;South Korea;;,5.0,"You, Tackgeun;Kim, Saehoon;Kim, Chiheon;Lee, Doyup;Han, Bohyung",57189343762;36630539000;57212002560;57219545528;8729746700,60013682-60032330;60121136;60121136;60121136;129867838-129868364-60013682,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"We propose a locally hierarchical auto-regressive model with multiple resolutions of discrete codes. In the first stage of our algorithm, we represent an image with a pyramid of codes using Hierarchically Quantized Variational AutoEncoder (HQ-VAE), which disentangles the information contained in the multi-level codes. For an example of two-level codes, we create two separate pathways to carry high-level coarse structures of input images using top codes while compensating for missing fine details by constructing a residual connection for bottom codes. An appropriate selection of resizing operations for code embedding maps enables top codes to capture maximal information within images and the first stage algorithm achieves better performance on both vector quantization and image generation. The second stage adopts Hierarchically Quantized Transformer (HQ-Transformer) to process a sequence of local pyramids, which consist of a single top code and its corresponding bottom codes. Contrary to other hierarchical models, we sample bottom codes in parallel by exploiting the conditional independence assumption on the bottom codes. This assumption is naturally harvested from our first-stage model, HQ-VAE, where the bottom code learns to describe local details. On class-conditional and text-conditional generation benchmarks, our model shows competitive performance to previous AR models in terms of fidelity of generated images while enjoying lighter computational budgets.",,10,0.0,,,SNU,2022R1A5A708390811,Seoul National University
2-s2.0-85203805239,,,,Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning,cp,Conference Paper,Zhao H.,131428035,EPFL,Geneva,Switzerland,4.0,"Zhao, Hao;Andriushchenko, Maksym;Croce, Francesco;Flammarion, Nicolas",58897795400;57202055148;57208002969;57190951738,131428035;131428035;131428035;131428035,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,60674-60703,"There is a consensus that instruction fine-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a quality scorer.We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses-that intuitively contain more learnable information and are harder to overfit-from standard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining competitive on the Open LLM benchmarks that test factual knowledge.We demonstrate this for several LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B-v0.1) and datasets (Alpaca-52k, Evol-Instruct-70k).In addition, a lightweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to obtain competitive results on MT-Bench and the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0, while training on only 1,000 examples and no extra preference data.We also conduct a thorough analysis of our models to ensure that their enhanced performance is not simply due to GPT-4's preference for longer responses.Overall, our findings suggest that fine-tuning on the longest responses should be the default baselinefor any work on instruction fine-tuning.We provide our code in this GitHub repository.",,0,0.0,,,ICML-UNAM,,"Instituto de Ciencias del Mar y Limnología, Universidad Nacional Autónoma de México"
2-s2.0-85137867246,10.24963/ijcai.2022/174,,,Long-Short Term Cross-Transformer in Compressed Domain for Few-Shot Video Classification,cp,Conference Paper,Luo W.,60027363;60018486;60278199;125298810,University of Chinese Academy of Sciences;Institute of Automation Chinese Academy of Sciences;Center for Excellence in Brain Science and Intelligence Technology;National Computer Network Emergency Response Technical Team,Beijing;Beijing;Shanghai;Beijing,China;China;China;China,6.0,"Luo, Wenyang;Liu, Yufan;Li, Bing;Hu, Weiming;Miao, Yanan;Li, Yangxi",57888543300;58453239600;57210580694;56703992500;57199229086;36087629400,60018486-60027363;60018486-60027363;60018486;60018486-60027363-60278199;125298810;125298810,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1247-1253,"Compared with image few-shot learning, most of the existing few-shot video classification methods perform worse on feature matching, because they fail to sufficiently exploit the temporal information and relation. Specifically, frames are usually evenly sampled, which may miss important frames. On the other hand, the heuristic model simply encodes the equally treated frames in sequence, which results in the lack of both long-term and short-term temporal modeling and interaction. To alleviate these limitations, we take advantage of the compressed domain knowledge and propose a long-short term Cross-Transformer (LSTC) for few-shot video classification. For short terms, the motion vector (MV) contains temporal cues and reflects the importance of each frame. For long terms, a video can be natively divided into a sequence of GOPs (Group Of Picture). Using this compressed domain knowledge helps to obtain a more accurate spatial-temporal feature space. Consequently, we design the long-short term selection module, short-term module, and long-term module to comprise the LSTC. Long-short term selection is performed to select informative compressed domain data. Long/short-term modules are utilized to sufficiently exploit the temporal information so that the query and support can be well-matched by cross-attention. Experimental results show the superiority of our method on various datasets.",,14,1.0,all publisherfree2read,All Open Access Bronze,NSFC,61721004,National Natural Science Foundation of China
2-s2.0-85137921018,10.24963/ijcai.2022/309,,,Long-term Spatio-Temporal Forecasting via Dynamic Multiple-Graph Attention,cp,Conference Paper,Shao W.,60014439;60025858;60028333;60025578;60011362;60139200;60105144,"University of California, Davis;ETH Zürich;UNSW Sydney;Xidian University;RMIT University;Ira A. Fulton Schools of Engineering;Qatar Mobility Innovations Center",Davis;Zurich;Sydney;Xi'an;Melbourne;Tempe;Doha,United States;Switzerland;Australia;China;Australia;United States;Qatar,9.0,"Shao, Wei;Jin, Zhiling;Wang, Shuo;Kang, Yufan;Xiao, Xiao;Menouar, Hamid;Zhang, Zhaofeng;Zhang, Junshan;Salim, Flora",57208885593;57239147200;57213427789;57701877000;36350444200;24470539100;57218794249;36105605300;7801420000,60139200;60025578;60025858;60011362;60025578;60105144;60139200;60014439;60028333,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2225-2232,"Many real-world ubiquitous applications, such as parking recommendations and air pollution monitoring, benefit significantly from accurate long-term spatio-temporal forecasting (LSTF). LSTF makes use of long-term dependency structure between the spatial and temporal domains, as well as the contextual information. Recent studies have revealed the potential of multi-graph neural networks (MGNNs) to improve prediction performance. However, existing MGNN methods do not work well when applied to LSTF due to several issues: the low level of generality, insufficient use of contextual information, and the imbalanced graph fusion approach. To address these issues, we construct new graph models to represent the contextual information of each node and exploit the long-term spatio-temporal data dependency structure. To aggregate the information across multiple graphs, we propose a new dynamic multi-graph fusion module to characterize the correlations of nodes within a graph and the nodes across graphs via the spatial attention and graph attention mechanisms. Furthermore, we introduce a trainable weight tensor to indicate the importance of each node in different graphs. Extensive experiments on two large-scale datasets demonstrate that our proposed approaches significantly improve the performance of existing graph neural network models in LSTF prediction tasks.",,46,1.0,all publisherfree2read repository repositoryam,All Open Access Bronze Green,QNRF,,Qatar National Research Fund
2-s2.0-85142060112,10.1613/jair.1.13854,,,Low-Rank Representation of Reinforcement Learning Policies,ar,Article,Mazoure B.,60002494;60027863;60113142,Université McGill;Université du Québec à Montréal;Montreal Institute for Learning Algorithms,Montreal;Montreal;Montreal,Canada;Canada;Canada,7.0,"Mazoure, Bogdan;Doan, Thang;Li, Tianyu;Makarenkov, Vladimir;Pineau, Joelle;Precup, Doina;Rabuseau, Guillaume",57195723464;57213864548;57209460908;35334891200;13404973100;6603288659;56613386300,60002494;60002494;60002494;60027863;60002494;60002494;60113142,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,597-636,"We propose a general framework for policy representation for reinforcement learning tasks. This framework involves finding a low-dimensional embedding of the policy on a reproducing kernel Hilbert space (RKHS). The usage of RKHS based methods allows us to derive strong theoretical guarantees on the expected return of the reconstructed policy. Such guarantees are typically lacking in black-box models, but are very desirable in tasks requiring stability and convergence guarantees. We conduct several experiments on classic RL domains. The results confirm that the policies can be robustly represented in a low-dimensional space while the embedded policy incurs almost no decrease in returns.",,2,1.0,all publisherfullgold,All Open Access Gold,CIFAR,,Canadian Institute for Advanced Research
2-s2.0-85137944909,10.24963/ijcai.2022/590,,,Low-Resource NER by Data Augmentation With Prompting,cp,Conference Paper,Liu J.,60022381,Beijing Jiaotong University,Beijing,China,3.0,"Liu, Jian;Chen, Yufeng;Xu, Jinan",57201559825;57114607100;54796280200,60022381;60022381;60022381,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4252-4258,"Named entity recognition (NER) is a fundamental information extraction task that seeks to identify entity mentions of certain types in text. Despite numerous advances, the existing NER methods rely on extensive supervision for model training, which struggle in a low-resource scenario with limited training data. In this paper, we propose a new data augmentation method for low-resource NER, by eliciting knowledge from BERT with prompting strategies. Particularly, we devise a label-conditioned word replacement strategy that can produce more label-consistent examples by capturing the underlying word-label dependencies, and a prompting with question answering method to generate new training data from unlabeled texts. The experimental results have widely confirmed the effectiveness of our approach. Particularly, in a low-resource scenario with only 150 training sentences, our approach outperforms previous methods without data augmentation by over 40% in F1 and prior best data augmentation methods by over 2.0% in F1. Furthermore, our approach also fits with a zero-shot scenario, yielding promising results without using any human-labeled data for the task.",,30,1.0,all publisherfree2read,All Open Access Bronze,NSFC,62106016,National Natural Science Foundation of China
2-s2.0-85174385467,,,,Low-Switching Policy Gradient with Exploration via Online Sensitivity Sampling,cp,Conference Paper,Li Y.,60027550;60021726,"University of California, Los Angeles;Microsoft Research",Los Angeles;Redmond,United States;United States,4.0,"Li, Yunfan;Wang, Yiran;Cheng, Yu;Yang, Lin",58453524800;58453524900;55421399200;55795985300,60027550;60027550;60021726;60027550,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,19407-19424,"Policy optimization methods are powerful algorithms in Reinforcement Learning (RL) for their flexibility to deal with policy parameterization and ability to handle model misspecification. However, these methods usually suffer from slow convergence rates and poor sample complexity. Hence it is important to design provably sample efficient algorithms for policy optimization. Yet, recent advances for this problems have only been successful in tabular and linear setting, whose benign structures cannot be generalized to non-linearly parameterized policies. In this paper, we address this problem by leveraging recent advances in value-based algorithms, including bounded eluder-dimension and online sensitivity sampling, to design a low-switching sample-efficient policy optimization algorithm, LPO, with general non-linear function approximation. We show that, our algorithm obtains an ε-optimal policy with only O<sup>e</sup>(<sup>poly</sup><inf>ε3</inf><sup>(d)</sup>) samples, where ε is the suboptimality gap and d is a complexity measure of the function class approximating the policy. This drastically improves previously best-known sample bound for policy optimization algorithms, O<sup>e</sup>(<sup>poly</sup><inf>ε8</inf><sup>(d)</sup>). Moreover, we empirically test our theory with deep neural nets to show the benefits of the theoretical inspiration.",,1,0.0,,,NSF,2221871,National Science Foundation
2-s2.0-105018579749,,,,Lower Complexity Bounds of Finite-Sum Optimization Problems: The Results and Construction,ar,Article,Han Y.,60014966,Peking University,Beijing,China,3.0,"Han, Yuze;Xie, Guangzeng;Zhang, Zhihua",57222867637;57205148815;56734646000,60014966;60014966;60014966,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"In this paper we study the lower complexity bounds for finite-sum optimization problems, where the objective is the average of n individual component functions. We consider a so-called proximal incremental first-order oracle (PIFO) algorithm, which employs the individual component function’s gradient and proximal information provided by PIFO to update the variable. To incorporate loopless methods, we also allow the PIFO algorithm to obtain the full gradient infrequently. We develop a novel approach to constructing the hard instances, which partitions the tridiagonal matrix of classical examples into n groups. This construction is friendly to the analysis of PIFO algorithms. Based on this construction, we establish the lower complexity bounds for finite-sum minimax optimization problems when the objective is convex-concave or nonconvex-strongly-concave and the class of component functions is L-average smooth. Most of these bounds are nearly matched by existing upper bounds up to log factors. We also derive similar lower bounds for finite-sum minimization problems as previous work under both smoothness and average smoothness assumptions. Our lower bounds imply that proximal oracles for smooth functions are not much more powerful than gradient oracles.",finite-sum optimization | lower bound | minimax optimization | proximal incremental first-order oracle,6,0.0,,,NNSF,12350001,National Natural Science Foundation of China
2-s2.0-85189543784,10.1609/aaai.v38i4.28152,,,"M3SOT: Multi-Frame, Multi-Field, Multi-Space 3D Single Object Tracking",cp,Conference Paper,Liu J.,60025578;60028628,Xidian University;Northeastern University,Xi'an;Boston,China;United States,7.0,"Liu, Jiaming;Wu, Yue;Gong, Maoguo;Miao, Qiguang;Ma, Wenping;Xu, Cai;Qin, Can",57222277278;56215531900;8933846400;9133503300;57205878746;57204475787;57215962325,60025578;60025578;60025578;60025578;60025578;60025578;60028628,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,4.0,,3630-3638,"3D Single Object Tracking (SOT) stands a forefront task of computer vision, proving essential for applications like autonomous driving. Sparse and occluded data in scene point clouds introduce variations in the appearance of tracked objects, adding complexity to the task. In this research, we unveil M3SOT, a novel 3D SOT framework, which synergizes multiple input frames (template sets), multiple receptive fields (continuous contexts), and multiple solution spaces (distinct tasks) in ONE model. Remarkably, M3SOT pioneers in modeling temporality, contexts, and tasks directly from point clouds, revisiting a perspective on the key factors influencing SOT. To this end, we design a transformer-based network centered on point cloud targets in the search area, aggregating diverse contextual representations and propagating target cues by employing historical frames. As M3SOT spans varied processing perspectives, we’ve streamlined the network—trimming its depth and optimizing its structure—to ensure a lightweight and efficient deployment for SOT applications. We posit that, backed by practical construction, M3SOT sidesteps the need for complex frameworks and auxiliary components to deliver sterling results. Extensive experiments on benchmarks such as KITTI, nuScenes, and Waymo Open Dataset demonstrate that M3SOT achieves state-of-the-art performance at 38 FPS. Our code and models are available at https://github.com/ywu0912/TeamCode.git.",,7,1.0,all publisherfullgold,All Open Access Gold,NSFC,62036006,National Natural Science Foundation of China
2-s2.0-85170383244,10.24963/ijcai.2023/470,,,MA2CL:Masked Attentive Contrastive Learning for Multi-Agent Reinforcement Learning,cp,Conference Paper,Song H.,60019118;127183403,University of Science and Technology of China;Hefei Comprehensive National Science Center,Hefei;Hefei,China;China,4.0,"Song, Haolin;Feng, Mingxiao;Zhou, Wengang;Li, Houqiang",58475115200;57224851783;8979446000;35956273100,60019118;60019118;60019118-127183403;60019118-127183403,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,4226-4234,"Recent approaches have utilized self-supervised auxiliary tasks as representation learning to improve the performance and sample efficiency of vision-based reinforcement learning algorithms in single-agent settings. However, in multi-agent reinforcement learning (MARL), these techniques face challenges because each agent only receives partial observation from an environment influenced by others, resulting in correlated observations in the agent dimension. So it is necessary to consider agent-level information in representation learning for MARL. In this paper, we propose an effective framework called Multi-Agent Masked Attentive Contrastive Learning (MA2CL), which encourages learning representation to be both temporal and agent-level predictive by reconstructing the masked agent observation in latent space. Specifically, we use an attention reconstruction model for recovering and the model is trained via contrastive learning. MA2CL allows better utilization of contextual information at the agent level, facilitating the training of MARL agents for cooperation tasks. Extensive experiments demonstrate that our method significantly improves the performance and sample efficiency of different MARL algorithms and outperforms other methods in various vision-based and state-based scenarios.",,6,1.0,all publisherfullgold,All Open Access Gold,NSFC,61836011,National Natural Science Foundation of China
2-s2.0-85189359029,10.1609/aaai.v38i1.27790,,,MCSSME: Multi-Task Contrastive Learning for Semi-supervised Singing Melody Extraction from Polyphonic Music,cp,Conference Paper,Yu S.,60010953,Donghua University,Shanghai,China,1.0,"Yu, Shuai",57206472390,60010953,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,1.0,,365-373,"Singing melody extraction is an important task in the field of music information retrieval (MIR). The development of data-driven models for this task have achieved great successes. However, the existing models have two major limitations: firstly, most of the existing singing melody extraction models have formulated this task as a pixel-level prediction task. The lack of labeling data has limited the model for further improvements. Secondly, the generalization of the existing models are prone to be disturbed by the music genres. To address the issues mentioned above, in this paper, we propose a multi-task contrastive learning framework for semisupervised singing melody extraction, termed as MCSSME. Specifically, to deal with data scarcity limitation, we propose a self-consistency regularization (SCR) method to train the model on the unlabeled data. Transformations are applied to the raw signal of polyphonic music, which makes the network to improve its representation capability via recognizing the transformations. We further propose a novel multi-task learning (MTL) approach to jointly learn singing melody extraction and classification of transformed data. To deal with generalization limitation, we also propose a contrastive embedding learning, which strengthens the intra-class compactness and inter-class separability. To improve the generalization on different music genres, we also propose a domain classification method to learn task-dependent features by mapping data from different music genres to shared subspace. MCSSME evaluates on a set of well-known public melody extraction datasets with promising performances. The experimental results demonstrate the effectiveness of the MCSSME framework for singing melody extraction from polyphonic music using very limited labeled data scenarios.",,9,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85149114126,,,,MEASURING CLEVRNESS: BLACK-BOX TESTING OF VISUAL REASONING MODELS,cp,Conference Paper,Mouselinos S.,60013756;60111161,University of Warsaw;DeepMind Technologies Limited,Warsaw;London,Poland;United Kingdom,3.0,"Mouselinos, Spyridon;Michalewski, Henryk;Malinowski, Mateusz",57209730728;6701557012;56735775000,60013756;60013756;60111161,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"How can we measure the reasoning capabilities of intelligence systems? Visual question answering provides a convenient framework for testing the model's abilities by interrogating the model through questions about the scene. However, despite scores of various visual QA datasets and architectures, which sometimes yield even a super-human performance, the question of whether those architectures can actually reason remains open to debate. To answer this, we extend the visual question answering framework and propose the following behavioral test in the form of a two-player game. We consider black-box neural models of CLEVR. These models are trained on a diagnostic dataset benchmarking reasoning. Next, we train an adversarial player that re-configures the scene to fool the CLEVR model. We show that CLEVR models, which otherwise could perform at a “human level”, can easily be fooled by our agent. Our results put in doubt whether data-driven approaches can do reasoning without exploiting the numerous biases that are often present in those datasets. Finally, we also propose a controlled experiment measuring the efficiency of such models to learn and perform reasoning.",,4,0.0,,,ERC,UMO-2018/29/B/ST6/02959,Intel Corporation
2-s2.0-85199631519,,,,MIXTURE OF LORA EXPERTS,cp,Conference Paper,Wu X.,60025278;60098464,Tsinghua University;Microsoft Research Asia,Beijing;Beijing,China;China,3.0,"Wu, Xun;Huang, Shaohan;Wei, Furu",57886520000;57188864311;23995914700,60098464-60025278;60098464;60098464,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Low-Rank Adaptation (LoRA) (Hu et al., 2021) has emerged as a pivotal technique for fine-tuning large pre-trained models, renowned for its efficacy across a wide array of tasks. The modular architecture of LoRA has catalyzed further research into the synergistic composition of multiple trained LoRAs, aiming to amplify performance across various tasks. However, the effective composition of these trained LoRAs presents a formidable challenge: (1) Linear arithmetic composition can lead to the diminution of the generative capabilities inherent in the original pre-trained models or the distinctive attributes of the individually trained LoRAs, potentially resulting in suboptimal outcomes. (2) Reference tuning-based composition exhibits limitations in adaptability and incurs significant computational costs due to the requirements to retrain a large model. In response to these challenges, we propose Mixture of LoRA Experts (MOLE). MOLE treats each layer of trained LoRAs as a distinct expert and implements hierarchical weight control by integrating a learnable gating function within each layer to learn optimal composition weights tailored specifically to the objectives of a given domain. MOLE not only demonstrates enhanced performance in LoRA composition but also preserves the essential flexibility necessary for effective composition of trained LoRAs with minimal computational overhead. Extensive experiments conducted in both Natural Language Processing (NLP) and Vision & Language (V&L) domains validate the effects of MOLE. Our code are available at https://github.com/yushuiwx/MoLE.git.",,43,0.0,,,,,
2-s2.0-85203796537,,,,MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation,cp,Conference Paper,Huang Q.,60012708,Stanford University,Stanford,United States,4.0,"Huang, Qian;Vora, Jian;Liang, Percy;Leskovec, Jure",57205118101;57219755103;56646712700;12241436100,60012708;60012708;60012708;60012708,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,20271-20309,"A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with 37.5% average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from 100% on well-established older datasets to as low as 0% on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination.",,2,0.0,,,,,
2-s2.0-85214314173,,,,MMD Aggregated Two-Sample Test,ar,Article,Schrab A.,60022148;60016912;60005899;60084280,University College London;Yonsei University;Université Toulouse - Jean Jaurès;Gatsby Computational Neuroscience Unit,London;Seoul;Toulouse;London,United Kingdom;South Korea;France;United Kingdom,6.0,"Schrab, Antonin;Kim, Ilmun;Albert, Mélisande;Laurent, Béatrice;Guedj, Benjamin;Gretton, Arthur",57222412629;56413435200;57203582193;7102324332;23388770500;6603257032,60084280;60016912;60005899;60005899;60022148;60084280,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"We propose two novel nonparametric two-sample kernel tests based on the Maximum Mean Discrepancy (MMD). First, for a fixed kernel, we construct an MMD test using either permutations or a wild bootstrap, two popular numerical procedures to determine the test threshold. We prove that this test controls the probability of type I error non-asymptotically. Hence, it can be used reliably even in settings with small sample sizes as it remains well-calibrated, which differs from previous MMD tests which only guarantee correct test level asymptotically. When the difference in densities lies in a Sobolev ball, we prove minimax optimality of our MMD test with a specific kernel depending on the smoothness parameter of the Sobolev ball. In practice, this parameter is unknown and, hence, the optimal MMD test with this particular kernel cannot be used. To overcome this issue, we construct an aggregated test, called MMDAgg, which is adaptive to the smoothness parameter. The test power is maximised over the collection of kernels used, without requiring held-out data for kernel selection (which results in a loss of test power), or arbitrary kernel choices such as the median heuristic. We prove that MMDAgg still controls the level non-asymptotically, and achieves the minimax rate over Sobolev balls, up to an iterated logarithmic term. Our guarantees are not restricted to a specific type of kernel, but hold for any product of one-dimensional translation invariant characteristic kernels. We provide a user-friendly parameter-free implementation of MMDAgg using an adaptive collection of bandwidths. We demonstrate that MMDAgg significantly outperforms alternative state-of-the-art MMD-based two-sample tests on synthetic data satisfying the Sobolev smoothness assumption, and that, on real-world image data, MMDAgg closely matches the power of tests leveraging the use of models such as neural networks.",kernel methods | minimax adaptivity | two-sample testing,29,0.0,,,NRF,RS-2023-00211073,National Research Foundation of Korea
2-s2.0-85204289537,,,,MMVQA: A Comprehensive Dataset for Investigating Multipage Multimodal Information Retrieval in PDF-based Visual Question Answering,cp,Conference Paper,Ding Y.,60026553;60025709;60031806,University of Melbourne;The University of Sydney;The University of Western Australia,Melbourne;Sydney;Perth,Australia;Australia;Australia,5.0,"Ding, Yihao;Ren, Kaixuan;Huang, Jiabin;Luo, Siwen;Han, Soyeon Caren",57748659500;58195033300;59443978000;57219788119;55361191400,60026553-60025709;60025709;60025709;60031806;60026553-60025709,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,6243-6251,"Document Question Answering (QA) presents a challenge in understanding visually-rich documents (VRD), particularly with lengthy textual content. Existing studies primarily focus on real-world documents with sparse text, while challenges persist in comprehending the hierarchical semantic relations among multiple pages to locate multimodal components. The paper introduces MMVQA, a dataset tailored for research journal articles, encompassing multiple pages and multimodal retrieval. Our approach aims to retrieve entire paragraphs containing answers or visually rich document entities like tables and figures. The main contribution is introducing a comprehensive PDF Document VQA dataset, allowing the examination of semantically hierarchical layout structures in text-dominant documents. We also present new VRD-QA frameworks to grasp textual contents and relations among document layouts simultaneously, extending page-level understanding to the entire multi-page document. We aim to enhance the capabilities of existing vision-and-language models in handling challenges posed by text-dominant documents in VRD-QA. Code and Appendix are in https://github.com/adlnlp/pdfmvqa.",,4,0.0,,,UNIMELB,,University of Melbourne
2-s2.0-85184986211,,,,MUFFIN: CURATING MULTI-FACETED INSTRUCTIONS FOR IMPROVING INSTRUCTION-FOLLOWING,cp,Conference Paper,Lou R.,60003500;60001439;60009860;60030398;60117660,The Ohio State University;Pennsylvania State University;Fudan University;Temple University;Westlake University,Columbus;University Park;Shanghai;Philadelphia;Hangzhou,United States;United States;China;United States;China,8.0,"Lou, Renze;Zhang, Kai;Xie, Jian;Sun, Yuxuan;Ahn, Janice;Xu, Hanzi;Yu, Su;Yin, Wenpeng",57231620300;59880442700;58316602900;57219469776;58775326300;57955571600;59248968900;55418415800,60001439;60003500;60009860;60117660;60001439;60030398;60003500;60001439,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair without requiring a separate input anymore. However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Additionally, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction-following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. Experimental results across four zero-shot benchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes, reveal that LLMs, at various scales, trained on MUFFIN generally demonstrate superior instruction-following capabilities compared to those trained on the two aforementioned schemes.",,9,0.0,,,,,
2-s2.0-85200534527,,,,MULTILINEAR OPERATOR NETWORKS,cp,Conference Paper,Cheng Y.,60032179;60028186,University of Wisconsin-Madison;École Polytechnique Fédérale de Lausanne,Madison;Lausanne,United States;Switzerland,4.0,"Cheng, Yixin;Chrysos, Grigorios G.;Georgopoulos, Markos;Cevher, Volkan",58904433600;57188639985;57193539057;6506127004,60028186;60032179;;60028186,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated. On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have yet to perform on par with modern architectures. In this work, we aim close this gap and propose MONet, which relies solely on multilinear operators. The core layer of MONet, called Mu-Layer, captures multiplicative interactions of the elements of the input token. MONet captures high-degree interactions of the input elements and we demonstrate the efficacy of our approach on a series of image recognition and scientific computing benchmarks. The proposed model outperforms prior polynomial networks and performs on par with modern architectures. We believe that MONet can inspire further research on models that use entirely multilinear operations. The source code is available at MONet.",,7,0.0,,,ARO,21043,Hasler Stiftung
2-s2.0-85189647522,10.1609/aaai.v38i17.29854,,,MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks,cp,Conference Paper,Qi J.,60157272,Virginia Tech College of Engineering,Blacksburg,United States,5.0,"Qi, Jingyuan;Liu, Minqian;Shen, Ying;Xu, Zhiyang;Huang, Lifu",57298825300;57853841000;58034116200;57668517800;57193240973,60157272;60157272;60157272;60157272;60157272,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,17.0,,18888-18896,"Automatically generating scripts (i.e. sequences of key steps described in text) from video demonstrations and reasoning about the subsequent steps are crucial to the modern AI virtual assistants to guide humans to complete everyday tasks, especially unfamiliar ones. However, current methods for generative script learning heavily rely on well-structured preceding steps described in text and/or images or are limited to a certain domain, resulting in a disparity with real-world user scenarios. To address these limitations, we present a new benchmark challenge – MULTISCRIPT, with two new tasks on task-oriented multimodal script learning: (1) multimodal script generation, and (2) subsequent step prediction. For both tasks, the input consists of a target task name and a video illustrating what has been done to complete the target task, and the expected output is (1) a sequence of structured step descriptions in text based on the demonstration video, and (2) a single text description for the subsequent step, respectively. Built from WikiHow, MULTISCRIPT covers multimodal scripts in videos and text descriptions for over 6,655 human everyday tasks across 19 diverse domains. To establish baseline performance on MULTISCRIPT, we propose two knowledge-guided multimodal generative frameworks that incorporate the task-related knowledge prompted from large language models such as Vicuna. Experimental results show that our proposed approaches significantly improve over the competitive baselines.",,3,1.0,all publisherfullgold,All Open Access Gold,DARPA,HR001122S0052,Defense Advanced Research Projects Agency
2-s2.0-85213816053,,,,MULTIZOO & MULTIBENCH: A Standardized Toolkit for Multimodal Deep Learning,ar,Article,Liang P.P.,60136640,School of Computer Science,Pittsburgh,United States,7.0,"Liang, Paul Pu;Lyu, Yiwei;Fan, Xiang;Agarwal, Arav;Cheng, Yun;Morency, Louis Philippe;Salakhutdinov, Ruslan",57203051940;57211207258;57226473039;58155030600;57226044843;6603047400;57203057355,60136640;60136640;60136640;60136640;60136640;60136640;60136640,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MULTIZOO, a public toolkit consisting of standardized implementations of > 20 core multimodal algorithms and MULTIBENCH, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MULTIBENCH paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community<sup>1</sup>",Benchmarks | Multimodal learning | Open Source Software | Representation learning,7,0.0,,,ARPA,DARPA/AFRL FA87502321015,Defense Advanced Research Projects Agency
2-s2.0-85203810084,,,,MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion,cp,Conference Paper,Chang D.,60029311;60159665,University of Southern California;ByteDance Ltd.,Los Angeles;Beijing,United States;China,10.0,"Chang, Di;Shi, Yichun;Gao, Quankai;Xu, Hongyi;Fu, Jessica;Song, Guoxian;Yan, Qing;Zhu, Yizhe;Yang, Xiao;Soleymani, Mohammad",58161383600;57200374012;57222872975;56292148300;58749294100;57204979749;58749752100;59606605900;57224941102;57188866370,60029311-60159665;60159665;60029311;60159665;60029311;60159665;60159665;60159665;60159665;60029311,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,6263-6285,"In this work, we propose MagicPose, a diffusion-based model for 2D human pose and facial expression retargeting. Specifically, given a reference image, we aim to generate a person's new images by controlling the poses and facial expressions while keeping the identity unchanged. To this end, we propose a two-stage training strategy to disentangle human motions and appearance (e.g., facial expressions, skin tone and dressing), consisting of (1) the pre-training of an appearance-control block and (2) learning appearance-disentangled pose control. Our novel design enables robust appearance control over generated human images, including body, facial attributes, and even background. By leveraging the prior knowledge of image diffusion models, MagicPose generalizes well to unseen human identities and complex poses without the need for additional fine-tuning. Moreover, the proposed model is easy to use and can be considered as a plug-in module/extension to Stable Diffusion. The project website is https://boese0601.github.io/magicdance/. The code is available at https://github.com/Boese0601/MagicDance.",,7,0.0,,,ARO,W911NF-20-2-0053,Army Research Office
2-s2.0-85173672994,,,,Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models,cp,Conference Paper,Huang R.,60003970;60014966;60159665,Zhejiang University;Peking University;ByteDance Ltd.,Hangzhou;Beijing;Beijing,China;China;China,10.0,"Huang, Rongjie;Huang, Jiawei;Yang, Dongchao;Ren, Yi;Liu, Luping;Li, Mingze;Ye, Zhenhui;Liu, Jinglin;Yin, Xiang;Zhao, Zhou",57225203822;58096202000;57224627147;57214747414;57477524500;58096051600;57219796529;57219735077;57218450691;55959624600,60003970;60003970;60014966;60159665;60003970;60003970;60003970;60003970;60159665;60003970,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,13916-13932,"Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with “No Modality Left Behind”, for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input.",,144,0.0,,,,,
2-s2.0-85204293574,,,,Manipulating Embeddings of Stable Diffusion Prompts,cp,Conference Paper,Deckers N.,60008042;60018123;131699492;131699490,Universität Leipzig;Universität Kassel;ScaDS.AI;hessian.AI,Leipzig;Kassel;;,Germany;Germany;;,3.0,"Deckers, Niklas;Peters, Julia;Potthast, Martin",57195962977;58570679100;23012600600,60008042-131699492;60008042-131699492;131699492-60018123-131699490,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,7636-7644,"Prompt engineering is still the primary way for users of generative text-to-image models to manipulate generated images in a targeted way. Based on treating the model as a continuous function and by passing gradients between the image space and the prompt embedding space, we propose and analyze a new method to directly manipulate the embedding of a prompt instead of the prompt text. We then derive three practical interaction tools to support users with image generation: (1) Optimization of a metric defined in the image space that measures, for example, the image style. (2) Supporting a user in creative tasks by allowing them to navigate in the image space along a selection of directions of “near” prompt embeddings. (3) Changing the embedding of the prompt to include information that a user has seen in a particular seed but has difficulty describing in the prompt. Compared to prompt engineering, user-driven prompt embedding manipulation enables a more fine-grained, targeted control that integrates a user's intentions. Our user study shows that our methods are considered less tedious and that the resulting images are often preferred.",,1,0.0,,,EC,GA 101070014,European Commission
2-s2.0-85148024417,,,,Mappings for Marginal Probabilities with Applications to Models in Statistical Physics,ar,Article,Molkaraie M.,60016849,University of Toronto,Toronto,Canada,1.0,"Molkaraie, Mehdi",6505700863,60016849,2022-08-01,1 August 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"We present local mappings that relate the marginal probabilities of a global probability mass function represented by its primal normal factor graph to the corresponding marginal probabilities in its dual normal factor graph. The mapping is based on the Fourier transform of the local factors of the models. Details of the mapping are provided for the Ising model, where it is proved that the local extrema of the fixed points are attained at the phase transition of the two-dimensional nearest-neighbor Ising model. The results are further extended to the Potts model, to the clock model, and to Gaussian Markov random fields. By employing the mapping, we can transform simultaneously all the estimated marginal probabilities from the dual domain to the primal domain (and vice versa), which is advantageous if estimating the marginals can be carried out more efficiently in the dual domain. An example of particular significance is the ferromagnetic Ising model in a positive external magnetic field. For this model, there exists a rapidly mixing Markov chain (called the subgraphs–world process) to generate configurations in the dual normal factor graph of the model. Our numerical experiments illustrate that the proposed procedure can provide more accurate estimates of marginal probabilities of a global probability mass function in various settings.",Factor Graph Duality | Fourier Transform | Gaussian Markov Random Fields | Ising Model | Marginal Probability | Monte Carlo Methods | Normal Factor Graph | Phase Transition | Potts Model | Subgraphs–World Process,0,0.0,,,,,
2-s2.0-85129646683,10.1613/jair.1.13233,,,Marginal Distance and Hilbert-Schmidt Covariances-Based Independence Tests for Multivariate Functional Data,ar,Article,Krzyśko M.,60009226;60014168;60204332,"Colorado State University;Uniwersytet im. Adama Mickiewicza w Poznaniu;Calisia University - Kalisz, Poland",Fort Collins;Poznan;Kalisz,United States;Poland;Poland,3.0,"Krzyśko, Mirosław;Smaga, Łukasz;Kokoszka, Piotr",56038568100;55191296000;6603891972,60204332;60014168;60009226,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,1355-1384,"We study the pairwise and mutual independence testing problem for multivariate functional data. Using a basis representation of functional data, we reduce this problem to testing the independence of multivariate data, which may be high-dimensional. For pairwise independence, we apply tests based on distance and Hilbert-Schmidt covariances as well as their marginal versions, which aggregate these covariances for coordinates of random processes. In the case of mutual independence, we study asymmetric and symmetric aggregating measures of pairwise dependence. A theoretical justification of the test procedures is established. In extensive simulation studies and examples based on a real economic data set, we investigate and compare the performance of the tests in terms of size control and power. An important finding is that tests based on distance and Hilbert-Schmidt covariances are usually more powerful than their marginal versions under linear dependence, while the reverse is true under non-linear dependence.",,3,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85163130480,,,,Markov Chain Monte Carlo for Continuous-Time Switching Dynamical Systems,cp,Conference Paper,KÃ¶hs L.,60011226,Technische Universität Darmstadt,Darmstadt,Germany,3.0,"KÃ¶hs, Lukas;Alt, Bastian;Koeppl, Heinz",58362151000;57200517467;6603491586,60011226;60011226;60011226,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,11430-11454,"Switching dynamical systems are an expressive model class for the analysis of time-series data. As in many fields within the natural and engineering sciences, the systems under study typically evolve continuously in time, it is natural to consider continuous-time model formulations consisting of switching stochastic differential equations governed by an underlying Markov jump process. Inference in these types of models is however notoriously difficult, and tractable computational schemes are rare. In this work, we propose a novel inference algorithm utilizing a Markov Chain Monte Carlo approach. The presented Gibbs sampler allows to efficiently obtain samples from the exact continuous-time posterior processes. Our framework naturally enables Bayesian parameter estimation, and we also include an estimate for the diffusion covariance, which is oftentimes assumed fixed in stochastic differential equations models. We evaluate our framework under the modeling assumption and compare it against an existing variational inference approach.",,5,0.0,,,ERC,773196,European Research Council
2-s2.0-85174390362,,,,Master-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation in ASR with Modular Learning,cp,Conference Paper,Yu Z.,60097290;60141075,College of Computing;MIT-IBM Watson AI Lab,Atlanta;Cambridge,United States;United States,7.0,"Yu, Zhongzhi;Zhang, Yang;Qian, Kaizhi;Wan, Cheng;Fu, Yonggan;Zhang, Yongan;Lin, Yingyan",57219630400;57189256436;57190755324;57219527760;57219270287;57221363022;58048621200,60097290;60141075;60141075;60097290;60097290;60097290;60097290,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,40475-40487,"Despite the impressive performance recently achieved by automatic speech recognition (ASR), we observe two primary challenges that hinder its broader applications: (1) The difficulty of introducing scalability into the model to support more languages with limited training, inference, and storage overhead; (2) The low-resource adaptation ability that enables effective low-resource adaptation while avoiding over-fitting and catastrophic forgetting issues. Inspired by recent findings, we hypothesize that we can address the above challenges with modules widely shared across languages. To this end, we propose an ASR framework, dubbed Master-ASR, that, for the first time, simultaneously achieves strong multilingual scalability and low-resource adaptation ability thanks to its modularize-then-assemble strategy. Specifically, Master-ASR learns a small set of generalizable sub-modules and adaptively assembles them for different languages to reduce the multilingual overhead and enable effective knowledge transfer for low-resource adaptation. Extensive experiments and visualizations demonstrate that Master-ASR can effectively discover language similarity and improve multilingual and low-resource ASR performance over state-of-the-art (SOTA) methods, e.g., under multilingual-ASR, our framework achieves a 0.13∼2.41 lower character error rate (CER) with 30% smaller inference overhead over SOTA solutions on multilingual ASR and a comparable CER, with nearly 50 times fewer trainable parameters over SOTA solutions on low-resource tuning, respectively.",,11,0.0,,,SRC,,Semiconductor Research Corporation
2-s2.0-85131112835,10.1609/aaai.v36i11.21495,,,Matching Market Design with Constraints,cp,Conference Paper,Aziz H.,60028333;60011047;60001434;60170234,UNSW Sydney;Kyushu University;Budapesti Corvinus Egyetem;HUN-REN Hungarian Research Network,Sydney;Fukuoka;Budapest;Budapest,Australia;Japan;Hungary;Hungary,3.0,"Aziz, Haris;Biró, Péter;Yokoo, Makoto",24824231400;56938236700;7006722060,60028333;60170234-60001434;60011047,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,12308-12316,"Two-sided matching is an important research area that has had a major impact on the design of real-world matching markets. One consistent feature in many of the real-world applications is that they impose new feasibility constraints that lead to research challenges. We survey developments in the field of two-sided matching with various constraints, including those based on regions, diversity, multi-dimensional capacities, and matroids.",,17,1.0,all publisherfullgold,All Open Access Gold,DSTG,FA2386-20-1-4063,Defence Science and Technology Group
2-s2.0-85178999239,10.1613/JAIR.1.14861,,,Maximisation of Admissible Multi-Objective Heuristics,ar,Article,Haslum P.,60008950,The Australian National University,Canberra,Australia,2.0,"Haslum, Patrik;Wang, Ryan Xiao",10242871500;58751286100,60008950;60008950,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,619-639,"In multi-objective (MO) heuristic search, solution costs, as well as heuristic values, are sets of multi-dimensional cost vectors, representing possible non-dominated trade-offs between objectives. The maximum of two or more such vector sets, which is an important operation in creating informative admissible MO heuristics, can be defined in several ways: Geißer et al. recently proposed two MO maximum operators, the component-wise maximum (comax) and the anti-dominance maximum (admax), which represent different trade-offs between informativeness and computational cost. We show that the anti-dominance maximum is not admissibility-preserving, and propose an alternative, the “select one” maximum (somax). We also show that the comax operator is the greatest admissibility-preserving MO maximum, and briefly investigate its efficient implementation. The conclusion of our experimental results is that somax achieves a trade-off similar to that intended with admax - cheaper to compute but less informed - also when compared to an improved comax implementation.",,0,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-105018471810,,,,Mean-Square Analysis of Discretized Itô Diffusions for Heavy-tailed Sampling,ar,Article,He Y.,60016849;60026851;60014439;60019647,"University of Toronto;University of Oxford;University of California, Davis;Georgia Institute of Technology",Toronto;Oxford;Davis;Atlanta,Canada;United Kingdom;United States;United States,4.0,"He, Ye;Farghly, Tyler;Balasubramanian, Krishnakumar;Erdogdu, Murat A.",57220893434;57222412396;57213949640;56123271800,60019647;60026851;60014439;60016849,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"We analyze the complexity of sampling from a class of heavy-tailed distributions by discretizing a natural class of It\^o diffusions associated with weighted Poincaré inequalities. Based on a mean-square analysis, we establish the iteration complexity for obtaining a sample whose distribution is \epsilon close to the target distribution in the Wasserstein-2 metric. In this paper, our results take the mean-square analysis to its limits, i.e., we invariably only require that the target density has finite variance, the minimal requirement for a mean-square analysis. To obtain explicit estimates, we compute upper bounds on certain moments associated with heavy-tailed targets under various assumptions. We also provide similar iteration complexity results for the case where only function evaluations of the unnormalized target density are available by estimating the gradients using a Gaussian smoothing technique. We provide illustrative examples based on the multivariate t-distribution.",Complexity of Sampling | Euler-Marayama discretization | It\^o diffusion | multivariate t-distribution | Weighted Poincaré inequalities,4,0.0,,,CIFAR,CCF-1934568,"Simons Institute for the Theory of Computing, University of California Berkeley"
2-s2.0-85163120494,,,,Meaningfully Debugging Model Mistakes using Conceptual Counterfactual Explanations,cp,Conference Paper,Abid A.,60012708;60141508,Stanford University;Stanford Engineering,Stanford;Stanford,United States;United States,3.0,"Abid, Abubakar;Yuksekgonul, Mert;Zou, James",57038324100;57219522935;26326649300,60141508;60141508;60012708,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,66-88,"Understanding and explaining the mistakes made by trained models is critical to many machine learning objectives, such as improving robustness, addressing concept drift, and mitigating biases. However, this is often an ad hoc process that involves manually looking at the model's mistakes on many test samples and guessing at the underlying reasons for those incorrect predictions. In this paper, we propose a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). We base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate our approach on well-known pretrained models, showing that it explains the models' mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample. On two challenging medical applications, CCE generated useful insights, confirmed by clinicians, into biases and mistakes the model makes in real-world settings. The code for CCE is publicly available at https://github.com/mertyg/debug-mistakes-cce.",,53,0.0,,,NSF,,National Science Foundation
2-s2.0-85162081959,10.1613/JAIR.1.14033,,,Measuring Fairness Under Unawareness of Sensitive Attributes: A Quantification-Based Approach,ar,Article,Fabris A.,60000481;60085207;60277517,Università degli Studi di Padova;Istituto di Scienza e Tecnologie dell'Informazione A. Faedo;Max Planck Institute for Security and Privacy,Padua;Pisa;Bochum,Italy;Italy;Germany,4.0,"Fabris, Alessandro;Esuli, Andrea;Moreo, Alejandro;Sebastiani, Fabrizio",59299168900;15044356100;55064772200;7004170314,60277517-60000481;60085207;60085207;60085207,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,1117-1180,"Algorithms and models are increasingly deployed to inform decisions about people, inevitably affecting their lives. As a consequence, those in charge of developing these models must carefully evaluate their impact on different groups of people and favour group fairness, that is, ensure that groups determined by sensitive demographic attributes, such as race or sex, are not treated unjustly. To achieve this goal, the availability (awareness) of these demographic attributes to those evaluating the impact of these models is fundamental. Unfortunately, collecting and storing these attributes is often in conflict with industry practices and legislation on data minimisation and privacy. For this reason, it can be hard to measure the group fairness of trained models, even from within the companies developing them. In this work, we tackle the problem of measuring group fairness under unawareness of sensitive attributes, by using techniques from quantification, a supervised learning task concerned with directly providing group-level prevalence estimates (rather than individual-level class labels). We show that quantification approaches are particularly suited to tackle the fairness-under-unawareness problem, as they are robust to inevitable distribution shifts while at the same time decoupling the (desirable) objective of measuring group fairness from the (undesirable) side effect of allowing the inference of sensitive attributes of individuals. More in detail, we show that fairness under unawareness can be cast as a quantification problem and solved with proven methods from the quantification literature. We show that these methods outperform previous approaches to measure demographic parity in five experimental protocols, corresponding to important challenges that complicate the estimation of classifier fairness under unawareness.",,20,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,MIUR,951911,"Ministero dell’Istruzione, dell’Università e della Ricerca"
2-s2.0-85174424819,,,,Memory-Based Meta-Learning on Non-Stationary Distributions,cp,Conference Paper,Genewein T.,60031101;60111161,University of Cambridge;DeepMind Technologies Limited,Cambridge;London,United Kingdom;United Kingdom,10.0,"Genewein, Tim;Delétang, Grégoire;Ruoss, Anian;Wenliang, Li Kevin;Catt, Elliot;Dutordoir, Vincent;Grau-Moya, Jordi;Orseau, Laurent;Hutter, Marcus;Veness, Joel",55414513500;57222379515;57219505618;57203203687;57195495059;57197835812;55331830200;36146416600;11042473800;21744020700,60111161;60111161;60111161;60111161;60111161;60111161-60031101;60111161;60111161;60111161;60111161,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,11173-11195,"Memory-based meta-learning is a technique for approximating Bayes-optimal predictors. Under fairly general conditions, minimizing sequential prediction error, measured by the log loss, leads to implicit meta-learning. The goal of this work is to investigate how far this interpretation can be realized by current sequence prediction models and training regimes. The focus is on piecewise stationary sources with unobserved switching-points, which arguably capture an important characteristic of natural language and action-observation sequences in partially observable environments. We show that various types of memory-based neural models, including Transformers, LSTMs, and RNNs can learn to accurately approximate known Bayes-optimal algorithms and behave as if performing Bayesian inference over the latent switching-points and the latent parameters governing the data distribution within each segment.",,5,0.0,,,,,
2-s2.0-85137902629,10.24963/ijcai.2022/273,,,Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting,cp,Conference Paper,Chen M.,60117751;60092530;60118460;60342847;60117750;128050768,"College of Computer Science and Technology, Zhejiang University;Huawei Technologies Co., Ltd.;Alibaba Group Holding Limited;ZJU-Hangzhou Global Scientific and Technological Innovation Center;School of Software Technology, Zhejiang University;Alibaba-Zhejiang University",Hangzhou;Shenzhen;Hangzhou;Hangzhou;Ningbo;Frontier,China;China;China;China;China;China,7.0,"Chen, Mingyang;Zhang, Wen;Yao, Zhen;Chen, Xiangnan;Ding, Mengxiao;Huang, Fei;Chen, Huajun",57221242759;56902283700;57483707200;57222381549;57701214400;57210150087;35268022500,60117751;60117750;60117750;60117750;60092530;60118460;60117751-60342847-128050768,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1966-1972,"We study the knowledge extrapolation problem to embed new components (i.e., entities and relations) that come with emerging knowledge graphs (KGs) in the federated setting. In this problem, a model trained on an existing KG needs to embed an emerging KG with unseen entities and relations. To solve this problem, we introduce the meta-learning setting, where a set of tasks are sampled on the existing KG to mimic the link prediction task on the emerging KG. Based on sampled tasks, we meta-train a graph neural network framework that can construct features for unseen components based on structural information and output embeddings for them. Experimental results show that our proposed method can effectively embed unseen components and outperforms models that consider inductive settings for KGs and baselines that directly use conventional KG embedding methods.",,24,1.0,all publisherfree2read,All Open Access Bronze,NSFC,2021J190,Natural Science Foundation of Ningbo
2-s2.0-85168248630,10.1609/aaai.v37i9.26238,,,MetaZSCIL: A Meta-Learning Approach for Generalized Zero-Shot Class Incremental Learning,cp,Conference Paper,Wu Y.,60022381;60022281;60033154,Beijing Jiaotong University;Beijing University of Technology;Concordia University,Beijing;Beijing;Montreal,China;China;Canada,7.0,"Wu, Yanan;Liang, Tengfei;Feng, Songhe;Jin, Yi;Lyu, Gengyu;Fei, Haojun;Wang, Yang",57224508424;57221809397;7402531247;42761595900;57208479355;58505455600;56233737200,60022381;60022381;60022381;60022381;60022281;;60033154,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,10408-10416,"Generalized zero-shot learning (GZSL) aims to recognize samples whose categories may not have been seen at training. Standard GZSL cannot handle dynamic addition of new seen and unseen classes. In order to address this limitation, some recent attempts have been made to develop continual GZSL methods. However, these methods require end-users to continuously collect and annotate numerous seen class samples, which is unrealistic and hampers the applicability in the real-world. Accordingly, in this paper, we propose a more practical and challenging setting named Generalized Zero-Shot Class Incremental Learning (CI-GZSL). Our setting aims to incrementally learn unseen classes without any training samples, while recognizing all classes previously encountered. We further propose a bi-level meta-learning based method called MetaZSCIL to directly optimize the network to learn how to incrementally learn. Specifically, we sample sequential tasks from seen classes during the offline training to simulate the incremental learning process. For each task, the model is learned using a meta-objective such that it is capable to perform fast adaptation without forgetting. Note that our optimization can be flexibly equipped with most existing generative methods to tackle CI-GZSL. This work introduces a feature generative framework that leverages visual feature distribution alignment to produce replayed samples of previously seen classes to reduce catastrophic forgetting. Extensive experiments conducted on five widely used benchmarks demonstrate the superiority of our proposed method.",,15,1.0,all publisherfullgold,All Open Access Gold,NSERC,61872032,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85137037635,10.1613/JAIR.1.13338,,,Metric-Distortion Bounds under Limited Information,ar,Article,Anagnostides I.,60027950;60002947,Carnegie Mellon University;National Technical University of Athens (NTUA),Pittsburgh;Athens,United States;Greece,3.0,"Anagnostides, Ioannis;Fotakis, Dimitris;Patsilinakos, Panagiotis",57219120307;6602118234;57207756957,60027950;60002947;60002947,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,1449-1483,"In this work, we study the metric distortion problem in voting theory under a limited amount of ordinal information. Our primary contribution is threefold. First, we consider mechanisms that perform a sequence of pairwise comparisons between candidates. We show that a popular deterministic mechanism employed in many knockout phases yields distortion O(logm) while eliciting only m - 1 out of the Θ(m2) possible pairwise comparisons, where m represents the number of candidates. Our analysis for this mechanism leverages a powerful technical lemma developed by Kempe (AAAI '20). We also provide a matching lower bound on its distortion. In contrast, we prove that any mechanism which performs fewer than m-1 pairwise comparisons is destined to have unbounded distortion. Moreover, we study the power of deterministic mechanisms under incomplete rankings. Most notably, when agents provide their k-top preferences we show an upper bound of 6m/k + 1 on the distortion, for any k ∈ {1, 2, . . . ,m}. Thus, we substantially improve over the previous bound of 12m/k established by Kempe (AAAI '20), and we come closer to matching the best-known lower bound. Finally, we are concerned with the sample complexity required to ensure near-optimal distortion with high probability. Our main contribution is to show that a random sample of Θ(m/ϵ2) voters suffices to guarantee distortion 3 + ϵ with high probability, for any sufficiently small ϵ > 0. This result is based on analyzing the sensitivity of the deterministic mechanism introduced by Gkatzelis, Halpern, and Shah (FOCS '20). Importantly, all of our sample-complexity bounds are distribution-independent. From an experimental standpoint, we present several empirical findings on real-life voting applications, comparing the scoring systems employed in practice with a mechanism explicitly minimizing (metric) distortion. Interestingly, for our case studies, we find that the winner in the actual competition is typically the candidate who minimizes the distortion.",,7,1.0,all publisherfullgold,All Open Access Gold,ΕΛ.ΙΔ.Ε.Κ,HFRI-FM17-1424,Hellenic Foundation for Research and Innovation
2-s2.0-85172793973,,,,Mirror Sinkhorn: Fast Online Optimization on Transport Polytopes,cp,Conference Paper,Ballu M.,60031101;60111161,University of Cambridge;DeepMind Technologies Limited,Cambridge;London,United Kingdom;United Kingdom,2.0,"Ballu, Marin;Berthet, Quentin",57219588763;55874328600,60031101;60111161,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,1595-1613,"Optimal transport is an important tool in machine learning, allowing to capture geometric properties of the data through a linear program on transport polytopes. We present a single-loop optimization algorithm for minimizing general convex objectives on these domains, utilizing the principles of Sinkhorn matrix scaling and mirror descent. The proposed algorithm is robust to noise, and can be used in an online setting. We provide theoretical guarantees for convex objectives and experimental results showcasing it effectiveness on both synthetic and real-world data.",,6,0.0,,,,,
2-s2.0-85167980342,10.1609/aaai.v37i12.26766,,,Misspecification in Inverse Reinforcement Learning,cp,Conference Paper,Skalse J.,60026851,University of Oxford,Oxford,United Kingdom,2.0,"Skalse, Joar;Abate, Alessandro",57219525715;56819871700,60026851;60026851,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,15136-15143,"The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function R from a policy π. To do this, we need a model of how π relates to R. In the current literature, the most common models are optimality, Boltzmann rationality, and causal entropy maximisation. One of the primary motivations behind IRL is to infer human preferences from human behaviour. However, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are misspecified, which raises the worry that they might lead to unsound inferences if applied to real-world data. In this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function R. We also introduce a framework for reasoning about misspecification in IRL, together with formal tools that can be used to easily derive the misspecification robustness of new IRL models.",,16,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85163147636,,,,Misspecified Phase Retrieval with Generative Priors,cp,Conference Paper,Liu Z.,60019499;60026851;60017161,Chinese Academy of Sciences;University of Oxford;National University of Singapore,Beijing;Oxford;Singapore City,China;United Kingdom;Singapore,3.0,"Liu, Zhaoqiang;Wang, Xinshao;Liu, Jiulong",57194721737;57214474156;56967527700,60017161;60026851;60019499,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"In this paper, we study phase retrieval under model misspecification and generative priors. In particular, we aim to estimate an n-dimensional signal x from m i.i.d. realizations of the single index model y = f(a<sup>T</sup> x), where f is an unknown and possibly random nonlinear link function and a ∈ R<sup>n</sup> is a standard Gaussian vector. We make the assumption Cov[y, (a<sup>T</sup> x)<sup>2</sup>] ≠ 0, which corresponds to the misspecified phase retrieval problem. In addition, the underlying signal x is assumed to lie in the range of an L-Lipschitz continuous generative model with bounded kdimensional inputs. We propose a two-step approach, for which the first step plays the role of spectral initialization and the second step refines the estimated vector produced by the first step iteratively. We show that both steps enjoy a statistical rate of order p(k log L) · (log m)/m under suitable conditions. Experiments on image datasets are performed to demonstrate that our approach performs on par with or even significantly outperforms several competing methods.",,6,0.0,,,NSFC,12288201,National Natural Science Foundation of China
2-s2.0-85196123006,10.1613/jair.1.15155,,,Mitigating Value Hallucination in Dyna-Style Planning via Multistep Predecessor Models,ar,Article,Aminmansour F.,60030835;60000812,University of Alberta;Harvey Mudd College,Edmonton;Claremont,Canada;United States,6.0,"Aminmansour, Farzane;Jafferjee, Taher;Imani, Ehsan;Talvitie, Erin J.;Bowling, Michael;White, Martha",56483207600;57219748982;57197832214;36835681200;57207509564;55392368300,60030835;60030835;60030835;60000812;60030835;60030835,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,441-473,"Dyna-style reinforcement learning (RL) agents improve sample efficiency over model-free RL agents by updating the value function with simulated experience generated by an environment model. However, it is often difficult to learn accurate models of environment dynamics, and even small errors may result in failure of Dyna agents. In this paper, we highlight that one potential cause of that failure is bootstrapping off of the values of simulated states, and introduce a new Dyna algorithm to avoid this failure. We discuss a design space of Dyna algorithms, based on using successor or predecessor models—simulating forwards or backwards—and using one-step or multi-step updates. Three of the variants have been explored, but surprisingly the fourth variant has not: using predecessor models with multi-step updates. We present the Hallucinated Value Hypothesis (HVH): updating the values of real states towards values of simulated states can result in misleading action values which adversely affect the control policy. We discuss and evaluate all four variants of Dyna amongst which three update real states toward simulated states — so potentially toward hallucinated values — and our proposed approach, which does not. The experimental results provide evidence for the HVH, and suggest that using predecessor models with multi-step updates is a promising direction toward developing Dyna algorithms that are more robust to model error.",,1,1.0,all publisherfullgold,All Open Access Gold,NSERC,,Natural Sciences and Engineering Research Council of Canada
2-s2.0-105018578022,,,,Model-Free Representation Learning and Exploration in Low-Rank MDPs,ar,Article,Modi A.,60021726;60006191;60026532;60282642,Microsoft Research;Google LLC;Microsoft Corporation;Siebel School of Computing and Data Science,Redmond;Mountain View;Redmond;Urbana,United States;United States;United States;United States,5.0,"Modi, Aditya;Chen, Jinglin;Krishnamurthy, Akshay;Jiang, Nan;Agarwal, Alekh",57219482021;57204474156;55208589700;56421285700;14831143400,60026532;60282642;60021726;60282642;60006191,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"The low-rank MDP has emerged as an important model for studying representation learning and exploration in reinforcement learning. With a known representation, several model-free exploration strategies exist. In contrast, all algorithms for the unknown representation setting are model-based, thereby requiring the ability to model the full dynamics. In this work, we present the first model-free representation learning algorithms for low-rank MDPs. The key algorithmic contribution is a new minimax representation learning objective, for which we provide variants with differing tradeoffs in their statistical and computational properties. We interleave this representation learning step with an exploration strategy to cover the state space in a reward-free manner. The resulting algorithms are provably sample efficient and can accommodate general function approximation to scale to complex environments.",low-rank MDPs | Reinforcement learning | representation learning | reward-free exploration | sample complexity analysis,19,0.0,,,NSF,IIS-1452099,University of Michigan
2-s2.0-105018665438,,,,Modeling Random Networks with Heterogeneous Reciprocity,ar,Article,Cirkovic D.,60020547;60009860,Texas A&M University;Fudan University,College Station;Shanghai,United States;China,2.0,"Cirkovic, Daniel;Wang, Tiandong",57223158958;57189584110,60020547;60009860,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Reciprocity, or the tendency of individuals to mirror behavior, is a key measure that describes information exchange in a social network. Users in social networks tend to engage in different levels of reciprocal behavior. Differences in such behavior may indicate the existence of communities that reciprocate links at varying rates. In this paper, we develop methodology to model the diverse reciprocal behavior in growing social networks. In particular, we present a preferential attachment model with heterogeneous reciprocity that imitates the attraction users have for popular users, plus the heterogeneous nature by which they reciprocate links. We compare Bayesian and frequentist model fitting techniques for large networks, as well as computationally efficient variational alternatives. Cases where the number of communities is known and unknown are both considered. We apply the presented methods to the analysis of Facebook and Reddit networks where users have nonuniform reciprocal behavior patterns. The fitted model captures the heavy-tailed nature of the empirical degree distributions in the datasets and identifies multiple groups of users that differ in their tendency to reply to and receive responses to wallposts and comments.",Bayesian methods | community detection | preferential attachment | Variational inference,3,0.0,,,NSF,DMS-2210735,National Science Foundation
2-s2.0-85142012022,10.1613/jair.1.13361,,,Motion Planning Under Uncertainty with Complex Agents and Environments via Hybrid Search,ar,Article,Strawser D.,60022195,Massachusetts Institute of Technology,Cambridge,United States,2.0,"Strawser, Daniel;Williams, Brian",55324641700;7404502203,60022195;60022195,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,1-81,"As autonomous systems and robots are applied to more real world situations, they must reason about uncertainty when planning actions. Mission success oftentimes cannot be guaranteed and the planner must reason about the probability of failure. Unfortunately, computing a trajectory that satisfies mission goals while constraining the probability of failure is difficult because of the need to reason about complex, multidimensional probability distributions. Recent methods have seen success using chance-constrained, model-based planning. However, the majority of these methods can only handle simple environment and agent models. We argue that there are two main drawbacks of current approaches to goal-directed motion planning under uncertainty. First, current methods suffer from an inability to deal with expressive environment models such as 3D non-convex obstacles. Second, most planners rely on considerable simplifications when computing trajectory risk including approximating the agent’s dynamics, geometry, and uncertainty. In this article, we apply hybrid search to the risk-bound, goal-directed planning problem. The hybrid search consists of a region planner and a trajectory planner. The region planner makes discrete choices by reasoning about geometric regions that the autonomous agent should visit in order to accomplish its mission. In formulating the region planner, we propose landmark regions that help produce obstacle-free paths. The region planner passes paths through the environment to a trajectory planner; the task of the trajectory planner is to optimize trajectories that respect the agent’s dynamics and the user’s desired risk of mission failure. We discuss three approaches to modeling trajectory risk: a CDF-based approach, a sampling-based collocation method, and an algorithm named Shooting Method Monte Carlo. These models allow computation of trajectory risk with more complex environments, agent dynamics, geometries, and models of uncertainty than past approaches. A variety of 2D and 3D test cases are presented including a linear case, a Dubins car model, and an underwater autonomous vehicle. The method is shown to outperform other methods in terms of speed and utility of the solution. Additionally, the models of trajectory risk are shown to better approximate risk in simulation.",,3,1.0,all publisherfullgold,All Open Access Gold,Toyota,,Exxon Mobil Corporation
2-s2.0-85204289659,,,,MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music,cp,Conference Paper,Wang Z.,60003970;60018308;129101456;131699486,Zhejiang University;Xi'an Jiaotong University;Innovation Center of Yangtze River Delta;Ltd.,Hangzhou;Xi'an;Jiaxing City;,China;China;China;,9.0,"Wang, Zihao;Li, Shuyu;Zhang, Tao;Wang, Qi;Yu, Pengfei;Luo, Jinyang;Liu, Yan;Xi, Ming;Zhang, Kejun",58851137600;58681091900;58906269000;59093058300;57901193500;58906614400;59236945900;58905746800;36106527100,60003970-131699486;131699486-60018308;131699486;131699486;131699486;131699486;131699486;131699486;60003970-129101456,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,7771-7779,"The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music. However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks. To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music. We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics. Utilizing this method, we built a dataset with multidimensional, high-precision music annotations, the Caichong Music Dataset (CaiMD), and carefully selected 1,000 high-quality entries to serve as the test set for MuChin. Based on MuChin, we analyzed the discrepancies between professionals and amateurs in terms of music description, and empirically demonstrated the effectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed MuChin to evaluate existing music understanding models on their ability to provide colloquial descriptions of music.",,6,0.0,,,NSFC,62272409,National Natural Science Foundation of China
2-s2.0-85130387154,,,,"Multi-Agent Online Optimization with Delays: Asynchronicity, Adaptivity, and Optimism",ar,Article,Hsieh Y.G.,60104653;128127768,Université Grenoble Alpes;Criteo AI Lab,Saint Martin d'Heres;,France;,4.0,"Hsieh, Yu Guan;Iutzeler, Franck;Malick, Jérôme;Mertikopoulos, Panayotis",57218717739;55224047100;12143395000;24773619100,60104653;60104653;60104653;60104653-128127768,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"In this paper, we provide a general framework for studying multi-agent online learning problems in the presence of delays and asynchronicities. Specifically, we propose and analyze a class of adaptive dual averaging schemes in which agents only need to accumulate gradient feedback received from the whole system, without requiring any between-agent coordination. In the single-agent case, the adaptivity of the proposed method allows us to extend a range of existing results to problems with potentially unbounded delays between playing an action and receiving the corresponding feedback. In the multi-agent case, the situation is significantly more complicated because agents may not have access to a global clock to use as a reference point; to overcome this, we focus on the information that is available for producing each prediction rather than the actual delay associated with each feedback. This allows us to derive adaptive learning strategies with optimal regret bounds, even in a fully decentralized, asynchronous environment. Finally, we also analyze an ""optimistic"" variant of the proposed algorithm which is capable of exploiting the predictability of problems with a slower variation and leads to improved regret bounds.",adaptive algorithms | asynchronous methods | delayed feedback | multi-agent systems | Online learning,17,0.0,,,COST,ANR-19-CE48-0018-01,European Cooperation in Science and Technology
2-s2.0-85142062990,10.1613/jair.1.13818,,,Multi-Agent Path Finding: A New Boolean Encoding,ar,Article,Achá R.A.,60012464;60029681;60001282;60281481,Universidad de Chile;Pontificia Universidad Católica de Chile;Universidad de Concepcion;Instituto Milenio Fundamentos de los Datos,Santiago;Santiago;Concepcion;Santiago,Chile;Chile;Chile;Chile,4.0,"Achá, Roberto Asín;López, Rodrigo;Hagedorn, Sebastián;Baier, Jorge A.",25928259300;57217165679;57452785400;55635780900,60001282;60012464-60029681;60029681;60029681-60281481,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,323-350,"Multi-agent pathfinding (MAPF) is an NP-hard problem. As such, dense maps may be very hard to solve optimally. In such scenarios, compilation-based approaches, via Boolean satisfiability (SAT) and answer set programming (ASP), have been shown to outperform heuristic-search-based approaches, such as conflict-based search (CBS). In this paper, we propose a new Boolean encoding for MAPF, and show how to implement it in ASP and MaxSAT. A feature that distinguishes our encoding from existing ones is that swap and follow conflicts are encoded using binary clauses, which can be exploited by current conflict-driven clause learning (CDCL) solvers. In addition, the number of clauses used to encode swap and follow conflicts do not depend on the number of agents, allowing us to scale better. For MaxSAT, we study different ways in which we may combine the MSU3 and LSU algorithms for maximum performance. In our experimental evaluation, we used square grids, ranging from 20 × 20 to 50 × 50 cells, and warehouse maps, with a varying number of agents and obstacles. We compared against representative solvers of the state-of-the-art, including the search-based algorithm CBS, the ASP-based solver ASP-MAPF, and the branch-and-cut-and-price hybrid solver, BCP. We observe that the ASP implementation of our encoding, ASP-MAPF2 outperforms other solvers in most of our experiments. The MaxSAT implementation of our encoding, MtMS shows best performance in relatively small warehouse maps when the number of agents is large, which are the instances with closer resemblance to hard puzzle-like problems.",,7,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85181806856,,,,Multi-Consensus Decentralized Accelerated Gradient Descent,ar,Article,Ye H.,60009860;60018308;60008928;60008592,Fudan University;Xi'an Jiaotong University;The Hong Kong Polytechnic University;Hong Kong University of Science and Technology,Shanghai;Xi'an;Hong Kong;Hong Kong,China;China;Hong Kong;Hong Kong,4.0,"Ye, Haishan;Luo, Luo;Zhou, Ziang;Zhang, Tong",57192661599;56022486000;57218845400;7404373332,60018308;60009860;60008928;60008592,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"This paper considers the decentralized convex optimization problem, which has a wide range of applications in large-scale machine learning, sensor networks, and control theory. We propose novel algorithms that achieve optimal computation complexity and near optimal communication complexity. Our theoretical results give affirmative answers to the open problem on whether there exists an algorithm that can achieve a communication complexity (nearly) matching the lower bound depending on the global condition number instead of the local one. Furthermore, the linear convergence of our algorithms only depends on the strong convexity of global objective and it does not require the local functions to be convex. The design of our methods relies on a novel integration of well-known techniques including Nesterov’s acceleration, multi-consensus and gradient-tracking. Empirical studies show the outperformance of our methods for machine learning applications.",accelerated gradient descent | composite optimization | consensus optimization | decentralized algorithm | gradient tracking,11,0.0,,,NSFC,12101491,National Natural Science Foundation of China
2-s2.0-85203831545,,,,Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions,cp,Conference Paper,Li W.,60097290,College of Computing,Atlanta,United States,4.0,"Li, Weihan;Li, Chengrui;Wang, Yule;Wu, Anqi",58719168800;57984556100;59117796100;57198629725,60097290;60097290;60097290;60097290,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,28112-28131,"Studying the complex interactions between different brain regions is crucial in neuroscience. Various statistical methods have explored the latent communication across multiple brain regions. Two main categories are the Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique strengths. The GP-based approach effectively discovers latent variables with frequency bands and communication directions. Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation. In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian Process (MRM-GP). Our work establishes a connection between an LDS and a multi-output GP that explicitly models frequencies and phase delays within the latent space of neural recordings. Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional representation, revealing communication directions across brain regions and separating oscillatory communications into different frequency bands.",,1,0.0,,,NIH,1U01NS131810,National Institutes of Health
,,,,Multi-Task Processes,,,,,,,,,,,,,,,,,,,,,,,,,13,,,,,,
2-s2.0-85205681197,,,,MultiVENT: Multilingual Videos of Events with Aligned Natural Text,cp,Conference Paper,Sanders K.,60005248,Johns Hopkins University,Baltimore,United States,4.0,"Sanders, Kate;Etter, David;Kriz, Reno;Van Durme, Benjamin",57933803500;56300590900;57207857948;26425343200,60005248;60005248;60005248;60005248,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Everyday news coverage has shifted from traditional broadcasts towards a wide range of presentation formats such as first-hand, unedited video footage. Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach models to benefit from this shift, but existing news video datasets focus on traditional news broadcasts produced for English-speaking audiences. We address this limitation by constructing MultiVENT, a dataset of multilingual, event-centric videos grounded in text documents across five target languages. MultiVENT includes both news broadcast videos and non-professional event footage, which we use to analyze the state of online news videos and how they can be leveraged to build robust, factually accurate models. Finally, we provide a model for complex, multilingual video retrieval to serve as a baseline for information retrieval using MultiVENT.",,3,0.0,,,,,
2-s2.0-85129580032,10.1613/jair.1.12699,,,Multilingual Machine Translation: Deep Analysis of Language-Specific Encoder-Decoders,ar,Article,Escolano C.,60007592,Universitat Politècnica de Catalunya,Barcelona,Spain,3.0,"Escolano, Carlos;Costa-Jussà, Marta R.;Fonollosa, José A.R.",57197861020;15519053500;55033531200,60007592;60007592;60007592,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,1535-1552,"State-of-the-art multilingual machine translation relies on a shared encoder-decoder. In this paper, we propose an alternative approach based on language-specific encoder-decoders, which can be easily extended to new languages by learning their corresponding modules. To establish a common interlingua representation, we simultaneously train N initial languages. Our experiments show that the proposed approach improves over the shared encoder-decoder for the initial languages and when adding new languages, without the need to retrain the remaining modules. All in all, our work closes the gap between shared and language-specific encoder-decoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings.",,5,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,H2020,947657,Horizon 2020 Framework Programme
2-s2.0-105018671725,,,,Multiple Descent in the Multiple Random Feature Model,ar,Article,Meng X.,60006541;60108865,"The University of Hong Kong;The Chinese University of Hong Kong, Shenzhen",Hong Kong;Shenzhen,Hong Kong;China,3.0,"Meng, Xuran;Yao, Jianfeng;Cao, Yuan",57706543900;7403503451;57211609976,60006541;60108865;60006541,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Recent works have demonstrated a double descent phenomenon in over-parameterized learning. Although this phenomenon has been investigated by recent works, it has not been fully understood in theory. In this paper, we investigate the multiple descent phenomenon in a class of multi-component prediction models. We first consider a “double random feature model” (DRFM) concatenating two types of random features, and study the excess risk achieved by the DRFM in ridge regression. We calculate the precise limit of the excess risk under the high dimensional framework where the training sample size, the dimension of data, and the dimension of random features tend to infinity proportionally. Based on the calculation, we further theoretically demonstrate that the risk curves of DRFMs can exhibit triple descent. We then provide a thorough experimental study to verify our theory. At last, we extend our study to the “multiple random feature model” (MRFM), and show that MRFMs ensembling K types of random features may exhibit (K + 1)-fold descent. Our analysis points out that risk curves with a specific number of descent generally exist in learning multi-component prediction models.",double random feature model | excess risk | multiple descent | multiple random feature model | Over-parameterization,2,0.0,,,,12350710179,
2-s2.0-85201302699,,,,MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models,cp,Conference Paper,Zhang Y.,60022109;60195969;131699491,Queen Mary University of London;Mohamed Bin Zayed University of Artificial Intelligence;Sony AI,London;Abu Dhabi;,United Kingdom;United Arab Emirates;,8.0,"Zhang, Yixiao;Ikemiya, Yukara;Xia, Gus;Murata, Naoki;Martínez-Ramírez, Marco A.;Liao, Wei Hsiang;Mitsufuji, Yuki;Dixon, Simon",57478996600;56303689300;57208212250;57202700032;57201131755;57311228400;55967134100;7201479437,60022109;131699491;60195969;131699491;131699491;131699491;131699491;60022109,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,7805-7813,"Recent advances in text-to-music generation models have opened new avenues in musical creativity. However, music generation usually involves iterative refinements, and how to edit the generated music remains a significant challenge. This paper introduces a novel approach to the editing of music generated by such models, enabling the modification of specific attributes, such as genre, mood and instrument, while maintaining other aspects unchanged. Our method transforms text editing to latent space manipulation while adding an extra constraint to enforce consistency. It seamlessly integrates with existing pretrained text-to-music diffusion models without requiring additional training. Experimental results demonstrate superior performance over both zero-shot and certain supervised baselines in style and timbre transfer evaluations. We also showcase the practical applicability of our approach in real-world music editing scenarios.",,9,0.0,,,UKRI,,UK Research and Innovation
2-s2.0-85134910981,,,,N-Penetrate: Active Learning of Neural Collision Handler for Complex 3D Mesh Deformations,cp,Conference Paper,Tan Q.,60114181;60151568;127299596,Tencent;Department of Computer Science;Meta Reality Labs Research,Shenzhen;College Park;Meta,China;United States;United States,5.0,"Tan, Qingyang;Pan, Zherong;Smith, Breannan;Shiratori, Takaaki;Manocha, Dinesh",57205541515;55785808300;55476077100;7005014907;35452853100,60151568;60114181;127299596;127299596;60151568,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,21037-21049,"We present a robust learning algorithm to detect and handle collisions in 3D deforming meshes. We first train a neural network to detect collisions and then use a numerical optimization algorithm to resolve penetrations guided by the network. Our learned collision handler can resolve collisions for unseen, high-dimensional meshes with thousands of vertices. To obtain stable network performance in such large and unseen spaces, we apply active learning by progressively inserting new collision data based on the network inferences. We automatically label these new data using an analytical collision detector and progressively fine-tune our detection networks. We evaluate our method for collision handling of complex, 3D meshes coming from several datasets with different shapes and topologies, including datasets corresponding to dressed and undressed human poses, cloth simulations, and human hand poses acquired using multi-view capture systems.",,3,0.0,,,ARO,W911NF1910069,Army Research Office
2-s2.0-85199898757,,,,NAGPHORMER: A TOKENIZED GRAPH TRANSFORMER FOR NODE CLASSIFICATION IN LARGE GRAPHS,cp,Conference Paper,Chen J.,60025761,Huazhong University of Science and Technology,Wuhan,China,4.0,"Chen, Jinsong;Gao, Kaiyuan;Li, Gaichao;He, Kun",57771352300;57771352400;57771402500;57204773777,60025761;60025761;60025761;60025761,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"The graph Transformer emerges as a new architecture and has shown superior performance on various graph mining tasks. In this work, we observe that existing graph Transformers treat nodes as independent tokens and construct a single long sequence composed of all node tokens so as to train the Transformer model, causing it hard to scale to large graphs due to the quadratic complexity on the number of nodes for the self-attention computation. To this end, we propose a Neighborhood Aggregation Graph Transformer (NAGphormer) that treats each node as a sequence containing a series of tokens constructed by our proposed Hop2Token module. For each node, Hop2Token aggregates the neighborhood features from different hops into different representations and thereby produces a sequence of token vectors as one input. In this way, NAGphormer could be trained in a mini-batch manner and thus could scale to large graphs. Moreover, we mathematically show that as compared to a category of advanced Graph Neural Networks (GNNs), the decoupled Graph Convolutional Network, NAGphormer could learn more informative node representations from the multi-hop neighborhoods. Extensive experiments on benchmark datasets from small to large are conducted to demonstrate that NAGphormer consistently outperforms existing graph Transformers and mainstream GNNs. Code is available at https://github.com/JHL-HUST/NAGphormer.",,86,0.0,,,NSFC,"U22B2017,62076105",National Natural Science Foundation of China
2-s2.0-85196966928,,,,NEGATIVE LABEL GUIDED OOD DETECTION WITH PRETRAINED VISION-LANGUAGE MODELS,cp,Conference Paper,Jiang X.,60026553;60025709;60023932;60105683;60032955;60014347;60195969,University of Melbourne;The University of Sydney;University of Technology Sydney;Southern University of Science and Technology;Huazhong Agricultural University;Hong Kong Baptist University;Mohamed Bin Zayed University of Artificial Intelligence,Melbourne;Sydney;Sydney;Shenzhen;Wuhan;Hong Kong;Abu Dhabi,Australia;Australia;Australia;China;China;Hong Kong;United Arab Emirates,7.0,"Jiang, Xue;Liu, Feng;Fang, Zhen;Chen, Hong;Liu, Tongliang;Zheng, Feng;Han, Bo",58088227100;57118471000;57211269319;56658950800;56297951800;57210574274;57191281044,60105683-60014347;60026553;60023932;60032955;60195969-60025709;60105683;60014347,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection methods take into account information from the text modality. In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the OOD score collaborated with negative labels. Theoretical analysis helps to understand the mechanism of negative labels. Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection benchmarks and generalizes well on multiple VLM architectures. Furthermore, our method NegLabel exhibits remarkable robustness against diverse domain shifts. The codes are available at https://github.com/tmlr-group/NegLabel.",,24,0.0,,,NSFC,LP220100527,National Natural Science Foundation of China
2-s2.0-85200597506,,,,NEUR2RO: NEURAL TWO-STAGE ROBUST OPTIMIZATION,cp,Conference Paper,Dumouchelle J.,60016849;60002483;60006288,University of Toronto;Universiteit van Amsterdam;Delft University of Technology,Toronto;Amsterdam;Delft,Canada;Netherlands;Netherlands,4.0,"Dumouchelle, Justin;Julien, Esther;Kurtz, Jannis;Khalil, Elias B.",57221323603;57886015300;57189342897;58904984100,60016849;60006288;60002483;60016849,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design. Embedding our neural network into CCG yields high-quality solutions quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital budgeting. For knapsack, Neur2RO finds solutions that are within roughly 2% of the best-known values in a few seconds compared to the three hours of the state-of-the-art exact branch-and-price algorithm; for larger and more complex instances, Neur2RO finds even better solutions. For capital budgeting, Neur2RO outperforms three variants of the k-adaptability algorithm, particularly on the largest instances, with a 10 to 100-fold reduction in solution time. Our code and data are available at https://github.com/khalil-research/Neur2RO.",,6,0.0,,,NSERC,,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85126978353,,,,NEURAL COLLAPSE UNDER MSE LOSS: PROXIMITY TO AND DYNAMICS ON THE CENTRAL PATH,cp,Conference Paper,Han X.Y.,60016849;60012708;60007776,University of Toronto;Stanford University;Cornell University,Toronto;Stanford;Ithaca,Canada;United States;United States,3.0,"Han, X. Y.;Papyan, Vardan;Donoho, David L.",57219428378;57192940042;7006144847,60007776;60016849;60012708,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and five benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: here. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC.",,58,0.0,,,DMS,1407813,Division of Mathematical Sciences
2-s2.0-85199911307,,,,NOISE-ROBUST DE-DUPLICATION AT SCALE,cp,Conference Paper,Silcock E.,60025038;60006303;60000650,"University of California, Berkeley;Harvard Faculty of Arts and Sciences;Harvard College",Berkeley;Cambridge;Cambridge,United States;United States;United States,4.0,"Silcock, Emily;D'Amico-Wong, Luca;Yang, Jinglin;Dell, Melissa",57933767900;57933707200;57933748500;35291870800,60006303;60000650;60025038;60006303,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Identifying near duplicates within large, noisy text corpora has a myriad of applications that range from de-duplicating training datasets, reducing privacy risk, and evaluating test set leakage, to identifying reproduced news articles and literature within large corpora. Across these diverse applications, the overwhelming majority of work relies on N-grams. Limited efforts have been made to evaluate how well N-gram methods perform, in part because it is unclear how one could create an unbiased evaluation dataset for a massive corpus. This study uses the unique timeliness of historical news wires to create a 27,210 document dataset, with 122,876 positive duplicate pairs, for studying noise-robust de-duplication. The time-sensitivity of news makes comprehensive hand labelling feasible - despite the massive overall size of the corpus - as duplicates occur within a narrow date range. The study then develops and evaluates a range of de-duplication methods: hashing and N-gram overlap (which predominate in the literature), a contrastively trained bi-encoder, and a “re-rank” style approach combining a bi- and cross-encoder. The neural approaches significantly outperform hashing and N-gram overlap. We show that the bi-encoder scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours. We also apply our pre-trained model to the RealNews and patent portions of C4 (Colossal Clean Crawled Corpus), illustrating that a neural approach can identify many near duplicates missed by hashing, in the presence of various types of noise. The public release of our NEWS-COPY de-duplication dataset, codebase, and the pre-trained models will facilitate further research and applications.",,8,0.0,,,,,
2-s2.0-85163162997,,,,Natural image synthesis for the retina with variational information bottleneck representation,cp,Conference Paper,Rahmani B.,121763255,EPFL,Ecublens,Switzerland,3.0,"Rahmani, Babak;Psaltis, Demetri;Moser, Christophe",57191407531;7102353967;7102765376,;121763255;121763255,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"In the early visual system, high dimensional natural stimuli are encoded into the trains of neuronal spikes that transmit the information to the brain to produce perception. However, is all the visual scene information required to explain the neuronal responses? In this work, we search for answers to this question by developing a joint model of the natural visual input and neuronal responses using the Information Bottleneck (IB) framework that can represent features of the input data into a few latent variables that play a role in the prediction of the outputs. The correlations between data samples acquired from published experiments on ex-vivo retinas are accounted for in the model by a Gaussian Process (GP) prior. The proposed IB-GP model performs competitively to the state-of-the-art feedforward convolutional networks in predicting spike responses to natural stimuli. Finally, the IB-GP model is used in a closed-loop iterative process to obtain reduced-complexity inputs that elicit responses as elicited by the original stimuli. We found three properties of the retina's IB-GP model. First, the reconstructed stimuli from the latent variables show robustness in spike prediction across models. Second, surprisingly the dynamics of the high-dimensional stimuli and RGCs' responses are very well represented in the embeddings of the IB-GP model. Third, the minimum stimuli consist of different patterns: Gabor-type locally high-frequency filters, on- and off-center Gaussians, or a mixture of both. Overall, this work demonstrates that the IB-GP model provides a principled approach for joint learning of the stimuli and retina codes, capturing dynamics of the stimuli-RGCs in the latent space which could help better understand the computation of the early visual system.",,2,0.0,,,,,
2-s2.0-85162129252,10.1613/JAIR.1.14020,,,Negative Human Rights as a Basis for Long-term AI Safety and Regulation,ar,Article,Bajgar O.,60026851;60016605,University of Oxford;Charles University,Oxford;Prague,United Kingdom;Czech Republic,2.0,"Bajgar, Ondrej;Horenovsky, Jan",57193222455;58203670000,60026851;60016605,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,1043-1075,"If autonomous AI systems are to be reliably safe in novel situations, they will need to incorporate general principles guiding them to recognize and avoid harmful behaviours. Such principles may need to be supported by a binding system of regulation, which would need the underlying principles to be widely accepted. They should also be specific enough for technical implementation. Drawing inspiration from law, this article explains how negative human rights could fulfil the role of such principles and serve as a foundation both for an international regulatory system and for building technical safety constraints for future AI systems.",,10,1.0,all publisherfullgold,All Open Access Gold,UK,EP/S024050/1,Univerzita Karlova v Praze
2-s2.0-85204287028,,,,Negative Prompt Driven Complementary Parallel Representation for Open-World 3D Object Retrieval,cp,Conference Paper,Xu Y.,60025278,Tsinghua University,Beijing,China,3.0,"Xu, Yang;Feng, Yifan;Gao, Yue",59227983000;57207868951;57198714223,60025278;60025278;60025278,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1498-1506,"The limited availability of supervised labels (positive information) poses a notable challenge for open-world retrieval. However, negative information is more easily obtained but remains underexploited in current methods. In this paper, we introduce the Negative Prompt Driven Complementary Parallel Representation (NPCP) framework, which navigates the complexities of open-world retrieval through the lens of Negative Prompts. Specifically, we employ the Parallel Exclusive Embedding (PEE) module to effectively utilize the prompt information, bilaterally capturing both explicit negative and implicit positive signals. To address the challenges of embedding unification and generalization, our method leverages high-order correlations among objects through the Complementary Structure Tuning (CST) module, by constructing a complementary hypergraph based on bi-directional and cross-category correlations. We have developed four multimodal datasets for open-world 3D object retrieval with negative prompts: NPMN, NPAB, NPNT, and NPES. Extensive experiments and ablation studies on these four benchmarks demonstrate the superiority of our method over current state-of-the-art approaches.",,4,0.0,,,NSFC,62088102,National Natural Science Foundation of China
2-s2.0-85148077665,,,,Network Regression with Graph Laplacians,ar,Article,Zhou Y.,60014439,"University of California, Davis",Davis,United States,2.0,"Zhou, Yidong;Müller, Hans Georg",57268046400;7404945300,60014439;60014439,2022-11-01,1 November 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,320,,"Network data are increasingly available in various research fields, motivating statistical analysis for populations of networks, where a network as a whole is viewed as a data point. The study of how a network changes as a function of covariates is often of paramount interest. However, due to the non-Euclidean nature of networks, basic statistical tools available for scalar and vector data are no longer applicable. This motivates an extension of the notion of regression to the case where responses are network data. Here we propose to adopt conditional Fréchet means implemented as M-estimators that depend on weights derived from both global and local least squares regression, extending the Fréchet regression framework to networks that are quantified by their graph Laplacians. The challenge is to characterize the space of graph Laplacians to justify the application of Fréchet regression. This characterization then leads to asymptotic rates of convergence for the corresponding M-estimators by applying empirical process methods. We demonstrate the usefulness and good practical performance of the proposed framework with simulations and with network data arising from resting-state fMRI in neuroimaging, as well as New York taxi records.",Fréchet mean | graph Laplacian | neuroimaging | power metric | sample of networks,13,0.0,,,NSF,DMS-2014626,National Science Foundation
2-s2.0-85124970182,10.1613/JAIR.1.13052,,,Neural Character-Level Syntactic Parsing for Chinese,ar,Article,Li Z.,60025084;60136640;60118847,Shanghai Jiao Tong University;School of Computer Science;School of Computing and Information Systems,Shanghai;Pittsburgh;Melbourne,China;United States;Australia,6.0,"Li, Zuchao;Zhou, Junru;Zhao, Hai;Zhang, Zhisong;Li, Haonan;Ju, Yuqi",57207860812;57212065601;55715822700;57189263574;57212087757;57205539591,60025084;60025084;60025084;60136640;60118847;60025084,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,461-509,"In this work, we explore character-level neural syntactic parsing for Chinese with two typical syntactic formalisms: the constituent formalism and a dependency formalism based on a newly released character-level dependency treebank. Prior works in Chinese parsing have struggled with whether to define words when modeling character interactions. We choose to integrate full character-level syntactic dependency relationships using neural representations from character embeddings and richer linguistic syntactic information from human-annotated character-level Parts-Of-Speech and dependency labels. This has the potential to better understand the deeper structure of Chinese sentences and provides a better structural formalism for avoiding unnecessary structural ambiguities. Specifically, we first compare two different character-level syntax annotation styles: constituency and dependency. Then, we discuss two key problems for character-level parsing: (1) how to combine constituent and dependency syntactic structure in full character-level trees and (2) how to convert from character-level to word-level for both constituent and dependency trees. In addition, we also explore several other key parsing aspects, including different character-level dependency annotations and joint learning of Parts-Of-Speech and syntactic parsing. Finally, we evaluate our models on the Chinese Penn Treebank (CTB) and our published Shanghai Jiao Tong University Chinese Character Dependency Treebank (SCDT). The results show the effectiveness of our model on both constituent and dependency parsing. We further provide empirical analysis and suggest several directions for future study.",,3,1.0,all publisherfullgold,All Open Access Gold,NSFC,61733011,National Natural Science Foundation of China
2-s2.0-105000531910,,,,NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human Connectomes,cp,Conference Paper,Wei Z.,60025111,The University of North Carolina at Chapel Hill,Chapel Hill,United States,4.0,"Wei, Ziquan;Dan, Tingting;Ding, Jiaqi;Wu, Guorong",57195631057;57216895686;57221459368;13905214600,60025111;60025111;60025111;60025111,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Although modern imaging technologies allow us to study connectivity between two distinct brain regions in-vivo, an in-depth understanding of how anatomical structure supports brain function and how spontaneous functional fluctuations emerge remarkable cognition is still elusive. Meanwhile, tremendous efforts have been made in the realm of machine learning to establish the nonlinear mapping between neuroimaging data and phenotypic traits. However, the absence of neuroscience insight in the current approaches poses significant challenges in understanding cognitive behavior from transient neural activities. To address this challenge, we put the spotlight on the coupling mechanism of structural connectivity (SC) and functional connectivity (FC) by formulating such network neuroscience question into an expressive graph representation learning problem for high-order topology. Specifically, we introduce the concept of topological detour to characterize how a ubiquitous instance of FC (direct link) is supported by neural pathways (detour) physically wired by SC, which forms a cyclic loop interacted by brain structure and function. In the cliché of machine learning, the multi-hop detour pathway underlying SC-FC coupling allows us to devise a novel multi-head self-attention mechanism within Transformer to capture multi-modal feature representation from paired graphs of SC and FC. Taken together, we propose a biological-inspired deep model, coined as NeuroPath, to find putative connectomic feature representations from the unprecedented amount of neuroimages, which can be plugged into various downstream applications such as task recognition and disease diagnosis. We have evaluated NeuroPath on large-scale public datasets including Human Connectome Project (HCP) and UK Biobank (UKB) under different experiment settings of supervised and zero-shot learning, where the state-of-the-art performance by our NeuroPath indicates great potential in network neuroscience.",,2,0.0,,,NIH,AG068399,National Institutes of Health
2-s2.0-85163105648,10.1016/j.patrec.2015.06.030,,,Neuron Dependency Graphs: A Causal Abstraction of Neural Networks,cp,Conference Paper,Hu Y.,60145790,Department of Computer Science,Ames,United States,2.0,"Hu, Yaojie;Tian, Jin",57668083600;56314888000,60145790;60145790,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,9020-9040,"We discover that neural networks exhibit approximate logical dependencies among neurons, and we introduce Neuron Dependency Graphs (NDG) that extract and present them as directed graphs. In an NDG, each node corresponds to the boolean activation value of a neuron, and each edge models an approximate logical implication from one node to another. We show that the logical dependencies extracted from the training dataset generalize well to the test set. In addition to providing symbolic explanations to the neural network's internal structure, NDGs can represent a Structural Causal Model. We empirically show that an NDG is a causal abstraction of the corresponding neural network that “unfolds” the same way under causal interventions using the theory by Geiger et al. (2021a). Code is available at https://github.com/phimachine/ndg.",,5,0.0,,,ONR,N000141712140,Office of Naval Research
2-s2.0-85137870796,10.24963/ijcai.2022/584,,,Neutral Utterances are Also Causes: Enhancing Conversational Causal Emotion Entailment with Social Commonsense Knowledge,cp,Conference Paper,Li J.,60027363;60273040;60114181,University of Chinese Academy of Sciences;Institute of Information Engineering;Tencent,Beijing;Beijing;Shenzhen,China;China;China,8.0,"Li, Jiangnan;Meng, Fandong;Lin, Zheng;Liu, Rui;Fu, Peng;Cao, Yanan;Wang, Weiping;Zhou, Jie",57221631302;55847567500;54581207500;57216694134;58725688100;55431244700;57272010000;57211746430,60273040-60027363;60114181;60273040-60027363;60273040-60027363;60273040;60273040-60027363;60273040;60114181,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4209-4215,"Conversational Causal Emotion Entailment aims to detect causal utterances for a non-neutral targeted utterance from a conversation. In this work, we build conversations as graphs to overcome implicit contextual modelling of the original entailment style. Following the previous work, we further introduce the emotion information into graphs. Emotion information can markedly promote the detection of causal utterances whose emotion is the same as the targeted utterance. However, it is still hard to detect causal utterances with different emotions, especially neutral ones. The reason is that models are limited in reasoning causal clues and passing them between utterances. To alleviate this problem, we introduce social commonsense knowledge (CSK) and propose a Knowledge Enhanced Conversation graph (KEC). KEC propagates the CSK between two utterances. As not all CSK is emotionally suitable for utterances, we therefore propose a sentiment-realized knowledge selecting strategy to filter CSK. To process KEC, we further construct the Knowledge Enhanced Directed Acyclic Graph networks. Experimental results show that our method outperforms baselines and infers more causes with different emotions from the targeted utterance.",,27,1.0,all publisherfree2read,All Open Access Bronze,NSFC,61906187,National Natural Science Foundation of China
2-s2.0-85200608396,,,,Never Train from Scratch: FAIR COMPARISON OF LONG-SEQUENCE MODELS REQUIRES DATA-DRIVEN PRIORS,cp,Conference Paper,Amos I.,60005681;60011048,Tel Aviv University;IBM Research,Tel Aviv-Yafo;Yorktown Heights,Israel;United States,3.0,"Amos, Ido;Berant, Jonathan;Gupta, Ankit",57427547600;52163188700;57050773300,60005681;60005681;60011048,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using only the downstream task data, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.",,11,0.0,,,ERC,DELPHI 802800,European Research Council
2-s2.0-85170373417,10.24963/ijcai.2023/302,,,New Algorithms for the Fair and Efficient Allocation of Indivisible Chores,cp,Conference Paper,Garg J.,60000745,University of Illinois Urbana-Champaign,Urbana,United States,3.0,"Garg, Jugal;Murhekar, Aniket;Qin, John",36677118000;57195278995;57323692000,60000745;60000745;60000745,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,2710-2718,"We study the problem of fairly and efficiently allocating indivisible chores among agents with additive disutility functions. We consider the widely-used envy-based fairness properties of EF1 and EFX, in conjunction with the efficiency property of fractional Pareto-optimality (fPO). Existence (and computation) of an allocation that is simultaneously EF1/EFX and fPO are challenging open problems, and we make progress on both of them. We show existence of an allocation that is • EF1+fPO, when there are three agents, • EF1+fPO, when there are at most two disutility functions, • EFX+fPO, for three agents with bivalued disutility functions. These results are constructive, based on strongly polynomial-time algorithms. We also investigate non-existence and show that an allocation that is EFX+fPO need not exist, even for two agents.",,15,1.0,all publisherfullgold,All Open Access Gold,NSF,CCF-1942321,National Science Foundation
2-s2.0-85170395727,10.24963/ijcai.2023/214,,,New Bounds and Constraint Programming Models for the Weighted Vertex Coloring Problem,cp,Conference Paper,Goudet O.,60012772,Université d’Angers,Angers,France,3.0,"Goudet, Olivier;Grelier, Cyril;Lesaint, David",56495360900;57456638100;6601970038,60012772;60012772;60012772,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,1927-1934,"This paper addresses the weighted vertex coloring problem (WVCP) which is an NP-hard variant of the graph coloring problem with various applications. Given a vertex-weighted graph, the problem consists of partitioning vertices in independent sets (colors) so as to minimize the sum of the maximum weights of the colors. We first present an iterative procedure to reduce the size of WVCP instances and prove new upper bounds on the objective value and the number of colors. Alternative constraint programming models are then introduced which rely on primal and dual encodings of the problem and use symmetry breaking constraints. A large number of experiments are conducted on benchmark instances. We analyze the impact of using specific bounds to reduce the search space and speed up the exact resolution of instances. New optimality proofs are reported for some benchmark instances.",,4,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85189624567,10.1609/aaai.v38i13.29412,,,No Internal Regret with Non-convex Loss Functions,cp,Conference Paper,Sharma D.,60027950,Carnegie Mellon University,Pittsburgh,United States,1.0,"Sharma, Dravyansh",57189371492,60027950,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,13.0,,14919-14927,"Internal regret is a measure of performance of an online learning algorithm, which measures the change in performance by substituting every occurrence of a given action i by an alternative action j. Algorithms for minimizing internal regret are known for the finite experts setting, including a general reduction to the problem of minimizing external regret for this case. The reduction however crucially depends on the finiteness of the action space. In this work we approach the problem of minimizing internal regret for a continuous action space. For the full information setting, we show how to obtain Õ(<sup>√</sup>T) internal regret for the class of Lipschitz functions, as well as non-Lipschitz dispersed functions, i.e. the non-Lipschitzness may not concentrate in a small region of the action space. We also consider extensions to partial feedback settings, and again obtain sublinear internal regret. Finally we discuss applications of internal regret minimization over continuous spaces to correlated equilibria in pricing problems and auction design, as well as to data-driven hyperparameter tuning.",,5,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85147735789,10.1609/aaai.v36i6.20561,,,NoiseGrad — Enhancing Explanations by Introducing Stochasticity to Model Weights,cp,Conference Paper,Bykov K.,60011604;60277287;132139046,Technische Universität Berlin;RIKEN Center for Advanced Intelligence Project;Understandable Machine Intelligence Lab.,Berlin;Tokyo;,Germany;Japan;,4.0,"Bykov, Kirill;Hedström, Anna;Nakajima, Shinichi;Höhne, Marina M.C.",57219761451;57339350700;7403109708;57219765782,60011604-132139046;60011604-132139046;60011604-60277287;60011604-132139046,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,6132-6140,"Many efforts have been made for revealing the decision-making process of black-box learning machines such as deep neural networks, resulting in useful local and global explanation methods. For local explanation, stochasticity is known to help: a simple method, called SmoothGrad, has improved the visual quality of gradient-based attribution by adding noise to the input space and averaging the explanations of the noisy inputs. In this paper, we extend this idea and propose NoiseGrad that enhances both local and global explanation methods. Specifically, NoiseGrad introduces stochasticity in the weight parameter space, such that the decision boundary is perturbed. NoiseGrad is expected to enhance the local explanation, similarly to SmoothGrad, due to the dual relationship between the input perturbation and the decision boundary perturbation. We evaluate NoiseGrad and its fusion with SmoothGrad — FusionGrad — qualitatively and quantitatively with several evaluation criteria, and show that our novel approach significantly outperforms the baseline methods. Both NoiseGrad and FusionGrad are method-agnostic and as handy as SmoothGrad using a simple heuristic for the choice of the hyperparameter setting without the need of fine-tuning.",,22,1.0,all publisherfullgold,All Open Access Gold,BMBF,01IS18025A,Bundesministerium für Bildung und Forschung
2-s2.0-85169909581,,,,Non-Asymptotic Guarantees for Robust Statistical Learning under Infinite Variance Assumption,ar,Article,Xu L.,60014966;60013789;60022317;112496222,Peking University;Beihang University;University of Macau;Technology Research Institute,Beijing;Beijing;Taipa;Zhuhai,China;China;Macao;China,4.0,"Xu, Lihu;Yao, Fang;Yao, Qiuran;Zhang, Huiming",54384383300;7102637741;57423339400;56421651600,60022317-112496222;60014966;60022317-112496222;60022317-112496222-60013789,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"There has been a surge of interest in developing robust estimators for models with heavy-tailed and bounded variance data in statistics and machine learning, while few works impose unbounded variance. This paper proposes two types of robust estimators, the ridge log-truncated M-estimator and the elastic net log-truncated M-estimator. The first estimator is applied to convex regressions such as quantile regression and generalized linear models, while the other one is applied to high dimensional non-convex learning problems such as regressions via deep neural networks. Simulations and real data analysis demonstrate the robustness of log-truncated estimations over standard estimations.",data with infinite variance | excess risk bounds | robust deep neural network (DNN) regressions | robust elastic net regressions | robust non-convex regressions | robust ridge regressions,13,0.0,,,,,
2-s2.0-85178938908,,,,Non-adversarial training of Neural SDEs with signature kernel scores,cp,Conference Paper,Issa Z.,60026851;60015150;60011520;60111768;60112639,University of Oxford;Imperial College London;King's College London;The Alan Turing Institute;Oxford-Man Institute of Quantitative Finance,Oxford;London;London;London;Oxford,United Kingdom;United Kingdom;United Kingdom;United Kingdom;United Kingdom,4.0,"Issa, Zacharia;Horvath, Blanka;Lemercier, Maud;Salvi, Cristopher",57326234700;57191824923;57219765435;57218718187,60011520;60026851-60112639-60111768;60026851-60111768;60111768-60015150,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Neural SDEs are continuous-time generative models for sequential data. State-of-the-art performance for irregular time series generation has been previously obtained by training these models adversarially as GANs. However, as typical for GAN architectures, training is notoriously unstable, often suffers from mode collapse, and requires specialised techniques such as weight clipping and gradient penalty to mitigate these issues. In this paper, we introduce a novel class of scoring rules on pathspace based on signature kernels and use them as objective for training Neural SDEs non-adversarially. By showing strict properness of such kernel scores and consistency of the corresponding estimators, we provide existence and uniqueness guarantees for the minimiser. With this formulation, evaluating the generator-discriminator pair amounts to solving a system of linear path-dependent PDEs which allows for memory-efficient adjoint-based backpropagation. Moreover, because the proposed kernel scores are well-defined for paths with values in infinite dimensional spaces of functions, our framework can be easily extended to generate spatiotemporal data. Our procedure permits conditioning on a rich variety of market conditions and significantly outperforms alternative ways of training Neural SDEs on a variety of tasks including the simulation of rough volatility models, the conditional probabilistic forecasts of real-world forex pairs where the conditioning variable is an observed past trajectory, and the mesh-free generation of limit order book dynamics.",,11,0.0,,,EPSRC,EP/S026347/1,Engineering and Physical Sciences Research Council
2-s2.0-85173078097,,,,Non-stationary Online Learning with Memory and Non-stochastic Control,ar,Article,Zhao P.,60033100;60142701,Nanjing University;The Robert Mehrabian College of Engineering,Nanjing;Santa Barbara,China;United States,4.0,"Zhao, Peng;Yan, Yu Hu;Wang, Yu Xiang;Zhou, Zhi Hua",57199758885;57219787283;58912060300;57218666267,60033100;60033100;60142701;60033100,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,206,,"We study the problem of Online Convex Optimization (OCO) with memory, which allows loss functions to depend on past decisions and thus captures temporal effects of learning problems. In this paper, we introduce dynamic policy regret as the performance measure to design algorithms robust to non-stationary environments, which competes algorithms’ decisions with a sequence of changing comparators. We propose a novel algorithm for OCO with memory that provably enjoys an optimal dynamic policy regret in terms of time horizon, non-stationarity measure, and memory length. The key technical challenge is how to control the switching cost, the cumulative movements of player’s decisions, which is neatly addressed by a novel switching-cost-aware online ensemble approach equipped with a new meta-base decomposition of dynamic policy regret and a careful design of meta-learner and base-learner that explicitly regularizes the switching cost. The results are further applied to tackle non-stationarity in online non-stochastic control (Agarwal et al., 2019), i.e., controlling a linear dynamical system with adversarial disturbance and convex cost functions. We derive a novel gradient-based controller with dynamic policy regret guarantees, which is the first controller provably competitive to a sequence of changing policies for online non-stochastic control.",dynamic policy regret | non-stationary environments | online convex optimization with memory | online ensemble | online learning | online non-stochastic control,5,0.0,,,NSFC,61921006,National Postdoctoral Program for Innovative Talents
2-s2.0-105018669423,,,,Nonparametric Inference under B-bits Quantization,ar,Article,Li K.,60029251;60021285;60022904;60002717,University of Miami;Texas Tech University;New Jersey Institute of Technology;Bristol Myers Squibb,Coral Gables;Lubbock;Newark;Princeton,United States;United States;United States;United States,4.0,"Li, Kexuan;Liu, Ruiqi;Xu, Ganggang;Shang, Zuofeng",57749136800;57211447557;57211887534;15045561600,60002717;60021285;60029251;60022904,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Statistical inference based on lossy or incomplete samples is often needed in research areas such as signal/image processing, medical image storage, remote sensing, signal transmission. In this paper, we propose a nonparametric testing procedure based on samples quantized to B bits through a computationally efficient algorithm. Under mild technical conditions, we establish the asymptotic properties of the proposed test statistic and investigate how the testing power changes as B increases. In particular, we show that if B exceeds a certain threshold, the proposed nonparametric testing procedure achieves the classical minimax rate of testing (Shang and Cheng, 2015) for spline models. We further extend our theoretical investigations to a nonparametric linearity test and an adaptive nonparametric test, expanding the applicability of the proposed methods. Extensive simulation studies together with a real-data analysis are used to demonstrate the validity and effectiveness of the proposed tests.",B-bits Quantization | Minimax Rates of Testing | Nonparametric Inference | Smoothing Splines,1,0.0,,,,,
2-s2.0-85168011523,10.1609/aaai.v37i6.25834,,,Normalizing Flow Ensembles for Rich Aleatoric and Epistemic Uncertainty Modeling,cp,Conference Paper,Berry L.,60002494,Université McGill,Montreal,Canada,2.0,"Berry, Lucas;Meger, David",58109795500;23009425800,60002494;60002494,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,6806-6814,"In this work, we demonstrate how to reliably estimate epistemic uncertainty while maintaining the flexibility needed to capture complicated aleatoric distributions. To this end, we propose an ensemble of Normalizing Flows (NF), which are state-of-the-art in modeling aleatoric uncertainty. The ensembles are created via sets of fixed dropout masks, making them less expensive than creating separate NF models. We demonstrate how to leverage the unique structure of NFs, base distributions, to estimate aleatoric uncertainty without relying on samples, provide a comprehensive set of baselines, and derive unbiased estimates for differential entropy. The methods were applied to a variety of experiments, commonly used to benchmark aleatoric and epistemic uncertainty estimation: 1D sinusoidal data, 2D windy grid-world (Wet Chicken), Pendulum, and Hopper. In these experiments, we setup an active learning framework and evaluate each model’s capability at measuring aleatoric and epistemic uncertainty. The results show the advantages of using NF ensembles in capturing complicated aleatoric while maintaining accurate epistemic uncertainty estimates.",,9,1.0,all publisherfullgold,All Open Access Gold,NSERC,,Natural Sciences and Engineering Research Council of Canada
2-s2.0-105000538442,,,,Not All Tokens Are What You Need for Pretraining,cp,Conference Paper,Lin Z.,60025278;60018205;60026532;60280914,Tsinghua University;Xiamen University;Microsoft Corporation;Shanghai Artificial Intelligence Laboratory,Beijing;Xiamen;Redmond;Shanghai,China;China;United States;China,11.0,"Lin, Zhenghao;Gou, Zhibin;Gong, Yeyun;Liu, Xiao;Shen, Yelong;Xu, Ruochen;Lin, Chen;Yang, Yujiu;Jiao, Jian;Duan, Nan;Chen, Weizhu",57916983800;57549839400;55953770000;57206739254;56729408300;57220778246;55730233100;35729585000;36470257500;52163366000;23007589000,60018205-60026532;60025278-60026532;60026532;60026532;60026532;60026532;60018205-60280914;60025278;60026532;60026532;60026532,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that “Not all tokens in a corpus are equally important for language model training”. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called RHO-1. Unlike traditional LMs that learn to predict every next token in a corpus, RHO-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, RHO-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, RHO-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, RHO-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both data efficiency and performance of the language model pre-training.",,20,0.0,,,NKRDPC,2022ZD0160501,National Key Research and Development Program of China
2-s2.0-85124241842,,,,Novel Min-Max Reformulations of Linear Inverse Problems,ar,Article,Sheriff M.R.,60014153,Indian Institute of Technology Bombay,Mumbai,India,2.0,"Sheriff, Mohammed Rayyan;Chatterjee, Debasish",57201863280;57193687946,60014153;60014153,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"In this article, we dwell into the class of so-called ill-posed Linear Inverse Problems (LIP) which simply refer to the task of recovering the entire signal from its relatively few random linear measurements. Such problems arise in a variety of settings with applications ranging from medical image processing, recommender systems, etc. We propose a slightly generalized version of the error constrained linear inverse problem and obtain a novel and equivalent convex-concave min-max reformulation by providing an exposition to its convex geometry. Saddle points of the min-max problem are completely characterized in terms of a solution to the LIP, and vice versa. Applying simple saddle point seeking ascend-descent type algorithms to solve the min-max problems provides novel and simple algorithms to find a solution to the LIP. Moreover, the reformulation of an LIP as the min-max problem provided in this article is crucial in developing methods to solve the dictionary learning problem with almost sure recovery constraints.",Dictionary learning | Linear inverse problems | Min-max problems,0,0.0,,,,,
2-s2.0-105018585474,,,,Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees,ar,Article,Terenin A.,60026851;60031101;60118653,University of Oxford;University of Cambridge;Secondmind Limited,Oxford;Cambridge;Cambridge,United Kingdom;United Kingdom;United Kingdom,7.0,"Terenin, Alexander;Burt, David R.;Artemev, Artem;Flaxman, Seth;van der Wilk, Mark;Rasmussen, Carl Edward;Ge, Hong",57195505017;57213689604;57219542414;35190257400;56737079800;7103365199;57189099854,60031101;60031101;60118653;60026851;60026851;60031101;60031101,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models. Within a system, the Gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. In this work, we study the numerical stability of scalable sparse approximations based on inducing points. To do so, we first review numerical stability, and illustrate typical situations in which Gaussian process models can be unstable. Building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. This is done via a modification of the cover tree data structure, which is of independent interest. We additionally propose an alternative sparse approximation for regression with a Gaussian likelihood which trades off a small amount of performance to further improve stability. We provide illustrative examples showing the relationship between stability of calculations and predictive performance of inducing point methods on spatial tasks.",,4,0.0,,,UKRI,EP/V002910/2,Qualcomm
2-s2.0-85130544681,,,,ON ROBUST PREFIX-TUNING FOR TEXT CLASSIFICATION,cp,Conference Paper,Yang Z.,60025278,Tsinghua University,Beijing,China,2.0,"Yang, Zonghan;Liu, Yang",57204467184;57211088579,60025278;60025278,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks. However, most currently developed defense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of prefix-tuning. In this work, we propose a robust prefix-tuning framework that preserves the efficiency and modularity of prefix-tuning. The core idea of our framework is leveraging the layerwise activations of the language model by correctly-classified training data as the standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned for each batch and added to the original prefix for robustness enhancement. Extensive experiments on three text classification benchmarks show that our framework substantially improves robustness over several strong baselines against five textual attacks of different types while maintaining comparable accuracy on clean texts. We also interpret our robust prefix-tuning framework from the optimal control perspective and pose several directions for future research.",,20,0.0,,,NSFC,61925601,National Natural Science Foundation of China
2-s2.0-85187546986,,,,ON THE SOFT-SUBNETWORK FOR FEW-SHOT CLASS INCREMENTAL LEARNING,cp,Conference Paper,Kang H.,60032144,Korea Advanced Institute of Science and Technology,Daejeon,South Korea,5.0,"Kang, Haeyong;Yoon, Jaehong;Madjid, Sultan Rizky;Hwang, Sung Ju;Yoo, Chang D.",57188750785;57215557616;57423411700;57687927300;7201746384,60032144;60032144;60032144;60032144;60032144,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Inspired by Regularized Lottery Ticket Hypothesis, which states that competitive smooth (non-binary) subnetworks exist within a dense network, we propose a few-shot class-incremental learning method referred to as Soft-SubNetworks (SoftNet). Our objective is to learn a sequence of sessions incrementally, where each session only includes a few training instances per class while preserving the knowledge of the previously learned ones. SoftNet jointly learns the model weights and adaptive non-binary soft masks at a base training session in which each mask consists of the major and minor subnetwork; the former aims to minimize catastrophic forgetting during training, and the latter aims to avoid overfitting to a few samples in each new training session. We provide comprehensive empirical validations demonstrating that our SoftNet effectively tackles the few-shot incremental learning problem by surpassing the performance of state-of-the-art baselines over benchmark datasets. The public code is available at https://github.com/ihaeyong/SoftNet-FSCIL.",,43,0.0,,,IITP,2021-0-01381,Institute for Information and Communications Technology Promotion
2-s2.0-85186087255,,,,ON-POLICY DISTILLATION OF LANGUAGE MODELS: LEARNING FROM SELF-GENERATED MISTAKES,cp,Conference Paper,Agarwal R.,60016849;60111161;123045532,University of Toronto;DeepMind Technologies Limited;Mila,Toronto;London;Quebec,Canada;United Kingdom;Canada,7.0,"Agarwal, Rishabh;Vieillard, Nino;Zhou, Yongchao;Stanczyk, Piotr;Ramos, Sabela;Geist, Matthieu;Bachem, Olivier",57214858852;57219808951;57969387700;56946758900;58486669200;25929145100;55916070200,60111161-123045532;60111161;60111161-60016849;60111161;60111161;60111161;60111161,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative loss functions between the student and teacher, which may be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning of language models. We demonstrate the efficacy of GKD for distilling auto-regressive T5 language models for task-specific distillation on summarization, translation, and reasoning tasks, and task-agnostic distillation for instruction tuning.",,86,0.0,,,,,
2-s2.0-85182725421,,,,ONE TRANSFORMER CAN UNDERSTAND BOTH 2D & 3D MOLECULAR DATA,cp,Conference Paper,Luo S.,60014966;60021726;60280914,Peking University;Microsoft Research;Shanghai Artificial Intelligence Laboratory,Beijing;Redmond;Shanghai,China;United States;China,7.0,"Luo, Shengjie;Chen, Tianlang;Xu, Yixian;Zheng, Shuxin;Liu, Tie Yan;Wang, Liwei;He, Di",57221147567;58437053000;57937849100;57202497062;57221068510;55721280000;57194207766,60014966;60014966-60280914;60014966;60021726;60021726;60014966;60014966,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. When the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. By training on 2D and 3D molecular data with properly designed supervised signals, Transformer-M automatically learns to leverage knowledge from different data modalities and correctly capture the representations. We conducted extensive experiments for Transformer-M. All empirical results show that Transformer-M can simultaneously achieve strong performance on 2D and 3D tasks, suggesting its broad applicability. The code and models will be made publicly available at https://github.com/lsj2408/Transformer-M.",,30,0.0,,,NKRDPC,2022ZD0114900,National Key Research and Development Program of China
2-s2.0-85150342955,,,,OPEN-VOCABULARY OBJECT DETECTION VIA VISION AND LANGUAGE KNOWLEDGE DISTILLATION,cp,Conference Paper,Gu X.,60006191;60076695,Google LLC;NVIDIA,Mountain View;Santa Clara,United States;United States,4.0,"Gu, Xiuye;Lin, Tsung Yi;Kuo, Weicheng;Cui, Yin",57223753777;55268148900;57189664596;57118588100,60006191;60076695;60006191;60006191,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP<inf>r</inf> with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP<inf>r</inf>. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP<inf>50</inf> on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art (Zareian et al., 2021) by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.",,316,0.0,,,,,
2-s2.0-85204310040,,,,Oasis: Data Curation and Assessment System for Pretraining of Large Language Models,cp,Conference Paper,Zhou T.,60027363;60018486;60280914;106228429,University of Chinese Academy of Sciences;Institute of Automation Chinese Academy of Sciences;Shanghai Artificial Intelligence Laboratory;Ltd.,Beijing;Beijing;Shanghai;Beijing,China;China;China;China,6.0,"Zhou, Tong;Chen, Yubo;Cao, Pengfei;Liu, Kang;Liu, Shengping;Zhao, Jun",57208686794;57935192700;57205099786;55729555700;57204396372;59157650500,60018486;60018486-60027363;60018486-60027363;60018486-60027363-60280914;106228429;60018486-60027363,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,8855-8859,"Data is one of the most critical elements in building a large language model. However, existing systems either fail to customize a corpus curation pipeline or neglect to leverage comprehensive corpus assessment for iterative optimization of the curation. To this end, we present a pretraining corpus curation and assessment platform called Oasis - a one-stop system for data quality improvement and quantification with user-friendly interactive interfaces. Specifically, the interactive modular rule filter module can devise customized rules according to explicit feedback. The debiased neural filter module builds the quality classification dataset in a negative-centric manner to remove the undesired bias. The adaptive document deduplication module could execute large-scale deduplication with limited memory resources. These three parts constitute the customized data curation module. And in the holistic data assessment module, a corpus can be assessed in local and global views, with three evaluation means including human, GPT-4, and heuristic metrics. In addition, an 800GB bilingual corpus curated by Oasis is publicly released.",,1,0.0,,,YIPA CAS,2022ZD0160503,Youth Innovation Promotion Association of the Chinese Academy of Sciences
2-s2.0-85163136378,,,,Object Permanence Emerges in a Random Walk along Memory,cp,Conference Paper,Tokmakov P.,60025038;127257731,"University of California, Berkeley;Toyota Research Institute",Berkeley;Chicago,United States;United States,4.0,"Tokmakov, Pavel;Jabri, Allan;Li, Jie;Gaidon, Adrien",57191433199;57191433104;58170702300;52263438200,127257731;60025038;127257731;127257731,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,21506-21519,"This paper proposes a self-supervised objective for learning representations that localize objects under occlusion - a property known as object permanence. A central question is the choice of learning signal in cases of total occlusion. Rather than directly supervising the locations of invisible objects, we propose a self-supervised objective that requires neither human annotation, nor assumptions about object dynamics. We show that object permanence can emerge by optimizing for temporal coherence of memory: we fit a Markov walk along a space-time graph of memories, where the states in each time step are non-Markovian features from a sequence encoder. This leads to a memory representation that stores occluded objects and predicts their motion, to better localize them. The resulting model outperforms existing approaches on several datasets of increasing complexity and realism, despite requiring minimal supervision, and hence being broadly applicable.",,6,0.0,,,DARPA,,Defense Advanced Research Projects Agency
2-s2.0-85161877266,10.1613/JAIR.1.13253,,,Object-agnostic Affordance Categorization via Unsupervised Learning of Graph Embeddings,ar,Article,Toumpa A.,60073652;60012070;60111768,Tongji University;University of Leeds;The Alan Turing Institute,Shanghai;Leeds;London,China;United Kingdom;United Kingdom,2.0,"Toumpa, Alexia;Cohn, Anthony G.",57204975052;57225810681,60012070;60012070-60111768-60073652,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,,77.0,,1-38,"Acquiring knowledge about object interactions and affordances can facilitate scene understanding and human-robot collaboration tasks. As humans tend to use objects in many different ways depending on the scene and the objects’ availability, learning object affordances in everyday-life scenarios is a challenging task, particularly in the presence of an open set of interactions and objects. We address the problem of affordance categorization for class-agnostic objects with an open set of interactions; we achieve this by learning similarities between object interactions in an unsupervised way and thus inducing clusters of object affordances. A novel depth-informed qualitative spatial representation is proposed for the construction of Activity Graphs (AGs), which abstract from the continuous representation of spatio-temporal interactions in RGB-D videos. These AGs are clustered to obtain groups of objects with similar affordances. Our experiments in a real-world scenario demonstrate that our method learns to create object affordance clusters with a high V-measure even in cluttered scenes. The proposed approach handles object occlusions by capturing effectively possible interactions and without imposing any object or scene constraints.",,2,1.0,all publisherfullgold,All Open Access Gold,H2020,825619,Horizon 2020 Framework Programme
2-s2.0-85163061806,,,,Off-Policy Fitted Q-Evaluation with Differentiable Function Approximators: Z-Estimation and Inference Theory,cp,Conference Paper,Zhang R.,60014966;60003269;60111161,Peking University;Princeton University;DeepMind Technologies Limited,Beijing;Princeton;London,China;United States;United Kingdom,4.0,"Zhang, Ruiqi;Zhang, Xuezhou;Ni, Chengzhuo;Wang, Mengdi",58454950000;57207319373;57211252049;57215322066,60014966;60003269;60003269;60003269-60111161,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,26713-26749,"Off-Policy Evaluation (OPE) serves as one of the cornerstones in Reinforcement Learning (RL). Fitted Q Evaluation (FQE) with various function approximators, especially deep neural networks, has gained practical success. While statistical analysis has proved FQE to be minimax-optimal with tabular, linear and several nonparametric function families, its practical performance with more general function approximator is less theoretically understood. We focus on FQE with general differentiable function approximators, making our theory applicable to neural function approximations. We approach this problem using the Z-estimation theory and establish the following results: The FQE estimation error is asymptotically normal with explicit variance determined jointly by the tangent space of the function class at the ground truth, the reward structure, and the distribution shift due to off-policy learning; The finite-sample FQE error bound is dominated by the same variance term, and it can also be bounded by function class-dependent divergence, which measures how the off-policy distribution shift intertwines with the function approximator. In addition, we study bootstrapping FQE estimators for error distribution inference and estimating confidence intervals, accompanied by a Cramer-Rao lower bound that matches our upper bounds. The Z-estimation analysis provides a generalizable theoretical framework for studying off-policy estimation in RL and provides sharp statistical theory for FQE with differentiable function approximators.",,3,0.0,,,NSF,CMMI-1653435,National Science Foundation
2-s2.0-85191163821,,,,Offline Reinforcement Learning with Differential Privacy,cp,Conference Paper,Qiao D.,60142701,The Robert Mehrabian College of Engineering,Santa Barbara,United States,2.0,"Qiao, Dan;Wang, Yu Xiang",57466476700;58912060300,60142701;60142701,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"The offline reinforcement learning (RL) problem is often motivated by the need to learn data-driven decision policies in financial, legal and healthcare applications. However, the learned policy could retain sensitive information of individuals in the training data (e.g., treatment and outcome of patients), thus susceptible to various privacy risks. We design offline RL algorithms with differential privacy guarantees which provably prevent such risks. These algorithms also enjoy strong instance-dependent learning bounds under both tabular and linear Markov Decision Process (MDP) settings. Our theory and simulation suggest that the privacy guarantee comes at (almost) no drop in utility comparing to the non-private counterpart for a medium-size dataset.",,11,0.0,,,NSF,2007117,National Science Foundation
2-s2.0-85153603782,,,,On Distribution Shift in Learning-based Bug Detectors,cp,Conference Paper,He J.,60025858,ETH Zürich,Zurich,Switzerland,3.0,"He, Jingxuan;Beurer-Kellner, Luca;Vechev, Martin",57204734521;57431541300;8876227900,60025858;60025858;60025858,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,8559-8580,"Deep learning has recently achieved initial success in program analysis tasks such as bug detection. Lacking real bugs, most existing works construct training and test data by injecting synthetic bugs into correct programs. Despite achieving high test accuracy (e.g., >90%), the resulting bug detectors are found to be surprisingly unusable in practice, i.e., <10% precision when used to scan real software repositories. In this work, we argue that this massive performance difference is caused by a distribution shift, i.e., a fundamental mismatch between the real bug distribution and the synthetic bug distribution used to train and evaluate the detectors. To address this key challenge, we propose to train a bug detector in two phases, first on a synthetic bug distribution to adapt the model to the bug detection domain, and then on a real bug distribution to drive the model towards the real distribution. During these two phases, we leverage a multi-task hierarchy, focal loss, and contrastive learning to further boost performance. We evaluate our approach extensively on three widely studied bug types, for which we construct new datasets carefully designed to capture the real bug distribution. The results demonstrate that our approach is practically effective and successfully mitigates the distribution shift: our learned detectors are highly performant on both our test set and the latest version of open source repositories. Our code, datasets, and models are publicly available at https://github.com/eth-sri/learning-real-bug-detector.",,11,0.0,,,,,
2-s2.0-85165165993,10.1613/jair.1.14481,,,On Dynamics in Structured Argumentation Formalisms,ar,Article,Rapberger A.,60008042;60018163,Universität Leipzig;TU Wien,Leipzig;Vienna,Germany;Austria,2.0,"Rapberger, Anna;Ulbricht, Markus",57211169022;57214569800,60018163;60008042,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,563-643,"This paper is a contribution to the research on dynamics in assumption-based argumentation (ABA). We investigate situations where a given knowledge base undergoes certain changes. We show that two frequently investigated problems, namely enforcement of a given target atom and deciding strong equivalence of two given ABA frameworks, are intractable in general. Notably, these problems are both tractable for abstract argumentation frameworks (AFs) which admit a close correspondence to ABA by constructing semantics-preserving instances. Inspired by this observation, we search for tractable fragments for ABA frameworks by means of the instantiated AFs. We argue that the usual instantiation procedure is not suitable for the investigation of dynamic scenarios since too much information is lost when constructing the abstract framework. We thus consider an extension of AFs, called cvAFs, equipping arguments with conclusions and vulnerabilities in order to better anticipate their role after the underlying knowledge base is extended. We investigate enforcement and strong equivalence for cvAFs and present syntactic conditions to decide them. We show that the correspondence between cvAFs and ABA frameworks is close enough to capture dynamics in ABA. This yields the desired tractable fragment. We furthermore discuss consequences for the corresponding problems for logic programs.",,18,1.0,all publisherfullgold,All Open Access Gold,BMBF,W 1255,Bundesministerium für Bildung und Forschung
2-s2.0-105018580133,,,,On Efficient and Scalable Computation of the Nonparametric Maximum Likelihood Estimator in Mixture Models,ar,Article,Zhang Y.,60019499;60030162;60017161;60121391,Chinese Academy of Sciences;Columbia University;National University of Singapore;UC Berkeley’s Industrial Engineering and Operations Research Department,Beijing;New York;Singapore City;Berkeley,China;United States;Singapore;United States,4.0,"Zhang, Yangjing;Cui, Ying;Sen, Bodhisattva;Toh, Kim Chuan",57203831699;57205503708;14021843900;24294588700,60019499;60121391;60030162;60017161,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"In this paper, we focus on the computation of the nonparametric maximum likelihood estimator (NPMLE) in multivariate mixture models. Our approach discretizes this infinite dimensional convex optimization problem by setting fixed support points for the NPMLE and optimizing over the mixing proportions. We propose an efficient and scalable semismooth Newton based augmented Lagrangian method (ALM). Our algorithm outperforms the state-of-the-art methods (Kim et al., 2020; Koenker and Gu, 2017), capable of handling n ≈ 10<sup>6</sup> data points with m ≈ 10<sup>4</sup> support points. A key advantage of our approach is its strategic utilization of the solution’s sparsity, leading to structured sparsity in Hessian computations. As a result, our algorithm demonstrates better scaling in terms of m when compared to the mixsqp method (Kim et al., 2020). The computed NPMLE can be directly applied to denoising the observations in the framework of empirical Bayes. We propose new denoising estimands in this context along with their consistent estimates. Extensive numerical experiments are conducted to illustrate the efficiency of our ALM. In particular, we employ our method to analyze two astronomy data sets: (i) Gaia-TGAS Catalog (Anderson et al., 2018) containing approximately 1.4 × 10<sup>6</sup> data points in two dimensions, and (ii) a data set from the APOGEE survey (Majewski et al., 2017) with approximately 2.7 × 10<sup>4</sup> data points.",Augmented Lagrangian method | denoising | empirical Bayes | Gaussian location mixture model | heteroscedastic errors | semismooth Newton method | sparse second-order information,4,0.0,,,NSFC,12201617,National Natural Science Foundation of China
2-s2.0-85163154524,,,,On Elimination Strategies for Bandit Fixed-Confidence Identification,cp,Conference Paper,Tirinzoni A.,60104665;127617877,Université de Lille;Meta AI,Lille;Paris,France;France,2.0,"Tirinzoni, Andrea;Degenne, Rémy",57204820100;56560076900,127617877;60104665,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Elimination algorithms for bandit identification, which prune the plausible correct answers sequentially until only one remains, are computationally convenient since they reduce the problem size over time. However, existing elimination strategies are often not fully adaptive (they update their sampling rule infrequently) and are not easy to extend to combinatorial settings, where the set of answers is exponentially large in the problem dimension. On the other hand, most existing fully-adaptive strategies to tackle general identification problems are computationally demanding since they repeatedly test the correctness of every answer, without ever reducing the problem size. We show that adaptive methods can be modified to use elimination in both their stopping and sampling rules, hence obtaining the best of these two worlds: the algorithms (1) remain fully adaptive, (2) suffer a sample complexity that is never worse of their non-elimination counterpart, and (3) provably eliminate certain wrong answers early. We confirm these benefits experimentally, where elimination improves significantly the computational complexity of adaptive methods on common tasks like best-arm identification in linear bandits.",,4,0.0,,,,,
2-s2.0-85153683372,10.1613/JAIR.1.13779,,,On Fair Division under Heterogeneous Matroid Constraints,ar,Article,Dror A.,60005681;60080064,Tel Aviv University;Ariel University,Tel Aviv-Yafo;Ariel,Israel;Israel,3.0,"Dror, Amitay;Feldman, Michal;Segal-Halevi, Erel",57681692800;35233000400;56422378800,60005681;60005681;60080064,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,567-611,"We study fair allocation of indivisible goods among additive agents with feasibility constraints. In these settings, every agent is restricted to get a bundle among a specified set of feasible bundles. Such scenarios have been of great interest to the AI community due to their applicability to real-world problems. Following some impossibility results, we restrict attention to matroid feasibility constraints that capture natural scenarios, such as the allocation of shifts to medical doctors and the allocation of conference papers to referees. We focus on the common fairness notion of envy-freeness up to one good (EF1). Previous algorithms for finding EF1 allocations are either restricted to agents with identical feasibility constraints or allow free disposal of items. An open problem is the existence of EF1 complete allocations among agents who differ both in their valuations and in their feasibility constraints. In this work, we make progress on this problem by providing positive and negative results for several matroid and valuation types. Among other results, we devise polynomial-time algorithms for finding EF1 allocations in the following settings: (i) n agents with heterogeneous (non-identical) binary valuations and partition matroids with heterogeneous capacities; (ii) two agents with heterogeneous additive valuations and partition matroids with heterogeneous capacities; and (iii) three agents with heterogeneous binary valuations and identical base-orderable matroid constraints.",,17,1.0,all publisherfullgold,All Open Access Gold,H2020,866132,Horizon 2020 Framework Programme
2-s2.0-85167865493,10.1609/aaai.v37i4.25574,,,On Generalized Degree Fairness in Graph Neural Networks,cp,Conference Paper,Liu Z.,60017161;60018933,National University of Singapore;Singapore Management University,Singapore City;Singapore City,Singapore;Singapore,3.0,"Liu, Zemin;Nguyen, Trung Kien;Fang, Yuan",57191689277;57753475300;55469295200,60017161;60018933;60018933,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,4525-4533,"Conventional graph neural networks (GNNs) are often confronted with fairness issues that may stem from their input, including node attributes and neighbors surrounding a node. While several recent approaches have been proposed to eliminate the bias rooted in sensitive attributes, they ignore the other key input of GNNs, namely the neighbors of a node, which can introduce bias since GNNs hinge on neighborhood structures to generate node representations. In particular, the varying neighborhood structures across nodes, manifesting themselves in drastically different node degrees, give rise to the diverse behaviors of nodes and biased outcomes. In this paper, we first define and generalize the degree bias using a generalized definition of node degree as a manifestation and quantification of different multi-hop structures around different nodes. To address the bias in the context of node classification, we propose a novel GNN framework called Generalized Degree Fairness-centric Graph Neural Network (DegFairGNN). Specifically, in each GNN layer, we employ a learnable debiasing function to generate debiasing contexts, which modulate the layer-wise neighborhood aggregation to eliminate the degree bias originating from the diverse degrees among nodes. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our model on both accuracy and fairness metrics.",,27,1.0,all publisherfullgold,All Open Access Gold,A*STAR,A20H6b0151,"Agency for Science, Technology and Research"
2-s2.0-85174409689,,,,"On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology",cp,Conference Paper,Di Giovanni F.,60026851;60031101;60032350;60021726,University of Oxford;University of Cambridge;Sapienza Università di Roma;Microsoft Research,Oxford;Cambridge;Rome;Redmond,United Kingdom;United Kingdom;Italy;United States,6.0,"Di Giovanni, Francesco;Giusti, Lorenzo;Barbero, Federico;Luise, Giulia;Liò, Pietro;Bronstein, Michael",57218954617;57546681800;57221158537;57208437972;7004223170;7005532788,60031101;60032350;60026851;60021726;60031101;60026851,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,7865-7885,"Message Passing Neural Networks (MPNNs) are instances of Graph Neural Networks that leverage the graph to send messages over the edges. This inductive bias leads to a phenomenon known as over-squashing, where a node feature is insensitive to information contained at distant nodes. Despite recent methods introduced to mitigate this issue, an understanding of the causes for oversquashing and of possible solutions are lacking. In this theoretical work, we prove that: (i) Neural network width can mitigate over-squashing, but at the cost of making the whole network more sensitive; (ii) Conversely, depth cannot help mitigate over-squashing: increasing the number of layers leads to over-squashing being dominated by vanishing gradients; (iii) The graph topology plays the greatest role, since over-squashing occurs between nodes at high commute time. Our analysis provides a unified framework to study different recent methods introduced to cope with over-squashing and serves as a justification for a class of methods that fall under graph rewiring.",,88,0.0,,,EC,274228,SK Innovation
2-s2.0-105018671484,,,,On Sufficient Graphical Models,ar,Article,Li B.,60001439;60001018,Pennsylvania State University;Ewha Womans University,University Park;Seoul,United States;South Korea,2.0,"Li, Bing;Kim, Kyongwon",60139129900;57257036800,60001439;60001018,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"We introduce a sufficient graphical model by applying the recently developed nonlinear sufficient dimension reduction techniques to the evaluation of conditional independence. The graphical model is nonparametric in nature, as it does not make distributional assumptions such as the Gaussian or copula Gaussian assumptions. However, unlike a fully nonparametric graphical model, which relies on the high-dimensional kernel to characterize conditional independence, our graphical model is based on conditional independence given a set of sufficient predictors with a substantially reduced dimension. In this way we avoid the curse of dimensionality that comes with a high-dimensional kernel. We develop the population-level properties, convergence rate, and variable selection consistency of our estimate. By simulation comparisons and an analysis of the DREAM 4 Challenge data set, we demonstrate that our method outperforms the existing methods when the Gaussian or copula Gaussian assumptions are violated, and its performance remains excellent in the high-dimensional setting.",conjoined conditional covariance operator | generalized sliced inverse regression | nonlinear sufficient dimension reduction | reproducing kernel Hilbert space,4,0.0,,,NRF,DMS-2210775,National Research Foundation of Korea
2-s2.0-85142066304,10.1613/jair.1.13575,,,On Tackling Explanation Redundancy in Decision Trees,ar,Article,Izza Y.,60019578;60027245;60030491,Monash University;Université de Toulouse;Institut de Recherche en Informatique de Toulouse,Melbourne;Toulouse;Toulouse,Australia;France;France,3.0,"Izza, Yacine;Ignatiev, Alexey;Marques-Silva, Joao",57190488304;56360244200;6603779463,60027245;60019578;60030491,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,261-321,"Decision trees (DTs) epitomize the ideal of interpretability of machine learning (ML) models. The interpretability of decision trees motivates explainability approaches by so-called intrinsic interpretability, and it is at the core of recent proposals for applying interpretable ML models in high-risk applications. The belief in DT interpretability is justified by the fact that explanations for DT predictions are generally expected to be succinct. Indeed, in the case of DTs, explanations correspond to DT paths. Since decision trees are ideally shallow, and so paths contain far fewer features than the total number of features, explanations in DTs are expected to be succinct, and hence interpretable. This paper offers both theoretical and experimental arguments demonstrating that, as long as interpretability of decision trees equates with succinctness of explanations, then decision trees ought not be deemed interpretable. The paper introduces logically rigorous path explanations and path explanation redundancy, and proves that there exist functions for which decision trees must exhibit paths with explanation redundancy that is arbitrarily larger than the actual path explanation. The paper also proves that only a very restricted class of functions can be represented with DTs that exhibit no explanation redundancy. In addition, the paper includes experimental results substantiating that path explanation redundancy is observed ubiquitously in decision trees, including those obtained using different tree learning algorithms, but also in a wide range of publicly available decision trees. The paper also proposes polynomial-time algorithms for eliminating path explanation redundancy, which in practice require negligible time to compute. Thus, these algorithms serve to indirectly attain irreducible, and so succinct, explanations for decision trees. Furthermore, the paper includes novel results related with duality and enumeration of explanations, based on using SAT solvers as witness-producing NP-oracles.",,61,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,
2-s2.0-105018572747,,,,On Tail Decay Rate Estimation of Loss Function Distributions,ar,Article,Haxholli E.,60110693,Université Côte d'Azur,Nice,France,2.0,"Haxholli, Etrit;Lorenzi, Marco",58475113900;26654433200,60110693;60110693,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"The study of loss-function distributions is critical to characterize a model’s behaviour on a given machine-learning problem. While model quality is commonly measured by the average loss assessed on a testing set, this quantity does not ascertain the existence of the mean of the loss distribution. Conversely, the existence of a distribution’s statistical moments can be verified by examining the thickness of its tails. Cross-validation schemes determine a family of testing loss distributions conditioned on the training sets. By marginalizing across training sets, we can recover the overall (marginal) loss distribution, whose tail-shape we aim to estimate. Small sample-sizes diminish the reliability and efficiency of classical tail-estimation methods like Peaks-Over-Threshold, and we demonstrate that this effect is notably significant when estimating tails of marginal distributions composed of conditional distributions with substantial tail-location variability. We mitigate this problem by utilizing a result we prove: under certain conditions, the marginal-distribution’s tail-shape parameter is the maximum tail-shape parameter across the conditional distributions underlying the marginal. We label the resulting approach as ‘cross-tail estimation (CTE)’. We test CTE in a series of experiments on simulated and real data<sup>1</sup>, showing the improved robustness and quality of tail estimation as compared to classical approaches.",Cross-Tail-Estimation | Extreme Value Theory | Model Ranking | Peaks-Over-Threshold | Tail Modelling,0,0.0,,,ANR,ANR-19-P3IA-0002,Agence Nationale de la Recherche
2-s2.0-105018578875,,,,On Truthing Issues in Supervised Classification,ar,Article,Su J.K.,60024592,Lincoln Laboratory,Lexington,United States,1.0,"Su, Jonathan K.",7401682295,60024592,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Ideal supervised classification assumes known correct labels, but various truthing issues can arise in practice: noisy labels; multiple, conflicting labels for a sample; missing labels; and different labeler combinations for different samples. Previous work introduced a noisy-label model, which views the observed noisy labels as random variables conditioned on the unobserved correct labels. It has mainly focused on estimating the conditional distribution of the noisy labels and the class prior, as well as estimating the correct labels or training with noisy labels. In a complementary manner, given the conditional distribution and class prior, we apply estimation theory to classifier testing, training, and comparison of different combinations of labelers. First, for binary classification, we construct a testing model and derive approximate marginal posteriors for accuracy, precision, recall, probability of false alarm, and F-score, and joint posteriors for ROC and precision-recall analysis. We propose minimum mean-square error (MMSE) testing, which employs empirical Bayes algorithms to estimate the testing-model parameters and then computes optimal point estimates and credible regions for the metrics. We extend the approach to multi-class classification to obtain optimal estimates of accuracy and individual confusion-matrix elements. Second, we present a unified view of training that covers probabilistic (i.e., discriminative or generative) and non-probabilistic models. For the former, we adjust maximum-likelihood or maximum a posteriori training for truthing issues; for the latter, we propose MMSE training, which minimizes the MMSE estimate of the empirical risk. We also describe suboptimal training that is compatible with existing infrastructure. Third, we observe that mutual information lets one express any labeler combination as an equivalent single labeler, implying that multiple mediocre labelers can be as informative as, or more informative than, a single expert labeler. Experiments demonstrate the effectiveness of the methods and confirm the implication.",Bayesian estimation | crowdsourcing | empirical Bayes | mutual information | noisy labels | supervised classification | truth errors,0,0.0,,,AF,FA8702-15-D-0001,Office of the Under Secretary of Defense for Research and Engineering
2-s2.0-85204976336,,,,"On Unbalanced Optimal Transport: Gradient Methods, Sparsity and Approximation Error",ar,Article,Nguyen Q.M.,60017366;60136858;60141072;60009253,IBM Thomas J. Watson Research Center;College of Engineering;MIT Department of Electrical Engineering and Computer Science;IBM Research - Almaden,Yorktown Heights;Atlanta;Cambridge;San Jose,United States;United States;United States;United States,4.0,"Nguyen, Quang Minh;Nguyen, Hoang H.;Zhou, Yi;Nguyen, Lam M.",57222270261;57962441500;57210954692;57190262358,60141072;60136858;60009253;60017366,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,384,,"We study the Unbalanced Optimal Transport (UOT) between two measures of possibly different masses with at most n components, where the marginal constraints of standard Optimal Transport (OT) are relaxed via Kullback-Leibler divergence with regularization factor τ. Although only Sinkhorn-based UOT solvers have been analyzed in the literature with the iteration complexity of (Formula presented) and per-iteration cost of O(n<sup>2</sup>) for achieving the desired error ε, their positively dense output transportation plans strongly hinder the practicality. On the other hand, while being vastly used as heuristics for computing UOT in modern deep learning applications and having shown success in sparse OT problem, gradient methods applied to UOT have not been formally studied. In this paper, we propose a novel algorithm based on Gradient Extrapolation Method (GEM-UOT) to find an ε-approximate solution to the UOT problem in (Formula presented) iterations with (Formula presented) per-iteration cost, where κ is the condition number depending on only the two input measures. Our proof technique is based on a novel dual formulation of the squared ℓ<inf>2</inf>-norm UOT objective, which fills the lack of sparse UOT literature and also leads to a new characterization of approximation error between UOT and OT. To this end, we further present a novel approach of OT retrieval from UOT, which is based on GEM-UOT with fine tuned τ and a post-process projection step. Extensive experiments on synthetic and real datasets validate our theories and demonstrate the favorable performance of our methods in practice. We showcase GEM-UOT on the task of color transfer in terms of both the quality of the transfer image and the sparsity of the transportation plan.",Convex Optimization | Gradient Methods | Unbalanced Optimal Transport,9,0.0,,,,,
2-s2.0-85153594611,10.1613/JAIR.1.13994,,,On the Complexity of Finding Set Repairs for Data-Graphs,ar,Article,Abriola S.,60001563,Universidad de Buenos Aires,Buenos Aires,Argentina,5.0,"Abriola, Sergio;Martinez, Maria Vanina;Pardal, Nina;Cifuentes, Santiago;Baque, Edwin Pin",55360150500;57195288338;57219684736;57289899000;57219460221,60001563;60001563;60001563;60001563;60001563,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,721-759,"In the deeply interconnected world we live in, pieces of information link domains all around us. As graph databases embrace effectively relationships among data and allow processing and querying these connections efficiently, they are rapidly becoming a popular platform for storage that supports a wide range of domains and applications. As in the relational case, it is expected that data preserves a set of integrity constraints that define the semantic structure of the world it represents. When a database does not satisfy its integrity constraints, a possible approach is to search for a 'similar' database that does satisfy the constraints, also known as a repair. In this work, we study the problem of computing subset and superset repairs for graph databases with data values using a notion of consistency based on having a set of Reg-GXPath expressions as integrity constraints. We show that for positive fragments of Reg-GXPath these problems admit a polynomialtime algorithm, while the full expressive power of the language renders them intractable.",,2,1.0,all publisherfullgold,All Open Access Gold,CONICET,RESCS-2020-345-E-UBA-REC,Consejo Nacional de Investigaciones Científicas y Técnicas
2-s2.0-85199696099,10.1613/jair.1.15305,,,On the Convergence of Swap Dynamics to Pareto-Optimal Matchings,ar,Article,Brandt F.,60019722;60106017,Technische Universität München;Université Paris-Saclay,Munich;Gif-sur-Yvette,Germany;France,2.0,"Brandt, Felix;Wilczynski, Anaëlle",35232736700;57193337202,60019722;60106017,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,1063-1098,"We study whether Pareto-optimal stable matchings can be reached via pairwise swaps in one-to-one matching markets with initial assignments. We consider housing markets, marriage markets, and roommate markets as well as three different notions of swap rationality. Our main results are as follows. While it can be efficiently determined whether a Pareto-optimal stable matching can be reached when defining swaps via blocking pairs, checking whether this is the case for all such sequences is computationally intractable. When defining swaps such that all involved agents need to be better off, even deciding whether a Pareto-optimal stable matching can be reached via some sequence is intractable. This confirms and extends a conjecture made by Damamme, Beynier, Chevaleyre, and Maudet (2015) who have shown that convergence to a Pareto-optimal matching is guaranteed in housing markets with single-peaked preferences. We prove that in marriage and roommate markets, single-peakedness is not sufficient for this to hold, but the stronger restriction of one-dimensional Euclidean preferences is.",,1,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,DFG,BR 2312/12-1,Deutsche Forschungsgemeinschaft
2-s2.0-105018670156,,,,On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks,ar,Article,Neumayer S.,60028186,École Polytechnique Fédérale de Lausanne,Lausanne,Switzerland,3.0,"Neumayer, Sebastian;Chizat, Lénaïc;Unser, Michael",57217635914;57191409756;7102049045,60028186;60028186;60028186,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"In supervised learning, the regularization path is sometimes used as a convenient theoretical proxy for the optimization path of gradient descent initialized from zero. In this paper, we study a modification of the regularization path for infinite-width 2-layer ReLU neural networks with nonzero initial distribution of the weights at different scales. By exploiting a link with unbalanced optimal-transport theory, we show that, despite the non-convexity of the 2-layer network training, this problem admits an infinite-dimensional convex counterpart. We formulate the corresponding functional-optimization problem and investigate its main properties. In particular, we show that, as the scale of the initialization ranges between 0 and +∞, the associated path interpolates continuously between the so-called kernel and rich regimes. Numerical experiments confirm that, in our setting, the scaling path and the final states of the optimization path behave similarly, even beyond these extreme points.",gradient-descent training | Hellinger–Kantorovich distance | neural tangent kernel | regularization path | Γ-convergence,0,0.0,,,ERC,101020573,European Research Council
2-s2.0-105018584948,,,,On the Generalization of Stochastic Gradient Descent with Momentum,ar,Article,Ramezani-Kebrya A.,60016849;60010348;60028186,University of Toronto;Universitetet i Oslo;École Polytechnique Fédérale de Lausanne,Toronto;Oslo;Lausanne,Canada;Norway;Switzerland,5.0,"Ramezani-Kebrya, Ali;Antonakopoulos, Kimon;Cevher, Volkan;Khisti, Ashish;Liang, Ben",55626621000;57218716787;6506127004;8337591400;7202071208,60010348;60028186;60028186;60016849;60016849,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"While momentum-based accelerated variants of stochastic gradient descent (SGD) are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In this work, we first show that there exists a convex loss function for which the stability gap for multiple epochs of SGD with standard heavy-ball momentum (SGDM) becomes unbounded. Then, for smooth Lipschitz loss functions, we analyze a modified momentum-based update rule, i.e., SGD with early momentum (SGDEM) under a broad range of step-sizes, and show that it can train machine learning models for multiple epochs with a guarantee for generalization. Finally, for the special case of strongly convex loss functions, we find a range of momentum such that multiple epochs of standard SGDM, as a special form of SGDEM, also generalizes. Extending our results on generalization, we also develop an upper bound on the expected true risk, in terms of the number of training steps, sample size, and momentum. Our experimental evaluations verify the consistency between the numerical results and our theoretical bounds. SGDEM improves the generalization error of SGDM when training ResNet-18 on ImageNet in practical distributed settings.",generalization error | heavy-ball momentum | non-convex | stochastic gradient descent | Uniform stability,11,0.0,,,NSERC,332645,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85203816760,,,,On the Hardness of Probabilistic Neurosymbolic Learning,cp,Conference Paper,Maene J.,60025063;60008141,KU Leuven;Örebro Universitet,Leuven;Orebro,Belgium;Sweden,3.0,"Maene, Jaron;Derkinderen, Vincent;De Raedt, Luc",57222420365;57216982292;55760010700,60025063;60025063;60025063-60008141,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,34203-34218,"The limitations of purely neural learning have sparked an interest in probabilistic neurosymbolic models, which combine neural networks with probabilistic logical reasoning. As these neurosymbolic models are trained with gradient descent, we study the complexity of differentiating probabilistic reasoning. We prove that although approximating these gradients is intractable in general, it becomes tractable during training. Furthermore, we introduce WeightME, an unbiased gradient estimator based on model sampling. Under mild assumptions, WeightME approximates the gradient with probabilistic guarantees using a logarithmic number of calls to a SAT solver. Lastly, we evaluate the necessity of these guarantees on the gradient. Our experiments indicate that the existing biased approximations indeed struggle to optimize even when exact solving is still feasible.",,1,0.0,,,FWO,G097720N,Vlaamse regering
2-s2.0-85174398614,,,,On the Importance of Feature Decorrelation for Unsupervised Representation Learning in Reinforcement Learning,cp,Conference Paper,Lee H.,60032144;126715494,Korea Advanced Institute of Science and Technology;Kakao Enterprise,Daejeon;Pohang,South Korea;South Korea,6.0,"Lee, Hojoon;Lee, Koanho;Hwang, Dongyoon;Lee, Hyunho;Lee, Byungkun;Choo, Jaegul",57239032300;58506652100;57238159100;58507758000;56313470700;24512223400,60032144-126715494;60032144;60032144;126715494;60032144;60032144,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,18974-18987,"Recently, unsupervised representation learning (URL) has improved the sample efficiency of Reinforcement Learning (RL) by pretraining a model from a large unlabeled dataset. The underlying principle of these methods is to learn temporally predictive representations by predicting future states in the latent space. However, an important challenge of this approach is the representational collapse, where the subspace of the latent representations collapses into a low-dimensional manifold. To address this issue, we propose a novel URL framework that causally predicts future states while increasing the dimension of the latent manifold by decorrelating the features in the latent space. Through extensive empirical studies, we demonstrate that our framework effectively learns predictive representations without collapse, which significantly improves the sample efficiency of state-of-the-art URL methods on the Atari 100k benchmark. The code is available at https://github.com/dojeon-ai/SimTPR.",,1,0.0,,,MSIP,2020-0-00368,"Ministry of Science, ICT and Future Planning"
2-s2.0-85163123508,,,,On the Impossibility of Learning to Cooperate with Adaptive Partner Strategies in Repeated Games,cp,Conference Paper,Loftin R.,60006288,Delft University of Technology,Delft,Netherlands,2.0,"Loftin, Robert;Oliehoek, Frans A.",58514845500;15043050300,60006288;60006288,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,14197-14209,"Learning to cooperate with other agents is challenging when those agents also possess the ability to adapt to our own behavior. Practical and theoretical approaches to learning in cooperative settings typically assume that other agents' behaviors are stationary, or else make very specific assumptions about other agents' learning processes. The goal of this work is to understand whether we can reliably learn to cooperate with other agents without such restrictive assumptions, which are unlikely to hold in real-world applications. Our main contribution is a set of impossibility results, which show that no learning algorithm can reliably learn to cooperate with all possible adaptive partners in a repeated matrix game, even if that partner is guaranteed to cooperate with some stationary strategy. Motivated by these results, we then discuss potential alternative assumptions which capture the idea that an adaptive partner will only adapt rationally to our behavior.",,1,0.0,,,OCW,024.004.022,"Ministerie van Onderwijs, Cultuur en Wetenschap"
2-s2.0-85137924121,10.24963/ijcai.2022/37,,,On the Ordinal Invariance of Power Indices on Coalitional Games,cp,Conference Paper,Doignon J.P.,60000145;60105740,Université Libre de Bruxelles;Laboratoire d’Analyse et de Modélisation de Systèmes pour l’Aide à la Décision,Brussels;Paris,Belgium;France,3.0,"Doignon, Jean Paul;Moretti, Stefano;Öztürk, Meltem",7005489550;7103215919;18434449200,60000145;60105740;60105740,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,258-264,"In a coalitional game, the coalitions are weakly ordered according to their worths in the game. When moreover a power index is given, the players are ranked according to the real numbers they are assigned by the power index. If any game inducing the same ordering of the coalitions generates the same ranking of the players then, by definition, the game is (ordinally) stable for the power index, which in turn is ordinally invariant for the game. If one is interested in ranking players of a game which is stable, re-computing the power index numbers when the coalitional worths slightly fluctuate or are uncertain becomes useless. Bivalued games are easy examples of games stable for any power index which is linear. Among general games, we characterize those that are stable for a given linear index. Note that the Shapley and Banzhaf indices, frequently used in AI, are particular semivalues, and all semivalues are linear indices. To check whether a game is stable for a specific semivalue, it suffices to inspect the ordering of the coalitions and to perform some direct computation based on the semivalue parameters.",,3,1.0,all publisherfree2read repository repositoryam,All Open Access Bronze Green,ANR,ANR-20-CE23-0018,Agence Nationale de la Recherche
2-s2.0-85170387694,10.24963/ijcai.2023/375,,,On the Paradox of Learning to Reason from Data,cp,Conference Paper,Zhang H.,60027550,"University of California, Los Angeles",Los Angeles,United States,5.0,"Zhang, Honghua;Li, Liunian Harold;Meng, Tao;Chang, Kai Wei;Van den Broeck, Guy",57219749561;57219620836;57216692617;24502911300;35185591300,60027550;60027550;60027550;60027550;60027550,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,3365-3373,"Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be trained end-to-end to solve logical reasoning problems presented in natural language? We attempt to answer this question in a confined problem space where there exists a set of parameters that perfectly simulates logical reasoning. We make observations that seem to contradict each other: BERT attains near-perfect accuracy on in-distribution test examples while failing to generalize to other data distributions over the exact same problem space. Our study provides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has, in fact, learned statistical features that inherently exist in logical reasoning problems. We also show that it is infeasible to jointly remove statistical features from data, illustrating the difficulty of learning to reason in general. Our result naturally extends to other neural models (e.g. T5) and unveils the fundamental difference between learning to reason and learning to achieve high performance on NLP benchmarks using statistical features.",,37,1.0,all publisherfullgold,All Open Access Gold,NSF,1837129,Samsung
2-s2.0-85179136483,10.1613/JAIR.1.14748,,,On the Parallel Parameterized Complexity of MaxSAT Variants,ar,Article,Bannach M.,60012345;60012408;60031159,Christian-Albrechts-Universität zu Kiel;Universität zu Lübeck;ESTEC - European Space Research and Technology Centre,Kiel;Lubeck;Noordwijk aan Zee,Germany;Germany;Netherlands,3.0,"Bannach, Max;Skambath, Malte;Tantau, Till",57120124800;57192679440;6602461912,60031159;60012345;60012408,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,673-707,"In the maximum satisfiability problem (max-sat) we are given a propositional formula in conjunctive normal form and have to find an assignment that satisfies as many clauses as possible. We study the parallel parameterized complexity of various versions of max-sat and provide the first constant-time algorithms parameterized either by the solution size or by the allowed excess relative to some guarantee. For the dual parameterized version where the parameter is the number of clauses we are allowed to leave unsatisfied, we present the first parallel algorithm for max-2sat (known as almost-2sat). The difficulty in solving almost-2sat in parallel comes from the fact that the iterative compression method, originally developed to prove that the problem is fixed-parameter tractable at all, is inherently sequential. We observe that a graph flow whose value is a parameter can be computed in parallel and develop a parallel algorithm for the vertex cover problem parameterized above the size of a given matching. Finally, we study the parallel complexity of max-sat parameterized by the vertex cover number, the treedepth, the feedback vertex set number, and the treewidth of the input's incidence graph. While max-sat is fixedparameter tractable for all of these parameters, we show that they allow different degrees of possible parallelization. For all four we develop dedicated parallel algorithms that are constructive, meaning that they output an optimal assignment - in contrast to results that can be obtained by parallel meta-theorems, which often only solve the decision version.",,0,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85150642689,,,,On the Representation Collapse of Sparse Mixture of Experts,cp,Conference Paper,Chi Z.,60014966;60016835;60026532,Peking University;Beijing Institute of Technology;Microsoft Corporation,Beijing;Beijing;Redmond,China;China;United States,12.0,"Chi, Zewen;Dong, Li;Huang, Shaohan;Dai, Damai;Ma, Shuming;Patra, Barun;Singhal, Saksham;Bajaj, Payal;Song, Xia;Mao, Xian Ling;Huang, Heyan;Wei, Furu",57211977105;57217793334;57188864311;57216617664;57200277125;57216617419;57219758188;57224850395;57192952195;56949703000;7405614195;23995914700,60016835;60026532;60026532;60014966;60026532;60026532;60026532;60026532;60026532;60016835;60016835;60026532,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Sparse mixture of experts provides larger model capacity while requiring a constant computational overhead. It employs the routing mechanism to distribute input tokens to the best-matched experts according to their hidden representations. However, learning such a routing mechanism encourages token clustering around expert centroids, implying a trend toward representation collapse. In this work, we propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. We conduct extensive experiments on cross-lingual language model pre-training and fine-tuning on downstream tasks. Experimental results across seven multilingual benchmarks show that our method achieves consistent gains. We also present a comprehensive analysis on the representation and routing behaviors of our models. Our method alleviates the representation collapse issue and achieves more consistent routing than the baseline mixture-of-experts methods.",,66,0.0,,,NSFC,2020AAA0106600,National Natural Science Foundation of China
2-s2.0-85131797951,,,,On the Robustness to Misspecification of α-posteriors and Their Variational Approximations,ar,Article,Medina M.A.,60030162,Columbia University,New York,United States,4.0,"Medina, Marco Avella;Olea, José Luis Montiel;Rush, Cynthia;Velez, Amilcar",56790031100;57211425435;57189364036;57214087273,60030162;60030162;60030162;60030162,2022-04-01,1 April 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"α-posteriors and their variational approximations distort standard posterior inference by downweighting the likelihood and introducing variational approximation errors. We show that such distortions, if tuned appropriately, reduce the Kullback–Leibler (KL) divergence from the true, but perhaps infeasible, posterior distribution when there is potential parametric model misspecification. To make this point, we derive a Bernstein–von Mises theorem showing convergence in total variation distance of α-posteriors and their variational approximations to limiting Gaussian distributions. We use these limiting distributions to evaluate the KL divergence between true and reported posteriors. We show that the KL divergence is minimized by choosing α strictly smaller than one, assuming there is a vanishingly small probability of model misspecification. The optimized value of α becomes smaller as the misspecification becomes more severe. The optimized KL divergence increases logarithmically in the magnitude of misspecification and not linearly as with the usual posterior. Moreover, the optimized variational approximations of α-posteriors can induce additional robustness to model misspecification beyond that obtained by optimally downweighting the likelihood.",model misspecification | robustness | variational inference | α-posterior,13,0.0,,,NSF,1849883,National Science Foundation
2-s2.0-105018457987,,,,On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control,ar,Article,Bedi A.S.,60017161;60003269;60154598;60026532;60028558,National University of Singapore;Princeton University;College of Engineering and Computer Science;Microsoft Corporation;JPMorgan Chase & Co.,Singapore City;Princeton;Orlando;Redmond;New York,Singapore;United States;United States;United States;United States,5.0,"Bedi, Amrit Singh;Parayil, Anjaly;Zhang, Junyu;Wang, Mengdi;Koppel, Alec",57147073300;57202265189;57190383048;57215322066;56304202600,60154598;60026532;60017161;60003269;60028558,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Reinforcement learning is a framework for interactive decision-making with incentives sequentially revealed across time without a system dynamics model. Due to its scaling to continuous spaces, we focus on policy search where one iteratively improves a parameterized policy with stochastic policy gradient (PG) updates. In tabular Markov Decision Problems (MDPs), under persistent exploration and suitable parameterization, global optimality may be obtained. By contrast, in continuous space, the non-convexity poses a pathological challenge as evidenced by existing convergence results being mostly limited to stationarity or arbitrary local extrema. To close this gap, we step towards persistent exploration in continuous space through policy parameterizations defined by distributions of heavier tails defined by tail-index parameter α, which increases the likelihood of jumping in state space. Doing so invalidates smoothness conditions of the score function common to PG. Thus, we establish how the convergence rate to stationarity depends on the policy’s tail index α, a H€older continuity parameter, integrability conditions, and an exploration tolerance parameter introduced here for the first time. Further, we characterize the dependence of the set of local maxima on the tail index through an exit and transition time analysis of a suitably defined Markov chain, identifying that policies associated with Lévy Processes of a heavier tail converge to wider peaks. This phenomenon yields improved stability to perturbations in supervised learning, which we corroborate also manifests in improved performance of policy search, especially when myopic and farsighted incentives are misaligned.",heavy-tailed distributions | non-convex optimization | Policy gradient algorithm,3,0.0,,,ONR,1006977,Office of Naval Research
2-s2.0-85135957534,10.1613/jair.1.13283,,,On the Tractability of SHAP Explanations,ar,Article,Van den Broeck G.,60015481;60027550,"University of Washington;University of California, Los Angeles",Seattle;Los Angeles,United States;United States,4.0,"Van den Broeck, Guy;Lykov, Anton;Schleich, Maximilian;Suciu, Dan",35185591300;57221146354;57190389877;7006812452,60027550;60015481;60015481;60015481,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,851-886,"Shap explanations are a popular feature-attribution mechanism for explainable AI. They use game-theoretic notions to measure the influence of individual features on the prediction of a machine learning model. Despite a lot of recent interest from both academia and industry, it is not known whether Shap explanations of common machine learning models can be computed efficiently. In this paper, we establish the complexity of computing the Shap explanation in three important settings. First, we consider fully-factorized data distributions, and show that the complexity of computing the Shap explanation is the same as the complexity of computing the expected value of the model. This fully-factorized setting is often used to simplify the Shap computation, yet our results show that the computation can be intractable for commonly used models such as logistic regression. Going beyond fully-factorized distributions, we show that computing Shap explanations is already intractable for a very simple setting: computing Shap explanations of trivial classifiers over naive Bayes distributions. Finally, we show that even computing Shap over the empirical distribution is #P-hard.",,330,1.0,all publisherfullgold,All Open Access Gold,DARPA,N66001-17-2-4032,Defense Advanced Research Projects Agency
2-s2.0-85141349496,,,,On the detrimental effect of invariances in the likelihood for variational inference,cp,Conference Paper,Kurle R.,60106550;60121951;128766331,Hasso-Plattner-Institut für Softwaresystemtechnik GmbH;Zalando SE;AWS AI Labs,Potsdam;Berlin;Pasadena,Germany;Germany;United States,5.0,"Kurle, Richard;Herbrich, Ralf;Januschowski, Tim;Wang, Yuyang;Gasthaus, Jan",57218718592;57203696645;37761498300;57198522502;34874812300,128766331;60106550;60121951;128766331;128766331,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Variational Bayesian posterior inference often requires simplifying approximations such as mean-field parametrisation to ensure tractability. However, prior work has associated the variational mean-field approximation for Bayesian neural networks with underfitting in the case of small datasets or large model sizes. In this work, we show that invariances in the likelihood function of over-parametrised models contribute to this phenomenon because these invariances complicate the structure of the posterior by introducing discrete and/or continuous modes which cannot be well approximated by Gaussian mean-field distributions. In particular, we show that the mean-field approximation has an additional gap in the evidence lower bound compared to a purpose-built posterior that takes into account the known invariances. Importantly, this invariance gap is not constant; it vanishes as the approximation reverts to the prior. We proceed by first considering translation invariances in a linear model with a single data point in detail. We show that, while the true posterior can be constructed from a mean-field parametrisation, this is achieved only if the objective function takes into account the invariance gap. Then, we transfer our analysis of the linear model to neural networks. Our analysis provides a framework for future work to explore solutions to the invariance problem.",,5,0.0,,,,,
2-s2.0-85148758146,,,,On-Demand Sampling: Learning Optimally from Multiple Distributions,cp,Conference Paper,Haghtalab N.,60025038,"University of California, Berkeley",Berkeley,United States,3.0,"Haghtalab, Nika;Jordan, Michael I.;Zhao, Eric",56394619900;57209168184;57224933126,60025038;60025038;60025038,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Societal and real-world considerations such as robustness, fairness, social welfare and multi-agent tradeoffs have given rise to multi-distribution learning paradigms, such as collaborative [5], group distributionally robust [36], and fair federated learning [27]. In each of these settings, a learner seeks to minimize its worst-case loss over a set of n predefined distributions, while using as few samples as possible. In this paper, we establish the optimal sample complexity of these learning paradigms and give algorithms that meet this sample complexity. Importantly, our sample complexity bounds exceed that of the sample complexity of learning a single distribution only by an additive factor of (Equation presented). These improve upon the best known sample complexity of agnostic federated learning by Mohri et al. [27] by a multiplicative factor of n, the sample complexity of collaborative learning by Nguyen and Zakynthinou [29] by a multiplicative factor (Equation presented), and give the first sample complexity bounds for the group DRO objective of Sagawa et al. [36]. To achieve optimal sample complexity, our algorithms learn to sample and learn from distributions on demand. Our algorithm design and analysis extends stochastic optimization techniques to solve zero-sum games in a new stochastic setting.",,30,0.0,,,NSF,CCF-2145898,National Science Foundation
2-s2.0-85170384101,10.24963/ijcai.2023/559,,,One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER,cp,Conference Paper,Chen X.,60003970;60118460;127617948,Zhejiang University;Alibaba Group Holding Limited;Donghai Laboratory,Hangzhou;Hangzhou;Donghai,China;China;China,8.0,"Chen, Xiang;Li, Lei;Qiao, Shuofei;Zhang, Ningyu;Tan, Chuanqi;Jiang, Yong;Huang, Fei;Chen, Huajun",57069246400;57420881800;57882121900;55923601900;57221151395;57195958145;57210150087;35268022500,60003970;60003970;60003970;60003970;60118460;60118460;60118460;60003970-127617948,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,5030-5038,"Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate the potential of PLMs to handle NER tasks across various domains. Experimental results on the Cross-NER benchmark show that the proposed approach has flexible transfer ability and performs better on both one-source and multiple-source cross-domain NER tasks.",,11,1.0,all publisherfullgold,All Open Access Gold,NSFC,A-0008542-00-00,Natural Science Foundation of Ningbo Municipality
2-s2.0-85170386308,10.24963/ijcai.2023/476,,,"One Model, Any CSP: Graph Neural Networks as Fast Global Search Heuristics for Constraint Satisfaction",cp,Conference Paper,Tönshoff J.,60016653,Rheinisch-Westfälische Technische Hochschule Aachen,Aachen,Germany,4.0,"Tönshoff, Jan;Kisin, Berke;Lindner, Jakob;Grohe, Martin",57312349600;57866194800;57866805500;7004249112,60016653;60016653;60016653;60016653,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,4280-4288,"We propose a universal Graph Neural Network architecture which can be trained as an end-2-end search heuristic for any Constraint Satisfaction Problem (CSP). Our architecture can be trained unsupervised with policy gradient descent to generate problem specific heuristics for any CSP in a purely data driven manner. The approach is based on a novel graph representation for CSPs that is both generic and compact and enables us to process every possible CSP instance with one GNN, regardless of constraint arity, relations or domain size. Unlike previous RL-based methods, we operate on a global search action space and allow our GNN to modify any number of variables in every step of the stochastic search. This enables our method to properly leverage the inherent parallelism of GNNs. We perform a thorough empirical evaluation where we learn heuristics for well known and important CSPs, both decision and optimisation problems, from random data, including graph coloring, MAXCUT, and MAX-k-SAT, and the general RB model. Our approach significantly outperforms prior end-2-end approaches for neural combinatorial optimization. It can compete with conventional heuristics and solvers on test instances that are several orders of magnitude larger and structurally more complex than those seen during training.",,8,1.0,all publisherfullgold,All Open Access Gold,DFG,GR 1492/16-1,Deutsche Forschungsgemeinschaft
2-s2.0-85181886412,10.1613/JAIR.1.14820,,,Online Bin Packing with Predictions,ar,Article,Angelopoulos S.,60001422;60009697;60033420,Sorbonne Université;University of Manitoba;York University,Paris;Winnipeg;Toronto,France;Canada;Canada,3.0,"Angelopoulos, Spyros;Kamali, Shahin;Shadkami, Kimia",57193202746;15845705100;57222275596,60001422;60033420;60009697,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,1111-1141,"Bin packing is a classic optimization problem with a wide range of applications, from load balancing to supply chain management. In this work, we study the online variant of the problem, in which a sequence of items of various sizes must be placed into a minimum number of bins of uniform capacity. The online algorithm is enhanced with a potentially erroneous prediction concerning the frequency of item sizes in the sequence. We design and analyze online algorithms with efficient tradeoffs between the consistency, which is the competitive ratio assuming no prediction error, and the robustness, which is the competitive ratio under adversarial error. Moreover, we demonstrate that the performance of our algorithm degrades near-optimally as a function of the prediction error. This is the first theoretical and experimental study of online bin packing under competitive analysis in the realistic setting of learnable predictions. Previous work addressed only extreme cases with respect to the prediction error and relied on overly powerful and error-free oracles.",,16,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,ANR,ANR-23-CE48-0010,Agence Nationale de la Recherche
2-s2.0-85129523271,10.1609/aaai.v36i6.20606,,,Online Certification of Preference-Based Fairness for Personalized Recommender Systems,cp,Conference Paper,Do V.,60105740;60355330,Laboratoire d’Analyse et de Modélisation de Systèmes pour l’Aide à la Décision;Meta Ai,Paris;Menlo Park,France;United States,4.0,"Do, Virginie;Corbett-Davies, Sam;Atif, Jamal;Usunier, Nicolas",57224481958;55579310100;23007702600;8558715300,60105740-60355330;60355330;60105740;60355330,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,6532-6540,"Recommender systems are facing scrutiny because of their growing impact on the opportunities we have access to. Current audits for fairness are limited to coarse-grained parity assessments at the level of sensitive groups. We propose to audit for envy-freeness, a more granular criterion aligned with individual preferences: every user should prefer their recommendations to those of other users. Since auditing for envy requires to estimate the preferences of users beyond their existing recommendations, we cast the audit as a new pure exploration problem in multi-armed bandits. We propose a sample-efficient algorithm with theoretical guarantees that it does not deteriorate user experience. We also study the tradeoffs achieved on real-world recommendation datasets.",,25,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85204300971,,,,Online Combinatorial Optimization with Group Fairness Constraints,cp,Conference Paper,Golrezaei N.,60014228;60112748;60074721,MIT Sloan School of Management;The University of Chicago Booth School of Business;Toyota Technological Institute at Chicago,Cambridge;Chicago;Chicago,United States;United States;United States,4.0,"Golrezaei, Negin;Niazadeh, Rad;Patel, Kumar Kshitij;Susan, Fransisca",48360936300;34768669800;57203859586;57222311991,60014228;60112748;60074721;60014228,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,394-402,"As digital marketplaces and services continue to expand, it is crucial to maintain a safe and fair environment for all users. This requires implementing fairness constraints into the sequential decision-making processes of these platforms to ensure equal treatment. However, this can be challenging as these processes often need to solve NP-complete problems with exponentially large decision spaces at each time step. To overcome this, we propose a general framework incorporating robustness and fairness into NP-complete problems, such as optimizing product ranking and maximizing submodular functions. Our framework casts the problem as a max-min game between a primal player aiming to maximize the platform's objective and a dual player in charge of group fairness constraints. We show that one can trace the entire Pareto fairness curve by changing the thresholds on the fairness constraints. We provide theoretical guarantees for our method and empirically evaluate it, demonstrating its effectiveness.",,0,0.0,,,DARPA,N00014-23-1-2584,Defense Advanced Research Projects Agency
2-s2.0-85204280897,,,,Online Learning of Capacity-Based Preference Models,cp,Conference Paper,Herin M.,60001422,Sorbonne Université,Paris,France,3.0,"Herin, Margot;Perny, Patrice;Sokolovska, Nataliya",57953881000;6603291973;25652236200,60001422;60001422;60001422,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,7118-7126,"In multicriteria decision making, sophisticated decision models often involve a non-additive set function (named capacity) to define the weights of all subsets of criteria. This makes it possible to model criteria interactions, leaving room for a diversity of attitudes in criteria aggregation. Fitting a capacity-based decision model to a given Decision Maker is a challenging problem and several batch learning methods have been proposed in the literature to derive the capacity from a database of preference examples. In this paper, we introduce an online algorithm for learning a sparse representation of the capacity, designed for decision contexts where preference examples become available sequentially. Our method based on regularized dual averaging is also well fitted to decision contexts involving a large number of preference examples or a large number of criteria. Moreover, we propose a variant making it possible to include normative constraints on the capacity (e.g., monotonicity, supermodularity) while preserving scalability, based on the alternating direction method of multipliers.",,1,0.0,,,,,
2-s2.0-85135541831,,,,Online and Consistent Correlation Clustering,cp,Conference Paper,Cohen-Addad V.,60028186;60006191,École Polytechnique Fédérale de Lausanne;Google LLC,Lausanne;Mountain View,Switzerland;United States,4.0,"Cohen-Addad, Vincent;Lattanzi, Silvio;Maggiori, Andreas;Parotsidis, Nikos",56786592300;24081026500;57214226943;55860165400,60006191;60006191;60028186;60006191,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,4157-4179,"In the correlation clustering problem the input is a signed graph where the sign indicates whether each pair of points should be placed in the same cluster or not. The goal of the problem is to compute a clustering which minimizes the number of disagreements with such recommendation. Thanks to its many practical applications, correlation clustering is a fundamental unsupervised learning problem and has been extensively studied in many different settings. In this paper we study the problem in the classic online setting with recourse; The vertices of the graphs arrive in an online manner and the goal is to maintain an approximate clustering while minimizing the number of times each vertex changes cluster. Our main contribution is an algorithm that achieves logarithmic recourse per vertex in the worst case. We also complement this result with a tight lower bound. Finally we show experimentally that our algorithm achieves better performances than state-of-the-art algorithms on real world data.",,14,0.0,,,,200020–182517,
2-s2.0-85204874277,10.1613/jair.1.15118,,,Opening the Analogical Portal to Explainability: Can Analogies Help Laypeople in AI-assisted Decision Making?,ar,Article,He G.,60006288,Delft University of Technology,Delft,Netherlands,5.0,"He, Gaole;Balayn, Agathe;Buijsman, Stefan;Yang, Jie;Gadiraju, Ujwal",57206481668;57193700401;57148418200;56370016500;56016850700,60006288;60006288;60006288;60006288;60006288,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,117-162,"Concepts are an important construct in semantics, based on which humans understand the world with various levels of abstraction. With the recent advances in explainable artificial intelligence (XAI), concept-level explanations are receiving an increasing amount of attention from the broad research community. However, laypeople may find such explanations difficult to digest due to the potential knowledge gap and the concomitant cognitive load. Inspired by prior work that has explored analogies and sensemaking, we argue that augmenting concept-level explanations with analogical inference information from commonsense knowledge can be a potential solution to tackle this issue. To investigate the validity of our proposition, we first designed an effective analogy-based explanation generation method and collected 600 analogy-based explanations from 100 crowd workers. Next, we proposed a set of structured dimensions for the qualitative assessment of such explanations, and conducted an empirical evaluation of the generated analogies with experts. Our findings revealed significant positive correlations between the qualitative dimensions of analogies and the perceived helpfulness of analogy-based explanations, suggesting the effectiveness of the dimensions. To understand the practical utility and the effectiveness of analogybased explanations in assisting human decision-making, we conducted a follow-up empirical study (N = 280) on a skin cancer detection task with non-expert humans and an imperfect AI system. Thus, we designed a between-subjects study spanning five different experimental conditions with varying types of explanations. The results of our study confirmed that a knowledge gap can prevent participants from understanding concept-level explanations. Consequently, when only the target domain of our designed analogy-based explanation was provided (in a specific experimental condition), participants demonstrated relatively more appropriate reliance on the AI system. In contrast to our expectations, we found that analogies were not effective in fostering appropriate reliance. We carried out a qualitative analysis of the open-ended responses from participants in the study regarding their perceived usefulness of explanations and analogies. Our findings suggest that human intuition and the perceived plausibility of analogies may have played a role in affecting user reliance on the AI system. We also found that the understanding of commonsense explanations varied with the varying experience of the recipient user, which points out the need for further work on personalization when leveraging commonsense explanations. In summary, although we did not find quantitative support for our hypotheses around the benefits of using analogies, we found considerable qualitative evidence suggesting the potential of high-quality analogies in aiding non-expert users in their decision making with AI-assistance. These insights can inform the design of future methods for the generation and use of effective analogy-based explanations.",,5,1.0,all publisherfullgold,All Open Access Gold,,EINF-3888,
2-s2.0-85174421764,,,,Opponent-Limited Online Search for Imperfect Information Games,cp,Conference Paper,Liu W.,60114181,Tencent,Shenzhen,China,4.0,"Liu, Weiming;Fu, Haobo;Fu, Qiang;Yang, Wei",57191071738;57219645569;57219496314;57211252391,60114181;60114181;60114181;60114181,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,22233-22248,"In recent years, online search has been playing an increasingly important role in imperfect information games (IIGs). Previous online search is known as common-knowledge subgame solving, which has to consider all the states in a common-knowledge closure. This is only computationally tolerable for medium size games, such as poker. To handle larger games, order-1 Knowledge-Limited Subgame Solving (1-KLSS) only considers the states in a knowledge-limited closure, which results in a much smaller subgame. However, 1-KLSS is unsafe. In this paper, we first extend 1-KLSS to Safe-1-KLSS and prove its safeness. To make Safe-1-KLSS applicable to even larger games, we propose Opponent-Limited Subgame Solving (OLSS) to limit how the opponent reaches a subgame and how it acts in the subgame. Limiting the opponent's strategy dramatically reduces the subgame size and improves the efficiency of subgame solving while still preserving some safety in the limit. Experiments in medium size poker show that Safe-1-KLSS and OLSS are orders of magnitude faster than previous common-knowledge subgame solving. Also, OLSS significantly improves the online performance in a two-player Mahjong game, whose game size prohibits the use of previous common-knowledge subgame-solving methods.",,1,0.0,,,,,
2-s2.0-105018577519,,,,"Optimal Bump Functions for Shallow ReLU networks: Weight Decay, Depth Separation, Curse of Dimensionality",ar,Article,Wojtowytsch S.,60015543,University of Pittsburgh,Pittsburgh,United States,1.0,"Wojtowytsch, Stephan",57191342051,60015543,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"In this note, we study how neural networks with a single hidden layer and ReLU activation interpolate data drawn from a radially symmetric distribution with target labels 1 at the origin and 0 outside the unit ball, if no labels are known inside the unit ball. With weight decay regularization and in the infinite neuron, infinite data limit, we prove that a unique radially symmetric minimizer exists, whose average parameters and Lipschitz constant grow as d and √d respectively. We furthermore show that the average weight variable grows exponentially in d if the label 1 is imposed on a ball of radius ε rather than just at the origin. By comparison, a neural networks with two hidden layers can approximate the target function without encountering the curse of dimensionality.",Barron space | compact support | curse of dimensionality | Deep learning | depth separation | explicit regularization | minimum norm solution | mollifier | radial symmetry | Radon-BV | symmetry learning | weight decay,2,0.0,,,,,
2-s2.0-85174405712,,,,Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning,cp,Conference Paper,Wang T.,60022195;60013372;60355330,Massachusetts Institute of Technology;The University of Texas at Austin;Meta Ai,Cambridge;Austin;Menlo Park,United States;United States;United States,4.0,"Wang, Tongzhou;Torralba, Antonio;Isola, Phillip;Zhang, Amy",57200619798;7005432728;50561507500;57204798871,60022195;60022195;60022195;60013372-60355330,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,36411-36430,"In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.",,21,0.0,,,MURI,N00014-22-1-2740,Google
2-s2.0-85162115531,10.1613/JAIR.1.14126,,,Optimal and Efficient Auctions for the Gradual Procurement of Strategic Service Provider Agents,ar,Article,Farhadi F.,60000891;60014551,Loughborough University;Aston University,Loughborough;Birmingham,United Kingdom;United Kingdom,3.0,"Farhadi, Farzaneh;Chli, Maria;Jennings, Nicholas R.",57201783774;8540855100;7005888644,60014551;60014551;60000891,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,76,,,959-1018,"We consider an outsourcing problem where a software agent procures multiple services from providers with uncertain reliabilities to complete a computational task before a strict deadline. The service consumer's goal is to design an outsourcing strategy (defining which services to procure and when) so as to maximize a specific objective function. This objective function can be different based on the consumer's nature; a socially-focused consumer often aims to maximize social welfare, while a self-interested consumer often aims to maximize its own utility. However, in both cases, the objective function depends on the providers' execution costs, which are privately held by the self-interested providers and hence may be misreported to influence the consumer's decisions. For such settings, we develop a unified approach to design truthful procurement auctions that can be used by both socially-focused and, separately, self-interested consumers. This approach benefits from our proposed weighted threshold payment scheme which pays the provably minimum amount to make an auction with a monotone outsourcing strategy incentive compatible. This payment scheme can handle contingent outsourcing plans, where additional procurement happens gradually over time and only if the success probability of the already hired providers drops below a time-dependent threshold. Using a weighted threshold payment scheme, we design two procurement auctions that maximize, as well as two low-complexity heuristic-based auctions that approximately maximize, the consumer's expected utility and expected social welfare, respectively. We demonstrate the effectiveness and strength of our proposed auctions through both game-theoretical and empirical analysis.",,2,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85172426522,10.1613/jair.1.14525,,,Optimality Guarantees for Particle Belief Approximation of POMDPs,ar,Article,Lim M.H.,60154476;60121438;60003743,College of Engineering and Applied Science;Department of Electrical Engineering and Computer Sciences;Department of Aeronautics and Astronautics,Boulder;Berkeley;Stanford,United States;United States;United States,5.0,"Lim, Michael H.;Becker, Tyler J.;Kochenderfer, Mykel J.;Tomlin, Claire J.;Sunberg, Zachary N.",57219629399;57938025900;24824356000;7005284849;55491073900,60121438;60154476;60003743;60121438;60154476,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,1591-1636,"Partially observable Markov decision processes (POMDPs) provide a flexible representation for real-world decision and control problems. However, POMDPs are notoriously difficult to solve, especially when the state and observation spaces are continuous or hybrid, which is often the case for physical systems. While recent online sampling-based POMDP algorithms that plan with observation likelihood weighting have shown practical effectiveness, a general theory characterizing the approximation error of the particle filtering techniques that these algorithms use has not previously been proposed. Our main contribution is bounding the error between any POMDP and its corresponding finite sample particle belief MDP (PB-MDP) approximation. This fundamental bridge between PB-MDPs and POMDPs allows us to adapt any sampling-based MDP algorithm to a POMDP by solving the corresponding particle belief MDP, thereby extending the convergence guarantees of the MDP algorithm to the POMDP. Practically, this is implemented by using the particle filter belief transition model as the generative model for the MDP solver. While this requires access to the observation density model from the POMDP, it only increases the transition sampling complexity of the MDP solver by a factor of O(C), where C is the number of particles. Thus, when combined with sparse sampling MDP algorithms, this approach can yield algorithms for POMDPs that have no direct theoretical dependence on the size of the state and observation spaces. In addition to our theoretical contribution, we perform five numerical experiments on benchmark POMDPs to demonstrate that a simple MDP algorithm adapted using PB-MDP approximation, Sparse-PFT, achieves performance competitive with other leading continuous observation POMDP solvers.",,12,1.0,all publisherfullgold,All Open Access Gold,DARPA,1752814,Defense Advanced Research Projects Agency
2-s2.0-85170374308,10.24963/ijcai.2023/836,,,Optimized Crystallographic Graph Generation for Material Science,cp,Conference Paper,Klipfel A.,60018178,Université d'Artois,Arras,France,4.0,"Klipfel, Astrid;Frégier, Yaël;Sayede, Adlane;Bouraoui, Zied",57886909500;24721389700;8546409000;57003243800,60018178;60018178;60018178;60018178,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,7145-7148,"Graph neural networks are widely used in machine learning applied to chemistry, and in particular for material science discovery. For crystalline materials, however, generating graph-based representation from geometrical information for neural networks is not a trivial task. The periodicity of crystalline needs efficient implementations to be processed in real-time under a massively parallel environment. With the aim of training graph-based generative models of new material discovery, we propose an efficient tool to generate cutoff graphs and k-nearest-neighbours graphs of periodic structures within GPU optimization. We provide pyMatGraph a Pytorch-compatible framework to generate graphs in real-time during the training of neural network architecture. Our tool can update a graph of a structure, making generative models able to update the geometry and process the updated graph during the forward propagation on the GPU side. Our code is publicly available at https://github.com/aklipf/mat-graph.",,2,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,ANR-20-THIA-0004,
2-s2.0-85204293115,,,,Optimizing Viscous Democracy,cp,Conference Paper,Armstrong B.,60014171;60027161,University of Waterloo;Ben-Gurion University of the Negev,Waterloo;Beer-Sheva,Canada;Israel,3.0,"Armstrong, Ben;Alouf-Heffetz, Shiri;Talmon, Nimrod",57226656875;57816391400;55537220900,60014171;60027161;60027161,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2643-2650,"Viscous democracy is a generalization of liquid democracy, a social choice framework in which voters may transitively delegate their votes.In viscous democracy, a”viscosity” factor decreases the weight of a delegation the further it travels, reducing the chance of excessive weight flowing between ideologically misaligned voters.We demonstrate that viscous democracy often significantly improves the quality of group decision-making over liquid democracy.We first show that finding optimal delegations within a viscous setting is NP-hard.However, simulations allow us to explore the practical effects of viscosity.Across social network structures, competence distributions, and delegation mechanisms we find high viscosity reduces the chance of “super-voters” attaining large amounts of weight and increases the number of voters that are able to affect the outcome of elections.This, in turn, improves group accuracy as a whole.As a result, we argue that viscosity should be considered a core component of liquid democracy.",,0,0.0,,,,,
2-s2.0-105000476001,,,,Oracle-Efficient Reinforcement Learning for Max Value Ensembles,cp,Conference Paper,Hussing M.,60102562;60145911,School of Engineering and Applied Science;Whiting School of Engineering,Philadelphia;Baltimore,United States;United States,5.0,"Hussing, Marcel;Kearns, Michael;Roth, Aaron;Sengupta, Sikata Bela;Sorrell, Jessica",57219621152;7005078406;8247437600;59193174200;57202917053,60102562;60102562;60102562;60102562;60145911,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Reinforcement learning (RL) in large or infinite state spaces is notoriously challenging, both theoretically (where worst-case sample and computational complexities must scale with state space cardinality) and experimentally (where function approximation and policy gradient techniques often scale poorly and suffer from instability and high variance). One line of research attempting to address these difficulties makes the natural assumption that we are given a collection of base or constituent policies (possibly heuristic) upon which we would like to improve in a scalable manner. In this work we aim to compete with the max-following policy, which at each state follows the action of whichever constituent policy has the highest value. The max-following policy is always at least as good as the best constituent policy, and may be considerably better. Our main result is an efficient algorithm that learns to compete with the max-following policy, given only access to the constituent policies (but not their value functions). In contrast to prior work in similar settings, our theoretical results require only the minimal assumption of an ERM oracle for value function approximation for the constituent policies (and not the global optimal policy or the max-following policy itself) on samplable distributions. We illustrate our algorithm's experimental effectiveness and behavior on several robotic simulation testbeds.",,1,0.0,,,ARO,W911NF2010080,Army Research Office
2-s2.0-85132023872,10.1613/jair.1.13317,,,Ordinal Maximin Share Approximation for Goods,ar,Article,Hosseini H.,60001439;60022054;60080064,Pennsylvania State University;Johns Hopkins University Applied Physics Laboratory;Ariel University,University Park;Laurel;Ariel,United States;United States;Israel,3.0,"Hosseini, Hadi;Searns, Andrew;Segal-Halevi, Erel",57197996369;57208051984;56422378800,60001439;60022054;60080064,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,353-391,"In fair division of indivisible goods, `-out-of-d maximin share (MMS) is the value that an agent can guarantee by partitioning the goods into d bundles and choosing the ` least preferred bundles. Most existing works aim to guarantee to all agents a constant fraction of their 1-out-of-n MMS. But this guarantee is sensitive to small perturbation in agents’ cardinal valuations. We consider a more robust approximation notion, which depends only on the agents’ ordinal rankings of bundles. We prove the existence of `-out-of-b(` + <sup>1</sup><inf>2</inf> )nc MMS allocations of goods for any integer ` ≥ 1, and present a polynomial-time algorithm that finds a 1-out-of-d<sup>3</sup><inf>2</inf><sup>n</sup> e MMS allocation when ` = 1. We further develop an algorithm that provides a weaker ordinal approximation to MMS for any ` > 1.",,18,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,ISF,712/20,Israel Science Foundation
2-s2.0-85203796268,,,,Outlier-Aware Slicing for Post-Training Quantization in Vision Transformer,cp,Conference Paper,Ma Y.,60018205;60271961;60159665,Xiamen University;Peng Cheng Laboratory;ByteDance Ltd.,Xiamen;Shenzhen;Beijing,China;China;China,9.0,"Ma, Yuexiao;Li, Huixia;Zheng, Xiawu;Ling, Feng;Xiao, Xuefeng;Wang, Rui;Wen, Shilei;Chao, Fei;Ji, Rongrong",57238548700;57220589057;57204472355;58960486000;57239031200;59804923300;58164292900;54794935400;23134935200,60018205;60159665;60018205-60271961;60159665;60159665;60159665;60159665;60018205;60018205,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,33811-33825,"Post-Training Quantization (PTQ) is a vital technique for network compression and acceleration, gaining prominence as model sizes increase. This paper addresses a critical challenge in PTQ: the severe impact of outliers on the accuracy of quantized transformer architectures. Specifically, we introduce the concept of 'reconstruction granularity' as a novel solution to this issue, which has been overlooked in previous works. Our work provides theoretical insights into the role of reconstruction granularity in mitigating the outlier problem in transformer models. This theoretical framework is supported by empirical analysis, demonstrating that varying reconstruction granularities significantly influence quantization performance. Our findings indicate that different architectural designs necessitate distinct optimal reconstruction granularities. For instance, the multi-stage Swin Transformer architecture benefits from finer granularity, a deviation from the trends observed in ViT and DeiT models. We further develop an algorithm for determining the optimal reconstruction granularity for various ViT models, achieving state-of-the-art (SOTA) performance in PTQ. For example, applying our method to 4-bit quantization, the Swin-Base model achieves a Top-1 accuracy of 82.24% on the ImageNet classification task. This result surpasses the RepQ-ViT by 3.92% (82.24% VS 78.32%). Similarly, our approach elevates the ViT-Small to a Top-1 accuracy of 80.50%, outperforming NoisyQuant by 3.64% (80.50% VS 76.86%).",,2,0.0,,,NSFC,62176223,National Natural Science Foundation of China
2-s2.0-85170381531,10.24963/ijcai.2023/453,,,Overlooked Implications of the Reconstruction Loss for VAE Disentanglement,cp,Conference Paper,Michlo N.,60016218,"University of the Witwatersrand, Johannesburg",Johannesburg,South Africa,3.0,"Michlo, Nathan;Klein, Richard;James, Steven",57543772900;57188701055;57195953434,60016218;60016218;60016218,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,4073-4081,"Learning disentangled representations with variational autoencoders (VAEs) is often attributed to the regularisation component of the loss. In this work, we highlight the interaction between data and the reconstruction term of the loss as the main contributor to disentanglement in VAEs. We show that standard benchmark datasets have unintended correlations between their subjective ground-truth factors and perceived axes in the data according to typical VAE reconstruction losses. Our work exploits this relationship to provide a theory for what constitutes an adversarial dataset under a given reconstruction loss. We verify this by constructing an example dataset that prevents disentanglement in state-of-the-art frameworks while maintaining human-intuitive ground-truth factors. Finally, we re-enable disentanglement by designing an example reconstruction loss that is once again able to perceive the ground-truth factors. Our findings demonstrate the subjective nature of disentanglement and the importance of considering the interaction between the ground-truth factors, data and notably, the reconstruction loss, which is under-recognised in the literature.",,3,1.0,all publisherfullgold,All Open Access Gold,,,"University of the Witwatersrand, Johannesburg"
2-s2.0-85189648965,10.1609/aaai.v38i19.30094,,,P2BPO: Permeable Penalty Barrier-Based Policy Optimization for Safe RL,cp,Conference Paper,Dey S.,60004750;60009778,Indian Institute of Technology Kharagpur;Synopsys Incorporated,Kharagpur;Sunnyvale,India;United States,3.0,"Dey, Sumanta;Dasgupta, Pallab;Dey, Soumyajit",57212483091;55737931500;14522293600,60004750;60009778;60004750,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,19.0,,21029-21036,"Safe Reinforcement Learning (SRL) algorithms aim to learn a policy that maximizes the reward while satisfying the safety constraints. One of the challenges in SRL is that it is often difficult to balance the two objectives of reward maximization and safety constraint satisfaction. Existing algorithms utilize constraint optimization techniques like penalty-based, barrier penalty-based, and Lagrangian-based dual or primal policy optimization methods. However, they suffer from training oscillations and approximation errors, which impact the overall learning objectives. This paper proposes the Permeable Penalty Barrier-based Policy Optimization (P2BPO) algorithm that addresses this issue by allowing a small fraction of penalty beyond the penalty barrier, and a parameter is used to control this permeability. In addition, an adaptive penalty parameter is used instead of a constant one, which is initialized with a low value and increased gradually as the agent violates the safety constraints. We have also provided theoretical proof of the proposed method's performance guarantee bound, which ensures that P2BPO can learn a policy satisfying the safety constraints with high probability while achieving a higher expected reward. Furthermore, we compare P2BPO with other SRL algorithms on various SRL tasks and demonstrate that it achieves better rewards while adhering to the constraints.",,2,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85124236261,,,,PAC Guarantees and Effective Algorithms for Detecting Novel Categories,ar,Article,Liu S.,60010261;60013402;60121438;60137364,Washington University in St. Louis;Oregon State University;Department of Electrical Engineering and Computer Sciences;College of Engineering,St. Louis;Corvallis;Berkeley;Corvallis,United States;United States;United States;United States,6.0,"Liu, Si;Garrepalli, Risheek;Hendrycks, Dan;Fern, Alan;Mondal, Debashis;Dietterich, Thomas G.",57204806381;57204808729;57204810366;12141549500;24765130600;6603937120,60013402;60137364;60121438;60137364;60010261;60137364,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Open category detection is the problem of detecting “alien” test instances that belong to categories or classes that were not present in the training data (Liu et al., 2018). In many applications, reliably detecting such aliens is central to ensuring the safety and accuracy of test set predictions. Unfortunately, there are no algorithms that provide theoretical guarantees on their ability to detect aliens under general assumptions. Further, while there are algorithms for open category detection, there are few empirical results that directly report alien detection rates. Thus, there are significant theoretical and empirical gaps in our understanding of open category detection. In this paper, we take a step toward addressing this gap by studying a simple, but practically-relevant variant of open category detection. In our setting, we are provided with a “clean” training set that contains only the target categories of interest and an unlabeled “contaminated” training set that contains a fraction α of alien examples. Under the assumption that we know an upper bound on α, we develop an algorithm that gives PAC-style guarantees on the alien detection rate, while aiming to minimize false alarms. Given an overall budget on the amount of training data, we also derive the optimal allocation of samples between the mixture and the clean data sets. Experiments on synthetic and standard benchmark datasets evaluate the regimes in which the algorithm can be effective and provide a baseline for further advancements. In addition, for the situation when an upper bound for α is not available, we employ nine different anomaly proportion estimators, and run experiments on both synthetic and standard benchmark data sets to compare their performance.",Alien detection rate | Anomaly detection | False positive rate | Open category detection | PAC guarantees,8,0.0,,,NSF,FA8750-19-C-0092,National Science Foundation
2-s2.0-85128313329,,,,PECOS: Prediction for Enormous and Correlated Output Spaces,ar,Article,Yu H.F.,60013372;60006191;60076757,"The University of Texas at Austin;Google LLC;Amazon.com, Inc.",Austin;Mountain View;Seattle,United States;United States;United States,5.0,"Yu, Hsiang Fu;Zhong, Kai;Zhang, Jiong;Chang, Wei Cheng;Dhillon, Inderjit S.",57141060300;56735682700;57192154042;57195631837;6603876297,60076757;60076757;60076757;60076757;60013372-60006191,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Many large-scale applications amount to finding relevant results from an enormous output space of potential candidates. For example, finding the best matching product from a large catalog or suggesting related search phrases on a search engine. The size of the output space for these problems can range from millions to billions, and can even be infinite in some applications. More- over, training data is often limited for the \long-tail""items in the output space. Fortunately, items in the output space are often correlated thereby presenting an opportunity to alleviate the data sparsity issue. In this paper, we propose the Prediction for Enormous and Correlated Out- put Spaces (PECOS) framework, a versatile and modular machine learning framework for solving prediction problems for very large output spaces, and apply it to the eXtreme Multilabel Rank- ing (XMR) problem: given an input instance, find and rank the most relevant items from an enormous but fixed and finite output space. We propose a three phase framework for PECOS: (i) in the first phase, PECOS organizes the output space using a semantic indexing scheme, (ii) in the second phase, PECOS uses the indexing to narrow down the output space by orders of mag- nitude using a machine learned matching scheme, and (iii) in the third phase, PECOS ranks the matched items using a final ranking scheme. The versatility and modularity of PECOS allows for easy plug-and-play of various choices for the indexing, matching, and ranking phases. The indexing and matching phases alleviate the data sparsity issue by leveraging correlations across different items in the output space. For the critical matching phase, we develop a recursive machine learned matching strategy with both linear and neural matchers. When applied to eXtreme Multilabel Ranking where the input instances are in textual form, we find that the recursive Transformer matcher gives state-of-the-art accuracy results, at the cost of two orders of magnitude increased training time compared to the recursive linear matcher. For example, on a dataset where the output space is of size 2.8 million, the recursive Transformer matcher results in a 6% increase in precision@1 (from 48.6% to 54.2%) over the recursive linear matcher but takes 100x more time to train. Thus it is up to the practitioner to evaluate the trade-offs and decide whether the increased training time and infrastructure cost is warranted for their application; indeed, the exibility of the PECOS framework seamlessly allows different strategies to be used. We also develop very fast inference procedures which allow us to perform XMR predictions in real time; for example, infer- ence takes less than 1 millisecond per input on the dataset with 2.8 million labels. The PECOS software is available at https://libpecos.org.",Extreme Multi-label Text Classification | Large Output Space Learning | Transform-ers,39,0.0,,,,,
2-s2.0-85174399725,,,,PINA: Leveraging Side Information in eXtreme Multi-label Classification via Predicted Instance Neighborhood Aggregation,cp,Conference Paper,Chien E.,60027550;60158506;60076757,"University of California, Los Angeles;The Grainger College of Engineering;Amazon.com, Inc.",Los Angeles;Urbana;Seattle,United States;United States;United States,7.0,"Chien, Eli;Zhang, Jiong;Hsieh, Cho Jui;Jiang, Jyun Yu;Chang, Wei Cheng;Milenkovic, Olgica;Yu, Hsiang Fu",57218450781;57192154042;24502954900;55536831700;57195631837;55946153300;57141060300,60158506;60076757;60027550;60076757;60076757;60158506;60076757,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,5616-5630,"The eXtreme Multi-label Classification (XMC) problem seeks to find relevant labels from an exceptionally large label space. Most of the existing XMC learners focus on the extraction of semantic features from input query text. However, conventional XMC studies usually neglect the side information of instances and labels, which can be of use in many real-world applications such as recommendation systems and e-commerce product search. We propose Predicted Instance Neighborhood Aggregation (PINA), a data enhancement method for the general XMC problem that leverages beneficial side information. Unlike most existing XMC frameworks that treat labels and input instances as featureless indicators and independent entries, PINA extracts information from the label metadata and the correlations among training instances. Extensive experimental results demonstrate the consistent gain of PINA on various XMC tasks compared to the state-of-the-art methods: PINA offers a gain in accuracy compared to standard XR-Transformers on five public benchmark datasets. Moreover, PINA achieves a ∼ 5% gain in accuracy on the largest dataset LF-AmazonTitles-1.3M. Our implementation is publicly available https://github.com/amzn/pecos/tree/mainline/examples/pina.",,1,0.0,,,NSF,1956384,National Science Foundation
2-s2.0-85190705096,,,,PLANCKIAN JITTER: COUNTERING THE COLOR-CRIPPLING EFFECTS OF COLOR JITTER ON SELF-SUPERVISED TRAINING,cp,Conference Paper,Zini S.,60021859;60023020;60012306;129016807,Università degli Studi di Firenze;Universitat Autònoma de Barcelona;Università degli Studi di Milano-Bicocca;IDEAS NCBR,Florence;Cerdanyola del Valles;Milan;Warsaw,Italy;Spain;Italy;Poland,6.0,"Zini, Simone;Gomez-Villa, Alex;Buzzelli, Marco;Twardowski, Bartłomiej;Bagdanov, Andrew D.;van de Weijer, Joost",57212482445;57214472745;55744003800;55438080100;16038484100;8725731800,60012306;60023020;60012306;60023020-129016807;60021859;60023020,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Several recent works on self-supervised learning are trained by mapping different augmentations of the same image to the same feature representation. The data augmentations used are of crucial importance to the quality of learned feature representations. In this paper, we analyze how the color jitter traditionally used in data augmentation negatively impacts the quality of the color features in learned feature representations. To address this problem, we propose a more realistic, physics-based color data augmentation - which we call Planckian Jitter - that creates realistic variations in chromaticity and produces a model robust to illumination changes that can be commonly observed in real life, while maintaining the ability to discriminate image content based on color information. Experiments confirm that such a representation is complementary to the representations learned with the currently-used color jitter augmentation and that a simple concatenation leads to significant performance gains on a wide range of downstream datasets. In addition, we present a color sensitivity analysis that documents the impact of different training methods on model neurons and shows that the performance of the learned features is robust with respect to illuminant variations. Official code available at: https://github.com/TheZino/PlanckianJitter.",,9,0.0,,,EC,RYC2021-032765-I,European Commission
2-s2.0-85137895147,10.24963/ijcai.2022/50,,,PLURALITYVETO: A Simple Voting Rule Achieving Optimal Metric Distortion,cp,Conference Paper,Kizilkaya F.E.,60029311,University of Southern California,Los Angeles,United States,2.0,"Kizilkaya, Fatih Erdem;Kempe, David",55908061300;6701573509,60029311;60029311,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,349-355,"The metric distortion framework posits that n voters and m candidates are jointly embedded in a metric space such that voters rank candidates that are closer to them higher. A voting rule's purpose is to pick a candidate with minimum total distance to the voters, given only the rankings, but not the actual distances. As a result, in the worst case, each deterministic rule picks a candidate whose total distance is at least three times larger than that of an optimal one, i.e., has distortion at least 3. A recent breakthrough result showed that achieving this bound of 3 is possible; however, the proof is nonconstructive, and the voting rule itself is a complicated exhaustive search. Our main result is an extremely simple voting rule, called PLURALITYVETO, which achieves the same optimal distortion of 3. Each candidate starts with a score equal to his number of first-place votes. These scores are then gradually decreased via an nround veto process in which a candidate drops out when his score reaches zero. One after the other, voters decrement the score of their bottom choice among the standing candidates, and the last standing candidate wins. We give a one-paragraph proof that this voting rule achieves distortion 3. This rule is also immensely practical, and it only makes two queries to each voter, so it has low communication overhead. We also show that a straightforward extension can be used to give a constructive proof of the more general Ranking-Matching Lemma of Gkatzelis et al. We also generalize PLURALITYVETO into a class of randomized voting rules in the following way: PLURALITYVETO is run only for k < n rounds; then, a candidate is chosen with probability proportional to his residual score. This general rule interpolates between RANDOMDICTATORSHIP (for k = 0) and PLURALITYVETO (for k = n-1), and k controls the variance of the output. We show that for all k, this rule has expected distortion at most 3.",,39,1.0,all publisherfree2read,All Open Access Bronze,,,
2-s2.0-85203821904,,,,PPFLOW: Target-Aware Peptide Design with Torsional Flow Matching,cp,Conference Paper,Lin H.,60003970;60117660,Zhejiang University;Westlake University,Hangzhou;Hangzhou,China;China,8.0,"Lin, Haitao;Zhang, Odin;Zhao, Huifeng;Jiang, Dejun;Wu, Lirong;Liu, Zicheng;Huang, Yufei;Li, Stan Z.",57221141078;58564101300;58250866600;57215577851;57219756658;57252253300;57938263600;34870116100,60003970-60117660;60003970;60003970;60003970;60003970-60117660;60003970-60117660;60003970-60117660;60117660,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,30510-30528,"Therapeutic peptides have proven to have great pharmaceutical value and potential in recent decades. However, methods of AI-assisted peptide drug discovery are not fully explored. To fill the gap, we propose a target-aware peptide design method called PPFLOW, based on conditional flow matching on torus manifolds, to model the internal geometries of torsion angles for the peptide structure design. Besides, we establish a protein-peptide binding dataset named PPBench2024 to fill the void of massive data for the task of structure-based peptide drug design and to allow the training of deep learning methods. Extensive experiments show that PPFLOW reaches state-of-the-art performance in tasks of peptide drug generation and optimization in comparison with baseline models, and can be generalized to other tasks including docking and side-chain packing.",,4,0.0,,,NSFC,U21A20427,Westlake University
2-s2.0-85189645805,10.1609/aaai.v38i13.29339,,,PPIDSG: A Privacy-Preserving Image Distribution Sharing Scheme with GAN in Federated Learning,cp,Conference Paper,Ma Y.,60019118;60002836,University of Science and Technology of China;Hefei University of Technology,Hefei;Hefei,China;China,3.0,"Ma, Yuting;Yao, Yuanzhi;Xu, Xiaohua",57194772650;55575156200;59873211600,60019118;60002836;60019118,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,13.0,,14272-14280,"Federated learning (FL) has attracted growing attention since it allows for privacy-preserving collaborative training on decentralized clients without explicitly uploading sensitive data to the central server. However, recent works have revealed that it still has the risk of exposing private data to adversaries. In this paper, we conduct reconstruction attacks and enhance inference attacks on various datasets to better understand that sharing trained classification model parameters to a central server is the main problem of privacy leakage in FL. To tackle this problem, a privacy-preserving image distribution sharing scheme with GAN (PPIDSG) is proposed, which consists of a block scrambling-based encryption algorithm, an image distribution sharing method, and local classification training. Specifically, our method can capture the distribution of a target image domain which is transformed by the block encryption algorithm, and upload generator parameters to avoid classifier sharing with negligible influence on model performance. Furthermore, we apply a feature extractor to motivate model utility and train it separately from the classifier. The extensive experimental results and security analyses demonstrate the superiority of our proposed scheme compared to other state-of-the-art defense methods. The code is available at https://github.com/ytingma/PPIDSG.",,8,1.0,all publisherfullgold,All Open Access Gold,NSFC,61802357,National Natural Science Foundation of China
2-s2.0-85162061979,,,,PROGRESSIVELY COMPRESSED AUTOENCODER FOR SELF-SUPERVISED REPRESENTATION LEARNING,cp,Conference Paper,Li J.,60025084;130702213,Shanghai Jiao Tong University;Huawei Cloud,Shanghai;Guiyang,China;China,9.0,"Li, Jin;Wang, Yaoming;Zhang, Xiaopeng;Chen, Yabo;Jiang, Dongsheng;Dai, Wenrui;Li, Chenglin;Xiong, Hongkai;Tian, Qi",58664050900;57220551229;57221514985;59294115000;57224547948;24723796900;36025559400;55617584700;57209993060,60025084;60025084;130702213;60025084;130702213;60025084;60025084;60025084;130702213,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"As a typical self-supervised learning strategy, Masked Image Modeling (MIM) is driven by recovering all masked patches from visible ones. However, patches from the same image are highly correlated and it is redundant to reconstruct all the masked patches. We find that this redundancy is neglected by existing MIM based methods and causes non-negligible overheads in computation that do not necessarily benefit self-supervised representation. In this paper, we present a novel approach named PCAE, short for Progressively Compressed AutoEncoder, to address the redundant reconstruction issue by progressively compacting tokens and only retaining necessary information for forward propagation and reconstruction. In particular, we identify those redundant tokens in an image via a simple yet effective similarity metric between each token with the mean of the token sequence. Those redundant tokens that other ones can probably represent are progressively dropped accordingly during the forward propagation, and importantly, we only focus on reconstructing these retained tokens. As a result, we are able to achieve a better trade-off between performance and efficiency for pre-training. Besides, benefitting from the flexible strategy, PCAE can be also directly employed for downstream fine-tuning tasks and enable scalable deployment. Experiments show that PCAE achieves comparable performance to MAE with only 1/8 GPU days. The code is available at https://github.com/caddyless/PCAE/.",,5,0.0,,,NSFC,62125109,National Natural Science Foundation of China
2-s2.0-85200602501,,,,PROTEIN DISCOVERY WITH DISCRETE WALK-JUMP SAMPLING,cp,Conference Paper,Frey N.C.,60021784;60026209;60147012,"New York University;Genentech, Inc;NYU CS Department",New York;South San Francisco;New York,United States;United States;United States,13.0,"Frey, Nathan C.;Berenberg, Daniel;Zadorozhny, Karina;Kleinhenz, Joseph;Lafrance-Vanasse, Julien;Hötzel, Isidro;Wu, Yan;Ra, Stephen;Bonneau, Richard;Cho, Kyunghyun;Loukas, Andreas;Gligorijević, Vladimir;Saremi, Saeed",57197038426;57209060747;57498038000;59607751200;8720938200;6603908403;7406891047;57695576300;7006793027;55722769200;27867908300;56419766300;55427569700,60026209;60026209-60147012;60026209;60026209;60026209;60026209;60026209;60026209;60026209;60026209-60147012-60021784;60026209;60026209;60026209,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our Discrete Walk-Jump Sampling formalism combines the contrastive divergence training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the distributional conformity score to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100% of generated samples are successfully expressed and purified and 70% of functional designs show equal or improved binding affinity compared to known functional antibodies on the first attempt in a single round of laboratory experiments. We also report the first demonstration of long-run fast-mixing MCMC chains where diverse antibody protein classes are visited in a single MCMC chain.",,13,0.0,,,,,
2-s2.0-85191149578,,,,PROTES: Probabilistic Optimization with Tensor Sampling,cp,Conference Paper,Batsheva A.,60107405,Skolkovo Institute of Science and Technology,Moscow,Russian Federation,4.0,"Batsheva, Anastasia;Ryzhakov, Gleb;Chertkov, Andrei;Oseledets, Ivan",58090943000;36622445500;57191283348;8529104000,60107405;60107405;60107405;60107405,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"We developed a new method PROTES for black-box optimization, which is based on the probabilistic sampling from a probability density function given in the low-parametric tensor train format. We tested it on complex multidimensional arrays and discretized multivariable functions taken, among others, from real-world applications, including unconstrained binary optimization and optimal control problems, for which the possible number of elements is up to 2<sup>1000</sup>. In numerical experiments, both on analytic model functions and on complex problems, PROTES outperforms popular discrete optimization methods (Particle Swarm Optimization, Covariance Matrix Adaptation, Differential Evolution, and others).",,3,0.0,,,Minobrnauka,075-10-2021-068,Ministry of Education and Science of the Russian Federation
2-s2.0-85150003777,,,,PROVING THE STRONG LOTTERY TICKET HYPOTHESIS FOR CONVOLUTIONAL NEURAL NETWORKS,cp,Conference Paper,da Cunha A.C.W.,60032385;60123660,Centre Inria d'Université Côte d'Azur;Institut de Recherche en Informatique Fondamentale (IRIF),Sophia Antipolis;Paris,France;France,3.0,"da Cunha, Arthur C.W.;Natale, Emanuele;Viennot, Laurent",57702992600;56278669100;6603066312,60032385;60032385;60123660,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"The lottery ticket hypothesis states that a randomly-initialized neural network contains a small subnetwork which, when trained in isolation, can compete with the performance of the original network. Recent theoretical works proved an even stronger version: every sufficiently overparameterized (dense) neural network contains a subnetwork that, even without training, achieves accuracy comparable to that of the trained large network. These works left as an open problem to extend the result to convolutional neural networks (CNNs). In this work we provide such generalization by showing that, with high probability, it is possible to approximate any CNN by pruning a random CNN whose size is larger by a logarithmic factor.",,35,0.0,,,UCA,2019650072,Université Côte d’Azur
2-s2.0-85167686793,10.1609/aaai.v37i2.25227,,,Panoramic Video Salient Object Detection with Ambisonic Audio Guidance,cp,Conference Paper,Li X.,60027950;60195969;60159665,Carnegie Mellon University;Mohamed Bin Zayed University of Artificial Intelligence;ByteDance Ltd.,Pittsburgh;Abu Dhabi;Beijing,United States;United Arab Emirates;China,6.0,"Li, Xiang;Cao, Haoyuan;Zhao, Shijie;Li, Junlin;Zhang, Li;Raj, Bhiksha",58870121100;57999170800;57304157300;57221559410;56649874600;7102615577,60027950;60159665;60159665;60159665;60159665;60027950-60195969,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,1424-1432,"Video salient object detection (VSOD), as a fundamental computer vision problem, has been extensively discussed in the last decade. However, all existing works focus on addressing the VSOD problem in 2D scenarios. With the rapid development of VR devices, panoramic videos have been a promising alternative to 2D videos to provide immersive feelings of the real world. In this paper, we aim to tackle the video salient object detection problem for panoramic videos, with their corresponding ambisonic audios. A multimodal fusion module equipped with two pseudo-siamese audio-visual context fusion (ACF) blocks is proposed to effectively conduct audio-visual interaction. The ACF block equipped with spherical positional encoding enables the fusion in the 3D context to capture the spatial correspondence between pixels and sound sources from the equirectangular frames and ambisonic audios. Experimental results verify the effectiveness of our proposed components and demonstrate that our method achieves state-of-the-art performance on the ASOD60K dataset.",,9,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85170391567,10.24963/ijcai.2023/620,,,Parameterized Local Search for Max c-Cut,cp,Conference Paper,Garvardt J.,60029507;60028453,"Friedrich-Schiller-Universität Jena;Fraunhofer Institute for Optronics, System Technologies and Image Exploitation IOSB",Jena;Karlsruhe,Germany;Germany,4.0,"Garvardt, Jaroslav;Grüttemeier, Niels;Komusiewicz, Christian;Morawietz, Nils",57426096400;57203903574;23135028100;57214776991,60029507;60028453;60029507;60029507,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,5586-5594,"In the NP-hard MAX c-CUT problem, one is given an undirected edge-weighted graph G and wants to color the vertices of G with c colors such that the total weight of edges with distinctly colored endpoints is maximal. The case with c = 2 is the famous MAX CUT problem. To deal with the NP-hardness of this problem, we study parameterized local search algorithms. More precisely, we study LS MAX c-CUT where we are also given a vertex coloring f and an integer k and the task is to find a better coloring f<sup>′</sup> that differs from f in at most k entries, if such a coloring exists; otherwise, f is k-optimal. We show that, for all c ≥ 2, LS MAX c-CUT presumably cannot be solved in g(k) · n<sup>O(1)</sup> time even on bipartite graphs. We then show an algorithm for LS MAX c-CUT with running time O((3e∆)<sup>k</sup> · c · k<sup>3</sup> · ∆ · n), where ∆ is the maximum degree of the input graph. Finally, we evaluate the practical performance of this algorithm in a hill-climbing approach as a post-processing for state-of-the-art heuristics for MAX c-CUT. We show that using parameterized local search, the results of this heuristic can be further improved on a set of standard benchmark instances.",,4,1.0,all publisherfullgold,All Open Access Gold,DFG,KO 3669/5-1,Deutsche Forschungsgemeinschaft
2-s2.0-85177447356,,,,"Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense",cp,Conference Paper,Krishna K.,60014313;60359365;131185486,University of Massachusetts Amherst;Google DeepMind;Google,Amherst;London;,United States;United Kingdom;,5.0,"Krishna, Kalpesh;Song, Yixiao;Karpinska, Marzena;Wieting, John;Iyyer, Mohit",57204049177;57943568600;57275350000;57197871618;44061578400,60014313-131185486;60014313;60014313;60359365;60014313,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Using DIPPER to paraphrase text generated by three large language models (including GPT3.5-davinci-003) successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics. To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data.",,142,0.0,,,,,Google
2-s2.0-85214114155,,,,Partial Order in Chaos: Consensus on Feature Attributions in the Rashomon Set,ar,Article,Laberge G.,60032619;60019141,Université Laval;Polytechnique Montréal,Quebec;Montreal,Canada;Canada,5.0,"Laberge, Gabriel;Pequignot, Yann;Mathieu, Alexandre;Khomh, Foutse;Marchand, Mario",57197840884;56380423300;57974931400;24724747600;7102059694,60019141;60032619;60032619;60019141;60032619,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,364,,"Post-hoc global/local feature attribution methods are progressively being employed to understand the decisions of complex machine learning models. Yet, because of limited amounts of data, it is possible to obtain a diversity of models with good empirical performance but that provide very different explanations for the same prediction, making it hard to derive insight from them. In this work, instead of aiming at reducing the underspecification of model explanations, we fully embrace it and extract logical statements about feature attributions that are consistent across all models with good empirical performance (i.e. all models in the Rashomon Set). We show that partial orders of local/global feature importance arise from this methodology enabling more nuanced interpretations by allowing pairs of features to be incomparable when there is no consensus on their relative importance. We prove that every relation among features present in these partial orders also holds in the rankings provided by existing approaches. Finally, we present three use cases employing hypothesis spaces with tractable Rashomon Sets (Additive models, Kernel Ridge, and Random Forests) and show that partial orders allow one to extract consistent local and global interpretations of models despite their under-specification.",Feature Attribution | Rashomon Set | Uncertainty | Under-Specification | XAI,2,0.0,,,CRIAQ,,Consortium de Recherche et d’innovation en Aérospatiale au Québec
2-s2.0-85186670432,,,,Parts of Speech-Grounded Subspaces in Vision-Language Models,cp,Conference Paper,Oldfield J.,60028900;60022109;60104388;129289336,"National and Kapodistrian University of Athens;Queen Mary University of London;Athena - Research and Innovation Center in Information, Communication and Knowledge Technologies;The Cyprus Institute",Athens;London;Athens;Aglandjia,Greece;United Kingdom;Greece;Cyprus,5.0,"Oldfield, James;Tzelepis, Christos;Panagakis, Yannis;Nicolaou, Mihalis A.;Patras, Ioannis",57211991046;55926342800;35503932300;36622278100;6603429806,60022109;60022109;60028900-60104388;129289336;60022109,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Latent image representations arising from vision-language models have proved immensely useful for a variety of downstream tasks. However, their utility is limited by their entanglement with respect to different visual attributes. For instance, recent work has shown that CLIP image representations are often biased towards specific visual properties (such as objects or actions) in an unpredictable manner. In this paper, we propose to separate representations of the different visual modalities in CLIP's joint vision-language space by leveraging the association between parts of speech and specific visual modes of variation (e.g. nouns relate to objects, adjectives describe appearance). This is achieved by formulating an appropriate component analysis model that learns subspaces capturing variability corresponding to a specific part of speech, while jointly minimising variability to the rest. Such a subspace yields disentangled representations of the different visual properties of an image or text in closed form while respecting the underlying geometry of the manifold on which the representations lie. What's more, we show the proposed model additionally facilitates learning subspaces corresponding to specific visual appearances (e.g. artists' painting styles), which enables the selective removal of entire visual themes from CLIP-based text-to-image synthesis. We validate the model both qualitatively, by visualising the subspace projections with a text-to-image model and by preventing the imitation of artists' styles, and quantitatively, through class invariance metrics and improvements to baseline zero-shot classification.",,6,0.0,,,EC,MIS 5154714,European Commission
2-s2.0-85136552255,10.1613/jair.1.13544,,,Path Counting for Grid-Based Navigation,ar,Article,Goldstein R.,60074802;128498290,Autodesk Inc.;Trax.Co,San Francisco;Toronto,United States;Canada,6.0,"Goldstein, Rhys;Walmsley, Kean;Bibliowicz, Jacobo;Tessier, Alexander;Breslav, Simon;Khan, Azam",26031391500;57215342796;57190586510;36702009900;15135261000;7404910952,60074802;60074802;60074802;60074802;128498290;128498290,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,917-955,"Counting the number of shortest paths on a grid is a simple procedure with close ties to Pascal’s triangle. We show how path counting can be used to select relatively direct grid paths for AI-related applications involving navigation through spatial environments. Typical implementations of Dijkstra’s algorithm and A* prioritize grid moves in an arbitrary manner, producing paths which stray conspicuously far from line-of-sight trajectories. We find that by counting the number of paths which traverse each vertex, then selecting the vertices with the highest counts, one obtains a path that is reasonably direct in practice and can be improved by refining the grid resolution. Central Dijkstra and Central A* are introduced as the basic methods for computing these central grid paths. Theoretical analysis reveals that the proposed grid-based navigation approach is related to an existing grid-based visibility approach, and establishes that central grid paths converge on clear sightlines as the grid spacing approaches zero. A more general property, that central paths converge on direct paths, is formulated as a conjecture.",,6,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85203810593,,,,PerceptAnon: Exploring the Human Perception of Image Anonymization Beyond Pseudonymization for GDPR,cp,Conference Paper,Patwari K.,60153736;131684014,College of Engineering;Sony AI,Davis;,United States;,4.0,"Patwari, Kartik;Chuah, Chen Nee;Lyu, Lingjuan;Sharma, Vivek",57713249200;7004251298;57189234207;58276065900,60153736;60153736;131684014;131684014,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,39955-39971,"Current image anonymization techniques, largely focus on localized pseudonymization, typically modify identifiable features like faces or full bodies and evaluate anonymity through metrics such as detection and re-identification rates. However, this approach often overlooks information present in the entire image post-anonymization that can compromise privacy, such as specific locations, objects/items, or unique attributes. Acknowledging the pivotal role of human judgment in anonymity, our study conducts a thorough analysis of perceptual anonymization, exploring its spectral nature and its critical implications for image privacy assessment, particularly in light of regulations such as the General Data Protection Regulation (GDPR). To facilitate this, we curated a dataset specifically tailored for assessing anonymized images. We introduce a learning-based metric, PerceptAnon, which is tuned to align with the human Perception of Anonymity. PerceptAnon evaluates both original-anonymized image pairs and solely anonymized images. Trained using human annotations, our metric encompasses both anonymized subjects and their contextual backgrounds, thus providing a comprehensive evaluation of privacy vulnerabilities. We envision this work as a milestone for understanding and assessing image anonymization, and establishing a foundation for future research. The codes and dataset are available in https://github.com/SonyResearch/gdpr_perceptanon.",,0,0.0,,,,,
2-s2.0-85188011916,10.1613/jair.1.15423,,,Performative Ethics From Within the Ivory Tower: How CS Practitioners Uphold Systems of Oppression,ar,Article,McFadden Z.,60004923,NC State University,Raleigh,United States,2.0,"McFadden, Zari;Alvarez, Lauren",57202043605;57219569148,60004923;60004923,2024-01-01,2024,Daimon,11300507.0,21100204105.0,19894651.0,Journal,79,,,777-799,"This paper analyzes where Artificial Intelligence (AI) ethics research fails and breaks down the dangers of well-intentioned but ultimately performative ethics research. A large majority of AI ethics research is criticized for not providing a comprehensive analysis of how AI is interconnected with sociological systems of oppression and power. Our work contributes to the handful of research that presents intersectional, Western systems of oppression and power as a framework for examining AI ethics work and the complexities of building less harmful technology; directly connecting technology to named systems such as capitalism and classism, colonialism, racism and white supremacy, patriarchy, and ableism. We then explore current AI ethics rhetoric's effect on the AI ethics domain. We conclude by providing an applied example to contextualize intersectional systems of oppression and AI interventions in the US justice system and present actionable steps for AI practitioners to participate in a less performative, critical analysis of AI.",,5,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-105018475679,,,,Personalized PCA: Decoupling Shared and Unique Features,ar,Article,Shi N.,60025778,"University of Michigan, Ann Arbor",Ann Arbor,United States,2.0,"Shi, Naichen;Kontar, Raed Al",57226463984;57191495988,60025778;60025778,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"In this paper, we tackle a significant challenge in PCA: heterogeneity. When data are collected from different sources with heterogeneous trends while still sharing some congruency, it is critical to extract shared knowledge while retaining the unique features of each source. To this end, we propose personalized PCA (PerPCA), which uses mutually orthogonal global and local principal components to encode both unique and shared features. We show that, under mild conditions, both unique and shared features can be identified and recovered by a constrained optimization problem, even if the covariance matrices are immensely different. Also, we design a fully federated algorithm inspired by distributed Stiefel gradient descent to solve the problem. The algorithm introduces a new group of operations called generalized retractions to handle orthogonality constraints, and only requires global PCs to be shared across sources. We prove the linear convergence of the algorithm under suitable assumptions. Comprehensive numerical experiments highlight PerPCA’s superior performance in feature extraction and prediction from heterogeneous datasets. As a systematic approach to decouple shared and unique features from heterogeneous datasets, PerPCA finds applications in several tasks, including video segmentation, topic extraction, and feature clustering.",heterogeneity | personalization | Principal component analysis,11,0.0,,,NSF,2144147,National Science Foundation
2-s2.0-85163217092,,,,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,cp,Conference Paper,Saharia C.,60016849;60006191,University of Toronto;Google LLC,Toronto;Mountain View,Canada;United States,14.0,"Saharia, Chitwan;Chan, William;Saxena, Saurabh;Li, Lala;Whang, Jay;Denton, Emily;Ghasemipour, Seyed Kamyar Seyed;Ayan, Burcu Karagol;Mahdavi, S. Sara;Gontijo-Lopes, Raphael;Salimans, Tim;Ho, Jonathan;Fleet, David J.;Norouzi, Mohammad",57210640805;55338413200;57213481797;57218718937;57220360238;55338354300;57210570038;57216965669;57712451000;57219498344;55320289800;59788557100;57206712712;59108337900,60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191-60016849;60006191,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, GLIDE and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.",,3577,0.0,,,,,
2-s2.0-85137894599,10.24963/ijcai.2022/44,,,Picking the Right Winner: Why Tie-Breaking in Crowdsourcing Contests Matters,cp,Conference Paper,Haggiag C.,60027161,Ben-Gurion University of the Negev,Beer-Sheva,Israel,3.0,"Haggiag, Coral;Oren, Sigal;Segev, Ella",57887619700;41862260500;36726938500,60027161;60027161;60027161,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,307-313,"We present a complete information game-theoretic model for crowdsourcing contests. We observe that in design contests, coding contests and other domains, separating low quality submissions from high quality ones is often easy. However, pinning down the best submission is more challenging since there is no objective measure. We model this situation by assuming that each contestant has an ability, which we interpret as its probability of submitting a high-quality submission. After the contestants decide whether or not they want to participate, the organizer of the contest needs to break ties between the high quality submissions. A common assumption in the literature is that the exact tie-breaking rule does not matter as long as ties are broken consistently. However, we show that the choice of the tie-breaking rule may have significant implications on the efficiency of the contest. Our results highlight both qualitative and quantitative differences between various deterministic tie-breaking rules. Perhaps counterintuitively, we show that in many scenarios, the utility of the organizer is maximized when ties are broken in favor of successful players with lower ability. Nevertheless, we show that the natural rule of choosing the submission of the successful player with the highest ability guarantees the organizer at least 1/3 of its utility under any tie-breaking rule. To complement these results, we provide an upper bound of H<inf>n</inf> ≈ ln(n) on the price of anarchy (the ratio between the social welfare of the optimal solution and the social welfare of the Nash equilibrium). We show that this ratio is tight when ties are broken in favor of players with higher abilities.",,4,1.0,all publisherfree2read,All Open Access Bronze,ISF,2167/19,Israel Science Foundation
2-s2.0-85140311439,10.1613/JAIR.1.13269,,,Planning with Critical Section Macros: Theory and Practice,ar,Article,Chrpa L.,60013323;128763377,Czech Technical University in Prague;University of Huddersffield,Prague;Huddersffield,Czech Republic;United Kingdom,2.0,"Chrpa, Lukáš;Vallati, Mauro",25640511300;36702078700,60013323;128763377,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,691-732,"Macro-operators (macros) are a well-known technique for enhancing performance of planning engines by providing \short-cuts""in the state space. Existing macro learning systems usually generate macros by considering most frequent action sequences in training plans. Unfortunately, frequent action sequences might not capture meaningful activities as a whole, leading to a limited beneficial impact for the planning process. In this paper, inspired by resource locking in critical sections in parallel computing, we propose a technique that generates macros able to capture whole activities in which limited resources (e.g., a robotic hand, or a truck) are used. Specifically, such a Critical Section macro starts by locking the resource (e.g., grabbing an object), continues by using the resource (e.g., manipulating the object) and finishes by releasing the resource (e.g., dropping the object). Hence, such a macro bridges states in which the resource is locked and cannot be used. We also introduce versions of Critical Section macros dealing with multiple resources and phased locks. Usefulness of macros is evaluated using a range of state-of-the-art planners, and a large number of benchmarks from the deterministic and learning tracks of recent editions of the International Planning Competition.",,5,1.0,all publisherfullgold repository repositoryvor repositoryam,All Open Access Gold Green,UKRI,CZ.02.1.01/0.0/0.0/16 019/0000765,UK Research and Innovation
2-s2.0-85142039009,10.1613/jair.1.13446,,,Planning with Perspectives – Decomposing Epistemic Planning using Functional STRIPS,ar,Article,Hu G.,60118847,School of Computing and Information Systems,Melbourne,Australia,3.0,"Hu, Guang;Miller, Tim;Lipovetzky, Nir",57219509364;7403948057;36701605000,60118847;60118847;60118847,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,489-539,"In this paper, we present a novel approach to epistemic planning called planning with perspectives (PWP) that is both more expressive and computationally more efficient than existing state-of-the-art epistemic planning tools. Epistemic planning — planning with knowledge and belief — is essential in many multi-agent and human-agent interaction domains. Most state-of-the-art epistemic planners solve epistemic planning problems by either compiling to propositional classical planning (for example, generating all possible knowledge atoms or compiling epistemic formulae to normal forms); or explicitly encoding Kripke-based semantics. However, these methods become computationally infeasible as problem sizes grow. In this paper, we decompose epistemic planning by delegating reasoning about epistemic formulae to an external solver. We do this by modelling the problem using Functional STRIPS, which is more expressive than standard STRIPS and supports the use of external, black-box functions within action models. Building on recent work that demonstrates the relationship between what an agent ‘sees’ and what it knows, we define the perspective of each agent using an external function, and build a solver for epistemic logic around this. Modellers can customise the perspective function of agents, allowing new epistemic logics to be defined without changing the planner. We ran evaluations on well-known epistemic planning benchmarks to compare an existing state-of-the-art planner, and on new scenarios that demonstrate the expressiveness of the PWP approach. The results show that our PWP planner scales significantly better than the state-of-the-art planner that we compared against, and can express problems more succinctly.",,4,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85141765537,,,,Point Transformer V2: Grouped Vector Attention and Partition-based Pooling,cp,Conference Paper,Wu X.,60006541;129867751;123858345,The University of Hong Kong;Intel Labs;Max Planck Institute,Hong Kong;;Hamburg,Hong Kong;;Germany,5.0,"Wu, Xiaoyang;Lao, Yixing;Jiang, Li;Liu, Xihui;Zhao, Hengshuang",57938220700;57208881308;57491002100;57200621033;57191430384,60006541;129867751;123858345;60006541;60006541,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"As a pioneering work exploring transformer architecture for 3D point cloud understanding, Point Transformer achieves impressive results on multiple highly competitive benchmarks. In this work, we analyze the limitations of the Point Transformer and propose our powerful and efficient Point Transformer V2 model with novel designs that overcome the limitations of previous work. In particular, we first propose group vector attention, which is more effective than the previous version of vector attention. Inheriting the advantages of both learnable weight encoding and multi-head attention, we present a highly effective implementation of grouped vector attention with a novel grouped weight encoding layer. We also strengthen the position information for attention by an additional position encoding multiplier. Furthermore, we design novel and lightweight partition-based pooling methods which enable better spatial alignment and more efficient sampling. Extensive experiments show that our model achieves better performance than its predecessor and achieves state-of-the-art on several challenging 3D point cloud understanding benchmarks, including 3D point cloud segmentation on ScanNet v2 and S3DIS and 3D point cloud classification on ModelNet40. Our code will be available at https://github.com/Gofinge/PointTransformerV2.",,441,0.0,,,,,
2-s2.0-85147704041,10.1609/aaai.v36i1.19920,,,Pose Adaptive Dual Mixup for Few-Shot Single-View 3D Reconstruction,cp,Conference Paper,Cheng T.Y.,60026851;60018029;60013651,"University of Oxford;National Tsing Hua University;Institute of Information Science, Academia Sinica",Oxford;Hsinchu;Taipei,United Kingdom;Taiwan;Taiwan,5.0,"Cheng, Ta Ying;Yang, Hsuan Ru;Trigoni, Niki;Chen, Hwann Tzong;Liu, Tyng Luh",57386377500;57386188100;23502091400;7501609595;7405910215,60013651-60018029;60013651;60026851;60018029;60013651,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,427-435,"We present a pose adaptive few-shot learning procedure and a two-stage data interpolation regularization, termed Pose Adaptive Dual Mixup (PADMix), for single-image 3D reconstruction. While augmentations via interpolating feature-label pairs are effective in classification tasks, they fall short in shape predictions potentially due to inconsistencies between interpolated products of two images and volumes when rendering viewpoints are unknown. PADMix targets this issue with two sets of mixup procedures performed sequentially. We first perform an input mixup which, combined with a pose adaptive learning procedure, is helpful in learning 2D feature extraction and pose adaptive latent encoding. The stagewise training allows us to build upon the pose invariant representations to perform a follow-up latent mixup under one-to-one correspondences between features and ground-truth volumes. PADMix significantly outperforms previous literature on few-shot settings over the ShapeNet dataset and sets new benchmarks on the more challenging real-world Pix3D dataset.",,7,1.0,all publisherfullgold,All Open Access Gold,EPSRC,EP/S030832/1,Engineering and Physical Sciences Research Council
2-s2.0-85168245553,10.1609/aaai.v37i7.26051,,,Positive Distribution Pollution: Rethinking Positive Unlabeled Learning from a Unified Perspective,cp,Conference Paper,Liang Q.,60019544;60117751;60121285,"Macquarie University;College of Computer Science and Technology, Zhejiang University;Ant group",Sydney;Hangzhou;Hangzhou,Australia;China;China,9.0,"Liang, Qianqiao;Zhu, Mengying;Wang, Yan;Wang, Xiuyuan;Zhao, Wanjia;Yang, Mengyuan;Wei, Hua;Han, Bing;Zheng, Xiaolin",57200563357;57189360235;7601520278;58539794200;58539686900;57879295600;57427159900;59590948500;57189368439,60117751;60117751;60019544;60117751;60117751;60117751;60121285;60121285;60117751,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,8737-8745,"Positive Unlabeled (PU) learning, which has a wide range of applications, is becoming increasingly prevalent. However, it suffers from problems such as data imbalance, selection bias, and prior agnostic in real scenarios. Existing studies focus on addressing part of these problems, which fail to provide a unified perspective to understand these problems. In this paper, we first rethink these problems by analyzing a typical PU scenario and come up with an insightful point of view that all these problems are inherently connected to one problem, i.e., positive distribution pollution, which refers to the inaccuracy in estimating positive data distribution under very little labeled data. Then, inspired by this insight, we devise a variational model named CoVPU, which addresses all three problems in a unified perspective by targeting the positive distribution pollution problem. CoVPU not only accurately separates the positive data from the unlabeled data based on discrete normalizing flows, but also effectively approximates the positive distribution based on our derived unbiased rebalanced risk estimator and supervises the approximation based on a novel prior-free variational loss. Rigorous theoretical analysis proves the convergence of CoVPU to an optimal Bayesian classifier. Extensive experiments demonstrate the superiority of CoVPU over the state-of-the-art PU learning methods under these problems.",,3,1.0,all publisherfullgold,All Open Access Gold,NSFC,2021R52001,National Natural Science Foundation of China
2-s2.0-105018578407,,,,Post-Regularization Confidence Bands for Ordinary Differential Equations,ar,Article,Dai X.,60025038;60029270,"University of California, Berkeley;UCLA Fielding School of Public Health",Berkeley;Los Angeles,United States;United States,2.0,"Dai, Xiaowu;Li, Lexin",57211961056;55540800600,60029270;60025038,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Ordinary differential equation (ODE) is an important tool to study a system of biological and physical processes. A central question in ODE modeling is to infer the significance of individual regulatory effect of one signal variable on another. However, building confidence band for ODE with unknown regulatory relations is challenging, and it remains largely an open question. In this article, we construct the post-regularization confidence band for the individual regulatory function in ODE with unknown functionals and noisy data observations. Our proposal is the first of its kind, and is built on two novel ingredients. The first is a new localized kernel learning approach that combines reproducing kernel learning with local Taylor approximation, and the second is a new de-biasing method that tackles infinite-dimensional functionals and additional measurement errors. We show that the constructed confidence band has the desired asymptotic coverage probability, and the recovered regulatory network approaches the truth with probability tending to one. We establish the theoretical properties when the number of variables in the system can be either smaller or larger than the number of sampling time points, and we study the regime-switching phenomenon. We demonstrate the efficacy of the proposed method through both simulations and illustrations with two data applications.",De-biasing | Local polynomial approximation | Ordinary differential equations | Reproducing kernel Hilbert space | Smoothing spline analysis of variance | Time series,3,0.0,,,NICHD,P2C-HD041022,Eunice Kennedy Shriver National Institute of Child Health and Human Development
2-s2.0-85168256626,10.1609/aaai.v37i8.26167,,,Post-hoc Uncertainty Learning Using a Dirichlet Meta-Model,cp,Conference Paper,Shen M.,60022195;60013959;60011048,Massachusetts Institute of Technology;University of Florida;IBM Research,Cambridge;Gainesville;Yorktown Heights,United States;United States;United States,6.0,"Shen, Maohao;Bu, Yuheng;Sattigeri, Prasanna;Ghosh, Soumya;Das, Subhro;Wornell, Gregory",57456690600;57189590397;35180457200;57213864973;55968369200;7003798695,60022195;60013959;60011048;60011048;60011048;60022195,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,9772-9781,"It is known that neural networks have the problem of being over-confident when directly using the output label distribution to generate uncertainty measures. Existing methods mainly resolve this issue by retraining the entire model to impose the uncertainty quantification capability so that the learned model can achieve desired performance in accuracy and uncertainty prediction simultaneously. However, training the model from scratch is computationally expensive and may not be feasible in many situations. In this work, we consider a more practical post-hoc uncertainty learning setting, where a well-trained base model is given, and we focus on the uncertainty quantification task at the second stage of training. We propose a novel Bayesian meta-model to augment pretrained models with better uncertainty quantification abilities, which is effective and computationally efficient. Our proposed method requires no additional training data and is flexible enough to quantify different uncertainties and easily adapt to different application settings, including out-of-domain data detection, misclassification detection, and trustworthy transfer learning. We demonstrate our proposed meta-model approach's flexibility and superior empirical performance on these applications over multiple representative image classification benchmarks.",,31,1.0,all publisherfullgold,All Open Access Gold,NSF,1816209,National Science Foundation
2-s2.0-85152386215,,,,Power and limitations of single-qubit native quantum neural networks,cp,Conference Paper,Yu Z.,60112903,"Baidu, Inc.",Beijing,China,4.0,"Yu, Zhan;Yao, Hongshun;Li, Mujin;Wang, Xin",57547834100;57719414900;57719398300;57191040606,60112903;60112903;60112903;60112903,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Quantum neural networks (QNNs) have emerged as a leading strategy to establish applications in machine learning, chemistry, and optimization. While the applications of QNN have been widely investigated, its theoretical foundation remains less understood. In this paper, we formulate a theoretical framework for the expressive ability of data re-uploading quantum neural networks that consist of interleaved encoding circuit blocks and trainable circuit blocks. First, we prove that single-qubit quantum neural networks can approximate any univariate function by mapping the model to a partial Fourier series. We in particular establish the exact correlations between the parameters of the trainable gates and the Fourier coefficients, resolving an open problem on the universal approximation property of QNN. Second, we discuss the limitations of single-qubit native QNNs on approximating multivariate functions by analyzing the frequency spectrum and the flexibility of Fourier coefficients. We further demonstrate the expressivity and limitations of single-qubit native QNNs via numerical experiments. We believe these results would improve our understanding of QNNs and provide a helpful guideline for designing powerful QNNs for machine learning tasks.",,32,0.0,,,,,
2-s2.0-105018579827,,,,"Power of Knockoff: The Impact of Ranking Algorithm, Augmented Design, and Symmetric Statistic",ar,Article,Ke Z.T.,60006303,Harvard Faculty of Arts and Sciences,Cambridge,United States,3.0,"Ke, Zheng Tracy;Liu, Jun S.;Ma, Yucong",56188663200;55886002200;57219765747,60006303;60006303;60006303,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"The knockoff filter is a recent false discovery rate (FDR) control method for high-dimensional linear models. We point out that knockoff has three key components: ranking algorithm, augmented design, and symmetric statistic, and each component admits multiple choices. By considering various combinations of the three components, we obtain a collection of variants of knockoff. All these variants guarantee finite-sample FDR control, and our goal is to compare their power. We assume a Rare and Weak signal model on regression coefficients and compare the power of different variants of knockoff by deriving explicit formulas of false positive rate and false negative rate. Our results provide new insights on how to improve power when controlling FDR at a targeted level. We also compare the power of knockoff with its propotype - a method that uses the same ranking algorithm but has access to an ideal threshold. The comparison reveals the additional price one pays by finding a data-driven threshold to control FDR.",CI-knockoff | Hamming error | phase diagram | Rare/Weak signal model | SDP-knockoff | variable ranking | variable selection,1,0.0,,,NSF,DMS-201541,National Science Foundation
2-s2.0-85170399774,10.24963/ijcai.2023/120,,,PowerBEV: A Powerful Yet Lightweight Framework for Instance Prediction in Bird's-Eye View,cp,Conference Paper,Li P.,60007493;60017246;60085150;60329533,Universität Bonn;Eberhard Karls Universität Tübingen;Mercedes-Benz AG;Lamarr - Institut für Maschinelles Lernen und Künstliche Intelligenz,Bonn;Tubingen;Stuttgart;Dortmund,Germany;Germany;Germany;Germany,6.0,"Li, Peizheng;Ding, Shuxiao;Chen, Xieyuanli;Hanselmann, Niklas;Cordts, Marius;Gall, Juergen",58452125000;57219528423;57193685774;57219761585;56406553600;23396675200,60085150-60017246;60085150-60007493;60007493;60085150-60017246;60085150;60007493-60329533,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,1080-1088,"Accurately perceiving instances and predicting their future motion are key tasks for autonomous vehicles, enabling them to navigate safely in complex urban traffic. While bird's-eye view (BEV) representations are commonplace in perception for autonomous driving, their potential in a motion prediction setting is less explored. Existing approaches for BEV instance prediction from surround cameras rely on a multi-task auto-regressive setup coupled with complex post-processing to predict future instances in a spatio-temporally consistent manner. In this paper, we depart from this paradigm and propose an efficient novel end-to-end framework named POWERBEV, which differs in several design choices aimed at reducing the inherent redundancy in previous methods. First, rather than predicting the future in an auto-regressive fashion, POWERBEV uses a parallel, multi-scale module built from lightweight 2D convolutional networks. Second, we show that segmentation and centripetal backward flow are sufficient for prediction, simplifying previous multi-task objectives by eliminating redundant output modalities. Building on this output representation, we propose a simple, flow warping-based post-processing approach which produces more stable instance associations across time. Through this lightweight yet powerful design, POWERBEV outperforms state-of-the-art baselines on the NuScenes Dataset and poses an alternative paradigm for BEV instance prediction. We made our code publicly available at: https://github.com/EdwardLeeLPZ/PowerBEV.",,16,1.0,all publisherfullgold,All Open Access Gold,ERC,101044724,European Research Council
2-s2.0-85172437202,,,,PreNAS: Preferred One-Shot Learning Towards Efficient Neural Architecture Search,cp,Conference Paper,Wang H.,60118460,Alibaba Group Holding Limited,Hangzhou,China,4.0,"Wang, Haibin;Ge, Ce;Chen, Hesen;Sun, Xiuyu",58743996100;57200371269;57215963445;57215969594,60118460;60118460;60118460;60118460,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,35642-35654,"The wide application of pre-trained models is driving the trend of once-for-all training in one-shot neural architecture search (NAS). However, training within a huge sample space damages the performance of individual subnets and requires much computation to search for an optimal model. In this paper, we present PreNAS, a search-free NAS approach that accentuates target models in one-shot training. Specifically, the sample space is dramatically reduced in advance by a zero-cost selector, and weight-sharing one-shot training is performed on the preferred architectures to alleviate update conflicts. Extensive experiments have demonstrated that PreNAS consistently outperforms state-of-the-art one-shot NAS competitors for both Vision Transformer and convolutional architectures, and importantly, enables instant specialization with zero search cost. Our code is available at https://github.com/tinyvision/PreNAS.",,19,0.0,,,,,
2-s2.0-85130092085,10.1613/JAIR.1.13510,,,Predicting Decisions in Language Based Persuasion Games,ar,Article,Apel R.,60022403,Technion - Israel Institute of Technology,Haifa,Israel,4.0,"Apel, Reut;Erev, Ido;Reichart, Roi;Tennenholtz, Moshe",57216893049;7003527328;51665948700;7003406059,60022403;60022403;60022403;60022403,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,1025-1091,"Sender-receiver interactions, and specifically persuasion games, are widely researched in economic modeling and artificial intelligence, and serve as a solid foundation for powerful applications. However, in the classic persuasion games setting, the messages sent from the expert to the decision-maker are abstract or well-structured application-specific signals rather than natural (human) language messages, although natural language is a very common communication signal in real-world persuasion setups. This paper addresses the use of natural language in persuasion games, exploring its impact on the decisions made by the players and aiming to construct effective models for the prediction of these decisions. For this purpose, we conduct an online repeated interaction experiment. At each trial of the interaction, an informed expert aims to sell an uninformed decision-maker a vacation in a hotel, by sending her a review that describes the hotel. While the expert is exposed to several scored reviews, the decision-maker observes only the single review sent by the expert, and her payoff in case she chooses to take the hotel is a random draw from the review score distribution available to the expert only. The expert’s payoff, in turn, depends on the number of times the decision-maker chooses the hotel. We also compare the behavioral patterns in this experiment to the equivalent patterns in similar experiments where the communication is based on the numerical values of the reviews rather than the reviews’ text, and observe substantial differences which can be explained through an equilibrium analysis of the game. We consider a number of modeling approaches for our verbal communication setup, differing from each other in the model type (deep neural network (DNN) vs. linear classifier), the type of features used by the model (textual, behavioral or both) and the source of the textual features (DNN-based vs. hand-crafted). Our results demonstrate that given a prefix of the interaction sequence, our models can predict the future decisions of the decision-maker, particularly when a sequential modeling approach and hand-crafted textual features are applied. Further analysis of the hand-crafted textual features allows us to make initial observations about the aspects of text that drive decision making in our setup.",,5,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,ERC,740435,European Research Council
2-s2.0-85174408078,,,,Predicting Ordinary Differential Equations with Transformers,cp,Conference Paper,Becker S.,60019722;60024007;60111161;60121131;130001030,"Technische Universität München;Helmholtz Center Munich German Research Center for Environmental Health;DeepMind Technologies Limited;OpenAI, Inc.;Apple",Munich;Oberschleissheim;London;San Francisco;Paris,Germany;Germany;United Kingdom;United States;France,5.0,"Becker, Sören;Klein, Michal;Neitz, Alexander;Parascandolo, Giambattista;Kilbertus, Niki",57972242200;57221351555;57189899915;57189593873;57202058195,60024007;130001030;60111161;60121131;60024007-60019722,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,1978-2002,"We develop a transformer-based sequence-to-sequence model that recovers scalar ordinary differential equations (ODEs) in symbolic form from irregularly sampled and noisy observations of a single solution trajectory. We demonstrate in extensive empirical evaluations that our model performs better or on par with existing methods in terms of accurate recovery across various settings. Moreover, our method is efficiently scalable: after one-time pretraining on a large set of ODEs, we can infer the governing law of a new observed solution in a few forward passes of the model.",,13,0.0,,,,,Helmholtz Association
2-s2.0-85124123720,10.1613/JAIR.1.12332,,,Preferences Single-Peaked on a Tree: Multiwinner Elections and Structural Results,ar,Article,Peters D.,60016849;60026851;60026306;60006191,University of Toronto;University of Oxford;University of Nebraska–Lincoln;Google LLC,Toronto;Oxford;Lincoln;Mountain View,Canada;United Kingdom;United States;United States,4.0,"Peters, Dominik;Yu, Lan;Chan, Hau;Elkind, Edith",57002231500;36492620400;36023465000;23008155200,60016849;60006191;60026306;60026851,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,231-276,"A preference profile is single-peaked on a tree if the candidate set can be equipped with a tree structure so that the preferences of each voter are decreasing from their top candidate along all paths in the tree. This notion was introduced by Demange (1982), and subsequently Trick (1989b) described an efficient algorithm for deciding if a given profile is single-peaked on a tree. We study the complexity of multiwinner elections under several variants of the Chamberlin–Courant rule for preferences single-peaked on trees. We show that in this setting the egalitarian version of this rule admits a polynomial-time winner determination algorithm. For the utilitarian version, we prove that winner determination remains NP-hard for the Borda scoring function; indeed, this hardness results extends to a large family of scoring functions. However, a winning committee can be found in polynomial time if either the number of leaves or the number of internal vertices of the underlying tree is bounded by a constant. To benefit from these positive results, we need a procedure that can determine whether a given profile is single-peaked on a tree that has additional desirable properties (such as, e.g., a small number of leaves). To address this challenge, we develop a structural approach that enables us to compactly represent all trees with respect to which a given profile is single-peaked. We show how to use this representation to efficiently find the best tree for a given profile for use with our winner determination algorithms: Given a profile, we can efficiently find a tree with the minimum number of leaves, or a tree with the minimum number of internal vertices among trees on which the profile is single-peaked. We then explore the power and limitations of this framework: we develop polynomial-time algorithms to find trees with the smallest maximum degree, diameter, or pathwidth, but show that it is NP-hard to check whether a given profile is single-peaked on a tree that is isomorphic to a given tree, or on a regular tree.",,5,1.0,all publisherfullgold repository repositoryvor repositoryam,All Open Access Gold Green,EPSRC,OISE-1209805,Engineering and Physical Sciences Research Council
2-s2.0-85213704625,10.1613/jair.1.16694,,,Preserving Fairness in AI under Domain Shift,ar,Article,Stan S.,60029311,University of Southern California,Los Angeles,United States,2.0,"Stan, Serban;Rostami, Mohammad",57221148504;55408018500,60029311;60029311,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,907-934,"Existing algorithms for ensuring fairness in AI use a single-shot training strategy, where an AI model is trained on an annotated training dataset with sensitive attributes and then fielded for utilization. This training strategy is effective in problems with stationary distributions, where both the training and testing data are drawn from the same distribution. However, it is vulnerable with respect to distributional shifts in the input space that may occur after the initial training phase. As a result, the time-dependent nature of data can introduce biases and performance degradation into the model predictions, even if the model is initially fair. Model retraining from scratch using a new annotated dataset is a naive solution that is expensive and time-consuming. We develop an algorithm to adapt a fair model to remain fair and generalizable under domain shift using solely new unannotated data points. We recast this learning setting as an unsupervised domain adaptation (UDA) problem. Our algorithm is based on updating the model such that the internal representation of data remains unbiased despite distributional shifts in the input space. We provide empirical validation on three common fairness datasets to show that the challenge exists in practical setting and to demonstrate the effectiveness of our algorithm.",,3,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85140371991,10.1609/aaai.v36i5.20528,,,Prevailing in the Dark: Information Walls in Strategic Games,cp,Conference Paper,Naumov P.,60025225;60004191,University of Southampton;Scripps College,Southampton;Claremont,United Kingdom;United States,2.0,"Naumov, Pavel;Zhang, Wenxuan",9534673600;58093998100,60025225;60004191,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,5842-5850,The paper studies strategic abilities that rise from restrictions on the information sharing in multiagent systems. The main technical result is a sound and complete logical system that describes the interplay between the knowledge and the strategic ability modalities.,,2,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85189517843,10.1609/aaai.v38i4.28161,,,Primitive-Based 3D Human-Object Interaction Modelling and Programming,cp,Conference Paper,Liu S.,60025084,Shanghai Jiao Tong University,Shanghai,China,6.0,"Liu, Siqi;Li, Yong Lu;Fang, Zhou;Liu, Xinpeng;You, Yang;Lu, Cewu",57219458634;57202366867;58783606000;57219509459;57203963060;23035659100,60025084;60025084;60025084;60025084;60025084;60025084,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,4.0,,3711-3719,"Embedding Human and Articulated Object Interaction (HAOI) in 3D is an important direction for a deeper human activity understanding. Different from previous works that use parametric and CAD models to represent humans and objects, in this work, we propose a novel 3D geometric primitive-based language to encode both humans and objects. Given our new paradigm, humans and objects are all compositions of primitives instead of heterogeneous entities. Thus, mutual information learning may be achieved between the limited 3D data of humans and different object categories. Moreover, considering the simplicity of the expression and the richness of the information it contains, we choose the superquadric as the primitive representation. To explore an effective embedding of HAOI for the machine, we build a new benchmark on 3D HAOI consisting of primitives together with their images and propose a task requiring machines to recover 3D HAOI using primitives from images. Moreover, we propose a baseline of single-view 3D reconstruction on HAOI. We believe this primitive-based 3D HAOI representation would pave the way for 3D HAOI studies. Our code and data are available at https://mvig-rhos.com/p3haoi.",,2,1.0,all publisherfullgold,All Open Access Gold,NSFC,62306175,National Natural Science Foundation of China
2-s2.0-85207829401,,,,Principled Out-of-Distribution Detection via Multiple Testing,ar,Article,Magesh A.,60158506;60000461,The Grainger College of Engineering;SRI International,Urbana;Menlo Park,United States;United States,4.0,"Magesh, Akshayaa;Veeravalli, Venugopal V.;Roy, Anirban;Jha, Susmit",57216374161;35579177800;56421078300;23476883200,60158506;60158506;60000461;60000461,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,378,,"We study the problem of out-of-distribution (OOD) detection, that is, detecting whether a machine learning (ML) model’s output can be trusted at inference time. While a number of tests for OOD detection have been proposed in prior work, a formal framework for studying this problem is lacking. We propose a definition for the notion of OOD that includes both the input distribution and the ML model, which provides insights for the construction of powerful tests for OOD detection. We also propose a multiple hypothesis testing inspired procedure to systematically combine any number of different statistics from the ML model using conformal p-values. We further provide strong guarantees on the probability of incorrectly classifying an in-distribution sample as OOD. In our experiments, we find that threshold-based tests proposed in prior work perform well in specific settings, but not uniformly well across different OOD instances. In contrast, our proposed method that combines multiple statistics performs uniformly well across different datasets and neural networks architectures.",Benjamini-Hochberg procedure | Conditional False Alarm Guarantees | Conformal p-values | OOD characterization,7,0.0,,,ARL,W911NF-17-2-0196,Army Research Laboratory
2-s2.0-85188556514,,,,Privacy Auditing with One (1) Training Run,cp,Conference Paper,Steinke T.,60111161,DeepMind Technologies Limited,London,United Kingdom,3.0,"Steinke, Thomas;Nasr, Milad;Jagielski, Matthew",56354113000;57191968336;57203204598,60111161;60111161;60111161,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"We propose a scheme for auditing differentially private machine learning systems with a single training run. This exploits the parallelism of being able to add or remove multiple training examples independently. We analyze this using the connection between differential privacy and statistical generalization, which avoids the cost of group privacy. Our auditing scheme requires minimal assumptions about the algorithm and can be applied in the black-box or white-box setting. We demonstrate the effectiveness of our framework by applying it to DP-SGD, where we can achieve meaningful empirical privacy lower bounds by training only one model. In contrast, standard methods would require training hundreds of models.",,50,0.0,,,,,
2-s2.0-85163137311,,,,Privacy for Free: How does Dataset Condensation Help Privacy?,cp,Conference Paper,Dong T.,60025084;60027272;129865667,Shanghai Jiao Tong University;The University of Edinburgh;Sony AI,Shanghai;Edinburgh;,China;United Kingdom;,3.0,"Dong, Tian;Zhao, Bo;Lyu, Lingjuan",57221418303;57194457536;57189234207,60025084-129865667;60027272;129865667,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,5378-5396,"To prevent unintentional data leakage, research community has resorted to data generators that can produce differentially private data for model training. However, for the sake of the data privacy, existing solutions suffer from either expensive training cost or poor generalization performance. Therefore, we raise the question whether training efficiency and privacy can be achieved simultaneously. In this work, we for the first time identify that dataset condensation (DC) which is originally designed for improving training efficiency is also a better solution to replace the traditional data generators for private data generation, thus providing privacy for free. To demonstrate the privacy benefit of DC, we build a connection between DC and differential privacy, and theoretically prove on linear feature extractors (and then extended to non-linear feature extractors) that the existence of one sample has limited impact (O(m/n)) on the parameter distribution of networks trained on m samples synthesized from n(n ≫ m) raw samples by DC. We also empirically validate the visual privacy and membership privacy of DC-synthesized data by launching both the loss-based and the state-of-the-art likelihood-based membership inference attacks. We envision this work as a milestone for data-efficient and privacy-preserving machine learning.",,76,0.0,,,,,
2-s2.0-85150325498,,,,Private optimization in the interpolation regime: faster rates and hardness results,cp,Conference Paper,Asi H.,60012708;60141508,Stanford University;Stanford Engineering,Stanford;Stanford,United States;United States,4.0,"Asi, Hilal;Chadha, Karan;Cheng, Gary;Duchi, John",57197709875;56031252700;57222059077;26221757400,60141508;60141508;60141508;60141508-60012708,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,1025-1045,"In non-private stochastic convex optimization, stochastic gradient methods converge much faster on interpolation problems-namely, problems where there exists a solution that simultaneously minimizes all of the sample losses-than on non-interpolating ones; similar improvements are not known in the private setting. In this paper, we investigate differentially private stochastic optimization in the interpolation regime. First, we show that without additional assumptions, interpolation problems do not exhibit an improved convergence rates with differential privacy. However, when the functions exhibit quadratic growth around the optimum, we show (near) exponential improvements in the private sample complexity. In particular, we propose an adaptive algorithm that improves the sample complexity to achieve expected error α from (Eqaution presented) for any fixed ρ > 0, while retaining the standard minimax-optimal sample complexity for non-interpolation problems. We prove a lower bound that shows the dimension-dependent term in the expression above is tight. Furthermore, we provide a superefficiency result which demonstrates the necessity of the polynomial term for adaptive algorithms: any algorithm that has a polylogarithmic sample complexity for interpolation problems cannot achieve the minimax-optimal rates for the family of non-interpolation problems.",,5,0.0,,,,,
2-s2.0-105018453102,,,,Probabilistic Forecasting with Generative Networks via Scoring Rule Minimization,ar,Article,Pacchiardi L.,60026851;60105683;60163091;60026710,"University of Oxford;Southern University of Science and Technology;Faculty of Science, Engineering and Medicine;European Centre for Medium-Range Weather Forecasts",Oxford;Shenzhen;Coventry;Reading,United Kingdom;China;United Kingdom;United Kingdom,4.0,"Pacchiardi, Lorenzo;Adewoyin, Rilwan A.;Dueben, Peter;Dutta, Ritabrata",57214232939;57219621345;54879515900;57189218756,60026851;60163091-60105683;60026710;60163091,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Probabilistic forecasting relies on past observations to provide a probability distribution for a future outcome, which is often evaluated against the realization using a scoring rule. Here, we perform probabilistic forecasting with generative neural networks, which parametrize distributions on high-dimensional spaces by transforming draws from a latent variable. Generative networks are typically trained in an adversarial framework. In contrast, we propose to train generative networks to minimize a predictive-sequential (or prequential) scoring rule on a recorded temporal sequence of the phenomenon of interest, which is appealing as it corresponds to the way forecasting systems are routinely evaluated. Adversarial-free minimization is possible for some scoring rules; hence, our framework avoids the cumbersome hyperparameter tuning and uncertainty underestimation due to unstable adversarial training, thus unlocking reliable use of generative networks in probabilistic forecasting. Further, we prove consistency of the minimizer of our objective with dependent data, while adversarial training assumes independence. We perform simulation studies on two chaotic dynamical models and a benchmark data set of global weather observations; for this last example, we define scoring rules for spatial data by drawing from the relevant literature. Our method outperforms state-of-the-art adversarial approaches, especially in probabilistic calibration, while requiring less hyperparameter tuning.",Adversarial-free | GAN | Generative Networks | Probabilistic Forecasting | Scoring Rules,11,0.0,,,EPSRC,EP/V025899/1,Engineering and Physical Sciences Research Council
2-s2.0-85203843008,,,,Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo,cp,Conference Paper,Zhao S.,60016849;60278837,University of Toronto;Vector Institute,Toronto;Toronto,Canada;Canada,4.0,"Zhao, Stephen;Brekelmans, Rob;Makhzani, Alireza;Grosse, Roger",57219741013;57202385248;55001812100;34875103900,60016849-60278837;60278837;60016849-60278837;60016849-60278837,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,60704-60748,"Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence.In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems.In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences.We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning.As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function.These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions.We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.",,9,0.0,,,,,Government of Ontario
2-s2.0-85189503881,10.1609/aaai.v38i18.30019,,,Probabilistic Offline Policy Ranking with Approximate Bayesian Computation,cp,Conference Paper,Da L.,60003892;60006832,Arizona State University;Brigham Young University,Tempe;Provo,United States;United States,5.0,"Da, Longchao;Jenkins, Porter;Schwantes, Trevor;Dotson, Jeffrey;Wei, Hua",57991810100;57209220353;58778534700;24490976300;57202789320,60003892;60006832;60006832;60006832;60003892,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,18.0,,20370-20378,"In practice, it is essential to compare and rank candidate policies offline before real-world deployment for safety and reliability. Prior work seeks to solve this offline policy ranking (OPR) problem through value-based methods, such as Off-policy evaluation (OPE). However, they fail to analyze special case performance (e.g., worst or best cases), due to the lack of holistic characterization of policies’ performance. It is even more difficult to estimate precise policy values when the reward is not fully accessible under sparse settings. In this paper, we present Probabilistic Offline Policy Ranking (POPR), a framework to address OPR problems by leveraging expert data to characterize the probability of a candidate policy behaving like experts, and approximating its entire performance posterior distribution to help with ranking. POPR does not rely on value estimation, and the derived performance posterior can be used to distinguish candidates in worst-, best-, and average-cases. To estimate the posterior, we propose POPR-EABC, an Energy-based Approximate Bayesian Computation (ABC) method conducting likelihood-free inference. POPR-EABC reduces the heuristic nature of ABC by a smooth energy function, and improves the sampling efficiency by a pseudo-likelihood. We empirically demonstrate that POPR-EABC is adequate for evaluating policies in both discrete and continuous action spaces across various experiment environments, and facilitates probabilistic comparisons of candidate policies before deployment.",,1,1.0,all publisherfullgold,All Open Access Gold,NSF,2153311,National Science Foundation
2-s2.0-85202026514,10.1613/jair.1.15679,,,Probabilities of the third type: Statistical relational learning and reasoning with relative frequencies,ar,Article,Weitkämper F.,60028717,Ludwig-Maximilians-Universität München,Munich,Germany,1.0,"Weitkämper, Felix",57219337277,60028717,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,1407-1436,"Dependencies on the relative frequency of a state in the domain are common when modelling probabilistic dependencies on relational data. For instance, the likelihood of a school closure during an epidemic might depend on the proportion of infected pupils exceeding a threshold. Often, rather than depending on discrete thresholds, dependencies are continuous: for instance, the likelihood of any one mosquito bite transmitting an illness depends on the proportion of carrier mosquitoes. Current approaches usually only consider probabilities over possible worlds rather than over domain elements themselves. An exception are the recently introduced Lifted Bayesian Networks for Conditional Probability Logic, which express discrete dependencies on probabilistic data. We introduce functional lifted Bayesian networks, a formalism that explicitly incorporates continuous dependencies on relative frequencies into statistical relational artificial intelligence, and compare and contrast them with lifted Bayesian networks for conditional probability logic. Incorporating relative frequencies is not only beneficial to modelling; it also provides a more rigorous approach to learning problems where training and test or application domains have different sizes. To this end, we provide a representation of the asymptotic probability distributions induced by functional lifted Bayesian networks on domains of increasing sizes. Since that representation has well-understood scaling behaviour across domain sizes, it can be used to estimate parameters for a large domain consistently from randomly sampled subpopulations. Furthermore, we show that in parametric families of FLBN, convergence is uniform in the parameters, which ensures a meaningful dependence of the asymptotic probabilities on the parameters of the model.",,0,1.0,all publisherfullgold,All Open Access Gold,BMBF,,Bundesministerium für Bildung und Forschung
2-s2.0-85163217770,,,,ProcTHOR: Large-Scale Embodied AI Using Procedural Generation,cp,Conference Paper,Deitke M.,60015481;126489860,University of Washington;PRIOR @ Allen Institute for AI,Seattle;Allen,United States;United States,11.0,"Deitke, Matt;VanderBilt, Eli;Herrasti, Alvaro;Weihs, Luca;Salvador, Jordi;Ehsani, Kiana;Han, Winson;Kolve, Eric;Farhadi, Ali;Kembhavi, Aniruddha;Mottaghi, Roozbeh",57219671843;57219666905;57191189350;57035350600;57219668313;57207777076;57219492602;57191433866;36099817000;57208117160;14632225600,126489860-60015481;126489860;126489860;126489860;126489860;126489860;126489860;126489860;60015481;126489860-60015481;126489860-60015481,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Massive datasets and high-capacity models have driven many recent advancements in computer vision and natural language understanding. This work presents a platform to enable similar success stories in Embodied AI. We propose PROCTHOR, a framework for procedural generation of Embodied AI environments. PROCTHOR enables us to sample arbitrarily large datasets of diverse, interactive, customizable, and performant virtual environments to train and evaluate embodied agents across navigation, interaction, and manipulation tasks. We demonstrate the power and potential of PROCTHOR via a sample of 10,000 generated houses and a simple neural model. Models trained using only RGB images on PROCTHOR, with no explicit mapping and no human task supervision produce state-of-the-art results across 6 embodied AI benchmarks for navigation, rearrangement, and arm manipulation, including the presently running Habitat 2022, AI2-THOR Rearrangement 2022, and RoboTHOR challenges. We also demonstrate strong 0-shot results on these benchmarks, via pre-training on PROCTHOR with no fine-tuning on the downstream benchmark, often beating previous state-of-the-art systems that access the downstream training data.",,137,0.0,,,,,
2-s2.0-105000464582,,,,Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation,cp,Conference Paper,Yuan K.,60019722;60103368;132184510,Technische Universität München;Université de Strasbourg;IHU Strasbourg,Munich;Strasbourg;Strasbourg,Germany;France;France,4.0,"Yuan, Kun;Srivastav, Vinkle;Navab, Nassir;Padoy, Nicolas",58115585300;57208333230;7003458998;24338962300,60103368-132184510-60019722;60103368-132184510;60019722;60103368-132184510,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Surgical video-language pretraining (VLP) faces unique challenges due to the knowledge domain gap and the scarcity of multi-modal data. This study aims to bridge the gap by addressing issues regarding textual information loss in surgical lecture videos and the spatial-temporal challenges of surgical VLP. To tackle these issues, we propose a hierarchical knowledge augmentation approach and a novel Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP) framework. The proposed knowledge augmentation approach uses large language models (LLM) to refine and enrich surgical concepts, thus providing comprehensive language supervision and reducing the risk of overfitting. The PeskaVLP framework combines language supervision with visual self-supervision, constructing hard negative samples and employing a Dynamic Time Warping (DTW) based loss function to effectively comprehend the cross-modal procedural alignment. Extensive experiments on multiple public surgical scene understanding and cross-modal retrieval datasets show that our proposed method significantly improves zero-shot transferring performance and offers a generalist visual representation for further advancements in surgical scene understanding. The source code will be available at https://github.com/CAMMA-public/PeskaVLP.",,9,0.0,,,EAES,AD011013704R1,European Association for Endoscopic Surgery and other Interventional Techniques
2-s2.0-85169700086,10.1613/JAIR.1.14394,,,Program Synthesis with Best-First Bottom-Up Search,ar,Article,Ameen S.,60030835,University of Alberta,Edmonton,Canada,2.0,"Ameen, Saqib;Lelis, Levi H.S.",58538664800;35812244500,60030835;60030835,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,1275-1310,"Cost-guided bottom-up search (BUS) algorithms use a cost function to guide the search to solve program synthesis tasks. In this paper, we show that current state-of-the-art costguided BUS algorithms suffer from a common problem: they can lose useful information given by the model and fail to perform the search in a best-first order according to a cost function. We introduce a novel best-first bottom-up search algorithm, which we call Bee Search, that does not suffer information loss and is able to perform cost-guided bottom-up synthesis in a best-first manner. Importantly, Bee Search performs best-first search with respect to the generation of programs, i.e., it does not even create in memory programs that are more expensive than the solution program. It attains best-first ordering with respect to generation by performing a search in an abstract space of program costs. We also introduce a new cost function that better uses the information provided by an existing cost model. Empirical results on string manipulation and bit-vector tasks show that Bee Search can outperform existing cost-guided BUS approaches when employing more complex domain-specific languages (DSLs); Bee Search and previous approaches perform equally well with simpler DSLs. Furthermore, our new cost function with Bee Search outperforms previous cost functions on string manipulation tasks.",,7,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,NSERC,,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85204127394,,,,Progressive Ensemble Distillation: Building Ensembles for Efficient Inference,cp,Conference Paper,Dennis D.K.,60025038;60027950;60026532,"University of California, Berkeley;Carnegie Mellon University;Microsoft Corporation",Berkeley;Pittsburgh;Redmond,United States;United States;United States,5.0,"Dennis, Don Kurian;Shetty, Abhishek;Sevekari, Anish;Koishida, Kazuhito;Smith, Virginia",57208443985;57209805441;57208437784;6601990788;55488836500,60027950;60025038;60027950;60026532;60027950,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"We study the problem of progressive ensemble distillation: Given a large, pretrained teacher model g, we seek to decompose the model into smaller, low-inference cost student models f<inf>i</inf>, such that progressively evaluating additional models in this ensemble leads to improved predictions. The resulting ensemble allows for flexibly tuning accuracy vs. inference cost at runtime, which is useful for a number of applications in on-device inference. The method we propose, B-DISTIL, relies on an algorithmic procedure that uses function composition over intermediate activations to construct expressive ensembles with similar performance as g, but with smaller student models. We demonstrate the effectiveness of B-DISTIL by decomposing pretrained models across standard image, speech, and sensor datasets. We also provide theoretical guarantees in terms of convergence and generalization.",,6,0.0,,,NSF,IIS2145670,National Science Foundation
2-s2.0-85148085692,,,,Project and Forget: Solving Large-Scale Metric Constrained Problems,ar,Article,Sonthalia R.,60025778;60005455,"University of Michigan, Ann Arbor;Yale University",Ann Arbor;New Haven,United States;United States,2.0,"Sonthalia, Rishi;Gilbert, Anna C.",57207776739;7201514859,60025778;60005455,2022-11-01,1 November 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,326,,"Many important machine learning problems can be formulated as highly constrained convex optimization problems. One important example is metric constrained problems. In this paper, we show that standard optimization techniques can not be used to solve metric constrained problem. To solve such problems, we provide a general active set framework, called Project and Forget, and several variants thereof that use Bregman projections. Project and Forget is a general purpose method that can be used to solve highly constrained convex problems with many (possibly exponentially) constraints. We provide a theoretical analysis of Project and Forget and prove that our algorithms converge to the global optimal solution and have a linear rate of convergence. We demonstrate that using our method, we can solve large problem instances of general weighted correlation clustering, metric nearness, information theoretic metric learning and quadratically regularized optimal transport; in each case, out-performing the state of the art methods with respect to CPU times and problem sizes.",Correlation Clustering | Large Scale Convex Optimization | Metric Constrained Optimization | Metric Learning,2,0.0,,,NSF,DMS-2027277,National Science Foundation
2-s2.0-85203702841,,,,Prompt Learning for Generalized Vehicle Routing,cp,Conference Paper,Liu F.,60013983;60105683;60119391,City University of Hong Kong;Southern University of Science and Technology;Huawei Noah's Ark Lab,Hong Kong;Shenzhen;Hong Kong,Hong Kong;China;Hong Kong,7.0,"Liu, Fei;Lin, Xi;Liao, Weiduo;Wang, Zhenkun;Zhang, Qingfu;Tong, Xialiang;Yuan, Mingxuan",55559931500;57192821375;57215412707;56403891000;7406718367;57210647349;19640750600,60013983;60013983;60013983;60105683;60013983;60119391;60119391,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,6976-6984,"Neural combinatorial optimization (NCO) is a promising learning-based approach to solving various vehicle routing problems without much manual algorithm design. However, the current NCO methods mainly focus on the in-distribution performance, while the real-world problem instances usually come from different distributions. A costly fine-tuning approach or generalized model retraining from scratch could be needed to tackle the out-of-distribution instances. Unlike the existing methods, this work investigates an efficient prompt learning approach in NCO for cross-distribution adaptation. To be concrete, we propose a novel prompt learning method to facilitate fast zero-shot adaptation of a pre-trained model to solve routing problem instances from different distributions. The proposed model learns a set of prompts among various distributions and then selects the best-matched one to prompt a pre-trained attention model for each problem instance. Extensive experiments show that the proposed prompt learning approach facilitates the fast adaptation of pre-trained routing models. It also outperforms existing generalized models on both in-distribution prediction and zero-shot generalization to a diverse set of new tasks. Our code implementation is available online at https://github.com/FeiLiu36/PromptVRP.",,5,0.0,,,研究資助局,CityU 11215723,"Research Grants Council, University Grants Committee"
2-s2.0-105000511172,,,,PromptFix: You Prompt and We Fix the Photo,cp,Conference Paper,Yu Y.,60027165;60021726,University of Rochester;Microsoft Research,Rochester;Redmond,United States;United States,5.0,"Yu, Yongsheng;Zeng, Ziyun;Hua, Hang;Fu, Jianlong;Luo, Jiebo",57865792400;59279020300;57204187963;36731082400;7404182441,60027165;60027165;60027165;60021726;60027165,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks. The dataset and code are available at https://www.yongshengyu.com/PromptFix-Page.",,2,0.0,,,,,
2-s2.0-85147257414,10.1613/jair.1.13811,,,Proofs and Certificates for Max-SAT,ar,Article,Py M.,60102127;60138276,Aix Marseille Université;Université Clermont Auvergne,Marseille;Clermont-Ferrand,France;France,3.0,"Py, Matthieu;Cherif, Mohamed Sami;Habet, Djamal",7003576402;57210811723;16642725100,60138276;60102127;60102127,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,1373-1400,"Current Max-SAT solvers are able to efficiently compute the optimal value of an input instance but they do not provide any certificate of its validity. In this paper, we present a tool, called MS-Builder, which generates certificates for the Max-SAT problem in the particular form of a sequence of equivalence-preserving transformations. To generate a certificate, MS-Builder iteratively calls a SAT oracle to get a SAT resolution refutation which is handled and adapted into a sound refutation for Max-SAT. In particular, we prove that the size of the computed Max-SAT refutation is linear with respect to the size of the initial refutation if it is semi-read-once, tree-like regular, tree-like or semi-tree-like. Additionally, we propose an extendable tool, called MS-Checker, able to verify the validity of any Max-SAT certificate using Max-SAT inference rules. Both tools are evaluated on the unweighted and weighted benchmark instances of the 2020 Max-SAT Evaluation.",,7,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,AMU,LIS - UMR 7020,Aix-Marseille Université
2-s2.0-85189306820,10.1609/aaai.v38i9.28813,,,Proportional Aggregation of Preferences for Sequential Decision Making,cp,Conference Paper,Chandak N.,60000163;60105740,"International Institute of Information Technology, Hyderabad;Laboratoire d’Analyse et de Modélisation de Systèmes pour l’Aide à la Décision",Hyderabad;Paris,India;France,3.0,"Chandak, Nikhil;Goel, Shashwat;Peters, Dominik",57700975300;58711735300;57002231500,60000163;60000163;60105740,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,9.0,,9573-9581,"We study the problem of fair sequential decision making given voter preferences. In each round, a decision rule must choose a decision from a set of alternatives where each voter reports which of these alternatives they approve. Instead of going with the most popular choice in each round, we aim for proportional representation, using axioms inspired by the multi-winner voting literature. The axioms require that every group of α% of the voters, if it agrees in every round (i.e., approves a common alternative), then those voters must approve at least α% of the decisions. A stronger version of the axioms requires that every group of α% of the voters that agrees in a β fraction of rounds must approve β · α% of the decisions. We show that three attractive voting rules satisfy axioms of this style. One of them (Sequential Phragmén) makes its decisions online, and the other two satisfy strengthened versions of the axioms but make decisions semi-online (Method of Equal Shares) or fully offline (Proportional Approval Voting). We present empirical results for these rules based on synthetic data and U.S. political elections. We also run experiments using the moral machine dataset about ethical dilemmas. We train preference models on user responses from different countries and let the models cast votes. We find that aggregating these votes using our rules leads to a more equal utility distribution across demographics than making decisions using a single global preference model.",,17,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85165198617,10.24963/ijcai.2023/282,,,Proportionality Guarantees in Elections with Interdependent Issues,cp,Conference Paper,Brill M.,60022020;60011604;60019507;130170399,University of Warwick;Technische Universität Berlin;Athens University of Economics and Business;Input Output Global (IOG),Coventry;Berlin;Athens;,United Kingdom;Germany;Greece;,4.0,"Brill, Markus;Markakis, Evangelos;Papasotiropoulos, Georgios;Peters, Jannik",35208118700;57200805370;57220547743;57216979552,60022020-60011604;60019507-130170399;60019507;60011604,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,2537-2545,"We consider a multi-issue election setting over a set of possibly interdependent issues with the goal of achieving proportional representation of the views of the electorate. To this end, we employ a proportionality criterion suggested recently in the literature, that guarantees fair representation for all groups of voters of sufficient size. For this criterion, there exist rules that perform well in the case where all the issues have a binary domain and are independent of each other. In particular, this has been shown for Proportional Approval Voting (PAV) and for the Method of Equal Shares (MES). In this paper, we go two steps further: we generalize these guarantees for issues with a non-binary domain, and, most importantly, we consider extensions to elections with dependencies among issues, where we identify restrictions that lead to analogous results. To achieve this, we define appropriate generalizations of PAV and MES to handle conditional ballots. In addition to proportionality considerations, we also examine the computational properties of the conditional version of MES. Our findings indicate that the conditional case poses additional challenges and differs significantly from the unconditional one, both in terms of proportionality guarantees and computational complexity.",,5,1.0,all publisherfullgold,All Open Access Gold,COST,CA16228,European Cooperation in Science and Technology
2-s2.0-85148278643,10.1609/aaai.v37i5.25686,,,Proportionality in Approval-Based Participatory Budgeting,cp,Conference Paper,Brill M.,60002483;60022020;60011604;60018163,Universiteit van Amsterdam;University of Warwick;Technische Universität Berlin;TU Wien,Amsterdam;Coventry;Berlin;Vienna,Netherlands;United Kingdom;Germany;Austria,5.0,"Brill, Markus;Forster, Stefan;Lackner, Martin;Maly, Jan;Peters, Jannik",35208118700;58109619000;55245740900;57203372240;57216979552,60011604-60022020;60018163;60018163;60002483;60011604,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,5524-5531,"The ability to measure the satisfaction of (groups of) voters is a crucial prerequisite for formulating proportionality axioms in approval-based participatory budgeting elections. Two common - but very different - ways to measure the satisfaction of a voter consider (i) the number of approved projects and (ii) the total cost of approved projects, respectively. In general, it is difficult to decide which measure of satisfaction best reflects the voters' true utilities. In this paper, we study proportionality axioms with respect to large classes of approval-based satisfaction functions. We establish logical implications among our axioms and related notions from the literature, and we ask whether outcomes can be achieved that are proportional with respect to more than one satisfaction function. We show that this is impossible for the two commonly used satisfaction functions when considering proportionality notions based on extended justified representation, but achievable for a notion based on proportional justified representation. For the latter result, we introduce a strengthening of priceability and show that it is satisfied by several polynomial-time computable rules, including the Method of Equal Shares and Phragmén's sequential rule.",,17,1.0,all publisherfullgold,All Open Access Gold,FWF,J4581,Austrian Science Fund
2-s2.0-85163073701,,,,Prototype-Anchored Learning for Learning with Imperfect Annotations,cp,Conference Paper,Zhou X.,60025278;60019616;60092945;60271961,Tsinghua University;Harbin Institute of Technology;King Abdullah University of Science and Technology;Peng Cheng Laboratory,Beijing;Harbin;Thuwal;Shenzhen,China;China;Saudi Arabia;China,6.0,"Zhou, Xiong;Liu, Xianming;Zhai, Deming;Jiang, Junjun;Gao, Xin;Ji, Xiangyang",57224783286;35230216700;35236030000;54902306100;55712115900;7402837796,60019616;60019616-60271961;60019616;60019616-60271961;60271961-60092945;60025278,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,27245-27267,"The success of deep neural networks greatly relies on the availability of large amounts of high-quality annotated data, which however are difficult or expensive to obtain. The resulting labels may be class imbalanced, noisy or human biased. It is challenging to learn unbiased classification models from imperfectly annotated datasets, on which we usually suffer from overfitting or underfitting. In this work, we thoroughly investigate the popular softmax loss and margin-based loss, and offer a feasible approach to tighten the generalization error bound by maximizing the minimal sample margin. We further derive the optimality condition for this purpose, which indicates how the class prototypes should be anchored. Motivated by theoretical analysis, we propose a simple yet effective method, namely prototype-anchored learning (PAL), which can be easily incorporated into various learning-based classification schemes to handle imperfect annotation. We verify the effectiveness of PAL on class-imbalanced learning and noise-tolerant learning by extensive experiments on synthetic and real-world datasets.",,7,0.0,,,NSFC,61922027,National Natural Science Foundation of China
2-s2.0-85191166903,,,,Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs,cp,Conference Paper,Abbe E.,125701908;119228941,EPFL;MIT,Geneva;Biel,Switzerland;Switzerland,3.0,"Abbe, Emmanuel;Cornacchia, Elisabetta;Lotfi, Aryo",16237841500;57218846417;57222185156,125701908;119228941;125701908,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Experimental results have shown that curriculum learning, i.e., presenting simpler examples before more complex ones, can improve the efficiency of learning. Some recent theoretical results also showed that changing the sampling distribution can help neural networks learn parities, with formal results only for large learning rates and one-step arguments. Here we show a separation result in the number of training steps with standard (bounded) learning rates on a common sample distribution: if the data distribution is a mixture of sparse and dense inputs, there exists a regime in which a 2-layer ReLU neural network trained by a curriculum noisy-GD (or SGD) algorithm that uses sparse examples first, can learn parities of sufficiently large degree, while any fully connected neural network of possibly larger width or depth trained by noisy-GD on the unordered samples cannot learn without additional steps. We also provide experimental results supporting the qualitative separation beyond the specific regime of the theoretical results.",,15,0.0,,,,,
2-s2.0-85174403106,,,,Pruning via Sparsity-indexed ODE: a Continuous Sparsity Viewpoint,cp,Conference Paper,Mo Z.,60005510;60002798;60078616,Nanyang Technological University;Chinese University of Hong Kong;School of Computer Science and Engineering,Singapore City;Hong Kong;Singapore City,Singapore;Hong Kong;Singapore,3.0,"Mo, Zhanfeng;Shi, Haosen;Pan, Sinno Jialin",57987798900;57224564342;22939024800,60078616;60078616-60005510;60078616-60002798,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,25018-25036,"Neural pruning, which involves identifying the optimal sparse subnetwork, is a key technique for reducing the complexity and improving the efficiency of deep neural networks. To address the challenge of solving neural pruning at a specific sparsity level directly, we investigate the evolution of optimal subnetworks with continuously increasing sparsity, which can provide insight into how to transform an unpruned dense model into an optimal subnetwork with any desired level of sparsity. In this paper, we proposed a novel pruning framework, coined Sparsity-indexed ODE (SpODE) that provides explicit guidance on how to best preserve model performance while ensuring an infinitesimal increase in model sparsity. On top of this, we develop a pruning algorithm, termed Pruning via Sparsity-indexed ODE (PSO), that enables effective pruning via traveling along the SpODE path. Empirical experiments show that PSO achieves either better or comparable performance compared to state-of-the-art baselines across various pruning settings. Our implementations are now available on GitHub.",,2,0.0,,,MSRA,,Microsoft Research Asia
2-s2.0-105018582080,,,,Pursuit of the Cluster Structure of Network Lasso: Recovery Condition and Non-convex Extension,ar,Article,Yagishita S.,60015336,Chuo University,Hachioji,Japan,2.0,"Yagishita, Shotaro;Gotoh, Jun Ya",57221868480;7007052927,60015336;60015336,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Network lasso (NL for short) is a technique for estimating models by simultaneously clustering data samples and fitting the models to them. It often succeeds in forming clusters thanks to the geometry of the sum of ℓ<inf>2</inf> norm employed therein, but there may be limitations due to the convexity of the regularizer. This paper focuses on clustering generated by NL and strengthens it by creating a non-convex extension, called network trimmed lasso (NTL for short). Specifically, we initially investigate a sufficient condition that guarantees the recovery of the latent cluster structure of NL on the basis of the result of Sun et al. (2021) for convex clustering, which is a special case of NL for ordinary clustering. Second, we extend NL to NTL to incorporate a cardinality (or, ℓ<inf>0</inf>-)constraint and rewrite the constrained optimization problem defined with the ℓ<inf>0</inf> norm, a discontinuous function, into an equivalent unconstrained continuous optimization problem. We develop ADMM algorithms to solve NTL and show their convergence results. Numerical illustrations indicate that the non-convex extension provides a more clear-cut cluster structure when NL fails to form clusters without incorporating prior knowledge of the associated parameters.",alternating direction method of multipliers (ADMM) | clustering | network lasso | network trimmed lasso | sparse modeling,2,0.0,,,JSPS,19H02379,Japan Society for the Promotion of Science
2-s2.0-105018464212,,,,Pygmtools: A Python Graph Matching Toolkit,ar,Article,Wang R.,60025084,Shanghai Jiao Tong University,Shanghai,China,13.0,"Wang, Runzhong;Guo, Ziao;Pan, Wenzheng;Ma, Jiale;Zhang, Yikai;Yang, Nan;Liu, Qi;Wei, Longxuan;Zhang, Hanxue;Liu, Chang;Jiang, Zetian;Yang, Xiaokang;Yan, Junchi",57215779042;57434578900;59983669200;59984778500;58873326400;60138137300;57188805838;59733593200;58451440000;56156416500;57221705930;7406503333;36026971200,60025084;60025084;60025084;60025084;60025084;60025084;60025084;60025084;60025084;60025084;60025084;60025084;60025084,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Graph matching aims to find node-to-node matching among multiple graphs, which is a fundamental yet challenging problem. To facilitate graph matching in scientific research and industrial applications, pygmtools is released, which is a Python graph matching toolkit that implements a comprehensive collection of two-graph matching and multi-graph matching solvers, covering both learning-free solvers as well as learning-based neural graph matching solvers. Our implementation supports numerical backends including Numpy, PyTorch, Jittor, Paddle, runs on Windows, MacOS and Linux, and is friendly to install and configure. Comprehensive documentations covering beginner’s guide, API reference and examples are available online. pygmtools is open-sourced under Mulan PSL v2 license.",combinatorial optimization | graph learning | graph matching | python toolkit,14,0.0,,,SJTU,AI3607,Shanghai Jiao Tong University
2-s2.0-85212288032,10.1613/jair.1.15522,,,QCDCL vs QBF Resolution: Further Insights,ar,Article,Böhm B.,60029507,Friedrich-Schiller-Universität Jena,Jena,Germany,2.0,"Böhm, Benjamin;Beyersdorff, Olaf",55934401900;14035089700,60029507;60029507,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,741-769,"We continue the investigation on the relations of QCDCL and QBF resolution systems. In particular, we introduce QCDCL versions that tightly characterise QU-Resolution and (a slight variant of) long-distance Q-Resolution. We show that most QCDCL variants – parameterised by different policies for decisions, unit propagations and reductions – lead to incomparable systems for almost all choices of these policies.",,1,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,CZS,BE 4209/3-1,Carl-Zeiss-Stiftung
2-s2.0-85137940368,10.24963/ijcai.2022/248,,,QCDCL with Cube Learning or Pure Literal Elimination - What is Best?,cp,Conference Paper,Böhm B.,60018163;60029507,TU Wien;Friedrich-Schiller-Universität Jena,Vienna;Jena,Austria;Germany,3.0,"Böhm, Benjamin;Peitl, Tomáš;Beyersdorff, Olaf",55934401900;57190126466;14035089700,60029507;60018163;60029507,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1781-1787,"Quantified conflict-driven clause learning (QCDCL) is one of the main approaches for solving quantified Boolean formulas (QBF). We formalise and investigate several versions of QCDCL that include cube learning and/or pure-literal elimination, and formally compare the resulting solving models via proof complexity techniques. Our results show that almost all of the QCDCL models are exponentially incomparable with respect to proof size (and hence solver running time), pointing towards different orthogonal ways how to practically implement QCDCL.",,10,1.0,all publisherfree2read,All Open Access Bronze,FWF,J 4361,Austrian Science Fund
2-s2.0-85163302392,,,,QUANTIZED COMPRESSED SENSING WITH SCORE-BASED GENERATIVE MODELS,cp,Conference Paper,Meng X.,60025272,The University of Tokyo,Tokyo,Japan,2.0,"Meng, Xiangming;Kabashima, Yoshiyuki",56495966800;7004020076,60025272;60025272,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"We consider the general problem of recovering a high-dimensional signal from noisy quantized measurements. Quantization, especially coarse quantization such as 1-bit sign measurements, leads to severe information loss and thus a good prior knowledge of the unknown signal is helpful for accurate recovery. Motivated by the power of score-based generative models (SGM, also known as diffusion models) in capturing the rich structure of natural signals beyond simple sparsity, we propose an unsupervised data-driven approach called quantized compressed sensing with SGM (QCS-SGM), where the prior distribution is modeled by a pre-trained SGM. To perform posterior sampling, an annealed pseudo-likelihood score called noise perturbed pseudo-likelihood score is introduced and combined with the prior score of SGM. The proposed QCS-SGM applies to an arbitrary number of quantization bits. Experiments on a variety of baseline datasets demonstrate that the proposed QCS-SGM significantly outperforms existing state-of-the-art algorithms by a large margin for both in-distribution and out-of-distribution samples. Moreover, as a posterior sampling method, QCS-SGM can be easily used to obtain confidence intervals or uncertainty estimates of the reconstructed results. The code is available at https://github.com/mengxiangming/QCS-SGM.",,4,0.0,,,,17H00764,
2-s2.0-85203787425,,,,QuRating: Selecting High-Quality Data for Training Language Models,cp,Conference Paper,Wettig A.,60141284,School of Engineering and Applied Science,Princeton,United States,4.0,"Wettig, Alexander;Gupta, Aatmik;Malik, Saumya;Chen, Danqi",57280169100;58906547500;58906547600;56121986100,60141284;60141284;60141284;60141284,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,52915-52971,"Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pretraining data that can capture human intuitions about data quality. In this paper, we investigate four qualities-writing style, required expertise, facts & trivia, and educational value-and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.",,12,0.0,,,UT,IIS-2211779,University of Texas at Austin
2-s2.0-85170405083,10.1613/jair.1.14345,,,Qualitative Reasoning about 2D Cardinal Directions using Answer Set Programming,ar,Article,Izmirlioglu Y.,60008991;60146886,Sabancı Üniversitesi;Department of Computer Science,Tuzla;Las Cruces,Turkey;United States,2.0,"Izmirlioglu, Yusuf;Erdem, Esra",57195417904;36847843900,60146886;60008991,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,1371-1453,"We introduce a formal framework (called NCDC-ASP ) for representing and reasoning about cardinal directions between extended spatial objects on a plane, using Answer Set Programming (ASP). NCDC-ASP preserves the meaning of cardinal directional relations as in Cardinal Directional Calculus (CDC), and provides solutions to all consistency checking problems in CDC under various conditions (i.e., for a complete/incomplete set of basic/disjunctive CDC constraints over connected/disconnected spatial objects). In particular, NCDC-ASP models a discretized version of the consistency checking problem in ASP, over a finite grid (rather than a plane), where we provide new lower bounds on the grid size to guarantee that it correctly characterizes solutions for the consistency checking in CDC. In addition, NCDC-ASP has the following two novelties important for applications. NCDC-ASP introduces default CDC constraints to represent and reason about background or commonsense knowledge that involves default qualitative directional relations (e.g., ""the ice cream truck is by default to the north of the playground""or ""the keyboard is normally placed in front of the monitor""). NCDC-ASP introduces inferred CDC constraints to allow inference of missing CDC relations and to provide them as explanations. We illustrate the uses and usefulness of NCDC-ASP with interesting scenarios from the real-world. We design and develop a variety of benchmark instances, and comprehensively evaluate NCDC-ASP from the perspectives of computational efficiency.",,4,1.0,all publisherfullgold,All Open Access Gold,TÜBİTAK,CA17124,Türkiye Bilimsel ve Teknolojik Araştırma Kurumu
2-s2.0-85168312821,10.24963/ijcai.2023/41,,,Quantifying Harm,cp,Conference Paper,Beckers S.,60007776;60011520;60002483,Cornell University;King's College London;Universiteit van Amsterdam,Ithaca;London;Amsterdam,United States;United Kingdom;Netherlands,3.0,"Beckers, Sander;Chockler, Hana;Halpern, Joseph Y.",55248095600;57204843574;7202823540,60002483;60011520;60007776,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,363-371,"In earlier work we defined a qualitative notion of harm: either harm is caused, or it is not. For practical applications, we often need to quantify harm; for example, we may want to choose the least harmful of a set of possible interventions. We first present a quantitative definition of harm in a deterministic context involving a single individual, then we consider the issues involved in dealing with uncertainty regarding the context and going from a notion of harm for a single individual to a notion of “societal harm”, which involves aggregating the harm to individuals. We show that the “obvious” way of doing this (just taking the expected harm for an individual and then summing the expected harm over all individuals) can lead to counterintuitive or inappropriate answers, and discuss alternatives, drawing on work from the decision-theory literature.",,8,1.0,all publisherfullgold,All Open Access Gold,UKRI,EP/V00784X/1,UK Research and Innovation
2-s2.0-85141751349,,,,Quantile regression with ReLU Networks: Estimators and minimax rates,ar,Article,Padilla O.H.M.,60027550;60008592;102070180,"University of California, Los Angeles;Hong Kong University of Science and Technology;Epidemiology and Biostatistics",Los Angeles;Hong Kong;New York,United States;Hong Kong;United States,3.0,"Padilla, Oscar Hernan Madrid;Tansey, Wesley;Chen, Yanzhen",57189343918;24766519700;57217315944,60027550;102070180;60008592,2022-08-01,1 August 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,A84,,"Quantile regression is the task of estimating a specified percentile response, such as the median (50<sup>th</sup> percentile), from a collection of known covariates. We study quantile regression with rectified linear unit (ReLU) neural networks as the chosen model class. We derive an upper bound on the expected mean squared error of a ReLU network used to estimate any quantile conditioning on a set of covariates. This upper bound only depends on the best possible approximation error, the number of layers in the network, and the number of nodes per layer. We further show upper bounds that are tight for two large classes of functions: compositions of Hölder functions and members of a Besov space. These tight bounds imply ReLU networks with quantile regression achieve minimax rates for broad collections of function types. Unlike existing work, the theoretical results hold under minimal assumptions and apply to general error distributions, including heavy-tailed distributions. Empirical simulations on a suite of synthetic response functions demonstrate the theoretical results translate to practical implementations of ReLU networks. Overall, the theoretical and empirical results provide insight into the strong performance of ReLU neural networks for quantile regression across a broad range of function classes and error distributions. All code for this paper is publicly available at https://github.com/tansey/quantile-regression.",Deep networks | minimax | robust regression | sparse networks,24,0.0,,,,,
2-s2.0-85213814650,,,,Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond,ar,Article,Hedström A.,60011604;60021763;60011055;128148841,Technische Universität Berlin;Universität Potsdam;Fraunhofer-Institut für Nachrichtentechnik Heinrich-Hertz-Institut;BIFOLD - Berlin Institute for the Foundations of Learning and Data,Berlin;Potsdam;Berlin;Berlin,Germany;Germany;Germany;Germany,8.0,"Hedström, Anna;Weber, Leander;Bareeva, Dilyara;Krakowczyk, Daniel;Motzkus, Franz;Samek, Wojciech;Lapuschkin, Sebastian;Höhne, Marina M.C.",57339350700;57219741231;57468869400;58258945100;57462121700;40762215900;57190868412;57219765782,60011604;60011055;60011604;60021763;60011055;60011604-60011055-128148841;60011055;60011604-128148841,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool with focus on XAI evaluation exists that exhaustively and speedily allows researchers to evaluate the performance of explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus—a comprehensive, evaluation toolkit in Python that includes a growing, wellorganised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under an open-source license on PyPi (or on https://github.com/understandable-machine-intelligence-lab/Quantus/).",explainability | open source | Python | reproducibility | responsible AI,166,0.0,,,BMBF,01IS20055,Bundesministerium für Bildung und Forschung
2-s2.0-85214290380,,,,Quasi-Equivalence between Width and Depth of Neural Networks,ar,Article,Fan F.L.,60025534,Rensselaer Polytechnic Institute,Troy,United States,3.0,"Fan, Feng Lei;Lai, Rongjie;Wang, Ge",57222365585;24824994200;7407148134,60025534;60025534;60025534,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"While classic studies proved that wide networks allow universal approximation, recent research and successes of deep learning demonstrate the power of deep networks. Based on a symmetric consideration, we investigate if the design of artificial neural networks should have a directional preference, and what the mechanism of interaction is between the width and depth of a network. Inspired by the De Morgan law, we address this fundamental question by establishing a quasi-equivalence between the width and depth of ReLU networks. We formulate two transforms for mapping an arbitrary ReLU network to a wide ReLU network and a deep ReLU network respectively, so that the essentially same capability of the original network can be implemented. Based on our findings, a deep network has a wide equivalent, and vice versa, subject to an arbitrarily small error.",Artificial neural networks | deep learning | quasi-equivalence | ReLU networks | wide learning,1,0.0,,,NIH,R01EB032716,National Institutes of Health
2-s2.0-85167866173,10.1609/aaai.v37i4.25613,,,Query-Aware Quantization for Maximum Inner Product Search,cp,Conference Paper,Zhang J.,60019118;60000937;129838867;130073281,University of Science and Technology of China;Shenzhen University;State Key Laboratory of Cognitive Intelligence;Hisense,Hefei;Shenzhen;Hefei;,China;China;China;,5.0,"Zhang, Jin;Lian, Defu;Zhang, Haodi;Wang, Baoyun;Chen, Enhong",57258739000;54403324000;57003171000;57733437400;35228685900,60019118;60019118-129838867;60000937;130073281;60019118-129838867,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,4875-4883,"Maximum Inner Product Search (MIPS) plays an essential role in many applications ranging from information retrieval, recommender systems to natural language processing. However, exhaustive MIPS is often expensive and impractical when there are a large number of candidate items. The state-of-the-art quantization method of approximated MIPS is product quantization with a score-aware loss, developed by assuming that queries are uniformly distributed in the unit sphere. However, in real-world datasets, the above assumption about queries does not necessarily hold. To this end, we propose a quantization method based on the distribution of queries combined with sampled softmax. Further, we introduce a general framework encompassing the proposed method and multiple quantization methods, and we develop an effective optimization for the proposed general framework. The proposed method is evaluated on three real-world datasets. The experimental results show that it outperforms the state-of-the-art baselines.",,7,1.0,all publisherfullgold,All Open Access Gold,NSFC,61976198,National Natural Science Foundation of China
2-s2.0-85183859685,10.1613/jair.1.14752,,,Query-driven Qualitative Constraint Acquisition,ar,Article,Belaid M.B.,60068730;60007853;60012050;60000645,"OsloMet – StorbyUniversitetet;Laboratoire d'Informatique, de Robotique et de Microélectronique de Montpellie;Norsk institutt for luftforskning;Simula Research Laboratory",Oslo;Montpellier;Skedsmokorset;Barum,Norway;France;Norway;Norway,5.0,"Belaid, Mohamed Bachir;Belmecheri, Nassim;Gotlieb, Arnaud;Lazaar, Nadjib;Spieker, Helge",57208898379;57545233800;56247674500;24833421000;57189329650,60012050-60068730;60000645;60000645;60007853;60000645,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,241-271,"Many planning, scheduling or multi-dimensional packing problems involve the design of subtle logical combinations of temporal or spatial constraints. Recently, we introduced GEQCA-I, which stands for Generic Qualitative Constraint Acquisition, as a new active constraint acquisition method for learning qualitative constraints using qualitative queries. In this paper, we revise and extend GEQCA-I to GEQCA-II with a new type of query, universal query, for qualitative constraint acquisition, with a deeper query-driven acquisition algorithm. Our extended experimental evaluation shows the efficiency and usefulness of the concept of universal query in learning randomly-generated qualitative networks, including both temporal networks based on Allen’s algebra and spatial networks based on region connection calculus. We also show the effectiveness of GEQCA-II in learning the qualitative part of real scheduling problems.",,2,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,EC,101059238,Norges Forskningsråd
2-s2.0-85167989235,10.1609/aaai.v37i11.26519,,,Question Decomposition Tree for Answering Complex Questions over Knowledge Bases,cp,Conference Paper,Huang X.,60033100,Nanjing University,Nanjing,China,5.0,"Huang, Xiang;Cheng, Sitao;Shu, Yiheng;Bao, Yuheng;Qu, Yuzhong",57292285300;58500229600;59819254500;58499089800;8400208900,60033100;60033100;60033100;60033100;60033100,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,12924-12932,"Knowledge base question answering (KBQA) has attracted a lot of interest in recent years, especially for complex questions which require multiple facts to answer. Question decomposition is a promising way to answer complex questions. Existing decomposition methods split the question into sub-questions according to a single compositionality type, which is not sufficient for questions involving multiple compositionality types. In this paper, we propose Question Decomposition Tree (QDT) to represent the structure of complex questions. Inspired by recent advances in natural language generation (NLG), we present a two-staged method called Clue-Decipher to generate QDT. It can leverage the strong ability of NLG model and simultaneously preserve the original questions. To verify that QDT can enhance KBQA task, we design a decomposition-based KBQA system called QDTQA. Extensive experiments show that QDTQA outperforms previous state-of-the-art methods on ComplexWebQuestions dataset. Besides, our decomposition method improves an existing KBQA system by 11% and sets a new state-of-the-art on LC-QuAD 1.0.",,15,1.0,all publisherfullgold,All Open Access Gold,NSFC,62072224,National Natural Science Foundation of China
2-s2.0-105000553996,,,,RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated Images with Provable Guarantees,cp,Conference Paper,Xian X.,60029445;60123802;126607508,University of Minnesota Twin Cities;Carlson School of Management;Cisco Research,Minneapolis;Minneapolis;Cisco,United States;United States;United States,7.0,"Xian, Xun;Wang, Ganghua;Bi, Xuan;Srinivasa, Jayanth;Kundu, Ashish;Hong, Mingyi;Ding, Jie",57219685785;57768824600;57189519887;57222187220;23482383700;34969184900;57169271900,60029445;60029445;60123802;126607508;126607508;60029445;60029445,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Safeguarding intellectual property and preventing potential misuse of AI-generated images are of paramount importance. This paper introduces a robust and agile plug-and-play watermark detection framework, referred to as RAW. As a departure from existing encoder-decoder methods, which incorporate fixed binary codes as watermarks within latent representations, our approach introduces learnable watermarks directly into the original image data. Subsequently, we employ a classifier that is jointly trained with the watermark to detect the presence of the watermark. The proposed framework is compatible with various generative architectures and supports on-the-fly watermark injection after training. By incorporating state-ofthe-art smoothing techniques, we show that the framework also provides provable guarantees regarding the false positive rate for misclassifying a watermarked image, even in the presence of adversarial attacks targeting watermark removal. Experiments on a diverse range of images generated by state-of-the-art diffusion models demonstrate substantially improved watermark encoding speed and watermark detection performance, under adversarial attacks, while maintaining image quality. Our code is publicly available here.",,1,0.0,,,ARO,W911NF2310315,Army Research Office
2-s2.0-85167962639,10.1609/aaai.v37i11.26535,,,RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL,cp,Conference Paper,Li H.,60014402;60125536;130082040,Renmin University of China;Key Laboratory of Data Engineering and Knowledge Engineering;Engineering Research Center of Ministry of Education on Database and BI,Beijing;Beijing;,China;China;,4.0,"Li, Haoyang;Zhang, Jing;Li, Cuiping;Chen, Hong",57848490600;57216205138;55696006300;56154659700,60125536-130082040-60014402;60125536-130082040-60014402;60125536-130082040-60014402;60125536-130082040-60014402,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,13067-13075,"One of the recent best attempts at Text-to-SQL is the pre-trained language model. Due to the structural property of the SQL queries, the seq2seq model takes the responsibility of parsing both the schema items (i.e., tables and columns) and the skeleton (i.e., SQL keywords). Such coupled targets increase the difficulty of parsing the correct SQL queries especially when they involve many schema items and logic operators. This paper proposes a ranking-enhanced encoding and skeleton-aware decoding framework to decouple the schema linking and the skeleton parsing. Specifically, for a seq2seq encoder-decode model, its encoder is injected by the most relevant schema items instead of the whole unordered ones, which could alleviate the schema linking effort during SQL parsing, and its decoder first generates the skeleton and then the actual SQL query, which could implicitly constrain the SQL parsing. We evaluate our proposed framework on Spider and its three robustness variants: Spider-DK, Spider-Syn, and Spider-Realistic. The experimental results show that our framework delivers promising performance and robustness. Our code is available at https://github.com/RUCKBReasoning/RESDSQL.",,153,1.0,all publisherfullgold,All Open Access Gold,NSFC,62072460,National Natural Science Foundation of China
2-s2.0-85192990403,,,,RESFIELDS: RESIDUAL NEURAL FIELDS FOR SPATIOTEMPORAL SIGNALS,cp,Conference Paper,Mihajlovic M.,60025858;60026532;60000869,ETH Zürich;Microsoft Corporation;Uniklinik Balgrist,Zurich;Redmond;Zurich,Switzerland;United States;Switzerland,4.0,"Mihajlovic, Marko;Prokudin, Sergey;Pollefeys, Marc;Tang, Siyu",57202978118;57195717314;7004040532;55914554200,60025858;60025858-60000869;60025858-60026532;60025858,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Neural fields, a category of neural networks trained to represent high-frequency signals, have gained significant attention in recent years due to their impressive performance in modeling complex 3D data, such as signed distance (SDFs) or radiance fields (NeRFs), via a single multi-layer perceptron (MLP). However, despite the power and simplicity of representing signals with an MLP, these methods still face challenges when modeling large and complex temporal signals due to the limited capacity of MLPs. In this paper, we propose an effective approach to address this limitation by incorporating temporal residual layers into neural fields, dubbed ResFields. It is a novel class of networks specifically designed to effectively represent complex temporal signals. We conduct a comprehensive analysis of the properties of ResFields and propose a matrix factorization technique to reduce the number of trainable parameters and enhance generalization capabilities. Importantly, our formulation seamlessly integrates with existing MLP-based neural fields and consistently improves results across various challenging tasks: 2D video approximation, dynamic shape modeling via temporal SDFs, and dynamic NeRF reconstruction. Lastly, we demonstrate the practical utility of ResFields by showcasing its effectiveness in capturing dynamic 3D scenes from sparse RGBD cameras of a lightweight capture system.",,8,0.0,,,,,
2-s2.0-85143657505,,,,RETHINKING THE EXPRESSIVE POWER OF GNNS VIA GRAPH BICONNECTIVITY,cp,Conference Paper,Zhang B.,60014966,Peking University,Beijing,China,4.0,"Zhang, Bohang;Luo, Shengjie;Wang, Liwei;He, Di",57221158414;57221147567;55721280000;57194207766,60014966;60014966;60014966;60014966,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.",,79,0.0,,,NKRDPC,2022ZD0114900,National Key Research and Development Program of China
2-s2.0-85151288941,,,,REVERSIBLE COLUMN NETWORKS,cp,Conference Paper,Cai Y.,60111058;126241012,"Beijing MEGVII Technology Co., Ltd.;Beijing Academy of Artificial Intelligence",Beijing;Beijing,China;China,7.0,"Cai, Yuxuan;Zhou, Yizhuang;Han, Qi;Sun, Jianjian;Kong, Xiangwen;Li, Jun;Zhang, Xiangyu",58727272200;57559065300;57220996844;57779806800;57846565600;58436944800;59742205200,60111058;60111058;60111058;60111058;60111058;60111058;60111058-126241012,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"We propose a new neural network design paradigm Reversible Column Network (RevCol). The main body of RevCol is composed of multiple copies of subnetworks, named columns respectively, between which multi-level reversible connections are employed. Such architectural scheme attributes RevCol very different behavior from conventional networks: during forward propagation, features in RevCol are learned to be gradually disentangled when passing through each column, whose total information is maintained rather than compressed or discarded as other network does. Our experiments suggest that CNN-style RevCol models can achieve very competitive performances on multiple computer vision tasks such as image classification, object detection and semantic segmentation, especially with large parameter budget and large dataset. For example, after ImageNet-22K pre-training, RevCol-XL obtains 88.2% ImageNet-1K accuracy. Given more pre-training data, our largest model RevCol-H reaches 90.0% on ImageNet-1K, 63.8% AP<inf>box</inf> on COCO detection minival set, 61.0% mIoU on ADE20k segmentation. To our knowledge, it is the best COCO detection and ADE20k segmentation result among pure (static) CNN models. Moreover, as a general macro architecture fashion, RevCol can also be introduced into transformers or other neural networks, which is demonstrated to improve the performances in both computer vision and NLP tasks. We release code and models at https://github.com/megvii-research/RevCol.",,34,0.0,,,NKRDPC,2017YFA0700800,National Key Research and Development Program of China
2-s2.0-85185611212,,,,REVISITING PRUNING AT INITIALIZATION THROUGH THE LENS OF RAMANUJAN GRAPH,cp,Conference Paper,Hoang D.,60150401,Cockrell School of Engineering,Austin,United States,4.0,"Hoang, Duc;Liu, Shiwei;Marculescu, Radu;Wang, Zhangyang",57221155290;57217824780;35555100900;56288839400,60150401;60150401;60150401;60150401,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Pruning neural networks at initialization (PaI) has received an upsurge of interest due to its end-to-end saving potential. PaI is able to find sparse subnetworks at initialization that can achieve comparable performance to the full networks. These methods can surpass the trivial baseline of random pruning but suffer from a significant performance gap compared to post-training pruning. Previous approaches firmly rely on weights, gradients, and sanity checks as primary signals when conducting PaI analysis. To better understand the underlying mechanism of PaI, we propose to interpret it through the lens of the Ramanujan Graph - a class of expander graphs that are sparse while being highly connected. It is often believed there should be a strong correlation between the Ramanujan graph and PaI since both are about finding sparse and well-connected neural networks. However, the finer-grained link relating highly sparse and connected networks to their relative performance (i.e., ranking of difference sparse structures at the same specific global sparsity) is still missing. We observe that not only the Ramanujan property for sparse networks shows no significant relationship to PaI's relative performance, but maximizing it can also lead to the formation of pseudo-random graphs with no structural meanings. We reveal the underlying cause to be Ramanujan Graph's strong assumption on the upper bound of the largest nontrivial eigenvalue (µ̂) of layers belonging to highly sparse networks. We hence propose Iterative Mean Difference of Bound (IMDB) as a mean to relax the µ̂ upper bound. Likewise, we also show there exists a lower bound for µ̂, which we call the Normalized Random Coefficient (NaRC), that gives us an accurate assessment for when sparse but highly connected structure degenerates into naive randomness. Finally, we systematically analyze the behavior of various PaI methods and demonstrate the utility of our proposed metrics in characterizing PaI performance. We show that subnetworks preserving better the IMDB property correlate higher in performance, while NaRC provides us with a possible mean to locate the region where highly connected, highly sparse, and non-trivial Ramanujan expanders exist. Our code is available at: https://github.com/VITA-Group/ramanujan-on-pai.",,26,0.0,,,,2133861,
2-s2.0-85149065397,,,,ROBBING THE FED: DIRECTLY OBTAINING PRIVATE DATA IN FEDERATED LEARNING WITH MODIFIED MODELS,cp,Conference Paper,Fowl L.,60020304;60021784,"University of Maryland, College Park;New York University",College Park;New York,United States;United States,5.0,"Fowl, Liam;Geiping, Jonas;Czaja, Wojtek;Goldblum, Micah;Goldstein, Tom",57211071247;57201420593;6603938615;57211069935;14055829100,60020304;60020304;60020304;60021784;60020304,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Federated learning has quickly gained popularity with its promises of increased user privacy and efficiency. Previous works have shown that federated gradient updates contain information that can be used to approximately recover user data in some situations. These previous attacks on user privacy have been limited in scope and do not scale to gradient updates aggregated over even a handful of data points, leaving some to conclude that data privacy is still intact for realistic training regimes. In this work, we introduce a new threat model based on minimal but malicious modifications of the shared model architecture which enable the server to directly obtain a verbatim copy of user data from gradient updates without solving difficult inverse problems. Even user data aggregated over large batches - where previous methods fail to extract meaningful content - can be reconstructed by these minimally modified models.",,60,0.0,,,DMS,,Division of Mathematical Sciences
2-s2.0-105000538892,,,,ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization,cp,Conference Paper,Huang H.,60029306,Wuhan University,Wuhan,China,3.0,"Huang, Huayang;Wu, Yu;Wang, Qian",57216280877;57206751173;56856235900,60029306;60029306;60029306,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods.",,2,0.0,,,NSFC,62372341,National Natural Science Foundation of China
2-s2.0-85167962456,10.1609/aaai.v37i3.25482,,,RSPT: Reconstruct Surroundings and Predict Trajectories for Generalizable Active Object Tracking,cp,Conference Paper,Zhong F.,60014966;60031031;60018554;126710833,Peking University;Shandong University;Zhengzhou University;Beijing Institute for General Artificial Intelligence (BIGAI),Beijing;Jinan;Zhengzhou;Beijing,China;China;China;China,5.0,"Zhong, Fangwei;Bi, Xiao;Zhang, Yudi;Zhang, Wei;Wang, Yizhou",57197844750;58198974300;58199516800;57201591153;7601522356,60014966-126710833;60014966;60031031;60031031;60014966-60018554,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,3705-3714,"Active Object Tracking (AOT) aims to maintain a specific relation between the tracker and object(s) by autonomously controlling the motion system of a tracker given observations. AOT has wide-ranging applications, such as in mobile robots and autonomous driving. However, building a generalizable active tracker that works robustly across different scenarios remains a challenge, especially in unstructured environments with cluttered obstacles and diverse layouts. We argue that constructing a state representation capable of modeling the geometry structure of the surroundings and the dynamics of the target is crucial for achieving this goal. To address this challenge, we present RSPT, a framework that forms a structure-aware motion representation by Reconstructing the Surroundings and Predicting the target Trajectory. Additionally, we enhance the generalization of the policy network by training in an asymmetric dueling mechanism. We evaluate RSPT on various simulated scenarios and show that it outperforms existing methods in unseen environments, particularly those with complex obstacles and layouts. We also demonstrate the successful transfer of RSPT to real-world settings.",,14,1.0,all publisherfullgold,All Open Access Gold,,BX2021008,National Postdoctoral Program for Innovative Talents
2-s2.0-85162019205,,,,RUMs from Head-to-Head Contests,cp,Conference Paper,Almanza M.,60032350;60006191;129864932,Sapienza Università di Roma;Google LLC;Algorand Labs,Rome;Mountain View;,Italy;United States;United States,5.0,"Almanza, Matteo;Chierichetti, Flavio;Kumar, Ravi;Panconesi, Alessandro;Tomkins, Andrew",57189853371;22937002700;58133248700;8787095100;57200940123,129864932-60032350;60032350;60006191;60032350;60006191,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,452-467,"Random utility models (RUMs) encode the likelihood that a particular item will be selected from a slate of competing items. RUMs are well-studied objects in both discrete choice theory and, more recently, in the machine learning community, as they encode a fairly broad notion of rational user behavior. In this paper, we focus on slates of size two representing head-to-head contests. Given a tournament matrix M such that M<inf>i,j</inf> is the probability that item j will be selected from (i, j), we consider the problem of finding the RUM that most closely reproduces M. For this problem we obtain a polynomial-time algorithm returning a RUM that approximately minimizes the average error over the pairs. Our experiments show that RUMs can perfectly represent many of the tournament matrices that have been considered in the literature; in fact, the maximum average error induced by RUMs on the matrices we considered is negligible (≈ 0.001). We also show that RUMs are competitive, on prediction tasks, with previous approaches.",,3,0.0,,,,2017K7XPAN,
2-s2.0-85168646798,10.24963/ijcai.2023/73,,,RZCR: Zero-shot Character Recognition via Radical-based Reasoning,cp,Conference Paper,Diao X.,60025858;60007711;60015986,ETH Zürich;Jilin University;Università di Trento,Zurich;Changchun;Trento,Switzerland;China;Italy,7.0,"Diao, Xiaolei;Shi, Daqian;Tang, Hao;Shen, Qiang;Li, Yanzeng;Wu, Lei;Xu, Hao",57221787270;57201184227;57208238003;57221113779;57814628700;58257538900;55493779900,60007711-60015986;60015986;60025858;60007711;60007711;60007711;60007711,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,654-662,"The long-tail effect is a common issue that limits the performance of deep learning models on real-world datasets. Character image datasets are also affected by such unbalanced data distribution due to differences in character usage frequency. Thus, current character recognition methods are limited when applied in the real world, especially for the categories in the tail that lack training samples, e.g., uncommon characters. In this paper, we propose a zero-shot character recognition framework via radical-based reasoning, called RZCR, to improve the recognition performance of few-sample character categories in the tail. Specifically, we exploit radicals, the graphical units of characters, by decomposing and reconstructing characters according to orthography. RZCR consists of a visual semantic fusion-based radical information extractor (RIE) and a knowledge graph character reasoner (KGR). RIE aims to recognize candidate radicals and their possible structural relations from character images in parallel. The results are then fed into KGR to recognize the target character by reasoning with a knowledge graph. We validate our method on multiple datasets, and RZCR shows promising experimental results, especially on few-sample character datasets.",,16,1.0,all publisherfullgold,All Open Access Gold,JLU,823783,Jilin University
2-s2.0-85183990543,,,,Radial Basis Approximation of Tensor Fields on Manifolds: From Operator Estimation to Manifold Learning,ar,Article,Harlim J.,60001439;60105232,Pennsylvania State University;ShanghaiTech University,University Park;Shanghai,United States;China,3.0,"Harlim, John;Jiang, Shixiao Willing;Peoples, John Wilson",9241987300;57224851020;57313463300,60001439;60105232;60001439,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,345,,"In this paper, we study the Radial Basis Function (RBF) approximation to differential operators on smooth tensor fields defined on closed Riemannian submanifolds of Euclidean space, identified by randomly sampled point cloud data. The formulation in this paper leverages a fundamental fact that the covariant derivative on a submanifold is the projection of the directional derivative in the ambient Euclidean space onto the tangent space of the submanifold. To differentiate a test function (or vector field) on the submanifold with respect to the Euclidean metric, the RBF interpolation is applied to extend the function (or vector field) in the ambient Euclidean space. When the manifolds are unknown, we develop an improved second-order local SVD technique for estimating local tangent spaces on the manifold. When the classical pointwise non-symmetric RBF formulation is used to solve Laplacian eigenvalue problems, we found that while accurate estimation of the leading spectra can be obtained with large enough data, such an approximation often produces irrelevant complex-valued spectra (or pollution) as the true spectra are real-valued and positive. To avoid such an issue, we introduce a symmetric RBF discrete approximation of the Laplacians induced by a weak formulation on appropriate Hilbert spaces. Unlike the non-symmetric approximation, this formulation guarantees non-negative real-valued spectra and the orthogonality of the eigenvectors. Theoretically, we establish the convergence of the eigenpairs of both the Laplace-Beltrami operator and Bochner Laplacian for the symmetric formulation in the limit of large data with convergence rates. Numerically, we provide supporting examples for approximations of the Laplace-Beltrami operator and various vector Laplacians, including the Bochner, Hodge, and Lichnerowicz Laplacians.",Laplace-Beltrami operators | local SVD | manifold learning | operator estimation | Radial Basis Functions (RBFs) | vector Laplacians,3,0.0,,,NSF,DMS-1854299,ShanghaiTech University
2-s2.0-85191067305,,,,Random Feature Amplification: Feature Learning and Generalization in Neural Networks,ar,Article,Frei S.,60025038;60141508,"University of California, Berkeley;Stanford Engineering",Berkeley;Stanford,United States;United States,3.0,"Frei, Spencer;Chatterji, Niladri S.;Bartlett, Peter L.",57192103008;57202056958;7202466727,60025038;60141508;60025038,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,303,,"In this work, we provide a characterization of the feature-learning process in two-layer ReLU networks trained by gradient descent on the logistic loss following random initialization. We consider data with binary labels that are generated by an XOR-like function of the input features. We permit a constant fraction of the training labels to be corrupted by an adversary. We show that, although linear classifiers are no better than random guessing for the distribution we consider, two-layer ReLU networks trained by gradient descent achieve generalization error close to the label noise rate. We develop a novel proof technique that shows that at initialization, the vast majority of neurons function as random features that are only weakly correlated with useful features, and the gradient descent dynamics ‘amplify’ these weak, random features to strong, useful features.",classification | feature learning | generalization | label noise | neural networks,7,0.0,,,NSF,DMS-2023505,National Science Foundation
2-s2.0-85203836874,,,,Randomized Confidence Bounds for Stochastic Partial Monitoring,cp,Conference Paper,Heuillet M.,60032619;129147347;131329604,Université Laval;Canada CIFAR AI;Thales Research and Technology (cortAIx),Quebec;;,Canada;Canada;Canada,3.0,"Heuillet, Maxime;Ahmad, Ola;Durand, Audrey",57275273900;59878835600;55328726000,60032619;60032619-131329604;60032619-129147347,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,18248-18283,"The partial monitoring (PM) framework provides a theoretical formulation of sequential learning problems with incomplete feedback. At each round, a learning agent plays an action while the environment simultaneously chooses an outcome. The agent then observes a feedback signal that is only partially informative about the (unobserved) outcome. The agent leverages the received feedback signals to select actions that minimize the (unobserved) cumulative loss. In contextual PM, the outcomes depend on some side information that is observable by the agent before selecting the action. In this paper, we consider the contextual and non-contextual PM settings with stochastic outcomes. We introduce a new class of PM strategies based on the randomization of deterministic confidence bounds. We also extend regret guarantees to settings where existing stochastic strategies are not applicable. Our experiments show that the proposed RandCBP and RandCBPside* strategies have competitive performance against state-of-the-art baselines in multiple PM games. To illustrate how the PM framework can benefit real world applications, we design a use case on the real-world problem of monitoring the error rate of any deployed classification system.",,0,0.0,,,CIFAR,,Mitacs
2-s2.0-85128962846,10.1613/JAIR.1.13030,,,Ranking Sets of Objects: The Complexity of Avoiding Impossibility Results,ar,Article,Maly J.,60018163,TU Wien,Vienna,Austria,1.0,"Maly, Jan",57203372240,60018163,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,1-65,"The problem of lifting a preference order on a set of objects to a preference order on a family of subsets of this set is a fundamental problem with a wide variety of applications in AI. The process is often guided by axioms postulating properties the lifted order should have. Well-known impossibility results by Kannai and Peleg and by Barberà and Pattanaik tell us that some desirable axioms – namely dominance and (strict) independence – are not jointly satisfiable for any linear order on the objects if all non-empty sets of objects are to be ordered. On the other hand, if not all non-empty sets of objects are to be ordered, the axioms are jointly satisfiable for all linear orders on the objects for some families of sets. Such families are very important for applications as they allow for the use of lifted orders, for example, in combinatorial voting. In this paper, we determine the computational complexity of recognizing such families. We show that it is Π<sup>p</sup><inf>2</inf>-complete to decide for a given family of subsets whether dominance and independence or dominance and strict independence are jointly satisfiable for all linear orders on the objects if the lifted order needs to be total. Furthermore, we show that the problem remains coNP-complete if the lifted order can be incomplete. Additionally, we show that the complexity of these problems can increase exponentially if the family of sets is not given explicitly but via a succinct domain restriction. Finally, we show that it is NP-complete to decide for a family of subsets whether dominance and independence or dominance and strict independence are jointly satisfiable for at least one linear order on the objects.",,1,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,FWF,P 31890,Austrian Science Fund
2-s2.0-85148017371,,,,Ranking and Tuning Pre-trained Models: A New Paradigm for Exploiting Model Hubs,ar,Article,You K.,60121438;60104026;60092530,"Department of Electrical Engineering and Computer Sciences;Beijing National Research Center for Information Science and Technology;Huawei Technologies Co., Ltd.",Berkeley;Beijing;Shenzhen,United States;China;China,6.0,"You, Kaichao;Liu, Yong;Zhang, Ziyang;Wang, Jianmin;Jordan, Michael I.;Long, Mingsheng",57214123971;56046941000;57310859900;57226847195;57209168184;36986782000,60104026;60104026;60092530;60104026;60121438;60104026,2022-06-01,1 June 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,209,,"Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain under-exploited—practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This naïve but common practice poses two obstacles to full exploitation of pre-trained model hubs: first, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively fine-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-fitting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes. The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models. This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks before fine-tuning. (2) The best ranked PTM can either be fine-tuned and deployed if we have no preference for the model’s architecture or the target PTM can be tuned by the top K ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as B-Tuning, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance.",Model Ranking | Model Tuning | Pre-trained Model Hub | Transfer Learning,22,0.0,,,NSFC,62021002,National Natural Science Foundation of China
2-s2.0-105018581574,,,,Rates of convergence for density estimation with generative adversarial networks,ar,Article,Puchkin N.,60014264;60020513;60195969,Universität Duisburg-Essen;HSE University;Mohamed Bin Zayed University of Artificial Intelligence,Duisburg;Moscow;Abu Dhabi,Germany;Russian Federation;United Arab Emirates,5.0,"Puchkin, Nikita;Samsonov, Sergey;Belomestny, Denis;Moulines, Eric;Naumov, Alexey",57219546561;57215609557;15069577900;7003817357;55178400400,60020513;60020513;60014264;60195969;60020513,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"In this work we undertake a thorough study of the non-asymptotic properties of the vanilla generative adversarial networks (GANs). We prove an oracle inequality for the Jensen-Shannon (JS) divergence between the underlying density p<sup>∗</sup> and the GAN estimate with a significantly better statistical error term compared to the previously known results. The advantage of our bound becomes clear in application to nonparametric density estimation. We show that the JS-divergence between the GAN estimate and p<sup>∗</sup> decays as fast as (log n/n)<sup>2</sup>β/(2β+d<sup>)</sup>, where n is the sample size and β determines the smoothness of p<sup>∗</sup>. This rate of convergence coincides (up to logarithmic factors) with minimax optimal for the considered class of densities.",generative model | Jensen-Shannon risk | minimax rates | nonparametric density estimation | oracle inequality,6,0.0,,,ACRF,000000D730321P5Q0002,Analytical Center for the Government of the Russian Federation
2-s2.0-85134711052,10.1609/aaai.v36i3.20182,,,ReMoNet: Recurrent Multi-Output Network for Efficient Video Denoising,cp,Conference Paper,Xiang L.,60025278;60030514;60104026;126954596,Tsinghua University;Aberystwyth University;Beijing National Research Center for Information Science and Technology;OPPO Inc.,Beijing;Aberystwyth;Beijing;Guangdong,China;United Kingdom;China;China,9.0,"Xiang, Liuyu;Zhou, Jundong;Liu, Jirui;Wang, Zerun;Huang, Haidong;Hu, Jie;Han, Jungong;Guo, Yuchen;Ding, Guiguang",57211746816;58094842700;58094816700;57219662497;57275407300;57301864600;14522692900;55924087200;8221502200,60104026-60025278;60104026-60025278;60104026;60104026-60025278;126954596;126954596;60030514;60104026;60104026-60025278,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,2786-2794,"While deep neural network-based video denoising methods have achieved promising results, it is still hard to deploy them on mobile devices due to their high computational cost and memory demands. This paper aims to develop a lightweight deep video denoising method that is friendly to resource-constrained mobile devices. Inspired by the facts that 1) consecutive video frames usually contain redundant temporal coherency, and 2) neural networks are usually over-parameterized, we propose a multi-input multi-output (MIMO) paradigm to process consecutive video frames within one-forward-pass. The basic idea is concretized to a novel architecture termed Recurrent Multi-output Network (ReMoNet), which consists of recurrent temporal fusion and temporal aggregation blocks and is further reinforced by similarity-based mutual distillation. We conduct extensive experiments on NVIDIA GPU and Qualcomm Snapdragon 888 mobile platform with Gaussian noise and simulated Image-Signal-Processor (ISP) noise. The experimental results show that ReMoNet is both effective and efficient on video denoising. Moreover, we show that ReMoNet is more robust under higher noise level scenarios.",,15,1.0,all publisherfullgold,All Open Access Gold,NSFC,61925107,National Natural Science Foundation of China
2-s2.0-105000555532,,,,ReactZyme: A Benchmark for Enzyme-Reaction Prediction,cp,Conference Paper,Hua C.,60111161;123045532;112797658;106402756;127881828,DeepMind Technologies Limited;Mila;McGill;SJTU;UdeM,London;Quebec;Montreal;;Montreal,United Kingdom;Canada;Canada;China;Canada,7.0,"Hua, Chenqing;Zhong, Bozitao;Luan, Sitao;Hong, Liang;Wolf, Guy;Precup, Doina;Zheng, Shuangjia",57219791986;57224410252;57218716420;59326247500;54396639400;6603288659;57205425874,112797658-123045532;106402756;112797658-123045532;106402756;123045532-127881828;112797658-123045532-60111161;106402756,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies. Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes. Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can recruit proteins for novel reactions and predict reactions in novel proteins, facilitating enzyme discovery and function annotation (https://github.com/WillHua127/ReactZyme).",,4,0.0,,,NSFC,10242,National Natural Science Foundation of China
2-s2.0-85204281542,,,,"Recall, Retrieve and Reason: Towards Better In-Context Relation Extraction",cp,Conference Paper,Li G.,60001604;60005244;60030904,Ministry of Education of the People's Republic of China;Southeast University;Institute of Computing Technology Chinese Academy of Sciences,Beijing;Nanjing;Beijing,China;China;China,8.0,"Li, Guozheng;Wang, Peng;Ke, Wenjun;Guo, Yikai;Ji, Ke;Shang, Ziyu;Liu, Jiajun;Xu, Zijie",57347169600;56182131700;57193995075;57997574500;58339210600;58203421300;57408916100;58311989800,60005244;60005244-60001604;60005244-60001604;60030904;60005244;60005244;60005244;60005244,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,6368-6376,"Relation extraction (RE) aims to identify relations between entities mentioned in texts. Although large language models (LLMs) have demonstrated impressive in-context learning (ICL) abilities in various tasks, they still suffer from poor performances compared to most supervised fine-tuned RE methods. Utilizing ICL for RE with LLMs encounters two challenges: (1) retrieving good demonstrations from training examples, and (2) enabling LLMs exhibit strong ICL abilities in RE. On the one hand, retrieving good demonstrations is a non-trivial process in RE, which easily results in low relevance regarding entities and relations. On the other hand, ICL with an LLM achieves poor performance in RE while RE is different from language modeling in nature or the LLM is not large enough. In this work, we propose a novel recall-retrieve-reason RE framework that synergizes LLMs with retrieval corpora (training examples) to enable relevant retrieving and reliable in-context reasoning. Specifically, we distill the consistently ontological knowledge from training datasets to let LLMs generate relevant entity pairs grounded by retrieval corpora as valid queries. These entity pairs are then used to retrieve relevant training examples from the retrieval corpora as demonstrations for LLMs to conduct better ICL via instruction tuning. Extensive experiments on different LLMs and RE datasets demonstrate that our method generates relevant and valid entity pairs and boosts ICL abilities of LLMs, achieving competitive or new state-of-the-art performance on sentence-level RE compared to previous supervised fine-tuning methods and ICL-based methods.",,5,0.0,,,NSFC,62376057,National Natural Science Foundation of China
2-s2.0-85139292665,10.1609/aaai.v36i6.20624,,,Recovering the Propensity Score from Biased Positive Unlabeled Data,cp,Conference Paper,Gerych W.,60006320;129167952,MIT Computer Science & Artificial Intelligence Laboratory;Worcester Polytechic Institute,Cambridge;Worcester,United States;United States,5.0,"Gerych, Walter;Hartvigsen, Thomas;Buiquicchio, Luke;Agu, Emmanuel;Rundensteiner, Elke",57208213488;57200215971;57211109516;12790256200;7005195084,129167952;60006320;129167952;129167952;129167952,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,6694-6702,"Positive-Unlabeled (PU) learning methods train a classifier to distinguish between the positive and negative classes given only positive and unlabeled data. While traditional PU methods require the labeled positive samples to be an unbiased sample of the positive distribution, in practice the labeled sample is often a biased draw from the true distribution. Prior work shows that if we know the likelihood that each positive instance will be selected for labeling, referred to as the propensity score, then the biased sample can be used for PU learning. Unfortunately, no prior work has been proposed an inference strategy for which the propensity score is identifiable. In this work, we propose two sets of assumptions under which the propensity score can be uniquely determined: one in which no assumption is made on the functional form of the propensity score (requiring assumptions on the data distribution), and the second which loosens the data assumptions while assuming a functional form for the propensity score. We then propose inference strategies for each case. Our empirical study shows that our approach significantly outperforms the state-of-the-art propensity estimation methods on a rich variety of benchmark datasets.",,16,1.0,all publisherfullgold,All Open Access Gold,DARPA,HR001117S0032,Defense Advanced Research Projects Agency
2-s2.0-85204313762,,,,Redefining Contributions: Shapley-Driven Federated Learning,cp,Conference Paper,Tastan N.,60195969,Mohamed Bin Zayed University of Artificial Intelligence,Abu Dhabi,United Arab Emirates,5.0,"Tastan, Nurbek;Fares, Samar;Aremu, Toluwani;Horvath, Samuel;Nandakumar, Karthik",57213520809;59138182900;57833577800;57215316342;8950463600,60195969;60195969;60195969;60195969;60195969,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,5009-5017,"Federated learning (FL) has emerged as a pivotal approach in machine learning, enabling multiple participants to collaboratively train a global model without sharing raw data. While FL finds applications in various domains such as healthcare and finance, it is challenging to ensure global model convergence when participants do not contribute equally and/or honestly. To overcome this challenge, principled mechanisms are required to evaluate the contributions made by individual participants in the FL setting. Existing solutions for contribution assessment rely on general accuracy evaluation, often failing to capture nuanced dynamics and class-specific influences. This paper proposes a novel contribution assessment method called ShapFed for fine-grained evaluation of participant contributions in FL. Our approach uses Shapley values from cooperative game theory to provide a granular understanding of class-specific influences. Based on ShapFed, we introduce a weighted aggregation method called ShapFed-WA, which outperforms conventional federated averaging, especially in class-imbalanced scenarios. Personalizing participant updates based on their contributions further enhances collaborative fairness by delivering differentiated models commensurate with the participant contributions. Experiments on CIFAR-10, Chest X-Ray, and Fed-ISIC2019 datasets demonstrate the effectiveness of our approach in improving utility, efficiency, and fairness in FL systems. The code can be found at https://github.com/tnurbek/shapfed.",,5,0.0,,,,,
2-s2.0-85144245397,10.1609/aaai.v36i10.21419,,,Reference-Based Speech Enhancement via Feature Alignment and Fusion Network,cp,Conference Paper,Yue H.,60019533;131846897,Tianjin University;Individual,Tianjin;,China;,4.0,"Yue, Huanjing;Duo, Wenxin;Peng, Xiulian;Yang, Jingyu",55440970800;58093769500;59840572200;56953627200,60019533;60019533;131846897;60019533,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,11648-11656,"Speech enhancement aims at recovering a clean speech from a noisy input, which can be classified into single speech enhancement and personalized speech enhancement. Personalized speech enhancement usually utilizes the speaker identity extracted from the noisy speech itself (or a clean reference speech) as a global embedding to guide the enhancement process. Different from them, we observe that the speeches of the same speaker are correlated in terms of frame-level short-time Fourier Transform (STFT) spectrogram. Therefore, we propose reference-based speech enhancement via a feature alignment and fusion network (FAF-Net). Given a noisy speech and a clean reference speech spoken by the same speaker, we first propose a feature-level alignment strategy to warp the clean reference with the noisy speech in frame level. Then, we fuse the reference feature with the noisy feature via a similarity-based fusion strategy. Finally, the fused features are skipped connected to the decoder, which generates the enhanced results. Experimental results demonstrate that the performance of the proposed FAF-Net is close to the state-of-the-art speech enhancement methods on both DNS and Voice Bank+DEMAND datasets. Our code is available at https://github.com/HieDean/FAF-Net.",,20,1.0,all publisherfullgold,All Open Access Gold,NSFC,62072331,National Natural Science Foundation of China
2-s2.0-85147606598,10.1609/aaai.v36i11.21471,,,ReforesTree: A Dataset for Estimating Tropical Forest Carbon Stock with Deep Learning and Aerial Imagery,cp,Conference Paper,Reiersen G.,60022195;60025858;60019722;60021784;60022020;120527915,Massachusetts Institute of Technology;ETH Zürich;Technische Universität München;New York University;University of Warwick;WWF Switzerland,Cambridge;Zurich;Munich;New York;Coventry;Bellinzona,United States;Switzerland;Germany;United States;United Kingdom;Switzerland,8.0,"Reiersen, Gyri;Dao, David;Lütjens, Björn;Klemmer, Konstantin;Amara, Kenza;Steinegger, Attila;Zhang, Ce;Zhu, Xiaoxiang",57226610176;57214704345;57210800420;57191155998;57386462700;57440630800;58502033600;55696622200,60019722-60025858;60025858;60022195;60022020-60021784;60025858;120527915;60025858;60019722,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,12119-12125,"Forest biomass is a key influence for future climate, and the world urgently needs highly scalable financing schemes, such as carbon offsetting certifications, to protect and restore forests. Current manual forest carbon stock inventory methods of measuring single trees by hand are time, labour, and cost intensive and have been shown to be subjective. They can lead to substantial overestimation of the carbon stock and ultimately distrust in forest financing. The potential for impact and scale of leveraging advancements in machine learning and remote sensing technologies is promising, but needs to be of high quality in order to replace the current forest stock protocols for certifications. In this paper, we present ReforesTree, a benchmark dataset of forest carbon stock in six agro-forestry carbon offsetting sites in Ecuador. Furthermore, we show that a deep learning-based end-to-end model using individual tree detection from low cost RGB-only drone imagery is accurately estimating forest carbon stock within official carbon offsetting certification standards. Additionally, our baseline CNN model outperforms state-of-the-art satellite-based forest biomass and carbon stock estimates for this type of small-scale, tropical agro-forestry sites. We present this dataset to encourage machine learning research in this area to increase accountability and transparency of monitoring, verification and reporting (MVR) in carbon offsetting projects, as well as scaling global reforestation financing through accurate remote sensing.",,25,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85174397939,,,,Regret-Minimizing Double Oracle for Extensive-Form Games,cp,Conference Paper,Tang X.,60022148;60014966;60025225;60027950,University College London;Peking University;University of Southampton;Carnegie Mellon University,London;Beijing;Southampton;Pittsburgh,United Kingdom;China;United Kingdom;United States,4.0,"Tang, Xiaohang;Dinh, Le Cong;McAleer, Stephen Marcus;Yang, Yaodong",57224898256;57219633688;57203463431;56167927700,60022148;60025225;60027950;60014966,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,33599-33615,"By incorporating regret minimization, double oracle methods have demonstrated rapid convergence to Nash Equilibrium (NE) in normal-form games and extensive-form games, through algorithms such as online double oracle (ODO) and extensive-form double oracle (XDO), respectively. In this study, we further examine the theoretical convergence rate and sample complexity of such regret minimization-based double oracle methods, utilizing a unified framework called Regret-Minimizing Double Oracle. Based on this framework, we extend ODO to extensive-form games and determine its sample complexity. Moreover, we demonstrate that the sample complexity of XDO can be exponential in the number of information sets |S|, owing to the exponentially decaying stopping threshold of restricted games. To solve this problem, we propose the Periodic Double Oracle (PDO) method, which has the lowest sample complexity among regret minimization-based double oracle methods, being only polynomial in |S|. Empirical evaluations on multiple poker and board games show that PDO achieves significantly faster convergence than previous double oracle algorithms and reaches a competitive level with state-of-the-art regret minimization methods.",,7,0.0,,,EPSRC,EP/T517793/1,Engineering and Physical Sciences Research Council
2-s2.0-85189607170,10.1609/aaai.v38i14.29546,,,Reliable Conflictive Multi-View Learning,cp,Conference Paper,Xu C.,60025578,Xidian University,Xi'an,China,6.0,"Xu, Cai;Si, Jiajun;Guan, Ziyu;Zhao, Wei;Wu, Yue;Gao, Xiyue",57204475787;58930862800;34868099700;57221014709;56215531900;57946178200,60025578;60025578;60025578;60025578;60025578;60025578,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,14.0,,16129-16137,"Multi-view learning aims to combine multiple features to achieve more comprehensive descriptions of data. Most previous works assume that multiple views are strictly aligned. However, real-world multi-view data may contain low-quality conflictive instances, which show conflictive information in different views. Previous methods for this problem mainly focus on eliminating the conflictive data instances by removing them or replacing conflictive views. Nevertheless, real-world applications usually require making decisions for conflictive instances rather than only eliminating them. To solve this, we point out a new Reliable Conflictive Multi-view Learning (RCML) problem, which requires the model to provide decision results and attached reliabilities for conflictive multi-view data. We develop an Evidential Conflictive Multi-view Learning (ECML) method for this problem. ECML first learns view-specific evidence, which could be termed as the amount of support to each category collected from data. Then, we can construct view-specific opinions consisting of decision results and reliability. In the multiview fusion stage, we propose a conflictive opinion aggregation strategy and theoretically prove this strategy can exactly model the relation of multi-view common and view-specific reliabilities. Experiments performed on 6 datasets verify the effectiveness of ECML. The code is released at https://github.com/jiajunsi/RCML.",,115,1.0,all publisherfullgold,All Open Access Gold,NSFC,MMC202105,Natural Science Basic Research Program of Shaanxi Province
2-s2.0-105000545824,,,,Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe,cp,Conference Paper,Jiang A.Q.,60031101;60027272;60013756;129016807;124880704,University of Cambridge;The University of Edinburgh;University of Warsaw;IDEAS NCBR;IMPAN,Cambridge;Edinburgh;Warsaw;Warsaw;Sopot,United Kingdom;United Kingdom;Poland;Poland;Poland,6.0,"Jiang, Albert Q.;Ziarko, Alicja;Piotrowski, Bartosz;Li, Wenda;Jamnik, Mateja;Miłos, Piotr",57712450600;59194806800;57202954328;57189242761;8403010100;26635635100,60031101;129016807-60013756-124880704;129016807;60027272;60031101;129016807-60013756-124880704,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Text embeddings are essential for many tasks, such as document retrieval, clustering, and semantic similarity assessment. In this paper, we study how to contrastively train text embedding models in a compute-optimal fashion, given a suite of pre-trained decoder-only language models. Our innovation is an algorithm that produces optimal configurations of model sizes, data quantities, and fine-tuning methods for text-embedding models at different computational budget levels. The resulting recipe, which we obtain through extensive experiments, can be used by practitioners to make informed design choices for their embedding models. Specifically, our findings suggest that full fine-tuning and low-rank adaptation fine-tuning produce optimal models at lower and higher computational budgets respectively.",,1,0.0,,,NCN,2019/35/O/ST6/03464,Narodowe Centrum Nauki
2-s2.0-105000524250,,,,Resource-Aware Federated Self-Supervised Learning with Global Class Representations,cp,Conference Paper,Li M.,60025084;60031031;60013789;60008592;132385600,Shanghai Jiao Tong University;Shandong University;Beihang University;Hong Kong University of Science and Technology;Coupang,Shanghai;Jinan;Beijing;Hong Kong;Hong Kong,China;China;China;Hong Kong;Hong Kong,9.0,"Li, Mingyi;Zhang, Xiao;Wang, Qi;Liu, Tengfei;Wu, Ruofan;Wang, Weiqiang;Zhuang, Fuzhen;Xiong, Hui;Yu, Dongxiao",58581617100;58872540500;59425083900;59701839900;59700939100;57225974160;59487689600;58136965900;30767911100,60031031;60031031;60031031;60008592;132385600;60025084;60013789;60008592;60031031,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Due to the heterogeneous architectures and class skew, the global representation models training in resource-adaptive federated self-supervised learning face with tricky challenges: deviated representation abilities and inconsistent representation spaces. In this work, we are the first to propose a multi-teacher knowledge distillation framework, namely FedMKD, to learn global representations with whole class knowledge from heterogeneous clients even under extreme class skew. Firstly, the adaptive knowledge integration mechanism is designed to learn better representations from all heterogeneous models with deviated representation abilities. Then the weighted combination of the self-supervised loss and the distillation loss can support the global model to encode all classes from clients into a unified space. Besides, the global knowledge anchored alignment module can make the local representation spaces close to the global spaces, which further improves the representation abilities of local ones. Finally, extensive experiments conducted on two datasets demonstrate the effectiveness of FedMKD which outperforms state-of-the-art baselines 4.78% under linear evaluation on average.",,1,0.0,,,NSFC,62122042,Bureau of Education of Guangzhou Municipality
2-s2.0-85191156262,,,,Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline,cp,Conference Paper,Zheng Z.,60017161;60119391,National University of Singapore;Huawei Noah's Ark Lab,Singapore City;Hong Kong,Singapore;Hong Kong,6.0,"Zheng, Zangwei;Ren, Xiaozhe;Xue, Fuzhao;Luo, Yang;Jiang, Xin;You, Yang",57223732242;57219588177;57210823440;58343095400;55960978700;59173239500,60017161;60119391;60017161;60017161;60119391;60017161,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Large language models (LLMs) have revolutionized the field of AI, demonstrating unprecedented capacity across various tasks. However, the inference process for LLMs comes with significant computational costs. In this paper, we propose an efficient LLM inference pipeline that harnesses the power of LLMs. Our approach begins by tapping into the potential of LLMs to accurately perceive and predict the response length with minimal overhead. By leveraging this information, we introduce an efficient sequence scheduling technique that groups queries with similar response lengths into micro-batches. We evaluate our approach on real-world instruction datasets using the LLaMA-based model, and our results demonstrate an impressive 86% improvement in inference throughput without compromising effectiveness. Notably, our method is orthogonal to other inference acceleration techniques, making it a valuable addition to many existing toolkits (e.g. FlashAttention, Quantization) for LLM inference.",,33,0.0,,,NUS,,National University of Singapore
2-s2.0-85174408047,,,,Rethinking Backdoor Attacks,cp,Conference Paper,Khaddaj A.,60022195,Massachusetts Institute of Technology,Cambridge,United States,7.0,"Khaddaj, Alaa;Leclerc, Guillaume;Makelov, Aleksandar;Georgiev, Kristian;Salman, Hadi;Ilyas, Andrew;Madry, Aleksander",57216954517;56853237600;57210646305;57223089094;57218718431;57204781168;24171757000,60022195;60022195;60022195;60022195;60022195;60022195;60022195,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,16216-16236,"In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. Defending against such attacks involves viewing inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them. In this work, we present a different approach to the backdoor attack problem. Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occuring features in the data-and thus impossible to “detect” in a general sense. Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make, and on which they depend. Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data. Under this assumption (which we make formal) we develop a new primitive for detecting backdoor attacks. Our primitive naturally gives rise to a detection algorithm that comes with theoretical guarantees, and is effective in practice.",,17,0.0,,,NSF,CNS-1815221,National Science Foundation
2-s2.0-85203828348,,,,Rethinking Specificity in SBDD: Leveraging Delta Score and Energy-Guided Diffusion,cp,Conference Paper,Gao B.,60019499;60027363;60025278;60014966;60018486;128374360,Chinese Academy of Sciences;University of Chinese Academy of Sciences;Tsinghua University;Peking University;Institute of Automation Chinese Academy of Sciences;Beijing Academy of Artificial Intelligence (BAAI),Beijing;Beijing;Beijing;Beijing;Beijing;Beijing,China;China;China;China;China;China,8.0,"Gao, Bowen;Ren, Minsi;Ni, Yuyan;Huang, Yanwen;Qiang, Bo;Ma, Zhi Ming;Ma, Wei Ying;Lan, Yanyan",58587034300;58655357500;58530726600;58968827800;57221543984;55479145500;36071778600;59301567800,60025278;60018486-60027363;60019499;60014966;60014966;60019499;60025278;60025278-128374360,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,14811-14825,"In the field of Structure-based Drug Design (SBDD), deep learning-based generative models have achieved outstanding performance in terms of docking score. However, further study shows that the existing molecular generative methods and docking scores both have lacked consideration in terms of specificity, which means that generated molecules bind to almost every protein pocket with high affinity. To address this, we introduce the Delta Score, a new metric for evaluating the specificity of molecular binding. To further incorporate this insight for generation, we develop an innovative energy-guided approach using contrastive learning, with active compounds as decoys, to direct generative models toward creating molecules with high specificity. Our empirical results show that this method not only enhances the delta score but also maintains or improves traditional docking scores, successfully bridging the gap between SBDD and real-world needs.",,1,0.0,,,NKRDPC,2021YFF1201600,National Key Research and Development Program of China
2-s2.0-85189611133,10.1609/aaai.v38i19.30086,,,Rethinking the Development of Large Language Models from the Causal Perspective: A Legal Text Prediction Case Study,cp,Conference Paper,Chen H.,60025278;60009860;60145911,Tsinghua University;Fudan University;Whiting School of Engineering,Beijing;Shanghai;Baltimore,China;China;United States,4.0,"Chen, Haotian;Zhang, Lingwei;Liu, Yiran;Yu, Yang",57222010251;57973521800;57986312400;56019147800,60009860;60145911;60025278;60025278,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,19.0,,20958-20966,"While large language models (LLMs) exhibit impressive performance on a wide range of NLP tasks, most of them fail to learn the causality from correlation, which disables them from learning rationales for predicting. Rethinking the whole developing process of LLMs is of great urgency as they are adopted in various critical tasks that need rationales, including legal text prediction (e.g., legal judgment prediction). In this paper, we first explain the underlying theoretical mechanism of their failure and argue that both the data imbalance and the omission of causality in model design and selection render the current training-testing paradigm failed to select the unique causality-based model from correlation-based models. Second, we take the legal text prediction task as the testbed and reconstruct the developing process of LLMs by simultaneously infusing causality into model architectures and organizing causality-based adversarial attacks for evaluation. Specifically, we base our reconstruction on our theoretical analysis and propose a causality-aware self-attention mechanism (CASAM), which prevents LLMs from entangling causal and non-causal information by restricting the interaction between causal and non-causal words. Meanwhile, we propose eight kinds of legal-specific attacks to form causality-based model selection. Our extensive experimental results demonstrate that our proposed CASAM achieves state-of-the-art (SOTA) performances and the strongest robustness on three commonly used legal text prediction benchmarks. We make our code publicly available at https://github.com/Carrot-Red/Rethink-LLM-development.",,4,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85189337809,10.1609/aaai.v38i3.28043,,,Retrieval-Augmented Primitive Representations for Compositional Zero-Shot Learning,cp,Conference Paper,Jing C.,60003970;60003977,Zhejiang University;Northwestern Polytechnical University,Hangzhou;Xi'an,China;China,4.0,"Jing, Chenchen;Li, Yukun;Chen, Hao;Shen, Chunhua",57201686232;57226614330;57188762580;7402859952,60003970;60003977;60003970;60003970,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,3.0,,2652-2660,"Compositional zero-shot learning (CZSL) aims to recognize unseen attribute-object compositions by learning from seen compositions. Composing the learned knowledge of seen primitives, i.e., attributes or objects, into novel compositions is critical for CZSL. In this work, we propose to explicitly retrieve knowledge of seen primitives for compositional zero-shot learning. We present a retrieval-augmented method, which augments standard multi-path classification methods with two retrieval modules. Specifically, we construct two databases storing the attribute and object representations of training images, respectively. For an input training/testing image, we use two retrieval modules to retrieve representations of training images with the same attribute and object, respectively. The primitive representations of the input image are augmented by using the retrieved representations, for composition recognition. By referencing semantically similar images, the proposed method is capable of recalling knowledge of seen primitives for compositional generalization. Experiments on three widely-used datasets show the effectiveness of the proposed method.",,17,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85204283274,,,,Revealing Hierarchical Structure of Leaf Venations in Plant Science via Label-Efficient Segmentation: Dataset and Method,cp,Conference Paper,Liu W.,60022414;60280914;130270274,Wuhan University of Technology;Shanghai Artificial Intelligence Laboratory;Yazhouwan National Laboratory,Wuhan;Shanghai;Sanya,China;China;China,11.0,"Liu, Weizhen;Li, Ao;Wu, Ze;Li, Yue;Ge, Baobin;Lan, Guangyu;Chen, Shilin;Li, Minghe;Liu, Yunfei;Yuan, Xiaohui;Dong, Nanqing",57192704781;59157100400;58859510100;59157100500;59156838700;59156437600;59156569700;59156437700;59156838800;55286920300;57202814178,60022414;60022414;60022414;60022414;60022414;60022414;60022414;60280914;60022414;60022414-130270274;60280914,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,7367-7375,"Hierarchical leaf vein segmentation is a crucial but under-explored task in agricultural sciences, where analysis of the hierarchical structure of plant leaf venation can contribute to plant breeding. While current segmentation techniques rely on data-driven models, there is no publicly available dataset specifically designed for hierarchical leaf vein segmentation. To address this gap, we introduce the HierArchical Leaf Vein Segmentation (HALVS) dataset, the first public hierarchical leaf vein segmentation dataset. HALVS comprises 5,057 real-scanned high-resolution leaf images collected from three plant species: soybean, sweet cherry, and London planetree. It also includes human-annotated ground truth for three orders of leaf veins, with a total labeling effort of 83.8 person-days. Based on HALVS, we further develop a label-efficient learning paradigm that leverages partial label information, i.e. missing annotations for tertiary veins. Empirical studies are performed on HALVS, revealing new observations, challenges, and research directions on leaf vein segmentation. Our dataset and code are available at https://github.com/WeizhenLiuBioinform/HALVS-Hierarchical-Vein-Segment.",,0,0.0,,,NSFC,32090061,National Natural Science Foundation of China
2-s2.0-85163960543,10.24963/ijcai.2023/360,,,Reverse Engineering of Temporal Queries Mediated by LTL Ontologies,cp,Conference Paper,Fortin M.,60025225;60020661;60009016;60123660,"University of Southampton;University of Liverpool;Birkbeck, University of London;Institut de Recherche en Informatique Fondamentale (IRIF)",Southampton;Liverpool;London;Paris,United Kingdom;United Kingdom;United Kingdom;France,6.0,"Fortin, Marie;Konev, Boris;Ryzhikov, Vladislav;Savateev, Yury;Wolter, Frank;Zakharyaschev, Michael",57224629685;6603374977;23467849100;24339205900;7005739306;6602982360,60123660;60020661;60009016;60025225;60020661;60009016,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,3230-3238,"In reverse engineering of database queries, we aim to construct a query from a given set of answers and non-answers; it can then be used to explore the data further or as an explanation of the answers and non-answers. We investigate this query-by-example problem for queries formulated in positive fragments of linear temporal logic LTL over timestamped data, focusing on the design of suitable query languages and the combined and data complexity of deciding whether there exists a query in the given language that separates the given answers from non-answers. We consider both plain LTL queries and those mediated by LTL-ontologies.",,6,1.0,all publisherfullgold,All Open Access Gold,EPSRC,EP/S032282,Engineering and Physical Sciences Research Council
2-s2.0-85191188280,,,,Reversible and irreversible bracket-based dynamics for deep graph neural networks,cp,Conference Paper,Gruber A.,60007843;60093807,"Sandia National Laboratories, New Mexico;School of Computing and Augmented Intelligence",Albuquerque;Tempe,United States;United States,3.0,"Gruber, Anthony;Lee, Kookjin;Trask, Nathaniel",57208782705;57201748069;36698409800,60007843;60093807;60093807,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth. It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance. Code is available at the Github repository https://github.com/natrask/BracketGraphs.",,7,0.0,,,OARC,CNS2210137,"Office of Advanced Research Computing, Rutgers, The State University of New Jersey"
2-s2.0-85124064682,10.1613/JAIR.1.12440,,,Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning,ar,Article,Icarte R.T.,60016849;60029681;60030838;60278837,University of Toronto;Pontificia Universidad Católica de Chile;Toronto Metropolitan University;Vector Institute,Toronto;Santiago;Toronto;Toronto,Canada;Chile;Canada;Canada,4.0,"Icarte, Rodrigo Toro;Klassen, Toryn Q.;Valenzano, Richard;McIlraith, Sheila A.",57196121113;56659739800;36702159500;6602586653,60029681-60278837;60278837-60016849;60030838;60278837-60016849,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,173-208,"Reinforcement learning (RL) methods usually treat reward functions as black boxes. As such, these methods must extensively interact with the environment in order to discover rewards and optimal policies. In most RL applications, however, users have to program the reward function and, hence, there is the opportunity to make the reward function visible – to show the reward function’s code to the RL agent so it can exploit the function’s internal structure to learn optimal policies in a more sample efficient manner. In this paper, we show how to accomplish this idea in two steps. First, we propose reward machines, a type of finite state machine that supports the specification of reward functions while exposing reward function structure. We then describe different methodologies to exploit this structure to support learning, including automated reward shaping, task decomposition, and counterfactual reasoning with off-policy learning. Experiments on tabular and continuous domains, across different tasks and RL agents, show the benefits of exploiting reward structure with respect to sample efficiency and the quality of resultant policies. Finally, by virtue of being a form of finite state machine, reward machines have the expressive power of a regular language and as such support loops, sequences and conditionals, as well as the expression of temporally extended properties typical of linear temporal logic and non-Markovian reward specification.",,199,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,CIFAR,,Microsoft Research
2-s2.0-85173849271,,,,Reward-Mixing MDPs with Few Latent Contexts are Learnable,cp,Conference Paper,Kwon J.,60032179;60150401;60076695;128343326;126809883,University of Wisconsin-Madison;Cockrell School of Engineering;NVIDIA;Meta;Technion,Madison;Austin;Santa Clara;New York;,United States;United States;United States;United States;Israel,4.0,"Kwon, Jeongyeol;Efroni, Yonathan;Caramanis, Constantine;Mannor, Shie",57219510631;57195997846;55967563700;8218747000,60032179;128343326;60150401;126809883-60076695,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,18057-18082,"We consider episodic reinforcement learning in reward-mixing Markov decision processes (RMMDPs): at the beginning of every episode nature randomly picks a latent reward model among M candidates and an agent interacts with the MDP throughout the episode for H time steps. Our goal is to learn a near-optimal policy that nearly maximizes the H time-step cumulative rewards in such a model. Prior work (Kwon et al., 2021a) established an upper bound for RMMDPs with M = 2. In this work, we resolve several open questions for the general RMMDP setting. We consider an arbitrary M ≥ 2 and provide a sample-efficient algorithm-EM<sup>2</sup> -that outputs an ϵ-optimal policy using O (ϵ<sup>−2</sup> · S<sup>d</sup>A<sup>d</sup> · poly(H, Z)<sup>d</sup>) episodes, where S, A are the number of states and actions respectively, H is the time-horizon, Z is the support size of reward distributions and d = O(min(M, H)). We also provide a (SA)<sup>Ω(√</sup>M<sup>)</sup>/ϵ<sup>2</sup> lower bound, supporting that super-polynomial sample complexity in M is necessary.",,5,0.0,,,ISF,2199/20,Israel Science Foundation
2-s2.0-85178167442,,,,Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards,cp,Conference Paper,Rame A.,60069769;60104109;60355330,Institut des Systèmes Intelligents et de Robotique;Valeo;Meta Ai,Paris;Paris;Menlo Park,France;France;United States,7.0,"Rame, Alexandre;Couairon, Guillaume;Shukor, Mustafa;Dancette, Corentin;Gaya, Jean Baptiste;Soulier, Laure;Cord, Matthieu",57201857412;57221318005;57521051200;57204210114;57313439200;55301159700;6701549439,60069769;60069769-60355330;60069769;60069769;60069769;60069769;60069769-60104109,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity.",,80,0.0,,,GENCI,AD011011953R1,Grand Équipement National De Calcul Intensif
2-s2.0-85146937073,,,,"Ridges, Neural Networks, and the Radon Transform",ar,Article,Unser M.,60028186,École Polytechnique Fédérale de Lausanne,Lausanne,Switzerland,1.0,"Unser, Michael",7102049045,60028186,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"A ridge is a function that is characterized by a one-dimensional profile (activation) and a multidimensional direction vector. Ridges appear in the theory of neural networks as functional descriptors of the effect of a neuron, with the direction vector being encoded in the linear weights. In this paper, we investigate properties of the Radon transform in relation to ridges and to the characterization of neural networks. We introduce a broad category of hyper-spherical Banach subspaces (including the relevant subspace of measures) over which the back-projection operator is invertible. We also give conditions under which the back-projection operator is extendable to the full parent space with its null space being identifiable as a Banach complement. Starting from first principles, we then characterize the sampling functionals that are in the range of the filtered Radon transform. Next, we extend the definition of ridges for any distributional profile and determine their (filtered) Radon transform in full generality. Finally, we apply our formalism to clarify and simplify some of the results and proofs on the optimality of ReLU networks that have appeared in the literature.",,18,0.0,,,SNF,200020-184646,Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung
2-s2.0-85183839748,10.1613/jair.1.15057,,,"Right Place, Right Time: Proactive Multi-Robot Task Allocation Under Spatiotemporal Uncertainty",ar,Article,Street C.,60026851;60019702;60112773,University of Oxford;University of Birmingham;Honda Research Institute Europe GmbH,Oxford;Birmingham;Offenbach,United Kingdom;United Kingdom;Germany,4.0,"Street, Charlie;Lacerda, Bruno;Mühlig, Manuel;Hawes, Nick",57220051354;54793193300;35410180900;14041693100,60019702;60026851;60112773;60026851,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,137-171,"For many multi-robot problems, tasks are announced during execution, where task announcement times and locations are uncertain. To synthesise multi-robot behaviour that is robust to early announcements and unexpected delays, multi-robot task allocation methods must explicitly model the stochastic processes that govern task announcement. In this paper, we model task announcement using continuous-time Markov chains which predict when and where tasks will be announced. We then present a task allocation framework which uses the continuous-time Markov chains to allocate tasks proactively, such that robots are near or at the task location upon its announcement. Our method seeks to minimise the expected total waiting duration for each task, i.e. the duration between task announcement and a robot beginning to service the task. Our framework can be applied to any multi-robot task allocation problem where robots complete spatiotemporal tasks which are announced stochastically. We demonstrate the efficacy of our approach in simulation, where we outperform baselines which do not allocate tasks proactively, or do not fully exploit our task announcement models.",,3,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,EP/R026084/1,
2-s2.0-105000498029,,,,Right this way: Can VLMs Guide Us to See More to Answer Questions?,cp,Conference Paper,Liu L.,60024941,"University of California, Santa Cruz",Santa Cruz,United States,7.0,"Liu, Li;Yang, Diji;Zhong, Sijia;Tholeti, Kalyana Suma Sree;Ding, Lei;Zhang, Yi;Gilpin, Leilani H.",59451896100;57207102600;58979264400;59451641000;59008983200;56018366600;56516595700,60024941;60024941;60024941;60024941;60024941;60024941;60024941,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"In question-answering scenarios, humans can assess whether the available information is sufficient and seek additional information if necessary, rather than providing a forced answer. In contrast, Vision Language Models (VLMs) typically generate direct, one-shot responses without evaluating the sufficiency of the information. To investigate this gap, we identify a critical and challenging task in the Visual Question Answering (VQA) scenario: can VLMs indicate how to adjust an image when the visual information is insufficient to answer a question? This capability is especially valuable for assisting visually impaired individuals who often need guidance to capture images correctly. To evaluate this capability of current VLMs, we introduce a human-labeled dataset as a benchmark for this task. Additionally, we present an automated framework that generates synthetic training data by simulating “where to know” scenarios. Our empirical results show significant performance improvements in mainstream VLMs when fine-tuned with this synthetic data. This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans. Our dataset and code are available at: https://github.com/LeoLee7/Directional_guidance.",,3,0.0,,,AFOSR,FA9550-24-1-0149,Air Force Office of Scientific Research
2-s2.0-85203799748,,,,Risk Estimation in a Markov Cost Process: Lower and Upper Bounds,cp,Conference Paper,Thoppe G.,60014097;60025757;60283297,Indian Institute of Science;Indian Institute of Technology Madras;TCS Research,Bengaluru;Chennai;Mumbai,India;India;India,3.0,"Thoppe, Gugan;Prashanth, L. A.;Bhat, Sanjay P.",55638413500;24825650400;7202757334,60014097-60025757;60025757;60283297,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,48124-48138,"We tackle the problem of estimating risk measures of the infinite-horizon discounted cost of a Markov cost process. The risk measures we study include variance, Value-at-Risk (VaR), and Conditional Value-at-Risk (CVaR). First, we show that estimating any of these risk measures with ϵ-accuracy, either in expected or high-probability sense, requires at least Ω(1/ϵ<sup>2</sup>) samples. Then, using a truncation scheme, we derive an upper bound for the CVaR and variance estimation. This bound matches our lower bound up to logarithmic factors. Finally, we discuss an extension of our estimation scheme that covers more general risk measures satisfying a certain continuity criterion, such as spectral risk measures and utility-based shortfall risk. To the best of our knowledge, our work is the first to provide lower and upper bounds for estimating any risk measure beyond the mean within a Markovian setting. Our lower bounds also extend to the infinite-horizon discounted costs’ mean. Even in that case, our lower bound of Ω(1/ϵ<sup>2</sup>) improves upon the existing Ω(1/ϵ) bound (Metelli et al., 2023).",,0,0.0,,,DST,CRG/2021/008330,"Department of Science and Technology, Ministry of Science and Technology, India"
2-s2.0-85192992732,,,,Robust Concept Erasure via Kernelized Rate-Distortion Maximization,cp,Conference Paper,Chowdhury S.B.R.,60025111;60006191;60111161,The University of North Carolina at Chapel Hill;Google LLC;DeepMind Technologies Limited,Chapel Hill;Mountain View;London,United States;United States;United Kingdom,5.0,"Chowdhury, Somnath Basu Roy;Monath, Nicholas;Dubey, Avinava;Ahmed, Amr;Chaturvedi, Snigdha",57195955398;57195598636;36016208000;57206405307;36766996800,60025111;60111161;60006191;60006191;60025111,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Distributed representations provide a vector space that captures meaningful relationships between data instances. The distributed nature of these representations, however, entangles together multiple attributes or concepts of data instances (e.g., the topic or sentiment of a text, characteristics of the author (age, gender, etc), etc). Recent work has proposed the task of concept erasure [50, 52], in which rather than making a concept predictable, the goal is to remove an attribute from distributed representations while retaining other information from the original representation space as much as possible. In this paper, we propose a new distance metric learning-based objective, the Kernelized Rate-Distortion Maximizer (KRaM), for performing concept erasure. KRaM fits a transformation of representations to match a specified distance measure (defined by a labeled concept to erase) using a modified rate-distortion function. Specifically, KRaM's objective function aims to make instances with similar concept labels dissimilar in the learned representation space while retaining other information. We find that optimizing KRaM effectively erases various types of concepts-categorical, continuous, and vector-valued variables-from data representations across diverse domains. We also provide a theoretical analysis of several properties of KRaM's objective. To assess the quality of the learned representations, we propose an alignment score to evaluate their similarity with the original representation space. Additionally, we conduct experiments to showcase KRaM's efficacy in various settings, from erasing binary gender variables in word embeddings to vector-valued variables in GPT-3 representations.",,4,0.0,,,,,Google Research
2-s2.0-85168013238,10.1609/aaai.v37i3.25492,,,Robust Feature Rectification of Pretrained Vision Models for Object Recognition,cp,Conference Paper,Zhou S.,60019499;60027363;60014347;60018486,Chinese Academy of Sciences;University of Chinese Academy of Sciences;Hong Kong Baptist University;Institute of Automation Chinese Academy of Sciences,Beijing;Beijing;Hong Kong;Beijing,China;China;Hong Kong;China,5.0,"Zhou, Shengchao;Meng, Gaofeng;Zhang, Zhaoxiang;Xu, Richard Yi Da;Xiang, Shiming",58537028000;16317032400;8066042700;8303122400;8938807200,60018486-60027363;60018486-60027363-60019499;60018486-60027363-60019499;60014347;60018486-60027363,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,3796-3804,"Pretrained vision models for object recognition often suffer a dramatic performance drop with degradations unseen during training. In this work, we propose a RObust FEature Rectification module (ROFER) to improve the performance of pretrained models against degradations. Specifically, ROFER first estimates the type and intensity of the degradation that corrupts the image features. Then, it leverages a Fully Convolutional Network (FCN) to rectify the features from the degradation by pulling them back to clear features. ROFER is a general-purpose module that can address various degradations simultaneously, including blur, noise, and low contrast. Besides, it can be plugged into pretrained models seamlessly to rectify the degraded features without retraining the whole model. Furthermore, ROFER can be easily extended to address composite degradations by adopting a beam search algorithm to find the composition order. Evaluations on CIFAR-10 and Tiny-ImageNet demonstrate that the accuracy of ROFER is 5% higher than that of SOTA methods on different degradations.With respect to composite degradations, ROFER improves the accuracy of a pretrained CNN by 10% and 6% on CIFAR-10 and Tiny-ImageNet respectively.",,0,1.0,all publisherfullgold,All Open Access Gold,NSFC,61976208,National Natural Science Foundation of China
2-s2.0-85170357298,10.24963/ijcai.2023/51,,,Robust Reinforcement Learning via Progressive Task Sequence,cp,Conference Paper,Li Y.,60022381,Beijing Jiaotong University,Beijing,China,5.0,"Li, Yike;Tian, Yunzhe;Tong, Endong;Niu, Wenjia;Liu, Jiqiang",57217875415;57221045656;42762175500;25028470300;55865525600,60022381;60022381;60022381;60022381;60022381,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,455-463,"Robust reinforcement learning (RL) has been a challenging problem due to the gap between simulation and the real world. Existing efforts typically address the robust RL problem by solving a max-min problem. The main idea is to maximize the cumulative reward under the worst-possible perturbations. However, the worst-case optimization either leads to overly conservative solutions or unstable training process, which further affects the policy robustness and generalization performance. In this paper, we tackle this problem from both formulation definition and algorithm design. First, we formulate the robust RL as a max-expectation optimization problem, where the goal is to find an optimal policy under both the worst cases and the non-worst cases. Then, we propose a novel framework DRRL to solve the max-expectation optimization. Given our definition of the feasible tasks, a task generation and sequencing mechanism is introduced to dynamically output tasks at appropriate difficulty level for the current policy. With these progressive tasks, DRRL realizes dynamic multi-task learning to improve the policy robustness and the training stability. Finally, extensive experiments demonstrate that the proposed method exhibits significant performance on the unmanned CarRacing game and multiple high-dimensional MuJoCo environments.",,2,1.0,all publisherfullgold,All Open Access Gold,NSFC,61802389,National Natural Science Foundation of China
2-s2.0-85163204578,,,,Robust ϕ-Divergence MDPs,cp,Conference Paper,Ho C.P.,60015150;60013983;60027646,Imperial College London;City University of Hong Kong;University System of New Hampshire,London;Hong Kong;Durham,United Kingdom;Hong Kong;United States,3.0,"Ho, Chin Pang;Petrik, Marek;Wiesemann, Wolfram",56145479500;16425736700;24823272600,60013983;60027646;60015150,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"In recent years, robust Markov decision processes (MDPs) have emerged as a prominent modeling framework for dynamic decision problems affected by uncertainty. In contrast to classical MDPs, which only account for stochasticity by modeling the dynamics through a stochastic process with a known transition kernel, robust MDPs additionally account for ambiguity by optimizing in view of the most adverse transition kernel from a prescribed ambiguity set. In this paper, we develop a novel solution framework for robust MDPs with s-rectangular ambiguity sets that decomposes the problem into a sequence of robust Bellman updates and simplex projections. Exploiting the rich structure present in the simplex projections corresponding to ϕ-divergence ambiguity sets, we show that the associated s-rectangular robust MDPs can be solved substantially faster than with state-of-the-art commercial solvers as well as a recent first-order solution scheme, thus rendering them attractive alternatives to classical MDPs in practical applications.",,9,0.0,,,NSF,9229076,National Science Foundation
2-s2.0-105000491411,,,,SAM-Guided Masked Token Prediction for 3D Scene Understanding,cp,Conference Paper,Chen Z.,60005248;60007033;60026610,Johns Hopkins University;The City University of New York;Clemson University,Baltimore;New York;Clemson,United States;United States;United States,5.0,"Chen, Zhimin;Yang, Liang;Li, Yingwei;Jing, Longlong;Li, Bing",57219165638;57192695601;57211277832;57200725782;57199786158,60026610;60007033;60005248;60007033;60026610,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Foundation models have significantly enhanced 2D task performance, and recent works like Bridge3D have successfully applied these models to improve 3D scene understanding through knowledge distillation, marking considerable advancements. Nonetheless, challenges such as the misalignment between 2D and 3D representations and the persistent long-tail distribution in 3D datasets still restrict the effectiveness of knowledge distillation from 2D to 3D using foundation models. To tackle these issues, we introduce a novel SAM-guided tokenization method that seamlessly aligns 3D transformer structures with region-level knowledge distillation, replacing the traditional KNN-based tokenization techniques. Additionally, we implement a group-balanced re-weighting strategy to effectively address the long-tail problem in knowledge distillation. Furthermore, inspired by the recent success of masked feature prediction, our framework incorporates a two-stage masked token prediction process in which the student model predicts both the global embeddings and the token-wise local embeddings derived from the teacher models trained in the first stage. Our methodology has been validated across multiple datasets, including SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and semantic segmentation. The results demonstrate significant improvements over current State-of-the-art self-supervised methods, establishing new benchmarks in this field.",,0,0.0,,,,,
2-s2.0-105000471893,,,,SARAD: Spatial Association-Aware Anomaly Detection and Diagnosis for Multivariate Time Series,cp,Conference Paper,Dai Z.,60019702;60012197;60163091,"University of Birmingham;University of Reading;Faculty of Science, Engineering and Medicine",Birmingham;Reading;Coventry,United Kingdom;United Kingdom;United Kingdom,4.0,"Dai, Zhihao;Yang, Shuang Hua;He, Ligang;Leeke, Matthew",57223429325;35726316400;8390041900;35731657000,60163091;60012197;60163091;60019702,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Anomaly detection in time series data is fundamental to the design, deployment, and evaluation of industrial control systems. Temporal modeling has been the natural focus of anomaly detection approaches for time series data. However, the focus on temporal modeling can obscure or dilute the spatial information that can be used to capture complex interactions in multivariate time series. In this paper, we propose SARAD, an approach that leverages spatial information beyond data autoencoding errors to improve the detection and diagnosis of anomalies. SARAD trains a Transformer to learn the spatial associations, the pairwise inter-feature relationships which ubiquitously characterize such feedback-controlled systems. As new associations form and old ones dissolve, SARAD applies subseries division to capture their changes over time. Anomalies exhibit association descending patterns, a key phenomenon we exclusively observe and attribute to the disruptive nature of anomalies detaching anomalous features from others. To exploit the phenomenon and yet dismiss non-anomalous descent, SARAD performs anomaly detection via autoencoding in the association space. We present experimental results to demonstrate that SARAD achieves state-of-the-art performance, providing robust anomaly detection and a nuanced understanding of anomalous events.",,9,0.0,,,EPSRC,EP/T022108/1,University of Warwick
2-s2.0-85167927876,10.24963/ijcai.2023/373,,,SAT-Based PAC Learning of Description Logic Concepts,cp,Conference Paper,Cate B.t.,60002483;60008042;60032991;125736302,Universiteit van Amsterdam;Universität Leipzig;Technische Universität Dortmund;Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI),Amsterdam;Leipzig;Dortmund;Leipzig,Netherlands;Germany;Germany;Germany,4.0,"Cate, Balder ten;Funk, Maurice;Jung, Jean Christoph;Lutz, Carsten",8964763900;57209282329;36166980800;7103325866,60002483;60008042-125736302;60032991;60008042-125736302,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,3347-3355,"We propose bounded fitting as a scheme for learning description logic concepts in the presence of ontologies. A main advantage is that the resulting learning algorithms come with theoretical guarantees regarding their generalization to unseen examples in the sense of PAC learning. We prove that, in contrast, several other natural learning algorithms fail to provide such guarantees. As a further contribution, we present the system SPELL which efficiently implements bounded fitting for the description logic ELH<sup>r</sup> based on a SAT solver, and compare its performance to a state-of-the-art learner.",,11,1.0,all publisherfullgold,All Open Access Gold,BMBF,57616814,Bundesministerium für Bildung und Forschung
2-s2.0-85166343322,10.1613/jair.1.14427,,,SAlign: A Graph Neural Attention Framework for Aligning Structurally Heterogeneous Networks,ar,Article,Saxena S.,60104342,Indian Institute of Technology Patna,Patna,India,2.0,"Saxena, Shruti;Chandra, Joydeep",57552385100;57191950294,60104342;60104342,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,949-969,"Network alignment techniques that map the same entities across multiple networks assume that the mapping nodes in two different networks have similar attributes and neighborhood proximity. However, real-world networks often violate such assumptions, having diverse attributes and structural properties. Node mapping across such structurally heterogeneous networks remains a challenge. Although capturing the nodes’ entire neighborhood (in low-dimensional embeddings) may help deal with these characteristic differences, the issue of over-smoothing in the representations that come from higher-order learning still remains a major problem. To address the above concerns, we propose SAlign: a supervised graph neural attention framework for aligning structurally heterogeneous networks that learns the correlation of structural properties of mapping nodes using a set of labeled (mapped) anchor nodes. SAlign incorporates nodes’ graphlet information with a novel structure-aware cross-network attention mechanism that transfers the required higher-order structure information across networks. The information exchanged across networks helps in enhancing the expressivity of the graph neural network, thereby handling any potential over-smoothing problem. Extensive experiments on three real datasets demonstrate that SAlign consistently outperforms the state-of-the-art network alignment methods by at least 1.3-8% in terms of accuracy score.",,3,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85178516097,,,,SCALABLE AND EQUIVARIANT SPHERICAL CNNS BY DISCRETE-CONTINUOUS (DISCO) CONVOLUTIONS,cp,Conference Paper,Ocampo J.,60022148;131506994,University College London;Kagenova Limited,London;,United Kingdom;,3.0,"Ocampo, Jeremy;Price, Matthew A.;McEwen, Jason D.",58845561300;57211852210;8559862200,131506994-60022148;131506994-60022148;131506994-60022148,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"No existing spherical convolutional neural network (CNN) framework is both computationally scalable and rotationally equivariant. Continuous approaches capture rotational equivariance but are often prohibitively computationally demanding. Discrete approaches offer more favorable computational performance but at the cost of equivariance. We develop a hybrid discrete-continuous (DISCO) group convolution that is simultaneously equivariant and computationally scalable to high-resolution. While our framework can be applied to any compact group, we specialize to the sphere. Our DISCO spherical convolutions exhibit SO(3) rotational equivariance, where SO(n) is the special orthogonal group representing rotations in n-dimensions. When restricting rotations of the convolution to the quotient space SO(3)/SO(2) for further computational enhancements, we recover a form of asymptotic SO(3) rotational equivariance. Through a sparse tensor implementation we achieve linear scaling in number of pixels on the sphere for both computational cost and memory usage. For 4k spherical images we realize a saving of 10<sup>9</sup> in computational cost and 10<sup>4</sup> in memory usage when compared to the most efficient alternative equivariant spherical convolution. We apply the DISCO spherical CNN framework to a number of benchmark dense-prediction problems on the sphere, such as semantic segmentation and depth estimation, on all of which we achieve the state-of-the-art performance.",,4,0.0,,,,,
2-s2.0-85149705392,,,,SCALE MIXTURES OF NEURAL NETWORK GAUSSIAN PROCESSES,cp,Conference Paper,Lee H.,60032144;60104544;60159728,"Korea Advanced Institute of Science and Technology;Institute for Basic Science, Daejeon;AItrics Co., Ltd.",Daejeon;Daejeon;Seoul,South Korea;South Korea;South Korea,4.0,"Lee, Hyungi;Yun, Eunggu;Yang, Hongseok;Lee, Juho",57226249122;57226256535;55153940300;57132050100,60032144;60032144;60032144-60104544;60032144-60159728,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Recent works have revealed that infinitely-wide feed-forward or recurrent neural networks of any architecture correspond to Gaussian processes referred to as Neural Network Gaussian Processes (NNGPs). While these works have extended the class of neural networks converging to Gaussian processes significantly, however, there has been little focus on broadening the class of stochastic processes that such neural networks converge to. In this work, inspired by the scale mixture of Gaussian random variables, we propose the scale mixture of NNGPs for which we introduce a prior distribution on the scale of the last-layer parameters. We show that simply introducing a scale prior on the last-layer parameters can turn infinitely-wide neural networks of any architecture into a richer class of stochastic processes. With certain scale priors, we obtain heavy-tailed stochastic processes, and in the case of inverse gamma priors, we recover Student's t processes. We further analyze the distributions of the neural networks initialized with our prior setting and trained with gradient descents and obtain similar results as for NNGPs. We present a practical posterior-inference algorithm for the scale mixture of NNGPs and empirically demonstrate its usefulness on regression and classification tasks. In particular, we show that in both tasks, the heavy-tailed stochastic processes obtained from our framework are robust to out-of-distribution data.",,6,0.0,,,MOE,NRF-2018R1A5A1059921,Ministry of Education
2-s2.0-85150385477,,,,SCALING LAWS FOR NEURAL MACHINE TRANSLATION,cp,Conference Paper,Ghorbani B.,60006191,Google LLC,Mountain View,United States,8.0,"Ghorbani, Behrooz;Firat, Orhan;Freitag, Markus;Bapna, Ankur;Krikun, Maxim;Garcia, Xavier;Chelba, Ciprian;Cherry, Colin",57215313981;55293162100;57215722263;57200080817;57216970977;57219508448;6507698988;55074594800,60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"We present an empirical study of the scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufficient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely influenced by composition bias of the train/test sets, which we define as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the model size, cross-entropy loss and the quality of the generated translations. We find two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study.",,20,0.0,,,,,
2-s2.0-85199911828,,,,SE(3)-EQUIVARIANT ATTENTION NETWORKS FOR SHAPE RECONSTRUCTION IN FUNCTION SPACE,cp,Conference Paper,Chatzipantazis E.,60006297,University of Pennsylvania,Philadelphia,United States,4.0,"Chatzipantazis, Evangelos;Pertigkiozoglou, Stefanos;Dobriban, Edgar;Daniilidis, Kostas",57364979200;57219641521;55711190100;57203069140,60006297;60006297;60006297;60006297,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"We propose a method for 3D shape reconstruction from unoriented point clouds. Our method consists of a novel SE(3)-equivariant coordinate-based network (TF-ONet), that parametrizes the occupancy field of the shape and respects the inherent symmetries of the problem. In contrast to previous shape reconstruction methods that align the input to a regular grid, we operate directly on the irregular point cloud. Our architecture leverages equivariant attention layers that operate on local tokens. This mechanism enables local shape modelling, a crucial property for scalability to large scenes. Given an unoriented, sparse, noisy point cloud as input, we produce equivariant features for each point. These serve as keys and values for the subsequent equivariant cross-attention blocks that parametrize the occupancy field. By querying an arbitrary point in space, we predict its occupancy score. We show that our method outperforms previous SO(3)-equivariant methods, as well as non-equivariant methods trained on SO(3)-augmented datasets. More importantly, local modelling together with SE(3)-equivariance create an ideal setting for SE(3) scene reconstruction. We show that by training only on single, aligned objects and without any pre-segmentation, we can reconstruct novel scenes containing arbitrarily many objects in random poses without any performance loss.",,12,0.0,,,,FRR 2220868,
2-s2.0-85189467890,,,,SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models,cp,Conference Paper,Gonzalez M.,60021784;60032640;60196608;131185761,New York University;Air Liquide;l’Institut de Recherche Technologique (IRT) SystemX;Safran,New York;Paris;Palaiseau;,United States;France;France;,6.0,"Gonzalez, Martin;Fernandez, Nelson;Tran, Thuy;Gherbi, Elies;Hajri, Hatem;Masmoudi, Nader",57219502135;58354485300;58317757200;57220205411;51461201600;7004021362,60196608;60032640;60196608;60196608;60196608-131185761;60021784,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"A potent class of generative models known as Diffusion Probabilistic Models (DPMs) has become prominent.A forward diffusion process adds gradually noise to data, while a model learns to gradually denoise.Sampling from pre-trained DPMs is obtained by solving differential equations (DE) defined by the learnt model, a process which has shown to be prohibitively slow.Numerous efforts on speeding-up this process have consisted on crafting powerful ODE solvers.Despite being quick, such solvers do not usually reach the optimal quality achieved by available slow SDE solvers.Our goal is to propose SDE solvers that reach optimal quality without requiring several hundreds or thousands of NFEs to achieve that goal.We propose Stochastic Explicit Exponential Derivative-free Solvers (SEEDS), improving and generalizing Exponential Integrator approaches to the stochastic case on several frameworks.After carefully analyzing the formulation of exact solutions of diffusion SDEs, we craft SEEDS to analytically compute the linear part of such solutions.Inspired by the Exponential Time-Differencing method, SEEDS use a novel treatment of the stochastic components of solutions, enabling the analytical computation of their variance, and contains high-order terms allowing to reach optimal quality sampling ∼ 3-5× faster than previous SDE methods.We validate our approach on several image generation benchmarks, showing that SEEDS outperform or are competitive with previous SDE solvers.Contrary to the latter, SEEDS are derivative and training free, and we fully prove strong convergence guarantees for them.Our code is publicly available in this link.",,6,0.0,,,,,
2-s2.0-85150366887,,,,SEQUENTIAL REPTILE: INTER-TASK GRADIENT ALIGNMENT FOR MULTILINGUAL LEARNING,cp,Conference Paper,Lee S.,60032144;60159728,"Korea Advanced Institute of Science and Technology;AItrics Co., Ltd.",Daejeon;Seoul,South Korea;South Korea,4.0,"Lee, Seanie;Lee, Hae Beom;Lee, Juho;Hwang, Sung Ju",57219689969;57204801406;57132050100;57687927300,60032144;60032144;60032144-60159728;60032144-60159728,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Multilingual models jointly pretrained on multiple languages have achieved remarkable performance on various multilingual downstream tasks. Moreover, models finetuned on a single monolingual downstream task have shown to generalize to unseen languages. In this paper, we first show that it is crucial for those tasks to align gradients between them in order to maximize knowledge transfer while minimizing negative transfer. Despite its importance, the existing methods for gradient alignment either have a completely different purpose, ignore inter-task alignment, or aim to solve continual learning problems in rather inefficient ways. As a result of the misaligned gradients between tasks, the model suffers from severe negative transfer in the form of catastrophic forgetting of the knowledge acquired from the pretraining. To overcome the limitations, we propose a simple yet effective method that can efficiently align gradients between tasks. Specifically, we perform each inner-optimization by sequentially sampling batches from all the tasks, followed by a Reptile outer update. Thanks to the gradients aligned between tasks by our method, the model becomes less vulnerable to negative transfer and catastrophic forgetting. We extensively validate our method on various multi-task learning and zero-shot cross-lingual transfer tasks, where our method largely outperforms all the relevant baselines we consider.",,2,0.0,,,MOE,2021-0-02068,Samsung
2-s2.0-85200567624,,,,SET LEARNING FOR ACCURATE AND CALIBRATED MODELS,cp,Conference Paper,Muttenthaler L.,60005273;60011604;60000256;60111161;124120654,Korea University;Technische Universität Berlin;Max Planck Institute for Informatics;DeepMind Technologies Limited;Berlin Institute for the Foundations of Learning and Data,Seoul;Berlin;Saarbrucken;London;Berlin,South Korea;Germany;Germany;United Kingdom;Germany,5.0,"Muttenthaler, Lukas;Vandermeulen, Robert A.;Zhang, Qiuyi;Unterthiner, Thomas;Müller, Klaus Robert",57212139934;56109429700;57200570786;37078167900;15042362900,60011604-124120654-60111161;60011604-124120654;60111161;60111161;60011604-124120654-60111161-60005273-60000256,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-k-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We demonstrate this in extensive experimental analyses and provide a mathematical theory to interpret our findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and a trained model can be applied to single examples at inference time, without significant run-time overhead or architecture changes.",,4,0.0,,,BMBF,BIFOLD22B,Bundesministerium für Bildung und Forschung
2-s2.0-85148087797,,,,SGD with Coordinate Sampling: Theory and Practice,ar,Article,Leluc R.,60116488;60028238,Institut Polytechnique de Paris;ENSAI École Nationale de la Statistique et de l'Analyse de l'Information,Palaiseau;Bruz,France;France,2.0,"Leluc, Rémi;Portier, François",57219523608;55444373400,60116488;60028238,2022-10-01,1 October 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,342,,"While classical forms of stochastic gradient descent algorithm treat the different coordinates in the same way, a framework allowing for adaptive (non uniform) coordinate sampling is developed to leverage structure in data. In a non-convex setting and including zeroth-order gradient estimate, almost sure convergence as well as non-asymptotic bounds are established. Within the proposed framework, we develop an algorithm, MUSKETEER, based on a reinforcement strategy: after collecting information on the noisy gradients, it samples the most promising coordinate (all for one); then it moves along the one direction yielding an important decrease of the objective (one for all). Numerical experiments on both synthetic and real data examples confirm the effectiveness of MUSKETEER in large scale problems.",adaptive methods | coordinate descent | stochastic gradient algorithms | stochastic optimization | zeroth-order optimization,5,0.0,,,,,
2-s2.0-85185597727,,,,SHAP-IQ: Unified Approximation of any-order Shapley Interactions,cp,Conference Paper,Fumagalli F.,60015595;60020238;60280918,Universität Bielefeld;Paderborn University;Munich Center for Machine Learning,Bielefeld;Paderborn;Munich,Germany;Germany;Germany,5.0,"Fumagalli, Fabian;Muschalik, Maximilian;Kolpaczki, Patrick;Hüllermeier, Eyke;Hammer, Barbara",57795307800;57795307700;57330112300;6701552637;55791279400,60015595;60280918;60020238;60280918;60015595,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature attributions for any black box model.Shapley interaction indices extend the SV to define any-order feature interactions.Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms.Moreover, each definition requires a specific approximation technique.Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e.interaction indices that satisfy the linearity, symmetry and dummy axiom.SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates.For the special case of SV, our approach reveals a novel representation of the SV and corresponds to Unbiased KernelSHAP with a greatly simplified calculation.We illustrate the computational efficiency and effectiveness by explaining language, image classification and high-dimensional synthetic models.",,33,0.0,,,DFG,TRR 318/1 2021-438445824,Deutsche Forschungsgemeinschaft
2-s2.0-85147665398,10.1609/aaai.v36i1.19911,,,SJDL-Vehicle: Semi-supervised Joint Defogging Learning for Foggy Vehicle Re-identification,cp,Conference Paper,Chen W.T.,60005429;129140991,National Taiwan University;ASUS Intelligent Cloud Services,Taipei;Taipei,Taiwan;Taiwan,6.0,"Chen, Wei Ting;Chen, I. Hsiang;Yeh, Chih Yuan;Yang, Hao Hsiang;Ding, Jian Jiun;Kuo, Sy Yen",55716092000;57226877999;57903807700;57209686527;7402608100;7401785539,60005429-129140991;60005429;60005429;60005429;60005429;60005429,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,347-355,"Vehicle re-identification (ReID) has attracted considerable attention in computer vision. Although several methods have been proposed to achieve state-of-the-art performance on this topic, re-identifying vehicle in foggy scenes remains a great challenge due to the degradation of visibility. To our knowledge, this problem is still not well-addressed so far. In this paper, to address this problem, we propose a novel training framework called Semi-supervised Joint Defogging Learning (SJDL) framework. First, the fog removal branch and the re-identification branch are integrated to perform simultaneous training. With the collaborative training scheme, defogged features generated by the defogging branch from input images can be shared to learn better representation for the re-identification branch. However, since the fog-free image of real-world data is intractable, this architecture can only be trained on the synthetic data, which may cause the domain gap problem between real-world and synthetic scenarios. To solve this problem, we design a semi-supervised defogging training scheme that can train two kinds of data alternatively in each iteration. Due to the lack of a dataset specialized for vehicle ReID in the foggy weather, we construct a dataset called FVRID which consists of real-world and synthetic foggy images to train and evaluate the performance. Experimental results show that the proposed method is effective and outperforms other existing vehicle ReID methods in the foggy weather. The code and dataset are available in https://github.com/Cihsaing/SJDL-Foggy-Vehicle-Re-Identification-AAAI2022.",,15,1.0,all publisherfullgold,All Open Access Gold,NSFC,108-2221-E-002-072-MY3,National Natural Science Foundation of China
2-s2.0-85167975876,10.1609/aaai.v37i6.25902,,,SKDBERT: Compressing BERT via Stochastic Knowledge Distillation,cp,Conference Paper,Ding Z.,60272178,Meituan,Beijing,China,5.0,"Ding, Zixiang;Jiang, Guoqing;Zhang, Shuai;Guo, Lin;Lin, Wei",57220726185;57999615500;58365650500;58382124400;58959377600,60272178;60272178;60272178;60272178;,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,7414-7422,"In this paper, we propose Stochastic Knowledge Distillation (SKD) to obtain compact BERT-style language model dubbed SKDBERT. In each iteration, SKD samples a teacher from a pre-defined teacher ensemble, which consists of multiple teachers with multi-level capacities, to transfer knowledge into student in an one-to-one manner. Sampling distribution plays an important role in SKD. We heuristically present three types of sampling distributions to assign appropriate probabilities for multi-level teachers. SKD has two advantages: 1) it can preserve the diversities of multi-level teachers via stochastically sampling single teacher in each iteration, and 2) it can also improve the efficacy of knowledge distillation via multi-level teachers when large capacity gap exists between the teacher and the student. Experimental results on GLUE benchmark show that SKDBERT reduces the size of a BERT model by 40% while retaining 99.5% performances of language understanding and being 100% faster.",,19,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85200551837,,,,SMOOTH ECE: PRINCIPLED RELIABILITY DIAGRAMS VIA KERNEL SMOOTHING,cp,Conference Paper,Błasiok J.,60030162;124945694,Columbia University;Apple,New York;Cambridge,United States;United States,2.0,"Błasiok, Jarosław;Nakkiran, Preetum",56017684200;56543740200,60030162;124945694,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most common constructions of reliability diagrams and calibration measures - binning and ECE - both suffer from well-known flaws (e.g. discontinuity). We show that a simple modification fixes both constructions: first smooth the observations using an RBF kernel, then compute the Expected Calibration Error (ECE) of this smoothed function. We prove that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved in the sense of Błasiok, Gopalan, Hu, and Nakkiran (2023) - a consistent calibration measure. We call this measure the SmoothECE. Moreover, the reliability diagram obtained from this smoothed function visually encodes the SmoothECE, just as binned reliability diagrams encode the BinnedECE. We also release a Python package with simple, hyperparameter-free methods for measuring and plotting calibration: `pip install relplot`. Code at: https://github.com/apple/ml-calibration.",,7,0.0,,,,,
2-s2.0-85150342808,,,,SOURCE-FREE ADAPTATION TO MEASUREMENT SHIFT VIA BOTTOM-UP FEATURE RESTORATION,cp,Conference Paper,Eastwood C.,60027272;60030569;60111768,The University of Edinburgh;Max Planck Institute for Intelligent Systems;The Alan Turing Institute,Edinburgh;Tubingen;London,United Kingdom;Germany;United Kingdom,4.0,"Eastwood, Cian;Mason, Ian;Williams, Christopher K.I.;Schölkopf, Bernhard",57210642314;57204397533;57199277310;7004460308,60027272-60030569;60027272;60027272-60111768;60030569,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift which can be resolved by restoring the source features rather than extracting new ones. In particular, we propose Feature Restoration (FR) wherein we: (i) store a lightweight and flexible approximation of the feature distribution under the source data; and (ii) adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We additionally propose a bottom-up training scheme which boosts performance, which we call Bottom-Up Feature Restoration (BUFR). On real and synthetic data, we demonstrate that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in the target domain.",,25,0.0,,,NUI,,National University of Ireland
2-s2.0-85178215651,,,,SPACETIME REPRESENTATION LEARNING,cp,Conference Paper,Law M.T.,60076695,NVIDIA,Santa Clara,United States,2.0,"Law, Marc T.;Lucas, James",57221812203;57204820834,60076695;60076695,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Much of the data we encounter in the real world can be represented as directed graphs. In this work, we introduce a general family of representations for directed graphs through connected time-oriented Lorentz manifolds, called “spacetimes” in general relativity. Spacetimes intrinsically contain a causal structure that indicates whether or not there exists a causal or even chronological order between points of the manifold, called events. This chronological order allows us to naturally represent directed edges via imposing the correct ordering when the nodes are embedded as events in the spacetime. Previous work in machine learning only considers embeddings lying on the simplest Lorentz manifold or does not exploit the connection between Lorentzian pre-length spaces and directed graphs. We introduce a well-defined approach to map data onto a general family of spacetimes. We empirically evaluate our framework in the tasks of hierarchy extraction of undirected graphs, directed link prediction and representation of directed graphs.",,4,0.0,,,,,
2-s2.0-85200573929,,,,SRL: SCALING DISTRIBUTED REINFORCEMENT LEARNING TO OVER TEN THOUSAND CORES,cp,Conference Paper,Mei Z.,60025278;130638464;131533992,Tsinghua University;Shanghai Qi Zhi Institute;OpenPsi Inc.,Beijing;Shanghai;,China;China;China,6.0,"Mei, Zhiyu;Fu, Wei;Gao, Jiaxuan;Wang, Guangju;Zhang, Huanchen;Wu, Yi",57258827600;57406754100;57313768000;58493179300;57190390737;55698586300,60025278-130638464;60025278-130638464;60025278-130638464;131533992;60025278-130638464;60025278-130638464-131533992,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed system to efficiently generate and process a massive amount of data. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies diverse RL training applications into a general framework. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL), which allows efficient and massively parallelized training and easy development of customized algorithms. Our evaluation shows that SRL outperforms existing academic libraries, reaching at most 21x higher training throughput in a distributed setting. On learning performance, beyond performing and scaling well on common RL benchmarks with different RL algorithms, SRL can reproduce the same solution in the challenging hide-and-seek environment as reported by OpenAI with up to 5x speedup in wall-clock time. Notably, SRL is the first in the academic community to perform RL experiments at a large scale with over 15k CPU cores. SRL source code is available at: https://github.com/openpsi-project/srl.",,3,0.0,,,,,
2-s2.0-85189510193,10.1609/aaai.v38i7.28489,,,STDiff: Spatio-Temporal Diffusion for Continuous Stochastic Video Prediction,cp,Conference Paper,Ye X.,60019141,Polytechnique Montréal,Montreal,Canada,2.0,"Ye, Xi;Bilodeau, Guillaume Alexandre",57574353700;6603906966,60019141;60019141,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,7.0,,6666-6674,"Predicting future frames of a video is challenging because it is difficult to learn the uncertainty of the underlying factors influencing their contents. In this paper, we propose a novel video prediction model, which has infinite-dimensional latent variables over the spatio-temporal domain. Specifically, we first decompose the video motion and content information, then take a neural stochastic differential equation to predict the temporal motion information, and finally, an image diffusion model autoregressively generates the video frame by conditioning on the predicted motion feature and the previous frame. The better expressiveness and stronger stochasticity learning capability of our model lead to state-of-the-art video prediction performances. As well, our model is able to achieve temporal continuous prediction, i.e., predicting in an unsupervised way the future video frames with an arbitrarily high frame rate. Our code is available at https://github.com/XiYe20/STDiffProject.",,12,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85189641220,10.1609/aaai.v38i8.28749,,,STEM: Unleashing the Power of Embeddings for Multi-Task Recommendation,cp,Conference Paper,Su L.,60025278;60114181,Tsinghua University;Tencent,Beijing;Shenzhen,China;China,7.0,"Su, Liangcai;Pan, Junwei;Wang, Ximei;Xiao, Xi;Quan, Shijie;Chen, Xihua;Jiang, Jie",57219503708;57207569483;57214129281;58774270700;58583287900;57999168100;56066182800,60025278;60114181;60114181;60025278;60114181;60114181;60114181,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,8.0,,9002-9010,"Multi-task learning (MTL) has gained significant popularity in recommender systems as it enables simultaneous optimization of multiple objectives. A key challenge in MTL is negative transfer, but existing studies explored negative transfer on all samples, overlooking the inherent complexities within them. We split the samples according to the relative amount of positive feedback among tasks. Surprisingly, negative transfer still occurs in existing MTL methods on samples that receive comparable feedback across tasks. Existing work commonly employs a shared-embedding paradigm, limiting the ability of modeling diverse user preferences on different tasks. In this paper, we introduce a novel Shared and Task-specific EMbeddings (STEM) paradigm that aims to incorporate both shared and task-specific embeddings to effectively capture task-specific user preferences. Under this paradigm, we propose a simple model STEM-Net, which is equipped with an All Forward Task-specific Backward gating network to facilitate the learning of task-specific embeddings and direct knowledge transfer across tasks. Remarkably, STEM-Net demonstrates exceptional performance on comparable samples, achieving positive transfer. Comprehensive evaluation on three public MTL recommendation datasets demonstrates that STEM-Net outperforms state-of-the-art models by a substantial margin. Our code is released at https://github.com/LiangcaiSu/STEM.",,27,1.0,all publisherfullgold,All Open Access Gold,NSFC,61972219,National Natural Science Foundation of China
2-s2.0-85186258349,,,,STR2STR: A SCORE-BASED FRAMEWORK FOR ZERO-SHOT PROTEIN CONFORMATION SAMPLING,cp,Conference Paper,Lu J.,60009507;60002970;101774889;125860468,University of Montreal;HEC Montréal;Quebec AI Institute;CIFAR AI,Montreal;Montreal;Quebec;,Canada;Canada;Canada;Canada,4.0,"Lu, Jiarui;Zhong, Bozitao;Zhang, Zuobai;Tang, Jian",57476205500;57224410252;57214940250;55713998400,101774889-60009507;101774889-60009507;101774889-60009507;101774889-60002970-125860468,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"The dynamic nature of proteins is crucial for determining their biological functions and properties, for which Monte Carlo (MC) and molecular dynamics (MD) simulations stand as predominant tools to study such phenomena. By utilizing empirically derived force fields, MC or MD simulations explore the conformational space through numerically evolving the system via Markov chain or Newtonian mechanics. However, the high-energy barrier of the force fields can hamper the exploration of both methods by the rare event, resulting in inadequately sampled ensemble without exhaustive running. Existing learning-based approaches perform direct sampling yet heavily rely on target-specific simulation data for training, which suffers from high data acquisition cost and poor generalizability. Inspired by simulated annealing, we propose STR2STR, a novel structure-to-structure translation framework capable of zero-shot conformation sampling with roto-translation equivariant property. Our method leverages an amortized denoising score matching objective trained on general crystal structures and has no reliance on simulation data during both training and inference. Experimental results across several benchmarking protein systems demonstrate that STR2STR outperforms previous state-of-the-art generative structure prediction models and can be orders of magnitude faster compared to long MD simulations. Our open-source implementation is available at https://github.com/lujiarui/Str2Str.",,14,0.0,,,NSERC,AI4D-CORE-06,Intel Corporation
2-s2.0-85168102900,,,,SUPERVISION COMPLEXITY AND ITS ROLE IN KNOWLEDGE DISTILLATION,cp,Conference Paper,Harutyunyan H.,60006191;60015400,Google LLC;Information Sciences Institute,Mountain View;Marina del Rey,United States;United States,5.0,"Harutyunyan, Hrayr;Rawat, Ankit Singh;Menon, Aditya Krishna;Kim, Seungyeon;Kumar, Sanjiv",57213670235;55406839900;36179673700;53663573000;57778027300,60015400;60006191;60006191;60006191;60006191,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Despite the popularity and efficacy of knowledge distillation, there is limited understanding of why it helps. In order to study the generalization behavior of a distilled student, we propose a new theoretical framework that leverages supervision complexity: a measure of alignment between teacher-provided supervision and the student's neural tangent kernel. The framework highlights a delicate interplay among the teacher's accuracy, the student's margin with respect to the teacher predictions, and the complexity of the teacher predictions. Specifically, it provides a rigorous justification for the utility of various techniques that are prevalent in the context of distillation, such as early stopping and temperature scaling. Our analysis further suggests the use of online distillation, where a student receives increasingly more complex supervision from teachers in different stages of their training. We demonstrate efficacy of online distillation and validate the theoretical findings on a range of image classification benchmarks and model architectures.",,13,0.0,,,,,
2-s2.0-85142758691,,,,SYMBOLIC LEARNING TO OPTIMIZE: TOWARDS IN-TERPRETABILITY AND SCALABILITY,cp,Conference Paper,Zheng W.,60013372;60020547,The University of Texas at Austin;Texas A&M University,Austin;College Station,United States;United States,4.0,"Zheng, Wenqing;Chen, Tianlong;Hu, Ting Kuei;Wang, Zhangyang",57216368678;57221072108;57201681484;56288839400,60013372;60013372;60020547;60013372,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Recent studies on Learning to Optimize (L2O) suggest a promising path to automating and accelerating the optimization procedure for complicated tasks. Existing L2O models parameterize optimization rules by neural networks, and learn those numerical rules via meta-training. However, they face two common pitfalls: (1) scalability: the numerical rules represented by neural networks create extra memory overhead for applying L2O models, and limit their applicability to optimizing larger tasks; (2) interpretability: it is unclear what an L2O model has learned in its black-box optimization rule, nor is it straightforward to compare different L2O models in an explainable way. To avoid both pitfalls, this paper proves the concept that we can “kill two birds by one stone”, by introducing the powerful tool of symbolic regression to L2O. In this paper, we establish a holistic symbolic representation and analysis framework for L2O, which yields a series of insights for learnable optimizers. Leveraging our findings, we further propose a lightweight L2O model that can be meta-trained on large-scale problems and outperformed human-designed and tuned optimizers. Our work is set to supply a brand-new perspective to L2O research. Codes are available at: https://github.com/VITA-Group/Symbolic-Learning-To-Optimize.",,16,0.0,,,,,
2-s2.0-85170386139,10.24963/ijcai.2023/637,,,Safe Reinforcement Learning via Probabilistic Logic Shields,cp,Conference Paper,Yang W.C.,60025063;60001565;60008141,KU Leuven;Stellenbosch University;Örebro Universitet,Leuven;Stellenbosch;Orebro,Belgium;South Africa;Sweden,4.0,"Yang, Wen Chi;Marra, Giuseppe;Rens, Gavin;De Raedt, Luc",57223722730;57204181546;36716764700;55760010700,60025063;60025063;60001565;60025063-60008141,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,5739-5749,"Safe Reinforcement learning (Safe RL) aims at learning optimal policies while staying safe. A popular solution to Safe RL is shielding, which uses a logical safety specification to prevent an RL agent from taking unsafe actions. However, traditional shielding techniques are difficult to integrate with continuous, end-to-end deep RL methods. To this end, we introduce Probabilistic Logic Policy Gradient (PLPG). PLPG is a model-based Safe RL technique that uses probabilistic logic programming to model logical safety constraints as differentiable functions. Therefore, PLPG can be seamlessly applied to any policy gradient algorithm while still providing the same convergence guarantees. In our experiments, we show that PLPG learns safer and more rewarding policies compared to other state-of-the-art shielding techniques.",,20,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,H2020,952215,Vlaamse regering
2-s2.0-85153850754,,,,Safety Guarantees for Neural Network Dynamic Systems via Stochastic Barrier Functions,cp,Conference Paper,Mazouz R.,60154476;60118244,College of Engineering and Applied Science;Delft Center for Systems and Control,Boulder;Delft,United States;Netherlands,5.0,"Mazouz, Rayan;Muvvala, Karan;Ratheesh, Akash;Laurenti, Luca;Lahijanian, Morteza",57221801150;57558285100;59579137600;56938263200;35105134400,60154476;60154476;60154476;60118244;60154476,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Neural Networks (NNs) have been successfully employed to represent the state evolution of complex dynamical systems. Such models, referred to as NN dynamic models (NNDMs), use iterative noisy predictions of NN to estimate a distribution of system trajectories over time. Despite their accuracy, safety analysis of NNDMs is known to be a challenging problem and remains largely unexplored. To address this issue, in this paper, we introduce a method of providing safety guarantees for NNDMs. Our approach is based on stochastic barrier functions, whose relation with safety are analogous to that of Lyapunov functions with stability. We first show a method of synthesizing stochastic barrier functions for NNDMs via a convex optimization problem, which in turn provides a lower bound on the system's safety probability. A key step in our method is the employment of the recent convex approximation results for NNs to find piece-wise linear bounds, which allow the formulation of the barrier function synthesis problem as a sum-of-squares optimization program. If the obtained safety probability is above the desired threshold, the system is certified. Otherwise, we introduce a method of generating controls for the system that robustly minimize the unsafety probability in a minimally-invasive manner. We exploit the convexity property of the barrier function to formulate the optimal control synthesis problem as a linear program. Experimental results illustrate the efficacy of the method. Namely, they show that the method can scale to multi-dimensional NNDMs with multiple layers and hundreds of neurons per layer, and that the controller can significantly improve the safety probability.",,23,0.0,,,NSF,2039062,National Science Foundation
2-s2.0-85189558764,10.1609/aaai.v38i12.29310,,,Sample-Level Cross-View Similarity Learning for Incomplete Multi-View Clustering,cp,Conference Paper,Liu S.,60024350;60006019;60011592;130608941,National University of Defense Technology China;China University of Geosciences;Qilu University of Technology;Intelligent Game and Decision Lab (IGDL),Changsha;Wuhan;Jinan;Beijing,China;China;China;China,10.0,"Liu, Suyuan;Zhang, Junpu;Wen, Yi;Yang, Xihong;Wang, Siwei;Zhang, Yi;Zhu, En;Tang, Chang;Zhao, Long;Liu, Xinwang",57702797800;57814410400;58146642000;57407105600;57209196796;55627871700;55363876800;55756740200;57193280443;36782771600,60024350;60024350;60024350;60024350;130608941;60024350;60024350;60006019;60011592;60024350,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,12.0,,14017-14025,"Incomplete multi-view clustering has attracted much attention due to its ability to handle partial multi-view data. Recently, similarity-based methods have been developed to explore the complete relationship among incomplete multiview data. Although widely applied to partial scenarios, most of the existing approaches are still faced with two limitations. Firstly, fusing similarities constructed individually on each view fails to yield a complete unified similarity. Moreover, incomplete similarity generation may lead to anomalous similarity values with column sum constraints, affecting the final clustering results. To solve the above challenging issues, we propose a Sample-level Cross-view Similarity Learning (SCSL) method for Incomplete Multi-view Clustering. Specifically, we project all samples to the same dimension and simultaneously construct a complete similarity matrix across views based on the inter-view sample relationship and the intra-view sample relationship. In addition, a simultaneously learning consensus representation ensures the validity of the projection, which further enhances the quality of the similarity matrix through the graph Laplacian regularization. Experimental results on six benchmark datasets demonstrate the ability of SCSL in processing incomplete multi-view clustering tasks. Our code is publicly available at https://github.com/Tracesource/SCSL.",,45,1.0,all publisherfullgold,All Open Access Gold,NKRDPC,2022ZD0209103,National Key Research and Development Program of China
2-s2.0-85190947288,,,,Sample-efficient Adversarial Imitation Learning,ar,Article,Jung D.,60120124,Department of Electrical and Computer Engineering,Seoul,South Korea,3.0,"Jung, Dahuin;Lee, Hyungyu;Yoon, Sungroh",57212136344;57219632377;58142622900,60120124;60120124;60120124,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,1-32,"Imitation learning, in which learning is performed by demonstration, has been studied and advanced for sequential decision-making tasks in which a reward function is not predefined. However, imitation learning methods still require numerous expert demonstration samples to successfully imitate an expert’s behavior. To improve sample efficiency, we utilize self-supervised representation learning, which can generate vast training signals from the given data. In this study, we propose a self-supervised representation-based adversarial imitation learning method to learn state and action representations that are robust to diverse distortions and temporally predictive, on non-image control tasks. In particular, in comparison with existing self-supervised learning methods for tabular data, we propose a different corruption method for state and action representations that is robust to diverse distortions. We theoretically and empirically observe that making an informative feature manifold with less sample complexity significantly improves the performance of imitation learning. The proposed method shows a 39% relative improvement over existing adversarial imitation learning methods on MuJoCo in a setting limited to 100 expert state-action pairs. Moreover, we conduct comprehensive ablations and additional experiments using demonstrations with varying optimality to provide insights into a range of factors.",adversarial imitation learning | data efficiency | imitation learning | self-supervised learning,4,0.0,,,NRF,2022R1A3B1077720,National Research Foundation of Korea
2-s2.0-85124188201,,,,Sampling Permutations for Shapley Value Estimation,ar,Article,Mitchell R.,60018179;60004424;60076695,University of South Carolina;University of Waikato;NVIDIA,Columbia;Hamilton;Santa Clara,United States;New Zealand;United States,4.0,"Mitchell, Rory;Cooper, Joshua;Frank, Eibe;Holmes, Geoffrey",55448682500;57203556577;7202332302;24779409500,60076695;60018179;60004424;60004424,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"Game-theoretic attribution techniques based on Shapley values are used to interpret black-box machine learning models, but their exact calculation is generally NP-hard, requiring approximation methods for non-trivial models. As the computation of Shapley values can be expressed as a summation over a set of permutations, a common approach is to sample a subset of these permutations for approximation. Unfortunately, standard Monte Carlo sampling methods can exhibit slow convergence, and more sophisticated quasi-Monte Carlo methods have not yet been applied to the space of permutations. To address this, we investigate new approaches based on two classes of approximation methods and compare them empirically. First, we demonstrate quadrature techniques in a RKHS containing functions of permutations, using the Mallows kernel in combination with kernel herding and sequential Bayesian quadrature. The RKHS perspective also leads to quasi-Monte Carlo type error bounds, with a tractable discrepancy measure defined on permutations. Second, we exploit connections between the hypersphere S<sup>d−</sup><sup>2</sup> and permutations to create practical algorithms for generating permutation samples with good properties. Experiments show the above techniques provide significant improvements for Shapley value estimates over existing methods, converging to a smaller RMSE in the same number of model evaluations.",Interpretability | Quasi-Monte Carlo | Shapley values,100,0.0,,,,,
2-s2.0-85162127350,,,,Sampling in Constrained Domains with Orthogonal-Space Variational Gradient Descent,cp,Conference Paper,Zhang R.,60017161;60009254;60150459,National University of Singapore;Purdue University;Department of Computer Science,Singapore City;West Lafayette;Austin,Singapore;United States;United States,3.0,"Zhang, Ruqi;Liu, Qiang;Tong, Xin T.",57218715795;55712150300;56341373400,60009254;60150459;60017161,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Sampling methods, as important inference and learning techniques, are typically designed for unconstrained domains. However, constraints are ubiquitous in machine learning problems, such as those on safety, fairness, robustness, and many other properties that must be satisfied to apply sampling results in real-life applications. Enforcing these constraints often leads to implicitly-defined manifolds, making efficient sampling with constraints very challenging. In this paper, we propose a new variational framework with a designed orthogonal-space gradient flow (O-Gradient) for sampling on a manifold G<inf>0</inf> defined by general equality constraints. O-Gradient decomposes the gradient into two parts: one decreases the distance to G<inf>0</inf> and the other decreases the KL divergence in the orthogonal space. While most existing manifold sampling methods require initialization on G<inf>0</inf>, O-Gradient does not require such prior knowledge. We prove that O-Gradient converges to the target constrained distribution with rate O<sup>e</sup>(1/the number of iterations) under mild conditions. Our proof relies on a new Stein characterization of conditional measure which could be of independent interest. We implement O-Gradient through both Langevin dynamics and Stein variational gradient descent and demonstrate its effectiveness in various experiments, including Bayesian deep neural networks.",,7,0.0,,,ONR,EAGER-2041327,Office of Naval Research
2-s2.0-85163057490,,,,Sanity Simulations for Saliency Methods,cp,Conference Paper,Kim J.S.,60136640,School of Computer Science,Pittsburgh,United States,3.0,"Kim, Joon Sik;Plumb, Gregory;Talwalkar, Ameet",57208438670;57213116779;24829809300,60136640;60136640;60136640,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,11173-11200,"Saliency methods are a popular class of feature attribution explanation methods that aim to capture a model's predictive reasoning by identifying ""important"" pixels in an input image. However, the development and adoption of these methods are hindered by the lack of access to ground-truth model reasoning, which prevents accurate evaluation. In this work, we design a synthetic benchmarking framework, SMERF, that allows us to perform ground-truth-based evaluation while controlling the complexity of the model's reasoning. Experimentally, SMERF reveals significant limitations in existing saliency methods and, as a result, represents a useful tool for the development of new saliency methods.",,5,0.0,,,NSF,IIS1705121,National Science Foundation
2-s2.0-85137931250,10.24963/ijcai.2022/138,,,SatFormer: Saliency-Guided Abnormality-Aware Transformer for Retinal Disease Classification in Fundus Image,cp,Conference Paper,Jiang Y.,60029310;60117933,Zhejiang University School of Medicine;National Key Laboratory of Computer-Aided Design and Graphics Systems,Hangzhou;Hangzhou,China;China,7.0,"Jiang, Yankai;Xu, Ke;Wang, Xinyue;Li, Yuan;Cui, Hongguang;Tao, Yubo;Lin, Hai",57218162346;58366389400;59652093800;57238031000;14624773700;8244401500;57188693150,60117933;60117933;60117933;60117933;60029310;60117933;60117933,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,987-994,"Automatic and accurate retinal disease diagnosis is critical to guide proper therapy and prevent potential vision loss. Previous works simply exploit the most discriminative features while ignoring the pathological visual clues of scattered subtle lesions. Therefore, without a comprehensive understanding of features from different lesion regions, they are vulnerable to noise from complex backgrounds and suffer from misclassification failures. In this paper, we address these limitations with a novel saliency-guided abnormality-aware transformer which explicitly captures the correlation between different lesion features from a global perspective with enhanced pathological semantics. The model has several merits. First, we propose a saliency enhancement module (SEM) which adaptively integrates disease related semantics and highlights potentially salient lesion regions. Second, to the best of our knowledge, this is the first work to explore comprehensive lesion feature dependencies via a tailored efficient self-attention. Third, with the saliency enhancement module and abnormality-aware attention, we propose a new variant of Vision Transformer models, called SatFormer, which outperforms the state-of-the-art methods on two public retinal disease classification benchmarks. Ablation study shows that the proposed components can be easily embedded into any Vision Transformers via a plug-and-play manner and effectively boost the performance.",,12,1.0,all publisherfree2read,All Open Access Bronze,NSFC,2021C03032,Key Technology Research and Development Program of Shandong
2-s2.0-85170367180,10.24963/ijcai.2023/402,,,Scalable Coupling of Deep Learning with Logical Reasoning,cp,Conference Paper,Defresne M.,60102124,Communauté d'universités et établissements de Toulouse,Toulouse,France,3.0,"Defresne, Marianne;Barbe, Sophie;Schiex, Thomas",57315339700;6603003733;6701725548,60102124;60102124;60102124,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,3615-3623,"In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs. In this paper, we introduce a scalable neural architecture and loss function dedicated to learning the constraints and criteria of NP-hard reasoning problems expressed as discrete Graphical Models. Our loss function solves one of the main limitations of Besag's pseudo-loglikelihood, enabling learning of high energies. We empirically show it is able to efficiently learn how to solve NP-hard reasoning problems from natural inputs as the symbolic, visual or many-solutions Sudoku problems as well as the energy optimization formulation of the protein design problem, providing data efficiency, interpretability, and a posteriori control over predictions.",,9,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,
2-s2.0-85204305818,10.1613/jair.1.15484,,,Scalable Distributed Algorithms for Size-Constrained Submodular Maximization in the MapReduce and Adaptive Complexity Models,ar,Article,Chen Y.,60148980;60144989,College of Engineering;Department of Computer Science,College Station;Tallahassee,United States;United States,3.0,"Chen, Yixin;Dey, Tonmoy;Kuhnle, Alan",57353556300;57219181768;56419692900,60148980;60144989;60148980,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,1575-1622,"Distributed maximization of a submodular function in the MapReduce (MR) model has received much attention, culminating in two frameworks that allow a centralized algorithm to be run in the MR setting without loss of approximation, as long as the centralized algorithm satisfies a certain consistency property - which had previously only been known to be satisfied by the standard greedy and continous greedy algorithms. A separate line of work has studied parallelizability of submodular maximization in the adaptive complexity model, where each thread may have access to the entire ground set. For the size-constrained maximization of a monotone and submodular function, we show that several sublinearly adaptive (highly parallelizable) algorithms satisfy the consistency property required to work in the MR setting, which yields practical, parallelizable and distributed algorithms. Separately, we develop the first distributed algorithm with linear query complexity for this problem. Finally, we provide a method to increase the maximum cardinality constraint for MR algorithms at the cost of additional MR rounds.",,0,1.0,all publisherfullgold,All Open Access Gold,FSU,,Florida State University
2-s2.0-85203847276,,,,Scalable Multiple Kernel Clustering: Learning Clustering Structure from Expectation,cp,Conference Paper,Liang W.,60024350;60003078,National University of Defense Technology China;Zhejiang Normal University,Changsha;Jinhua,China;China,6.0,"Liang, Weixuan;Zhu, En;Yu, Shengju;Xu, Huiying;Zhu, Xinzhong;Liu, Xinwang",57219787404;55363876800;57204110175;55703813700;7406187646;36782771600,60024350;60024350;60024350;60003078;60003078;60024350,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,29700-29719,"In this paper, we derive an upper bound of the difference between a kernel matrix and its expectation under a mild assumption. Specifically, we assume that the true distribution of the training data is an unknown isotropic Gaussian distribution. When the kernel function is a Gaussian kernel, and the mean of each cluster is sufficiently separated, we find that the expectation of a kernel matrix can be close to a rank-k matrix, where k is the cluster number. Moreover, we prove that the normalized kernel matrix of the training set deviates (w.r.t. Frobenius norm) from its expectation in the order of Õ(1/√d), where d is the dimension of samples. Based on the above theoretical results, we propose a novel multiple kernel clustering framework which attempts to learn the information of the expectation kernel matrices. First, we aim to minimize the distance between each base kernel and a rank-k matrix, which is a proxy of the expectation kernel. Then, we fuse these rank-k matrices into a consensus rank-k matrix to find the clustering structure. Using an anchor-based method, the proposed framework is flexible with the sizes of input kernel matrices and able to handle large-scale datasets. We also provide the approximation guarantee by deriving two non-asymptotic bounds for the consensus kernel and clustering indicator matrices. Finally, we conduct extensive experiments to verify the clustering performance of the proposed method and the correctness of the proposed theoretical results.",,4,0.0,,,NSFC,62376039,National Natural Science Foundation of China
2-s2.0-85178997369,10.1613/JAIR.1.15027,,,Scalable Neural-Probabilistic Answer Set Programming,ar,Article,Skryagin A.,60011226;60075096,Technische Universität Darmstadt;German Research Center for Artificial Intelligence (DFKI),Darmstadt;Kaiserslautern,Germany;Germany,4.0,"Skryagin, Arseny;Ochs, Daniel;Dhami, Devendra Singh;Kersting, Kristian",57219292666;57305286600;57194685256;57189007679,60011226;60011226;60011226;60075096,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,579-617,"The goal of combining the robustness of neural networks and the expressiveness of symbolic methods has rekindled the interest in Neuro-Symbolic AI. Deep Probabilistic Programming Languages (DPPLs) have been developed for probabilistic logic programming to be carried out via the probability estimations of deep neural networks (DNNs). However, recent SOTA DPPL approaches allow only for limited conditional probabilistic queries and do not offer the power of true joint probability estimation. In our work, we propose an easy integration of tractable probabilistic inference within a DPPL. To this end, we introduce SLASH, a novel DPPL that consists of Neural-Probabilistic Predicates (NPPs) and a logic program, united via answer set programming (ASP). NPPs are a novel design principle allowing for combining all deep model types and combinations thereof to be represented as a single probabilistic predicate. In this context, we introduce a novel +/− notation for answering various types of probabilistic queries by adjusting the atom notations of a predicate. To scale well, we show how to prune the stochastically insignificant parts of the (ground) program, speeding up reasoning without sacrificing the predictive performance. We evaluate SLASH on various tasks, including the benchmark task of MNIST addition and Visual Question Answering (VQA).",,5,1.0,all publisherfullgold,All Open Access Gold,BMBF,952215,Bundesministerium für Bildung und Forschung
2-s2.0-85170378500,10.24963/ijcai.2023/485,,,Scalable Optimal Margin Distribution Machine,cp,Conference Paper,Wang Y.,60025761,Huazhong University of Science and Technology,Wuhan,China,5.0,"Wang, Yilin;Cao, Nan;Zhang, Teng;Shi, Xuanhua;Jin, Hai",59453278700;57470921600;56355330300;8935128100;56434989100,60025761;60025761;60025761;60025761;60025761,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,4362-4370,"Optimal margin Distribution Machine (ODM) is a newly proposed statistical learning framework rooting in the latest margin theory, which demonstrates better generalization performance than the traditional large margin based counterparts. However, it suffers from the ubiquitous scalability problem regarding both computation time and memory storage as other kernel methods. This paper proposes a scalable ODM, which can achieve nearly ten times speedup compared to the original ODM training method. For nonlinear kernels, we put forward a novel distribution-aware partition method to make the local ODM trained on each partition be close and converge fast to the global one. When linear kernel is applied, we extend a communication efficient SVRG method to accelerate the training further. Extensive empirical studies validate that our proposed method is highly computational efficient and almost never worsen the generalization.",,0,1.0,all publisherfullgold,All Open Access Gold,NSFC,2020BAA020,National Natural Science Foundation of China
2-s2.0-85196098521,10.1613/jair.1.14972,,,Scalable Primal Heuristics Using Graph Neural Networks for Combinatorial Optimization,ar,Article,Cantürk F.,60006288;60086558,Delft University of Technology;Özyeğin Üniversitesi,Delft;Istanbul,Netherlands;Turkey,4.0,"Cantürk, Furkan;Varol, Taha;Aydoğan, Reyhan;Özener, Okan Örsan",57227251700;58158815800;24463667000;57215569521,60086558;60086558;60086558-60006288;60086558,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,327-376,"By examining the patterns of solutions obtained for various instances, one can gain insights into the structure and behavior of combinatorial optimization (CO) problems and develop efficient algorithms for solving them. Machine learning techniques, especially Graph Neural Networks (GNNs), have shown promise in parametrizing and automating this laborious design process. The inductive bias of GNNs allows for learning solutions to mixed-integer programming (MIP) formulations of constrained CO problems with a relational representation of decision variables and constraints. The trained GNNs can be leveraged with primal heuristics to construct high-quality feasible solutions to CO problems quickly. However, current GNN-based end-to-end learning approaches have limitations for scalable training and generalization on larger-scale instances; therefore, they have been mostly evaluated over small-scale instances. Addressing this issue, our study builds on supervised learning of optimal solutions to the downscaled instances of given large-scale CO problems. We introduce several improvements on a recent GNN model for CO to generalize on instances of a larger scale than those used in training. We also propose a two-stage primal heuristic strategy based on uncertainty-quantification to automatically configure how solution search relies on the predicted decision values. Our models can generalize on 16x upscaled instances of commonly benchmarked five CO problems. Unlike the regressive performance of existing GNN-based CO approaches as the scale of problems increases, the CO pipelines using our models offer an incremental performance improvement relative to CPLEX. The proposed uncertainty-based primal heuristics provide 6-75% better optimality gap values and 45-99% better primal gap values for the 16x upscaled instances and brings immense speedup to obtain high-quality solutions. All these gains are achieved through a computationally efficient modeling approach without sacrificing solution quality.",,1,1.0,all publisherfullgold,All Open Access Gold,TÜBİTAK,120N680,Türkiye Bilimsel ve Teknolojik Araştırma Kurumu
2-s2.0-85136126047,,,,Scalable Spike-and-Slab,cp,Conference Paper,Biswas N.,60009982;60021726,Harvard University;Microsoft Research,Cambridge;Redmond,United States;United States,3.0,"Biswas, Niloy;Mackey, Lester;Meng, Xiao Li",57204163522;57214520890;7401629915,60009982;60021726;60009982,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,2021-2040,"Spike-and-slab priors are commonly used for Bayesian variable selection, due to their interpretability and favorable statistical properties. However, existing samplers for spike-and-slab posteriors incur prohibitive computational costs when the number of variables is large. In this article, we propose Scalable Spike-and-Slab (S<sup>3</sup>), a scalable Gibbs sampling implementation for high-dimensional Bayesian regression with the continuous spike-and-slab prior of George & McCulloch (1993). For a dataset with n observations and p covariates, S<sup>3</sup> has order max(n<sup>2</sup>p<inf>t</inf>, np) computational cost at iteration t where p<inf>t</inf> never exceeds the number of covariates switching spike- and-slab states between iterations t and t − 1 of the Markov chain. This improves upon the order n<sup>2</sup>p per-iteration cost of state-of-the-art implementations as, typically, p<inf>t</inf> is substantially smaller than p. We apply S<sup>3</sup> on synthetic and real-world datasets, demonstrating orders of magnitude speed-ups over existing exact samplers and significant gains in inferential quality over approximate samplers with comparable cost.",,8,0.0,,,NSF,DMS-1844695,National Science Foundation
2-s2.0-85137901725,10.24963/ijcai.2022/135,,,ScaleFormer: Revisiting the Transformer-based Backbones from a Scale-wise Perspective for Medical Image Segmentation,cp,Conference Paper,Huang H.,60003970;60020739;60031848;125070492,Zhejiang University;Yamaguchi University;Ritsumeikan University;Zhejiang Lab,Hangzhou;Yamaguchi;Kyoto;Hangzhou,China;Japan;Japan;China,7.0,"Huang, Huimin;Xie, Shiao;Lin, Lanfen;Iwamoto, Yutaro;Han, Xian Hua;Chen, Yen Wei;Tong, Ruofeng",57218083631;57345847900;7404131803;54408573200;24080007200;56036268200;7006595990,60003970;60003970;60003970;60031848;60020739;60031848;60003970-125070492,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,964-971,"Recently, a variety of vision transformers have been developed as their capability of modeling long-range dependency. In current transformer-based backbones for medical image segmentation, convolutional layers were replaced with pure transformers, or transformers were added to the deepest encoder to learn global context. However, there are mainly two challenges in a scale-wise perspective: (1) intra-scale problem: the existing methods lacked in extracting local-global cues in each scale, which may impact the signal propagation of small objects; (2) inter-scale problem: the existing methods failed to explore distinctive information from multiple scales, which may hinder the representation learning from objects with widely variable size, shape and location. To address these limitations, we propose a novel backbone, namely ScaleFormer, with two appealing designs: (1) A scale-wise intra-scale transformer is designed to couple the CNN-based local features with the transformer-based global cues in each scale, where the row-wise and column-wise global dependencies can be extracted by a lightweight Dual-Axis MSA. (2) A simple and effective spatial-aware inter-scale transformer is designed to interact among consensual regions in multiple scales, which can highlight the cross-scale dependency and resolve the complex scale variations. Experimental results on different benchmarks demonstrate that our ScaleFormer outperforms the current state-of-the-art methods. The code is publicly available at: https://github.com/ZJUGiveLab/ScaleFormer.",,27,1.0,all publisherfree2read,All Open Access Bronze,MEXT,2020ND8AD01,"Ministry of Education, Culture, Sports, Science and Technology"
2-s2.0-85205740026,,,,Scaling Data-Constrained Language Models,cp,Conference Paper,Muennighoff N.,60009982;60006876;129019764,Harvard University;Turun yliopisto;Hugging Face,Cambridge;Turku;Berkeley,United States;Finland;United States,9.0,"Muennighoff, Niklas;Rush, Alexander M.;Barak, Boaz;Le Scao, Teven;Piktus, Aleksandra;Tazi, Nouamane;Pyysalo, Sampo;Wolf, Thomas;Raffel, Colin",57221839796;51665746500;55909951700;57219591230;57216968819;57869458000;8537000700;57207883536;55354986300,129019764;129019764;60009982;129019764;129019764;129019764;60006876;129019764;129019764,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.",,130,0.0,,,CSC,101070350,China Scholarship Council
2-s2.0-85124248843,,,,Scaling Laws from the Data Manifold Dimension,ar,Article,Sharma U.,60005248,Johns Hopkins University,Baltimore,United States,2.0,"Sharma, Utkarsh;Kaplan, Jared",57213032897;15762464300,60005248;60005248,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"When data is plentiful, the test loss achieved by well-trained neural networks scales as a power-law L / N- in the number of network parameters N. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension d. This simple theory predicts that the scaling exponents α ≈ 4=d for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of d and α by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.",Data manifold | Intrinsic dimension | Model capacity | Scaling laws | Under-parameterized,34,0.0,,,NSF,PHY-1454083,National Science Foundation
2-s2.0-85203792454,,,,Scaling Rectified Flow Transformers for High-Resolution Image Synthesis,cp,Conference Paper,Esser P.,131684026,Stability AI,,,14.0,"Esser, Patrick;Kulal, Sumith;Blattmann, Andreas;Entezari, Rahim;Müller, Jonas;Saini, Harry;Levi, Yam;Lorenz, Dominik;Sauer, Axel;Boesel, Frederic;Podell, Dustin;Dockhorn, Tim;English, Zion;Rombach, Robin",57206656989;57192925328;57212344164;56483297600;57963135800;58960696200;58749818500;57214473930;59886259500;57222440668;58505785100;57219636645;58504693400;57219706721,131684026;131684026;131684026;131684026;131684026;131684026;131684026;131684026;131684026;131684026;131684026;131684026;131684026;131684026,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,12606-12633,"Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models. Stability AI is considering making experimental data, code, and model weights publicly available.",,123,0.0,,,,,
2-s2.0-85174401427,,,,SeMAIL: Eliminating Distractors in Visual Imitation via Separated Models,cp,Conference Paper,Wan S.,60033100,Nanjing University,Nanjing,China,5.0,"Wan, Shenghua;Wang, Yucen;Shao, Minghao;Chen, Ruying;Zhan, De Chuan",57959354900;58476502000;58476736400;58476038600;56186431200,60033100;60033100;60033100;60033100;60033100,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,35426-35443,"Model-based imitation learning (MBIL) is a popular reinforcement learning method that improves sample efficiency on high-dimension input sources, such as images and videos. Following the convention of MBIL research, existing algorithms are highly deceptive by task-irrelevant information, especially moving distractors in videos. To tackle this problem, we propose a new algorithm - named Separated Model-based Adversarial Imitation Learning (SeMAIL) - decoupling the environment dynamics into two parts by task-relevant dependency, which is determined by agent actions, and training separately. In this way, the agent can imagine its trajectories and imitate the expert behavior efficiently in task-relevant state space. Our method achieves near-expert performance on various visual control tasks with complex observations and the more challenging tasks with different backgrounds from expert observations.",,8,0.0,,,NKRDPC,2022ZD0114805,National Key Research and Development Program of China
2-s2.0-105018671617,,,,Seeded Graph Matching for the Correlated Gaussian Wigner Model via the Projected Power Method,ar,Article,Araya E.,60016648;60107837,Riken;Laboratoire Paul Painlevé (LPP),Wako;Villeneuve-d'Ascq,Japan;France,3.0,"Araya, Ernesto;Braun, Guillaume;Tyagi, Hemant",59454752000;57222421675;55257535000,60107837;60016648;60107837,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"In the graph matching problem we observe two graphs G, H and the goal is to find an assignment (or matching) between their vertices such that some measure of edge agreement is maximized. We assume in this work that the observed pair G, H has been drawn from the Correlated Gaussian Wigner (CGW) model – a popular model for correlated weighted graphs – where the entries of the adjacency matrices of G and H are independent Gaussians and each edge of G is correlated with one edge of H (determined by the unknown matching) with the edge correlation described by a parameter σ ∈ [0, 1). In this paper, we analyse the performance of the projected power method (PPM) as a seeded graph matching algorithm where we are given an initial partially correct matching (called the seed) as side information. We prove that if the seed is close enough to the ground-truth matching, then with high probability, PPM iteratively improves the seed and recovers the ground-truth matching (either partially or exactly) in O(log n) iterations. Our results prove that PPM works even in regimes of constant σ, thus extending the analysis in (Mao et al., 2023) for the sparse Correlated Erdős-Renyi (CER) model to the (dense) CGW model. As a byproduct of our analysis, we see that the PPM framework generalizes some of the state-of-art algorithms for seeded graph matching. We support and complement our theoretical findings with numerical experiments on synthetic data.",correlated Wigner model | graph matching | projected power method,4,0.0,,,,,
2-s2.0-85184855196,10.1613/JAIR.1.14365,,,Select and Augment: Enhanced Dense Retrieval Knowledge Graph Augmentation,ar,Article,Abaho M.,60020661;60110514,University of Liverpool;University of Tabuk,Liverpool;Tabuk,United Kingdom;Saudi Arabia,2.0,"Abaho, Micheal;Alfaifi, Yousef H.",57205728689;57195110483,60020661;60110514,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,78,,,269-285,"Injecting textual information into knowledge graph (KG) entity representations has been a worthwhile expedition in terms of improving performance in KG oriented tasks within the NLP community. External knowledge often adopted to enhance KG embeddings ranges from semantically rich lexical dependency parsed features to a set of relevant key words to entire text descriptions supplied from an external corpus such as wikipedia and many more. Despite the gains this innovation (Text-enhanced KG embeddings) has made, the proposal in this work suggests that it can be improved even further. Instead of using a single text description (which would not sufficiently represent an entity because of the inherent lexical ambiguity of text), we propose a multi-task framework that jointly selects a set of text descriptions relevant to KG entities as well as align or augment KG embeddings with text descriptions. Different from prior work that plugs formal entity descriptions declared in knowledge bases, this framework leverages a retriever model to selectively identify richer or highly relevant text descriptions to use in augmenting entities. Furthermore, the framework treats the number of descriptions to use in augmentation process as a parameter, which allows the flexibility of enumerating across several numbers before identifying an appropriate number. Experiment results for Link Prediction demonstrate a 5.5% and 3.5% percentage increase in the Mean Reciprocal Rank (MRR) and Hits@10 scores respectively, in comparison to text-enhanced knowledge graph augmentation methods using traditional CNNs.",,4,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85191149606,,,,Selective Sampling and Imitation Learning via Online Regression,cp,Conference Paper,Sekhari A.,60022195;60007776,Massachusetts Institute of Technology;Cornell University,Cambridge;Ithaca,United States;United States,4.0,"Sekhari, Ayush;Sridharan, Karthik;Sun, Wen;Wu, Runzhe",26640679500;56731229500;57218716268;58128090600,60022195;60007776;60007776;60007776,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries. Our algorithm for selective sampling leverages function approximation, and relies on an online regression oracle w.r.t. the given model class to predict actions, and to decide whether to query the expert for its label. On the theoretical side, the regret bound of our algorithm is upper bounded by the regret of the online regression oracle, while the query complexity additionally depends on the eluder dimension of the model class. We complement this with a lower bound that demonstrates that our results are tight. We extend our selective sampling algorithm for IL with general function approximation and provide bounds on both the regret and the number of queries made to the noisy expert. A key novelty here is that our regret and query complexity bounds only depend on the number of times the optimal policy (and not the noisy expert, or the learner) go to states that have a small margin.",,6,0.0,,,SF,DMS-2031883,Simons Foundation
2-s2.0-85189546541,10.1609/aaai.v38i6.28419,,,Selective and Orthogonal Feature Activation for Pedestrian Attribute Recognition,cp,Conference Paper,Wu J.,60017605;60018486;112080275;130565683,Fuzhou University;Institute of Automation Chinese Academy of Sciences;Xiamen Meiya Pico Information Company Ltd.;Xiamen Meiya Pico Information Security Research Institute Company Ltd.,Fuzhou;Beijing;Xiamen;Xiamen,China;China;China;China,7.0,"Wu, Junyi;Huang, Yan;Gao, Min;Niu, Yuzhen;Yang, Mingjing;Gao, Zhipeng;Zhao, Jianqiang",57209892407;58465657800;57217750696;8914089700;37025279500;57222389327;57816024400,112080275-130565683-60017605;60018486;60017605;60017605;60017605;112080275-130565683;112080275-130565683,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,6.0,,6039-6047,"Pedestrian Attribute Recognition (PAR) involves identifying the attributes of individuals in person images. Existing PAR methods typically rely on CNNs as the backbone network to extract pedestrian features. However, CNNs process only one adjacent region at a time, leading to the loss of long-range inter-relations between different attribute-specific regions. To address this limitation, we leverage the Vision Transformer (ViT) instead of CNNs as the backbone for PAR, aiming to model long-range relations and extract more robust features. However, PAR suffers from an inherent attribute imbalance issue, causing ViT to naturally focus more on attributes that appear frequently in the training set and ignore some pedestrian attributes that appear less. The native features extracted by ViT are not able to tolerate the imbalance attribute distribution issue. To tackle this issue, we propose two novel components: the Selective Feature Activation Method (SFAM) and the Orthogonal Feature Activation Loss. SFAM smartly suppresses the more informative attribute-specific features, compelling the PAR model to capture discriminative features from regions that are easily overlooked. The proposed loss enforces an orthogonal constraint on the original feature extracted by ViT and the suppressed features from SFAM, promoting the complementarity of features in space. We conduct experiments on several benchmark PAR datasets, including PETA, PA100K, RAPv1, and RAPv2, demonstrating the effectiveness of our method. Specifically, our method outperforms existing state-of-the-art approaches by GRL, IAA-Caps, ALM, and SSC in terms of mA on the four datasets, respectively.",,15,1.0,all publisherfullgold,All Open Access Gold,CAS,2023J01067,Natural Science Foundation of Fujian Province
2-s2.0-85174399634,,,,Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains,cp,Conference Paper,Doshi V.,60279548;60112142,NC State College of Engineering;IQVIA Inc.,Raleigh;Durham,United States;United States,3.0,"Doshi, Vishwaraj;Hu, Jie;Eun, Do Young",57217278835;57222095129;6603631181,60112142;60279548;60279548,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,8403-8423,"We consider random walks on discrete state spaces, such as general undirected graphs, where the random walkers are designed to approximate a target quantity over the network topology via sampling and neighborhood exploration in the form of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain corresponding to a target probability distribution, we design a self-repellent random walk (SRRW) which is less likely to transition to nodes that were highly visited in the past, and more likely to transition to seldom visited nodes. For a class of SRRWs parameterized by a positive real α, we prove that the empirical distribution of the process converges almost surely to the the target (stationary) distribution of the underlying Markov chain kernel. We then provide a central limit theorem and derive the exact form of the arising asymptotic co-variance matrix, which allows us to show that the SRRW with a stronger repellence (larger α) always achieves a smaller asymptotic covariance, in the sense of Loewner ordering of co-variance matrices. Especially for SRRW-driven MCMC algorithms, we show that the decrease in the asymptotic sampling variance is of the order O(1/α), eventually going down to zero. Finally, we provide numerical simulations complimentary to our theoretical results, also empirically testing a version of SRRW with α increasing in time to combine the benefits of smaller asymptotic variance due to large α, with empirically observed faster mixing properties of SRRW with smaller α.",,4,0.0,,,NSF,CNS-1824518,National Science Foundation
2-s2.0-85147549205,10.1609/aaai.v36i10.21334,,,Self-Supervised Audio-and-Text Pre-training with Extremely Low-Resource Parallel Data,cp,Conference Paper,Kang Y.,60114181;125292625,Tencent;TAL Education Group,Shenzhen;Beijing,China;China,5.0,"Kang, Yu;Liu, Tianqiao;Li, Hang;Hao, Yang;Ding, Wenbiao",57220878218;57209222567;59888753600;57219694585;57207458619,125292625;125292625;125292625;125292625;125292625-60114181,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,10875-10883,"Multimodal pre-training for audio-and-text has recently been proved to be effective and has significantly improved the performance of many downstream speech understanding tasks. However, these state-of-the-art pre-training audio-text models work well only when provided with large amount of parallel audio-and-text data, which brings challenges on many languages that are rich in unimodal corpora but scarce of parallel cross-modal corpus. In this paper, we investigate whether it is possible to pre-train an audio-text multimodal model with extremely low-resource parallel data and extra non-parallel unimodal data. Our pre-training framework consists of the following components: (1) Intra-modal Denoising Auto-Encoding (IDAE), which is able to reconstruct input text (audio) representations from a noisy version of itself. (2) Cross-modal Denoising Auto-Encoding (CDAE), which is pre-trained to reconstruct the input text (audio), given both a noisy version of the input text (audio) and the corresponding translated noisy audio features (text embeddings). (3) Iterative Denoising Process (IDP), which iteratively translates raw audio (text) and the corresponding text embeddings (audio features) translated from previous iteration into the new less-noisy text embeddings (audio features). We adapt a dual cross-modal Transformer as our backbone model which consists of two unimodal encoders for IDAE and two cross-modal encoders for CDAE and IDP. Our method achieves comparable performance on multiple downstream speech understanding tasks compared with the model pre-trained on fully parallel data, demonstrating the great potential of the proposed method.",,5,1.0,all publisherfullgold,All Open Access Gold,BMSTC,Z201100006820068,Beijing Nova Program
2-s2.0-85148701218,,,,Self-Supervised Models of Audio Effectively Explain Human Cortical Responses to Speech,cp,Conference Paper,Vaidya A.R.,60013372,The University of Texas at Austin,Austin,United States,3.0,"Vaidya, Aditya R.;Jain, Shailee;Huth, Alexander G.",57224852222;57208446516;24391233100,;;60013372,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,21927-21944,"Self-supervised language models are very effective at predicting high-level cortical responses during language comprehension. However, the best current models of lower-level auditory processing in the human brain rely on either hand-constructed acoustic filters or representations from supervised audio neural networks. In this work, we capitalize on the progress of self-supervised speech representation learning (SSL) to create new state-of-the-art models of the human auditory system. Compared against acoustic baselines, phonemic features, and supervised models, representations from the middle layers of self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently yield the best prediction performance for fMRI recordings within the auditory cortex (AC). Brain areas involved in low-level auditory processing exhibit a preference for earlier SSL model layers, whereas higher-level semantic areas prefer later layers. We show that these trends are due to the models' ability to encode information at multiple linguistic levels (acoustic, phonetic, and lexical) along their representation depth. Overall, these results show that self-supervised models effectively capture the hierarchy of information relevant to different stages of speech processing in human cortex.",,19,0.0,,,APSF,,Alfred P. Sloan Foundation
2-s2.0-85204283578,,,,Self-supervised Weighted Information Bottleneck for Multi-view Clustering,cp,Conference Paper,Lou Z.,60018554,Zhengzhou University,Zhengzhou,China,6.0,"Lou, Zhengzheng;Zhang, Chaoyang;Xue, Hang;Ye, Yangdong;Zhou, Qinglei;Hu, Shizhe",36572300400;55703894300;59132190400;7401627537;24451497400;57201311056,60018554;60018554;60018554;60018554;60018554;60018554,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4643-4650,"Multi-view clustering (MVC) is a long-standing topic in machine learning and data mining community, focusing on investigating and utilizing the relationships among views for final consistent data cluster structure discovery. Generally, weighted MVC is one of the popular methods working by learning and applying the view weight/importance on each view for fully exploring the complementary information across views. However, most existing weighted MVCs only consider the quality of each view, ignoring the vital role of pseudo label self-supervision information in weight learning. In this work, we propose a novel self-supervised weighted information bottleneck (SWIB) method for solving the multi-view clustering problem. It combines the weighted information from different views based on information bottleneck theory, and the view weight learning mechanism is newly designed by simultaneously taking into accounting both the quality of view-contained information and the self-supervised information on the data partition of each view. Experimental results on multi-view text, multi-feature image, multi-angle video, and multi-modal text-image dataset as well as large-scale datasets show the superiority of the SWIB method. To our knowledge, this is the first work incorporating the self-supervised learning into weighted multi-view clustering.",,3,0.0,,,NSFC,62206254,National Natural Science Foundation of China
2-s2.0-85162639367,,,,Semantic Diffusion Network for Semantic Segmentation,cp,Conference Paper,Tan H.,60112903,"Baidu, Inc.",Beijing,China,3.0,"Tan, Haoru;Wu, Sitong;Pi, Jimin",57394051400;57224850055;57195220788,60112903;60112903;60112903,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Precise and accurate predictions over boundary areas are essential for semantic segmentation. However, the commonly-used convolutional operators tend to smooth and blur local detail cues, making it difficult for deep models to generate accurate boundary predictions. In this paper, we introduce an operator-level approach to enhance semantic boundary awareness, so as to improve the prediction of the deep semantic segmentation model. Specifically, we first formulate the boundary feature enhancement as an anisotropic diffusion process. We then propose a novel learnable approach called semantic diffusion network (SDN) to approximate the diffusion process, which contains a parameterized semantic difference convolution operator followed by a feature fusion module. Our SDN aims to construct a differentiable mapping from the original feature to the inter-class boundary-enhanced feature. The proposed SDN is an efficient and flexible module that can be easily plugged into existing encoder-decoder segmentation models. Extensive experiments show that our approach can achieve consistent improvements over several typical and state-of-the-art segmentation baseline models on challenging public benchmarks.",,32,0.0,,,,,
2-s2.0-85204308358,,,,SemanticMask: A Contrastive View Design for Anomaly Detection in Tabular Data,cp,Conference Paper,Tao S.,60003970;60117751,"Zhejiang University;College of Computer Science and Technology, Zhejiang University",Hangzhou;Hangzhou,China;China,4.0,"Tao, Shuting;Zhu, Tongtian;Wang, Hongwei;Meng, Xiangming",57216410699;57790287200;54785526500;56495966800,60117751-60003970;60117751;60117751-60003970;60117751-60003970,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2370-2378,"Contrastive learning based on data augmentation techniques has recently achieved substantial advancement in learning a representation well-suited for anomaly detection in image domain.However, due to the lack of spatial structure, designing effective data augmentation methods for tabular data remains challenging.Conventional techniques, such as random mask, disregard the inter-feature correlations and fail to accurately represent the data.To address this issue, we propose a novel augmentation technique called SemanticMask which leverages the semantic information from column names to generate better augmented views.SemanticMask aims to ensure that the shared information between views contains sufficient information for anomaly detection without redundancy.We analyze the relationship between shared information and anomaly detection performance and empirically demonstrate that good views for tabular anomaly detection tasks are feature-dependent.Our experiment results validate the superiority of SemanticMask over the state-of-the-art anomaly detection methods and existing augmentation techniques for tabular data.In further evaluations of the multi-class novelty detection task, SemanticMask also significantly outperforms the baseline.",,3,0.0,,,NSFC,62276230,National Natural Science Foundation of China
2-s2.0-85168010137,10.1609/aaai.v37i6.25890,,,Semi-Supervised Deep Regression with Uncertainty Consistency and Variational Model Ensembling via Bayesian Neural Networks,cp,Conference Paper,Dai W.,60008592,Hong Kong University of Science and Technology,Hong Kong,Hong Kong,3.0,"Dai, Weihang;Li, Xiaomeng;Cheng, Kwang Ting",57387090900;57192493244;7402997957,60008592;60008592;60008592,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,7304-7313,"Deep regression is an important problem with numerous applications. These range from computer vision tasks such as age estimation from photographs, to medical tasks such as ejection fraction estimation from echocardiograms for disease tracking. Semi-supervised approaches for deep regression are notably under-explored compared to classification and segmentation tasks, however. Unlike classification tasks, which rely on thresholding functions for generating class pseudo-labels, regression tasks use real number target predictions directly as pseudo-labels, making them more sensitive to prediction quality. In this work, we propose a novel approach to semi-supervised regression, namely Uncertainty-Consistent Variational Model Ensembling (UCVME), which improves training by generating high-quality pseudo-labels and uncertainty estimates for heteroscedastic regression. Given that aleatoric uncertainty is only dependent on input data by definition and should be equal for the same inputs, we present a novel uncertainty consistency loss for co-trained models. Our consistency loss significantly improves uncertainty estimates and allows higher quality pseudo-labels to be assigned greater importance under heteroscedastic regression. Furthermore, we introduce a novel variational model ensembling approach to reduce prediction noise and generate more robust pseudo-labels. We analytically show our method generates higher quality targets for unlabeled data and further improves training. Experiments show that our method outperforms state-of-the-art alternatives on different tasks and can be competitive with supervised methods that use full labels. Code is available at https://github.com/xmed-lab/UCVME.",,16,1.0,all publisherfullgold,All Open Access Gold,GRF,HCIC-004,Glaucoma Research Foundation
2-s2.0-85146274905,,,,Semiparametric Inference for Causal Effects in Graphical Models with Hidden Variables,ar,Article,Bhattacharya R.,60000928;60145911;60021497,Emory University;Whiting School of Engineering;Williams College,Atlanta;Baltimore;Williamstown,United States;United States;United States,3.0,"Bhattacharya, Rohit;Nabi, Razieh;Shpitser, Ilya",57190120426;57194652473;15056964800,60021497;60000928;60145911,2022-10-01,1 October 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,295,,"Identification theory for causal effects in causal models associated with hidden variable directed acyclic graphs (DAGs) is well studied. However, the corresponding algorithms are underused due to the complexity of estimating the identifying functionals they output. In this work, we bridge the gap between identification and estimation of population-level causal effects involving a single treatment and a single outcome. We derive influence function based estimators that exhibit double robustness for the identified effects in a large class of hidden variable DAGs where the treatment satisfies a simple graphical criterion; this class includes models yielding the adjustment and front-door functionals as special cases. We also provide necessary and sufficient conditions under which the statistical model of a hidden variable DAG is nonparametrically saturated and implies no equality constraints on the observed data distribution. Further, we derive an important class of hidden variable DAGs that imply observed data distributions observationally equivalent (up to equality constraints) to fully observed DAGs. In these classes of DAGs, we derive estimators that achieve the semiparametric efficiency bounds for the target of interest where the treatment satisfies our graphical criterion. Finally, we provide a sound and complete identification algorithm that directly yields a weight based estimation strategy for any identifiable effect in hidden variable causal models.",doubly robust estimation | efficient influence function | nonparametric saturation | Unmeasured confounding,17,0.0,,,,,
2-s2.0-85162169113,10.1613/jair.1.13970,,,Semiring Reasoning Frameworks in AI and Their Computational Complexity,ar,Article,Eiter T.,60018163,TU Wien,Vienna,Austria,2.0,"Eiter, Thomas;Kiesel, Rafael",7005085805;57216394393,60018163;60018163,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,207-293,"Many important problems in AI, among them #SAT, parameter learning and probabilistic inference go beyond the classical satisfiability problem. Here, instead of finding a solution we are interested in a quantity associated with the set of solutions, such as the number of solutions, the optimal solution or the probability that a query holds in a solution. To model such quantitative problems in a uniform manner, a number of frameworks, e.g. Algebraic Model Counting and Semiring-based Constraint Satisfaction Problems, employ what we call the semiring paradigm. In the latter the abstract algebraic structure of the semiring serves as a means of parameterizing the problem definition, thus allowing for different modes of quantitative computations by choosing different semirings. While efficiently solvable cases have been widely studied, a systematic study of the computational complexity of such problems depending on the semiring parameter is missing. In this work, we characterize the latter by NP(R), a novel generalization of NP over semiring R, and obtain NP(R)-completeness results for a selection of semiring frameworks. To obtain more tangible insights into the hardness of NP(R), we link it to well-known complexity classes from the literature. Interestingly, we manage to connect the computational hardness to properties of the semiring. Using this insight, we see that, on the one hand, NP(R) is always at least as hard as NP or ModpP depending on the semiring R and in general unlikely to be in FPSpace(poly). On the other hand, for broad subclasses of semirings relevant in practice we can employ reductions to NP, ModpP and #P. These results show that in many cases solutions are only mildly harder to compute than functions in NP, ModpP and #P, give us new insights into how problems that involve counting on semirings can be approached, and provide a means of assessing whether an algorithm is appropriate for a given class of problems.",,4,1.0,all publisherfullgold,All Open Access Gold,FWF,W1255-N23,Austrian Science Fund
2-s2.0-85204306920,,,,Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation,cp,Conference Paper,Wei J.,60027363;60019118;60008227,University of Chinese Academy of Sciences;University of Science and Technology of China;Shenyang Institute of Computing Technology Chinese Academy of Sciences,Beijing;Hefei;Shenyang,China;China;China,6.0,"Wei, Jingxuan;Sun, Linzhuang;Leng, Yichong;Tan, Xu;Yu, Bihui;Guo, Ruifeng",57222556629;58515248400;57216622850;57204467223;57221978081;35233558200,60008227-60027363;60008227-60027363;60019118;;60008227-60027363;60008227-60027363,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,6531-6540,"Knowledge distillation, transferring knowledge from a teacher model to a student model, has emerged as a powerful technique in neural machine translation for compressing models or simplifying training targets. Knowledge distillation encompasses two primary methods: sentence-level distillation and token-level distillation. In sentence-level distillation, the student model is trained to align with the output of the teacher model, which can alleviate the training difficulty and give student model a comprehensive understanding of global structure. Differently, token-level distillation requires the student model to learn the output distribution of the teacher model, facilitating a more fine-grained transfer of knowledge. Studies have revealed divergent performances between sentence-level and token-level distillation across different scenarios, leading to the confusion on the empirical selection of knowledge distillation methods. In this study, we argue that token-level distillation, with its more complex objective (i.e., distribution), is better suited for “simple” scenarios, while sentence-level distillation excels in “complex” scenarios. To substantiate our hypothesis, we systematically analyze the performance of distillation methods by varying the model size of student models, the complexity of text, and the difficulty of decoding procedure. While our experimental results validate our hypothesis, defining the complexity level of a given scenario remains a challenging task. So we further introduce a novel hybrid method that combines token-level and sentence-level distillation through a gating mechanism, aiming to leverage the advantages of both individual methods. Experiments demonstrate that the hybrid method surpasses the performance of token-level or sentence-level distillation methods and the previous works by a margin, demonstrating the effectiveness of the proposed hybrid method.",,4,0.0,,,,2022JH2/101300258,
2-s2.0-85137879357,10.1609/aaai.v36i9.21190,,,Shaping Noise for Robust Attributions in Neural Stochastic Differential Equations,cp,Conference Paper,Jha S.K.,60028609;60154598;60000461;60143652;60011972,Argonne National Laboratory;College of Engineering and Computer Science;SRI International;Department of Computer Science;Air Force Research Laboratory Information Directorate,Lemont;Orlando;Menlo Park;San Antonio;Rome,United States;United States;United States;United States;United States,5.0,"Jha, Sumit Kumar;Ewetz, Rickard;Velasquez, Alvaro;Ramanathan, Arvind;Jha, Susmit",57218716753;55641700700;56404337600;57204334274;23476883200,60143652;60154598;60011972;60028609;60000461,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,9567-9574,"Neural SDEs with Brownian motion as noise lead to smoother attributions than traditional ResNets. Various attribution methods such as saliency maps, integrated gradients, DeepSHAP and DeepLIFT have been shown to be more robust for neural SDEs than for ResNets using the recently proposed sensitivity metric. In this paper, we show that neural SDEs with adaptive attribution-driven noise lead to even more robust attributions and smaller sensitivity metrics than traditional neural SDEs with Brownian motion as noise. In particular, attribution-driven shaping of noise leads to 6.7%, 6.9% and 19.4% smaller sensitivity metric for integrated gradients computed on three discrete approximations of neural SDEs with standard Brownian motion noise: stochastic ResNet-50, WideResNet-101 and ResNeXt-101 models respectively. The neural SDE model with adaptive attribution-driven noise leads to 25.7% and 4.8% improvement in the SIC metric over traditional ResNets and Neural SDEs with Brownian motion as noise. To the best of our knowledge, we are the first to propose the use of attributions for shaping the noise injected in neural SDEs, and demonstrate that this process leads to more robust attributions than traditional neural SDEs with standard Brownian motion as noise.",,11,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85189650112,10.1609/aaai.v38i16.29781,,,ShareBERT: Embeddings Are Capable of Learning Hidden Layers,cp,Conference Paper,Hu J.C.,60004591,Università degli Studi di Modena e Reggio Emilia,Modena,Italy,4.0,"Hu, Jia Cheng;Cavicchioli, Roberto;Berardinelli, Giulia;Capotondi, Alessandro",57213146696;55061566600;58738383700;55825787600,60004591;60004591;60004591;60004591,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,16.0,,18225-18233,"The deployment of Pre-trained Language Models in memory-limited devices is hindered by their massive number of parameters, which motivated the interest in developing smaller architectures. Established works in the model compression literature showcased that small models often present a noticeable performance degradation and need to be paired with transfer learning methods, such as Knowledge Distillation. In this work, we propose a parameter-sharing method that consists of sharing parameters between embeddings and the hidden layers, enabling the design of near-zero parameter encoders. To demonstrate its effectiveness, we present an architecture design called ShareBERT, which can preserve up to 95.5% of BERT Base performances, using only 5M parameters (21.9× fewer parameters) without the help of Knowledge Distillation. We demonstrate empirically that our proposal does not negatively affect the model learning capabilities and that it is even beneficial for representation learning. Code will be available at https://github.com/jchenghu/sharebert.",,2,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,101120726,
2-s2.0-85137900889,10.24963/ijcai.2022/640,,,Shared Autonomy Systems with Stochastic Operator Models,cp,Conference Paper,Costen C.,60026851,University of Oxford,Oxford,United Kingdom,4.0,"Costen, Clarissa;Rigter, Marc;Lacerda, Bruno;Hawes, Nick",57888747600;57207877670;54793193300;14041693100,60026851;60026851;60026851;60026851,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4614-4620,"We consider shared autonomy systems where multiple operators (AI and human), can interact with the environment, e.g. by controlling a robot. The decision problem for the shared autonomy system is to select which operator takes control at each timestep, such that a reward specifying the intended system behaviour is maximised. The performance of the human operator is influenced by unobserved factors, such as fatigue or skill level. Therefore, the system must reason over stochastic models of operator performance. We present a framework for stochastic operators in shared autonomy systems (SO-SAS), where we represent operators using rich, partially observable models. We formalise SO-SAS as a mixed-observability Markov decision process, where environment states are fully observable and internal operator states are hidden. We test SO-SAS on a simulated domain and a computer game, empirically showing it results in better performance compared to traditional formulations of shared autonomy systems.",,9,1.0,all publisherfree2read,All Open Access Bronze,Dstl,EP/V000748/1,Defence Science and Technology Laboratory
2-s2.0-85137865094,10.24963/ijcai.2022/106,,,Shielding Federated Learning: Robust Aggregation with Adaptive Client Selection,cp,Conference Paper,Wan W.,60025761;60018805;127686813;132110527;132111365;132111540,Huazhong University of Science and Technology;Deakin University;Hubei Engineering Research Center on Big Data Security;National Engineering Research Center for Big Data Technology and System;Services Computing Technology and System Lab;Cluster and Grid Computing Lab,Wuhan;Geelong;;;;,China;Australia;China;;;,6.0,"Wan, Wei;Hu, Shengshan;Lu, Jianrong;Zhang, Leo Yu;Jin, Hai;He, Yuanyuan",57345317300;57020312600;57345008500;57249742600;56434989100;57085469800,60025761-132110527-132111365-127686813;60025761-132110527-132111365-127686813;60025761-132110527-132111365-127686813;60018805;60025761-132110527-132111365-132111540;60025761,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,753-760,"Federated learning (FL) enables multiple clients to collaboratively train an accurate global model while protecting clients' data privacy. However, FL is susceptible to Byzantine attacks from malicious participants. Although the problem has gained significant attention, existing defenses have several flaws: the server irrationally chooses malicious clients for aggregation even after they have been detected in previous rounds; the defenses perform ineffectively against sybil attacks or in the heterogeneous data setting. To overcome these issues, we propose MAB-RFL, a new method for robust aggregation in FL. By modelling the client selection as an extended multi-armed bandit (MAB) problem, we propose an adaptive client selection strategy to choose honest clients that are more likely to contribute high-quality updates. We then propose two approaches to identify malicious updates from sybil and non-sybil attacks, based on which rewards for each client selection decision can be accurately evaluated to discourage malicious behaviors. MAB-RFL achieves a satisfying balance between exploration and exploitation on the potential benign clients. Extensive experimental results show that MAB-RFL outperforms existing defenses in three attack scenarios under different percentages of attackers.",,24,1.0,all publisherfree2read repository repositoryam,All Open Access Bronze Green,NSFC,61702221,National Natural Science Foundation of China
2-s2.0-85147404274,,,,Shuffle Private Linear Contextual Bandits,cp,Conference Paper,Chowdhury S.R.,60019674;60009408,Boston University;Wayne State University,Boston;Detroit,United States;United States,2.0,"Chowdhury, Sayak Ray;Zhou, Xingyu",57195955397;57203513849,60019674;60009408,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,3984-4009,"Differential privacy (DP) has been recently introduced to linear contextual bandits to formally address the privacy concerns in its associated personalized services to participating users (e.g., recommendations). Prior work largely focus on two trust models of DP - the central model, where a central server is responsible for protecting users' sensitive data, and the (stronger) local model, where information needs to be protected directly on users' side. However, there remains a fundamental gap in the utility achieved by learning algorithms under these two privacy models, e.g., if all users are unique within a learning horizon T, O<sup>e</sup>(<sup>√</sup>T) regret in the central model as compared to O<sup>e</sup>(T<sup>3</sup>/<sup>4</sup>) regret in the local model. In this work, we aim to achieve a stronger model of trust than the central model, while suffering a smaller regret than the local model by considering recently popular shuffle model of privacy. We propose a general algorithmic framework for linear contextual bandits under the shuffle trust model, where there exists a trusted shuffler - in between users and the central server- that randomly permutes a batch of users data before sending those to the server. We then instantiate this framework with two specific shuffle protocols - one relying on privacy amplification of local mechanisms, and another incorporating a protocol for summing vectors and matrices of bounded norms. We prove that both these instantiations lead to regret guarantees that significantly improve on that of the local model, and can potentially be of the order O<sup>e</sup>(T<sup>3</sup>/<sup>5</sup>) if all users are unique. We also verify this regret behavior with simulations on synthetic data. Finally, under the practical scenario of non-unique users, we show that the regret of our shuffle private algorithm scale as O<sup>e</sup>(T<sup>2</sup>/<sup>3</sup>), which matches what the central model could achieve in this case.",,7,0.0,,,BU,,Boston University
2-s2.0-85170362330,10.24963/ijcai.2023/662,,,Sign Language-to-Text Dictionary with Lightweight Transformer Models,cp,Conference Paper,Fink J.,60006635;60276198;60278731,"Université de Namur;Namur Digital Institute;Namur Institute of Language, Text and Transmediality (NALTT)",Namur;Namur;Namur,Belgium;Belgium;Belgium,8.0,"Fink, Jérôme;Poitier, Pierre;André, Maxime;Meurice, Loup;Frénay, Benoît;Cleve, Anthony;Dumas, Bruno;Meurant, Laurence",57211803140;58569592600;57188748829;55945475600;36476003500;8694759600;23967790500;56582452200,60276198-60278731-60006635;60276198-60006635;60276198-60006635;60276198-60006635;60276198-60006635;60276198-60006635;60276198-60006635;60278731-60006635,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,5968-5976,"The recent advances in deep learning have been beneficial to automatic sign language recognition (SLR). However, free-to-access, usable, and accessible tools are still not widely available to the deaf community. The need for a sign language-to-text dictionary was raised by a bilingual deaf school in Belgium and linguist experts in sign languages (SL) in order to improve the autonomy of students. To meet that need, an efficient SLR system was built based on a specific transformer model. The proposed system is able to recognize 700 different signs, with a top-10 accuracy of 83%. Those results are competitive with other systems in the literature while using 10 times less parameters than existing solutions. The integration of this model into a usable and accessible web application for the dictionary is also introduced. A user-centered human-computer interaction (HCI) methodology was followed to design and implement the user interface. To the best of our knowledge, this is the first publicly released sign language-to-text dictionary using video captured by a standard camera.",,11,1.0,all publisherfullgold,All Open Access Gold,FNRS,,Fonds De La Recherche Scientifique - FNRS
2-s2.0-85147605670,10.1609/aaai.v36i2.20040,,,SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-training for Spatial-Aware Visual Representations,cp,Conference Paper,Li Z.,60025278;60019616;60002798;132138809;132139701,Tsinghua University;Harbin Institute of Technology;Chinese University of Hong Kong;SenseTime Research;University of Science and Technology,Beijing;Harbin;Hong Kong;;,China;China;Hong Kong;;,9.0,"Li, Zhenyu;Chen, Zehui;Li, Ang;Fang, Liangji;Jiang, Qinhong;Liu, Xianming;Jiang, Junjun;Zhou, Bolei;Zhao, Hang",60066127600;57226331687;57714583500;57425666100;57219698653;35230216700;54902306100;36697366200;57222479659,60019616;132139701;132138809;132138809;132138809;60019616;60019616;60002798;60025278,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,1500-1508,"Pre-training has become a standard paradigm in many computer vision tasks. However, most of the methods are generally designed on the RGB image domain. Due to the discrepancy between the two-dimensional image plane and the three-dimensional space, such pre-trained models fail to perceive spatial information and serve as sub-optimal solutions for 3D-related tasks. To bridge this gap, we aim to learn a spatial-aware visual representation that can describe the three-dimensional space and is more suitable and effective for these tasks. To leverage point clouds, which are much more superior in providing spatial information compared to images, we propose a simple yet effective 2D Image and 3D Point cloud Unsupervised pre-training strategy, called SimIPU. Specifically, we develop a multi-modal contrastive learning framework that consists of an intra-modal spatial perception module to learn a spatial-aware representation from point clouds and an inter-modal feature interaction module to transfer the capability of perceiving spatial information from the point cloud encoder to the image encoder, respectively. Positive pairs for contrastive losses are established by the matching algorithm and the projection matrix. The whole framework is trained in an unsupervised end-to-end fashion. To the best of our knowledge, this is the first study to explore contrastive learning pre-training strategies for outdoor multi-modal datasets, containing paired camera images and LIDAR point clouds.",,53,1.0,all publisherfullgold,All Open Access Gold,NSFC,61922027,National Natural Science Foundation of China
2-s2.0-85196073332,10.1613/jair.1.15693,,,Similarity-Based Adaptation for Task-Aware and Task-Free Continual Learning,ar,Article,Adel T.,60031101,University of Cambridge,Cambridge,United Kingdom,1.0,"Adel, Tameem",57192975847,60031101,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,377-417,"Continual learning (CL) is a paradigm which addresses the issue of how to learn from sequentially arriving tasks. The goal of this paper is to introduce a CL framework which can both learn from a global multi-task architecture and locally adapt this learning to the task at hand. In addition to the global knowledge, we conjecture that it is also beneficial to further focus on the most relevant pieces of previous knowledge. Using a prototypical network as a proxy, the proposed framework bases its adaptation on the similarity between the current data stream and the previously encountered data. We develop two algorithms, one for the standard task-aware CL and another for the more challenging task-free setting where boundaries between tasks are unknown. We correspondingly derive a generalization upper bound on the error of an upcoming task. Experiments demonstrate that the introduced algorithms lead to improved performance on several CL benchmarks.",,2,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85148721327,10.1609/aaai.v37i9.26283,,,Simple and Efficient Heterogeneous Graph Neural Network,cp,Conference Paper,Yang X.,60027363;60032987;60030904,University of Chinese Academy of Sciences;Griffith University;Institute of Computing Technology Chinese Academy of Sciences,Beijing;Brisbane;Beijing,China;Australia;China,5.0,"Yang, Xiaocheng;Yan, Mingyu;Pan, Shirui;Ye, Xiaochun;Fan, Dongrui",57223433761;57211109655;55522732400;7401898954;12782252600,60030904;60030904;60032987;60030904;60030904-60027363,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,10816-10824,"Heterogeneous graph neural networks (HGNNs) have the powerful capability to embed rich structural and semantic information of a heterogeneous graph into node representations. Existing HGNNs inherit many mechanisms from graph neural networks (GNNs) designed for homogeneous graphs, especially the attention mechanism and the multi-layer structure. These mechanisms bring excessive complexity, but seldom work studies whether they are really effective on heterogeneous graphs. In this paper, we conduct an in-depth and detailed study of these mechanisms and propose the Simple and Efficient Heterogeneous Graph Neural Network (SeHGNN). To easily capture structural information, SeHGNN pre-computes the neighbor aggregation using a light-weight mean aggregator, which reduces complexity by removing overused neighbor attention and avoiding repeated neighbor aggregation in every training epoch. To better utilize semantic information, SeHGNN adopts the single-layer structure with long metapaths to extend the receptive field, as well as a transformer-based semantic fusion module to fuse features from different metapaths. As a result, SeHGNN exhibits the characteristics of a simple network structure, high prediction accuracy, and fast training speed. Extensive experiments on five real-world heterogeneous graphs demonstrate the superiority of SeHGNN over the state-of-the-arts on both accuracy and training speed.",,212,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,NSFC,61732018,National Natural Science Foundation of China
2-s2.0-85188725044,10.1609/aaai.v38i18.29997,,,Simplifying Complex Observation Models in Continuous POMDP Planning with Probabilistic Guarantees and Practice,cp,Conference Paper,Lev-Yehudi I.,60022403,Technion - Israel Institute of Technology,Haifa,Israel,3.0,"Lev-Yehudi, Idan;Barenboim, Moran;Indelman, Vadim",58146592000;57219436260;23090922300,60022403;60022403;60022403,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,18.0,,20176-20184,"Solving partially observable Markov decision processes (POMDPs) with high dimensional and continuous observations, such as camera images, is required for many real life robotics and planning problems. Recent researches suggested machine learned probabilistic models as observation models, but their use is currently too computationally expensive for online deployment. We deal with the question of what would be the implication of using simplified observation models for planning, while retaining formal guarantees on the quality of the solution. Our main contribution is a novel probabilistic bound based on a statistical total variation distance of the simplified model. We show that it bounds the theoretical POMDP value w.r.t. original model, from the empirical planned value with the simplified model, by generalizing recent results of particle-belief MDP concentration bounds. Our calculations can be separated into offline and online parts, and we arrive at formal guarantees without having to access the costly model at all during planning, which is also a novel result. Finally, we demonstrate in simulation how to integrate the bound into the routine of an existing continuous online POMDP solver.",,5,1.0,all publisherfullgold,All Open Access Gold,ISF,,Israel Science Foundation
2-s2.0-85197348662,10.1613/jair.1.15579,,,Simulating Counterfactuals,ar,Article,Karvanen J.,60032398,University of Jyväskylä,Jyvaskyla,Finland,3.0,"Karvanen, Juha;Tikka, Santtu;Vihola, Matti",56243965800;57193616850;19934590600,60032398;60032398;60032398,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,835-857,"Counterfactual inference considers a hypothetical intervention in a parallel world that shares some evidence with the factual world. If the evidence specifies a conditional distribution on a manifold, counterfactuals may be analytically intractable. We present an algorithm for simulating values from a counterfactual distribution where conditions can be set on both discrete and continuous variables. We show that the proposed algorithm can be presented as a particle filter leading to asymptotically valid inference. The algorithm is applied to fairness analysis in credit-scoring.",,0,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,AKA,346311,Research Council of Finland
2-s2.0-85174415959,,,,SinDDM: A Single Image Denoising Diffusion Model,cp,Conference Paper,Kulikov V.,60022403,Technion - Israel Institute of Technology,Haifa,Israel,4.0,"Kulikov, Vladimir;Yadin, Shahar;Kleiner, Matan;Michaeli, Tomer",58003268900;58003036100;58003350900;18042574300,60022403;60022403;60022403;60022403,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,17920-17930,"Denoising diffusion models (DDMs) have led to staggering performance leaps in image generation, editing and restoration. However, existing DDMs use very large datasets for training. Here, we introduce a framework for training a DDM on a single image. Our method, which we coin SinDDM, learns the internal statistics of the training image by using a multi-scale diffusion process. To drive the reverse diffusion process, we use a fully-convolutional light-weight denoiser, which is conditioned on both the noise level and the scale. This architecture allows generating samples of arbitrary dimensions, in a coarse-to-fine manner. As we illustrate, SinDDM generates diverse high-quality samples, and is applicable in a wide array of tasks, including style transfer and harmonization. Furthermore, it can be easily guided by external supervision. Particularly, we demonstrate text-guided generation from a single image using a pre-trained CLIP model. Results, code and the Supplementary Material are available on the project's webpage.",,70,0.0,,,ISF,2318/22,Israel Science Foundation
2-s2.0-85203791039,,,,Smooth Min-Max Monotonic Networks,cp,Conference Paper,Igel C.,60030840,Københavns Universitet,Copenhagen,Denmark,1.0,"Igel, Christian",6602116076,60030840,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,20908-20923,"Monotonicity constraints are powerful regular-izers in statistical modelling. They can support fairness in computer-aided decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of partial derivatives of the MM nonlinearities being zero. We propose a simple modification of the MM network using strictly-increasing smooth minimum and maximum functions that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is conceptually simple and computationally less demanding than state-of-the-art neural networks for monotonic modelling. Our experiments show that this does not come with a loss in generalization performance compared to alternative neural and non-neural approaches.",,2,0.0,,,DNRF,,Villum Fonden
2-s2.0-85170394014,10.24963/ijcai.2023/227,,,Solving Quantum-Inspired Perfect Matching Problems via Tutte-Theorem-Based Hybrid Boolean Constraints,cp,Conference Paper,Vardi M.Y.,60005286,Rice University,Houston,United States,2.0,"Vardi, Moshe Y.;Zhang, Zhiwei",7005334525;57219630094,60005286;60005286,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,2039-2048,"Determining the satisfiability of Boolean constraint-satisfaction problems with different types of constraints, that is hybrid constraints, is a well-studied problem with important applications. We study a new application of hybrid Boolean constraints, which arises in quantum computing. The problem relates to constrained perfect matching in edge-colored graphs. While general-purpose hybrid constraint solvers can be powerful, we show that direct encodings of the constrained-matching problem as hybrid constraints scale poorly and special techniques are still needed. We propose a novel encoding based on Tutte's Theorem in graph theory as well as optimization techniques. Empirical results demonstrate that our encoding, in suitable languages with advanced SAT solvers, scales significantly better than a number of competing approaches on constrained-matching benchmarks. Our study identifies the necessity of designing problem-specific encodings when applying powerful general-purpose constraint solvers.",,1,1.0,all publisherfullgold,All Open Access Gold,,1830549,
2-s2.0-85189525267,10.1609/aaai.v38i7.28593,,,SpFormer: Spatio-Temporal Modeling for Scanpaths with Transformer,cp,Conference Paper,Zhong W.,60003977,Northwestern Polytechnical University,Xi'an,China,5.0,"Zhong, Wenqi;Yu, Linzhi;Xia, Chen;Han, Junwei;Zhang, Dingwen",58894179700;58972053200;56102195500;24450644400;56024706800,60003977;60003977;60003977;60003977;60003977,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,7.0,,7605-7613,"Saccadic scanpath, a data representation of human visual behavior, has received broad interest in multiple domains.Scanpath is a complex eye-tracking data modality that includes the sequences of fixation positions and fixation duration, coupled with image information.However, previous methods usually face the spatial misalignment problem of fixation features and loss of critical temporal data (including temporal correlation and fixation duration).In this study, we propose a Transformer-based scanpath model, SpFormer, to alleviate these problems.First, we propose a fixation-centric paradigm to extract the aligned spatial fixation features and tokenize the scanpaths.Then, according to the visual working memory mechanism, we design a local meta attention to reduce the semantic redundancy of fixations and guide the model to focus on the meta scanpath.Finally, we progressively integrate the duration information and fuse it with the fixation features to solve the problem of ambiguous location with the Transformer block increasing.We conduct extensive experiments on four databases under three tasks.The SpFormer establishes new state-of-the-art results in distinct settings, verifying its flexibility and versatility in practical applications.The code can be obtained from https://github.com/wenqizhong/SpFormer.",,9,1.0,all publisherfullgold,All Open Access Gold,NSFC,62172334,National Natural Science Foundation of China
2-s2.0-85189637019,10.1609/aaai.v38i16.29743,,,Spanning the Spectrum of Hatred Detection: A Persian Multi-Label Hate Speech Dataset with Annotator Rationales,cp,Conference Paper,Delbari Z.,60001881;60023998;60264839,The University of Sheffield;Cardiff University;Khatam University,Sheffield;Cardiff;Tehran,United Kingdom;United Kingdom;Iran,3.0,"Delbari, Zahra;Moosavi, Nafise Sadat;Pilehvar, Mohammad Taher",59956307500;36968334800;56349749800,60264839;60001881;60023998,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,16.0,,17889-17897,"With the alarming rise of hate speech in online communities, the demand for effective NLP models to identify instances of offensive language has reached a critical point. However, the development of such models heavily relies on the availability of annotated datasets, which are scarce, particularly for less-studied languages. To bridge this gap for the Persian language, we present a novel dataset specifically tailored to multi-label hate speech detection. Our dataset, called PHATE, consists of an extensive collection of over seven thousand manually-annotated Persian tweets, offering a rich resource for training and evaluating hate speech detection models on this language. Notably, each annotation in our dataset specifies the targeted group of hate speech and includes a span of the tweet which elucidates the rationale behind the assigned label. The incorporation of these information expands the potential applications of our dataset, facilitating the detection of targeted online harm or allowing the benchmark to serve research on interpretability of hate speech detection models. The dataset, annotation guideline, and all associated codes are accessible at https://github.com/Zahra-D/Phate.",,5,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,
2-s2.0-85148001487,,,,Sparse Continuous Distributions and Fenchel-Young Losses,ar,Article,Martins A.F.T.,60002483;60004956;60006191,Universiteit van Amsterdam;Instituto Superior Técnico;Google LLC,Amsterdam;Lisbon;Mountain View,Netherlands;Portugal;United States,7.0,"Martins, André F.T.;Treviso, Marcos;Farinhas, António;Aguiar, Pedro M.Q.;Figueiredo, Mário A.T.;Blondel, Mathieu;Niculae, Vlad",55937372200;57194682222;57219733679;36014956700;34769730500;36622153000;55369344500,60004956;60004956;60004956;60004956;60004956;60006191;60002483,2022-07-01,1 July 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,A94,,"Exponential families are widely used in machine learning, including many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, recent work on sparse alternatives to softmax (e.g., sparsemax, α-entmax, and fusedmax), has led to distributions with varying support. This paper develops sparse alternatives to continuous distributions, based on several technical contributions: First, we define Ω-regularized prediction maps and Fenchel-Young losses for arbitrary domains (possibly countably infinite or continuous). For linearly parametrized families, we show that minimization of Fenchel-Young losses is equivalent to moment matching of the statistics, generalizing a fundamental property of exponential families. When Ω is a Tsallis negentropy with parameter α, we obtain “deformed exponential families,” which include α-entmax and sparsemax (α = 2) as particular cases. For quadratic energy functions, the resulting densities are β-Gaussians, an instance of elliptical distributions that contain as particular cases the Gaussian, biweight, triweight, and Epanechnikov densities, and for which we derive closed-form expressions for the variance, Tsallis entropy, and Fenchel-Young loss. When Ω is a total variation or Sobolev regularizer, we obtain a continuous version of the fusedmax. Finally, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for α ∈ {1, <sup>4</sup>/3, <sup>3</sup>/2, 2}. Using these algorithms, we demonstrate our sparse continuous distributions for attention-based audio classification and visual question answering, showing that they allow attending to time intervals and compact regions.",attention mechanisms | deformed exponential families | Fenchel-Young losses | Sparse continuous distributions,5,0.0,,,ERC,DeepSPIN 758969,European Research Council
2-s2.0-85189113452,,,,Sparse GCA and Thresholded Gradient Descent,ar,Article,Gao S.,60006297,University of Pennsylvania,Philadelphia,United States,2.0,"Gao, Sheng;Ma, Zongming",58775366900;54952343700,60006297;60006297,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"Generalized correlation analysis (GCA) is concerned with uncovering linear relationships across multiple data sets. It generalizes canonical correlation analysis that is designed for two data sets. We study sparse GCA when there are potentially multiple leading generalized correlation tuples in data that are of interest and the loading matrix has a small number of nonzero rows. It includes sparse CCA and sparse PCA of correlation matrices as special cases. We first formulate sparse GCA as a generalized eigenvalue problem at both population and sample levels via a careful choice of normalization constraints. Based on a Lagrangian form of the sample optimization problem, we propose a thresholded gradient descent algorithm for estimating GCA loading vectors and matrices in high dimensions. We derive tight estimation error bounds for estimators generated by the algorithm with proper initialization. We also demonstrate the prowess of the algorithm on a number of synthetic data sets.",generalized eigenvalue problem | non-convex optimization | sparse CCA | sparse GCA | thresholded gradient descent,4,0.0,,,,,
2-s2.0-85213877569,,,,Sparse Markov Models for High-dimensional Inference,ar,Article,Ost G.,60000036;60023857,Universidade Federal do Rio de Janeiro;Universidade Federal do Rio Grande do Norte,Rio de Janeiro;Natal,Brazil;Brazil,2.0,"Ost, Guilherme;Takahashi, Daniel Y.",56862159700;8241472000,60000036;60023857,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"Finite-order Markov models are well-studied models for dependent finite alphabet data. Despite their generality, application in empirical work is rare when the order d is large relative to the sample size n (e.g., d = O(n)). Practitioners rarely use higher-order Markov models because (1) the number of parameters grows exponentially with the order, (2) the sample size n required to estimate each parameter grows exponentially with the order, and (3) the interpretation is often difficult. Here, we consider a subclass of Markov models called Mixture of Transition Distribution (MTD) models, proving that when the set of relevant lags is sparse (i.e., O(log(n))), we can consistently and efficiently recover the lags and estimate the transition probabilities of high-dimensional (d = O(n)) MTD models. Moreover, the estimated model allows straightforward interpretation. The key innovation is a recursive procedure for a priori selection of the relevant lags of the model. We prove a new structural result for the MTD and an improved martingale concentration inequality to prove our results. Using simulations, we show that our method performs well compared to other relevant methods. We also illustrate the usefulness of our method on weather data where the proposed method correctly recovers the long-range dependence.",High-dimensional inference | Markov Chains | Mixture Transition Distribution,0,0.0,,,FAPESP,2013/07699-0,Fundação de Amparo à Pesquisa do Estado de São Paulo
2-s2.0-105018471379,,,,Sparse NMF with Archetypal Regularization: Computational and Robustness Properties,ar,Article,Behdin K.,60022195;60014228,Massachusetts Institute of Technology;MIT Sloan School of Management,Cambridge;Cambridge,United States;United States,2.0,"Behdin, Kayhan;Mazumder, Rahul",57215040648;16319404900,60022195;60014228,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"We consider the problem of sparse nonnegative matrix factorization (NMF) using archetypal regularization. The goal is to represent a collection of data points as nonnegative linear combinations of a few nonnegative sparse factors with appealing geometric properties, arising from the use of archetypal regularization. We generalize the notion of robustness studied in Javadi and Montanari (2019) (without sparsity) to the notions of (a) strong robustness that implies each estimated archetype is close to the underlying archetypes and (b) weak robustness that implies there exists at least one recovered archetype that is close to the underlying archetypes. Our theoretical results on robustness guarantees hold under minimal assumptions on the underlying data, and applies to settings where the underlying archetypes need not be sparse. We present theoretical results and illustrative examples to strengthen the insights underlying the notions of robustness. We propose new algorithms for our optimization problem; and present numerical experiments on synthetic and real data sets that shed further insights into our proposed framework and theoretical developments.",Archetypal Analysis | Model misspecification | Nonconvex Optimization | Robustness to Perturbation | Sparse Nonnegative Matrix Factorization,1,0.0,,,IBMC,ONR-N000141812298,International Business Machines Corporation
2-s2.0-85147703523,10.1609/aaai.v36i9.21275,,,Sparsification of Decomposable Submodular Functions,cp,Conference Paper,Rafiey A.,60018491;60028928,Simon Fraser University;Research Organization of Information and Systems National Institute of Informatics,Burnaby;Tokyo,Canada;Japan,2.0,"Rafiey, Akbar;Yoshida, Yuichi",57023733400;35575532400,60018491;60028928,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,10336-10344,"Submodular functions are at the core of many machine learning and data mining tasks. The underlying submodular functions for many of these tasks are decomposable, i.e., they are sum of several simple submodular functions. In many data intensive applications, however, the number of underlying submodular functions in the original function is so large that we need prohibitively large amount of time to process it and/or it does not even fit in the main memory. To overcome this issue, we introduce the notion of sparsification for decomposable submodular functions whose objective is to obtain an accurate approximation of the original function that is a (weighted) sum of only a few submodular functions. Our main result is a polynomial-time randomized sparsification algorithm such that the expected number of functions used in the output is independent of the number of underlying submodular functions in the original function. We also study the effectiveness of our algorithm under various constraints such as matroid and cardinality constraints. We complement our theoretical analysis with an empirical study of the performance of our algorithm.",,8,1.0,all publisherfullgold,All Open Access Gold,NSERC,20H05965,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85189634883,10.1609/aaai.v38i8.28707,,,Spatio-Temporal Pivotal Graph Neural Networks for Traffic Flow Forecasting,cp,Conference Paper,Kong W.,60021182;129951188,Sun Yat-Sen University;Guangdong Key Laboratory of Big Data Analysis and Processing,Guangzhou;Guangzhou,China;China,3.0,"Kong, Weiyang;Guo, Ziyu;Liu, Yubao",57220549396;58973942300;35409590200,60021182;60021182;60021182-129951188,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,8.0,,8627-8635,"Traffic flow forecasting is a classical spatio-temporal data mining problem with many real-world applications. Recently, various methods based on Graph Neural Networks (GNN) have been proposed for the problem and achieved impressive prediction performance. However, we argue that the majority of existing methods disregarding the importance of certain nodes (referred to as pivotal nodes) that naturally exhibit extensive connections with multiple other nodes. Predicting on pivotal nodes poses a challenge due to their complex spatio-temporal dependencies compared to other nodes. In this paper, we propose a novel GNN-based method called Spatio-Temporal Pivotal Graph Neural Networks (STPGNN) to address the above limitation. We introduce a pivotal node identification module for identifying pivotal nodes. We propose a novel pivotal graph convolution module, enabling precise capture of spatio-temporal dependencies centered around pivotal nodes. Moreover, we propose a parallel framework capable of extracting spatio-temporal traffic features on both pivotal and non-pivotal nodes. Experiments on seven real-world traffic datasets verify our proposed method's effectiveness and efficiency compared to state-of-the-art baselines.",,67,1.0,all publisherfullgold,All Open Access Gold,NSFC,61572537,National Natural Science Foundation of China
2-s2.0-85204296524,,,,Spear: Evaluate the Adversarial Robustness of Compressed Neural Models,cp,Conference Paper,Yu C.,60009860,Fudan University,Shanghai,China,4.0,"Yu, Chong;Chen, Tao;Gan, Zhongxue;Fan, Jiayuan",55214565300;57192810199;54419884000;56428257800,60009860;60009860;60009860;60009860,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1598-1606,"As Artificial Intelligence evolves, the neural models vulnerable to adversarial attacks may produce fatal results in critical applications.This paper mainly discusses the robustness of the compressed neural models facing adversarial attacks.A few studies discuss the interaction between model compression and adversarial attack.However, they focus on the robustness against the traditional attacks designed for the dense models, not the attacks intended explicitly for the compressed models, using sparsity and quantization techniques.Compressed models often have fewer parameters and smaller sizes that are more friendly to resource-limited devices than dense models, so they are widely deployed in various edge and mobile devices.However, introducing the sparsity and quantization into neural models further imposes higher attack risks.A specific adversarial attack method (Spear) is proposed to generate the particular adversarial attack samples for evaluating the robustness of the compressed models.The Spear attack finds minimal perturbations to create the attack samples to maximize the different behaviors between the compressed and dense reference models.We demonstrate the proposed Spear attack technique can generally be applied to various networks and tasks through quantitative and ablation experiments.",,1,0.0,,,STCSM,2021SHZDZX0103,Fudan University
2-s2.0-85170385119,10.24963/ijcai.2023/487,,,Speeding Up Multi-Objective Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-Structured Parzen Estimator,cp,Conference Paper,Watanabe S.,60025641;60024621,Universität Freiburg;National Institute of Advanced Industrial Science and Technology,Freiburg im Breisgau;Tsukuba,Germany;Japan,4.0,"Watanabe, Shuhei;Awad, Noor;Onishi, Masaki;Hutter, Frank",57215670828;59870139500;7102474552;55931808800,60025641;60025641;60024621;60025641,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,4380-4388,"Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 competition on “Multiobjective Hyperparameter Optimization for Transformers”. See https://arxiv.org/abs/2212.06751 for the latest version with Appendix.",,3,1.0,all publisherfullgold,All Open Access Gold,EC,101045765,European Commission
2-s2.0-85140426982,,,,Stable Conformal Prediction Sets,cp,Conference Paper,Ndiaye E.,60136858,College of Engineering,Atlanta,United States,1.0,"Ndiaye, Eugene",57189097699,60136858,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,16462-16479,"When one observes a sequence of variables (x<inf>1</inf>, y<inf>1</inf>),..., (x<inf>n</inf>, y<inf>n</inf>), Conformal Prediction (CP) is a methodology that allows to estimate a confidence set for y<inf>n+1</inf> given x<inf>n+1</inf> by merely assuming that the distribution of the data is exchangeable. CP sets have guaranteed coverage for any finite population size n. While appealing, the computation of such a set turns out to be infeasible in general, e.g., when the unknown variable y<inf>n+1</inf> is continuous. The bottleneck is that it is based on a procedure that readjusts a prediction model on data where we replace the unknown target by all its possible values in order to select the most probable one. This requires computing an infinite number of models, which often makes it intractable. In this paper, we combine CP techniques with classical algorithmic stability bounds to derive a prediction set computable with a single model fit. We demonstrate that our proposed confidence set does not lose any coverage guarantees while avoiding the need for data splitting as currently done in the literature. We provide some numerical experiments to illustrate the tightness of our estimation when the sample size is sufficiently large, on both synthetic and real datasets.",,19,0.0,,,,,
2-s2.0-85165204320,10.1613/jair.1.14563,,,Stackelberg Security Games with Contagious Attacks on a Network: Reallocation to the Rescue,ar,Article,Bai R.,60017161;60013983;60023237;60022422;60202520,National University of Singapore;City University of Hong Kong;Beijing Normal University;Ocean University of China;State Key Laboratory of Internet of Things for Smart City,Singapore City;Hong Kong;Beijing;Qingdao;Taipa,Singapore;Hong Kong;China;China;Macao,6.0,"Bai, Rufan;Lin, Haoxing;Yang, Xinyu;Wu, Xiaowei;Li, Minming;Jia, Weijia",57192013325;57218844106;59827053800;56198779900;8948338100;26643193700,60202520;60017161;60202520;60202520;60013983-60022422;60023237,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,487-515,"In the classic network security games, the defender distributes defending resources to the nodes of the network, and the attacker attacks a node, with the objective of maximizing the damage caused. In this paper, we consider the network defending problem against contagious attacks, e.g., the attack at a node u spreads to the neighbors of u and can cause damage at multiple nodes. Existing works that study shared resources assume that the resource allocated to a node can be shared or duplicated between neighboring nodes. However, in the real world, sharing resource naturally leads to a decrease in defending power of the source node, especially when defending against contagious attacks. Therefore, we study the model in which resources allocated to a node can only be transferred to its neighboring nodes, which we refer to as a reallocation process. We show that the problem of computing optimal defending strategy is NP-hard even for some very special cases. For positive results, we give a mixed integer linear program formulation for the problem and a bi-criteria approximation algorithm. Our experimental results demonstrate that the allocation and reallocation strategies our algorithm computes perform well in terms of minimizing the damage due to contagious attacks.",,4,1.0,all publisherfullgold,All Open Access Gold,NSFC,28712217900001,National Natural Science Foundation of China
2-s2.0-85144909363,,,,Statistical Optimality and Computational Efficiency of Nyström Kernel PCA,ar,Article,Sterge N.,60001439,Pennsylvania State University,University Park,United States,2.0,"Sterge, Nicholas;Sriperumbudur, Bharath K.",57222398497;18937221200,60001439;60001439,2022-10-01,1 October 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,337,,"Kernel methods provide an elegant framework for developing nonlinear learning algorithms from simple linear methods. Though these methods have superior empirical performance in several real data applications, their usefulness is inhibited by the significant computational burden incurred in large sample situations. Various approximation schemes have been proposed in the literature to alleviate these computational issues, and the approximate kernel machines are shown to retain the empirical performance. However, the theoretical properties of these approximate kernel machines are less well understood. In this work, we theoretically study the trade-off between computational complexity and statistical accuracy in Nyström approximate kernel principal component analysis (KPCA), wherein we show that the Nyström approximate KPCA matches the statistical performance of (non-approximate) KPCA while remaining computationally beneficial. Additionally, we show that Nyström approximate KPCA outperforms the statistical behavior of another popular approximation scheme, the random feature approximation, when applied to KPCA.",covariance operator | kernel PCA | Nyström approximation | Principal component analysis | reproducing kernel Hilbert space | U-statistics,6,0.0,,,,,
2-s2.0-85174858630,,,,Statistical Robustness of Empirical Risks in Machine Learning,ar,Article,Guo S.,60004538;60002798,Dalian University of Technology;Chinese University of Hong Kong,Dalian;Hong Kong,China;Hong Kong,3.0,"Guo, Shaoyan;Xu, Huifu;Zhang, Liwei",55827930700;7407451817;36092139000,60004538;60002798;60004538,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"This paper studies convergence of empirical risks in reproducing kernel Hilbert spaces (RKHS). A conventional assumption in the existing research is that empirical training data are generated by the unknown true probability distribution but this may not be satisfied in some practical circumstances. Consequently the existing convergence results may not provide a guarantee as to whether the empirical risks are reliable or not when the data are potentially corrupted (generated by a distribution perturbed from the true). In this paper, we fill out the gap from robust statistics perspective (Krätschmer, Schied and Zähle (2012); Krätschmer, Schied and Zähle (2014); Guo and Xu (2020)). First, we derive moderate sufficient conditions under which the expected risk changes stably (continuously) against small perturbation of the probability distributions of the underlying random variables and demonstrate how the cost function and kernel affect the stability. Second, we examine the difference between laws of the statistical estimators of the expected optimal loss based on pure data and contaminated data using Prokhorov metric and Kantorovich metric, and derive some asymptotic qualitative and non-asymptotic quantitative statistical robustness results. Third, we identify appropriate metrics under which the statistical estimators are uniformly asymptotically consistent. These results provide theoretical grounding for analysing asymptotic convergence and examining reliability of the statistical estimators in a number of regression models.",asymptotic qualitative statistical robustness | Empirical risks | non-asymptotic quantitative statistical robustness | stability analysis | uniform consistency,3,0.0,,,NKPs,2022YFA1004000,National Key Research and Development Program of China
2-s2.0-85202356603,,,,Stealing Part of a Production Language Model,cp,Conference Paper,Carlini N.,60002494;60025858;60111161;60121131;127691826,"Université McGill;ETH Zürich;DeepMind Technologies Limited;OpenAI, Inc.;University of Washington",Montreal;Zurich;London;San Francisco;Sequim,Canada;Switzerland;United Kingdom;United States;United States,13.0,"Carlini, Nicholas;Paleka, Daniel;Dvijotham, Krishnamurthy;Steinke, Thomas;Hayase, Jonathan;Feder Cooper, A.;Lee, Katherine;Jagielski, Matthew;Nasr, Milad;Conmy, Arthur;Wallace, Eric;Rolnick, David;Tramèr, Florian",57194977162;57814722800;36470045000;56354113000;57223805521;57219588873;57219622918;57203204598;57191968336;57311205200;57207856436;50263066200;56878876400,60111161;60025858;60111161;60111161;127691826;60111161;60111161;60111161;60111161;60111161;60121131;60002494;60025858,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,5680-5705,"We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under $20 USD, our attack extracts the entire projection matrix of OpenAI's ada and babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",,6,0.0,,,,,
2-s2.0-105018583008,,,,"Stochastic Modified Flows, Mean-Field Limits and Dynamics of Stochastic Gradient Descent",ar,Article,Gess B.,60028229;60015595;60025237;60020022,"Universität Hamburg;Universität Bielefeld;National Academy of Sciences in Ukraine, Institute of Mathematics;Max Planck Institute for Mathematics in the Sciences",Hamburg;Bielefeld;Kyiv;Leipzig,Germany;Germany;Ukraine;Germany,3.0,"Gess, Benjamin;Kassing, Sebastian;Konarovskyi, Vitalii",36699482000;57219494017;56429732500,60015595-60020022;60015595;60028229-60025237,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"We propose new limiting dynamics for stochastic gradient descent in the small learning rate regime called stochastic modified flows. These SDEs are driven by a cylindrical Brownian motion and improve the so-called stochastic modified equations by having regular diffusion coefficients and by matching the multi-point statistics. As a second contribution, we introduce distribution dependent stochastic modified flows which we prove to describe the fluctuating limiting dynamics of stochastic gradient descent in the small learning rate - infinite width scaling regime.",fluctuation mean field limit | machine learning | overparametrization | stochastic gradient descent | stochastic modified equation,7,0.0,,,MIS,SFB 1283/2 2021 – 317210226,Universität Bielefeld
2-s2.0-105000502184,,,,Stochastic Optimal Control Matching,cp,Conference Paper,Domingo-Enrich C.,60021784;60105360;119273346;131425073,New York University;Simons Foundation;Meta;NYU Data Science & Meta AI FAIR,New York;New York;Meta;Meta,United States;United States;United States;United States,5.0,"Domingo-Enrich, Carles;Han, Jiequn;Amos, Brandon;Bruna, Joan;Chen, Ricky T.Q.",57200516157;57221974701;55848755000;50461069300;57814726700,131425073;60105360;119273346;60021784;119273346,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Stochastic optimal control, which has the goal of driving the behavior of noisy systems, is broadly applicable in science, engineering and artificial intelligence. Our work introduces Stochastic Optimal Control Matching (SOCM), a novel Iterative Diffusion Optimization (IDO) technique for stochastic optimal control that stems from the same philosophy as the conditional score matching loss for diffusion models. That is, the control is learned via a least squares problem by trying to fit a matching vector field. The training loss, which is closely connected to the cross-entropy loss, is optimized with respect to both the control function and a family of reparameterization matrices which appear in the matching vector field. The optimization with respect to the reparameterization matrices aims at minimizing the variance of the matching vector field. Experimentally, our algorithm achieves lower error than all the existing IDO techniques for stochastic optimal control for three out of four control problems, in some cases by an order of magnitude. The key idea underlying SOCM is the path-wise reparameterization trick, a novel technique that may be of independent interest.",,2,0.0,,,,,
2-s2.0-105000504649,,,,Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators,cp,Conference Paper,Shi Z.,60017161,National University of Singapore,Singapore City,Singapore,4.0,"Shi, Zekun;Hu, Zheyuan;Lin, Min;Kawaguchi, Kenji",58797632700;57224666465;55926433700;57189097784,60017161;60017161;60017161;60017161,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Optimizing neural networks with loss that contain high-dimensional and high-order differential operators is expensive to evaluate with back-propagation due to O(d<sup>k</sup>) scaling of the derivative tensor size and the O(2<sup>k−1</sup>L) scaling in the computation graph, where d is the dimension of the domain, L is the number of ops in the forward computation graph, and k is the derivative order. In previous works, the polynomial scaling in d was addressed by amortizing the computation over the optimization process via randomization. Separately, the exponential scaling in k for univariate functions (d = 1) was addressed with high-order auto-differentiation (AD). In this work, we show how to efficiently perform arbitrary contraction of the derivative tensor of arbitrary order for multivariate functions, by properly constructing the input tangents to univariate high-order AD, which can be used to efficiently randomize any differential operator. When applied to Physics-Informed Neural Networks (PINNs), our method provides >1000× speed-up and >30× memory reduction over randomization with first-order AD, and we can now solve 1-million-dimensional PDEs in 8 minutes on a single NVIDIA A100 GPU<sup>1</sup>. This work opens the possibility of using high-order differential operators in large-scale problems.",,4,0.0,,,,,
2-s2.0-85167965766,10.1609/aaai.v37i3.25421,,,Stop-Gradient Softmax Loss for Deep Metric Learning,cp,Conference Paper,Yang L.,60003977;130082189,Northwestern Polytechnical University;National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology,Xi'an;,China;China,3.0,"Yang, Lu;Wang, Peng;Zhang, Yanning",59046495400;57309244300;56075029000,60003977-130082189;60003977-130082189;60003977-130082189,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,3164-3172,"Deep metric learning aims to learn a feature space that models the similarity between images, and feature normalization is a critical step for boosting performance. However directly optimizing L2-normalized softmax loss cause the network to fail to converge. Therefore some SOTA approaches appends a scale layer after the inner product to relieve the convergence problem, but it incurs a new problem that it's difficult to learn the best scaling parameters. In this letter, we look into the characteristic of softmax-based approaches and propose a novel learning objective function Stop-Gradient Softmax Loss (SGSL) to solve the convergence problem in softmaxbased deep metric learning with L2-normalization. In addition, we found a useful trick named Remove the last BNReLU (RBR). It removes the last BN-ReLU in the backbone to reduce the learning burden of the model. Experimental results on four fine-grained image retrieval benchmarks show that our proposed approach outperforms most existing approaches, i.e., our approach achieves 75.9% on CUB-200-2011, 94.7% on CARS196 and 83.1% on SOP which outperforms other approaches at least 1.7%, 2.9% and 1.7% on Recall@1.",,4,1.0,all publisherfullgold,All Open Access Gold,NSFC,2021KWZ-03,National Natural Science Foundation of China
2-s2.0-85137909107,10.24963/ijcai.2022/75,,,Strategy Proof Mechanisms for Facility Location with Capacity Limits,cp,Conference Paper,Walsh T.,60028333,UNSW Sydney,Sydney,Australia,1.0,"Walsh, Toby",55806690200,60028333,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,527-533,"An important feature of many real world facility location problems are capacity limits on the number of agents served by each facility. We provide a comprehensive picture of strategy proof mechanisms for facility location problems with capacity constraints that are anonymous and Pareto optimal. First, we prove a strong characterization theorem. For locating two identical facilities with capacity limits and no spare capacity, the INNERPOINT mechanism is the unique strategy proof mechanism that is both anonymous and Pareto optimal. Second, when there is spare capacity, we identify a more general class of strategy proof mechanisms that interpolates smoothly between INNERPOINT and ENDPOINT which are anonymous and Pareto optimal. Third, with two facilities of different capacities, we prove a strong impossibility theorem that no mechanism can be both anonymous and Pareto optimal except when the capacities differ by just a single agent. Fourth, with three or more facilities we prove a second impossibility theorem that no mechanism can be both anonymous and Pareto optimal even when facilities have equal capacity. Our characterization and impossibility results are all minimal as multiple mechanisms exist if we drop one property.",,12,1.0,all publisherfree2read,All Open Access Bronze,ARC,FL200100204,Australian Research Council
2-s2.0-85204287604,,,,Streamflow Prediction with Uncertainty Quantification for Water Management: A Constrained Reasoning and Learning Approach,cp,Conference Paper,Gharsallaoui M.A.,60018208,Washington State University Pullman,Pullman,United States,7.0,"Gharsallaoui, Mohammed Amine;Singh, Bhupinderjeet;Savalkar, Supriya;Deshwal, Aryan;Kalyanaraman, Ananth;Rajagopalan, Kirti;Doppa, Janardhan Rao",57219489955;58235374700;58235825000;57211256632;35117700300;55530724200;35324430700,60018208;60018208;60018208;60018208;60018208;60018208;60018208,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,7269-7277,"Predicting the spatiotemporal variation in streamflow along with uncertainty quantification enables decision-making for sustainable management of scarce water resources. Process-based hydrological models (aka physics-based models) are based on physical laws, but using simplifying assumptions which can lead to poor accuracy. Data-driven approaches offer a powerful alternative, but they require large amount of training data and tend to produce predictions that are inconsistent with physical laws. This paper studies a constrained reasoning and learning (CRL) approach where physical laws represented as logical constraints are integrated as a layer in the deep neural network. To address small data setting, we develop a theoretically-grounded training approach to improve the generalization accuracy of deep models. For uncertainty quantification, we combine the synergistic strengths of Gaussian processes (GPs) and deep temporal models (i.e., deep models for time-series forecasting) by passing the learned latent representation as input to a standard distance-based kernel. Experiments on multiple real-world datasets demonstrate the effectiveness of both CRL and GP with deep kernel approaches over strong baseline methods.",,2,0.0,,,USDA,2021-67021-35344,U.S. Department of Agriculture
2-s2.0-85203838035,,,,Structure Your Data: Towards Semantic Graph Counterfactuals,cp,Conference Paper,Dimitriou A.,60002947,National Technical University of Athens (NTUA),Athens,Greece,5.0,"Dimitriou, Angeliki;Lymperaiou, Maria;Filandrianos, Giorgos;Thomas, Konstantinos;Stamou, Giorgos",58853578500;57900314700;57223044127;58954177200;7004137698,60002947;60002947;60002947;60002947;60002947,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,10897-10926,"Counterfactual explanations (CEs) based on concepts are explanations that consider alternative scenarios to understand which high-level semantic features contributed to particular model predictions. In this work, we propose CEs based on the semantic graphs accompanying input data to achieve more descriptive, accurate, and human-aligned explanations. Building upon state-of-the-art (SotA) conceptual attempts, we adopt a model-agnostic edit-based approach and introduce leveraging GNNs for efficient Graph Edit Distance (GED) computation. With a focus on the visual domain, we represent images as scene graphs and obtain their GNN embeddings to bypass solving the NP-hard graph similarity problem for all input pairs, an integral part of CE computation process. We apply our method to benchmark and real-world datasets with varying difficulty and availability of semantic annotations. Testing on diverse classifiers, we find that our CEs outperform previous SotA explanation models based on semantics, including both white and black-box as well as conceptual and pixel-level approaches. Their superiority is proven quantitatively and qualitatively, as validated by human subjects, highlighting the significance of leveraging semantic edges in the presence of intricate relationships. Our model-agnostic graph-based approach is widely applicable and easily extensible, producing actionable explanations across different contexts. The code is available at https://github.com/aggeliki-dimitriou/SGCE.",,2,0.0,,,EuroHPC JU,101093457,European High Performance Computing Joint Undertaking
2-s2.0-85126031315,10.1613/JAIR.1.12370,,,Sum-of-Products with Default Values: Algorithms and Complexity Results,ar,Article,Ganian R.,60018163;60008924,TU Wien;Université Paris-Dauphine,Vienna;Paris,Austria;France,4.0,"Ganian, Robert;Kim, Eun Jung;Slivovsky, Friedrich;Szeider, Stefan",35118870500;55568531620;55321141700;6602786002,60018163;60008924;60018163;60018163,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,73,,,535-552,"Weighted Counting for Constraint Satisfaction with Default Values (#CSPD) is a powerful special case of the sum-of-products problem that admits succinct encodings of #CSP, #SAT, and inference in probabilistic graphical models. We investigate #CSPD under the fundamental parameter of incidence treewidth (i.e., the treewidth of the incidence graph of the constraint hypergraph). We show that if the incidence treewidth is bounded, #CSPD can be solved in polynomial time. More specifically, we show that the problem is fixed-parameter tractable for the combined parameter incidence treewidth, domain size, and support size (the maximum number of non-default tuples in a constraint). This generalizes known results on the fixed-parameter tractability of #CSPD under the combined parameter primal treewidth and domain size. We further prove that the problem is not fixed-parameter tractable if any of the three components is dropped from the parameterization.",,3,1.0,all publisherfullgold,All Open Access Gold,FWF,P 27721,Austrian Science Fund
2-s2.0-85146110054,10.1609/aaai.v36i3.20239,,,Suppressing Static Visual Cues via Normalizing Flows for Self-Supervised Video Representation Learning,cp,Conference Paper,Zhang M.,60001604;60021182,Ministry of Education of the People's Republic of China;Sun Yat-Sen University,Beijing;Guangzhou,China;China,3.0,"Zhang, Manlin;Wang, Jinpeng;Ma, Andy J.",57226277642;57224010012;54958223900,60021182;60021182;60021182-60001604,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,3300-3308,"Despite the great progress in video understanding made by deep convolutional neural networks, feature representation learned by existing methods may be biased to static visual cues. To address this issue, we propose a novel method to suppress static visual cues (S<sup>2</sup>VC) based on probabilistic analysis for self-supervised video representation learning. In our method, video frames are first encoded to obtain latent variables under standard normal distribution via normalizing flows. By modelling static factors in a video as a random variable, the conditional distribution of each latent variable becomes shifted and scaled normal. Then, the less-varying latent variables along time are selected as static cues and suppressed to generate motion-preserved videos. Finally, positive pairs are constructed by motion-preserved videos for contrastive learning to alleviate the problem of representation bias to static cues. The less-biased video representation can be better generalized to various downstream tasks. Extensive experiments on publicly available benchmarks demonstrate that the proposed method outperforms the state of the art when only single RGB modality is used for pre-training.",,7,1.0,all publisherfullgold,All Open Access Gold,NSFC,61906218,National Natural Science Foundation of China
2-s2.0-85191592704,,,,Surrogate Assisted Semi-supervised Inference for High Dimensional Risk Prediction,ar,Article,Hou J.,60032499;60013223;60120411,Harvard T.H. Chan School of Public Health;School of Public Health;Department of Statistics and Biostatistics,Boston;Minneapolis;Piscataway,United States;United States;United States,3.0,"Hou, Jue;Guo, Zijian;Cai, Tianxi",57225924112;56201814200;7102610149,60013223;60120411;60032499,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,265,,"Risk modeling with electronic health records (EHR) data is challenging due to no direct observations of the disease outcome and the high-dimensional predictors. In this paper, we develop a surrogate assisted semi-supervised learning approach, leveraging small labeled data with annotated outcomes and extensive unlabeled data of outcome surrogates and high-dimensional predictors. We propose to impute the unobserved outcomes by constructing a sparse imputation model with outcome surrogates and high-dimensional predictors. We further conduct a one-step bias correction to enable interval estimation for the risk prediction. Our inference procedure is valid even if both the imputation and risk prediction models are misspecified. Our novel way of ultilizing unlabelled data enables the high-dimensional statistical inference for the challenging setting with a dense risk prediction model. We present an extensive simulation study to demonstrate the superiority of our approach compared to existing supervised methods. We apply the method to genetic risk prediction of type-2 diabetes mellitus using an EHR biobank cohort.",generalized linear models | high dimensional inference | model mis-specification | risk prediction | semi-supervised learning,7,0.0,,,,,
2-s2.0-105018463797,,,,Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee,ar,Article,Chen G.H.,60027950,Carnegie Mellon University,Pittsburgh,United States,1.0,"Chen, George H.",7407502219,60027950,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Kernel survival analysis models estimate individual survival distributions with the help of a kernel function, which measures the similarity between any two data points. Such a kernel function can be learned using deep kernel survival models. In this paper, we present a new deep kernel survival model called a survival kernet, which scales to large datasets in a manner that is amenable to model interpretation and also theoretical analysis. Specifically, the training data are partitioned into clusters based on a recently developed training set compression scheme for classification and regression called kernel netting that we extend to the survival analysis setting. At test time, each data point is represented as a weighted combination of these clusters, and each such cluster can be visualized. For a special case of survival kernets, we establish a finite-sample error bound on predicted survival distributions that is, up to a log factor, optimal. Whereas scalability at test time is achieved using the aforementioned kernel netting compression strategy, scalability during training is achieved by a warm-start procedure based on tree ensembles such as xgboost and a heuristic approach to accelerating neural architecture search. On four standard survival analysis datasets of varying sizes (up to roughly 3 million data points), we show that survival kernets are highly competitive compared to various baselines tested in terms of time-dependent concordance index.",interpretability | kernel methods | neural networks | scalability | survival analysis,3,0.0,,,NSF,2047981,National Science Foundation
2-s2.0-85185374444,,,,T-Cal: An Optimal Test for the Calibration of Predictive Models,ar,Article,Lee D.,60006297;60102562,University of Pennsylvania;School of Engineering and Applied Science,Philadelphia;Philadelphia,United States;United States,4.0,"Lee, Donghwan;Huang, Xinmeng;Hassani, Hamed;Dobriban, Edgar",59444921500;57216760747;57192302656;55711190100,60006297;60006297;60102562;60006297,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,335,,"The prediction accuracy of machine learning methods is steadily increasing, but the calibration of their uncertainty predictions poses a significant challenge. Numerous works focus on obtaining well-calibrated predictive models, but less is known about reliably assessing model calibration. This limits our ability to know when algorithms for improving calibration have a real effect, and when their improvements are merely artifacts due to random noise in finite datasets. In this work, we consider detecting mis-calibration of predictive models using a finite validation dataset as a hypothesis testing problem. The null hypothesis is that the predictive model is calibrated, while the alternative hypothesis is that the deviation from calibration is sufficiently large. We find that detecting mis-calibration is only possible when the conditional probabilities of the classes are sufficiently smooth functions of the predictions. When the conditional class probabilities are Hölder continuous, we propose T-Cal, a minimax optimal test for calibration based on a debiased plug-in estimator of the ℓ<inf>2</inf>-Expected Calibration Error (ECE). We further propose adaptive T-Cal, a version that is adaptive to unknown smoothness. We verify our theoretical findings with a broad range of experiments, including with several popular deep neural net architectures and several standard post-hoc calibration methods. T-Cal is a practical general-purpose tool, which—combined with classical tests for discrete-valued predictors—can be used to test the calibration of virtually any probabilistic classification method. T-Cal is available at https://github.com/dh7401/T-Cal.",calibration | hypothesis testing | minimax optimality | nonparametric statistics | uncertainty quantification,5,0.0,,,AFOSR,TRIPODS 1934960,Air Force Office of Scientific Research
2-s2.0-85189635325,10.1609/aaai.v38i17.29866,,,TACIT: A Target-Agnostic Feature Disentanglement Framework for Cross-Domain Text Classification,cp,Conference Paper,Song R.,60007711;60015986,Jilin University;Università di Trento,Changchun;Trento,China;Italy,5.0,"Song, Rui;Giunchiglia, Fausto;Li, Yingji;Tian, Mingjie;Xu, Hao",57201394498;7005768672;58094003000;57484158600;55493779900,60007711;60007711-60015986;60007711;60007711;60007711,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,17.0,,18999-19007,"Cross-domain text classification aims to transfer models from label-rich source domains to label-poor target domains, giving it a wide range of practical applications. Many approaches promote cross-domain generalization by capturing domain-invariant features. However, these methods rely on unlabeled samples provided by the target domains, which renders the model ineffective when the target domain is agnostic. Furthermore, the models are easily disturbed by shortcut learning in the source domain, which also hinders the improvement of domain generalization ability. To solve the aforementioned issues, this paper proposes TACIT, a target domain agnostic feature disentanglement framework which adaptively decouples robust and unrobust features by Variational Auto-Encoders. Additionally, to encourage the separation of unrobust features from robust features, we design a feature distillation task that compels unrobust features to approximate the output of the teacher. The teacher model is trained with a few easy samples that are easy to carry potential unknown shortcuts. Experimental results verify that our framework achieves comparable results to state-of-the-art baselines while utilizing only source domain data.",,9,1.0,all publisherfullgold,All Open Access Gold,NSFC,JJKH20200993K,Education Department of Jilin Province
2-s2.0-85174351024,,,,TANGOS: REGULARIZING TABULAR NEURAL NETWORKS THROUGH GRADIENT ORTHOGONALIZATION AND SPECIALIZATION,cp,Conference Paper,Jeffares A.,60027550;60031101,"University of California, Los Angeles;University of Cambridge",Los Angeles;Cambridge,United States;United Kingdom,5.0,"Jeffares, Alan;Liu, Tennison;Crabbé, Jonathan;Imrie, Fergus;van der Schaar, Mihaela",57310120700;57984912300;57221317222;57196214743;35605361700,60031101;60031101;60031101;60027550;60031101,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Despite their success with unstructured data, deep neural networks are not yet a panacea for structured tabular data. In the tabular domain, their efficiency crucially relies on various forms of regularization to prevent overfitting and provide strong generalization performance. Existing regularization techniques include broad modelling decisions such as choice of architecture, loss functions, and optimization methods. In this work, we introduce Tabular Neural Gradient Orthogonalization and Specialization (TANGOS), a novel framework for regularization in the tabular setting built on latent unit attributions. The gradient attribution of an activation with respect to a given input feature suggests how the neuron attends to that feature, and is often employed to interpret the predictions of deep networks. In TANGOS, we take a different approach and incorporate neuron attributions directly into training to encourage orthogonalization and specialization of latent attributions in a fully-connected network. Our regularizer encourages neurons to focus on sparse, non-overlapping input features and results in a set of diverse and specialized latent units. In the tabular domain, we demonstrate that our approach can lead to improved out-of-sample generalization performance, outperforming other popular regularization methods. We provide insight into why our regularizer is effective and demonstrate that TANGOS can be applied jointly with existing methods to achieve even greater generalization performance.",,16,0.0,,,CF,1722516,Cystic Fibrosis Trust
2-s2.0-85199918722,,,,TEMPORAL DEPENDENCIES IN FEATURE IMPORTANCE FOR TIME SERIES PREDICTION,cp,Conference Paper,Leung K.K.,60014171;119273346;121657972,University of Waterloo;Meta;Layer 6 AI,Waterloo;Meta;Toronto,Canada;United States;Canada,5.0,"Leung, Kin Kwan;Rooke, Clayton;Smith, Jonathan;Zuberi, Saba;Volkovs, Maksims",57219620921;58189136300;58189645400;57219433096;22939798200,121657972;60014171;119273346;121657972;121657972,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Time series data introduces two key challenges for explainability methods: firstly, observations of the same feature over subsequent time steps are not independent, and secondly, the same feature can have varying importance to model predictions over time. In this paper, we propose Windowed Feature Importance in Time (WinIT), a feature removal based explainability approach to address these issues. Unlike existing feature removal explanation methods, WinIT explicitly accounts for the temporal dependence between different observations of the same feature in the construction of its importance score. Furthermore, WinIT captures the varying importance of a feature over time, by summarizing its importance over a window of past time steps. We conduct an extensive empirical study on synthetic and real-world data, compare against a wide range of leading explainability methods, and explore the impact of various evaluation strategies. Our results show that WinIT achieves significant gains over existing methods, with more consistent performance across different evaluation metrics.",,14,0.0,,,,,
2-s2.0-85204287141,,,,TFLOP: Table Structure Recognition Framework with Layout Pointer Mechanism,cp,Conference Paper,Khang M.,128913890,Upstage AI,Seoul,South Korea,2.0,"Khang, Minsoo;Hong, Teakgyu",58587815400;57221090314,128913890;128913890,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,947-955,"Table Structure Recognition (TSR) is a task aimed at converting table images into a machine-readable format (e.g. HTML), to facilitate other applications such as information retrieval. Recent works tackle this problem by identifying the HTML tags and text regions, where the latter is used for text extraction from the table document. These works however, suffer from misalignment issues when mapping text into the identified text regions. In this paper, we introduce a new TSR framework, called TFLOP (TSR Framework with LayOut Pointer mechanism), which reformulates the conventional text region prediction and matching into a direct text region pointing problem. Specifically, TFLOP utilizes text region information to identify both the table's structure tags and its aligned text regions, simultaneously. Without the need for region prediction and alignment, TFLOP circumvents the additional text region matching stage, which requires finely-calibrated post-processing. TFLOP also employs span-aware contrastive supervision to enhance the pointing mechanism in tables with complex structure. As a result, TFLOP achieves the state-of-the-art performance across multiple benchmarks such as PubTabNet, FinTabNet, and SynthTabNet. In our extensive experiments, TFLOP not only exhibits competitive performance but also shows promising results on industrial document TSR scenarios such as documents with watermarks or in non-English domain. Source code of our work is publicly available at: https://github.com/UpstageAI/TFLOP.",,1,0.0,,,,,
2-s2.0-85140192920,,,,THE EFFECTS OF REWARD MISSPECIFICATION: MAPPING AND MITIGATING MISALIGNED MODELS,cp,Conference Paper,Pan A.,60025038;60031581,"University of California, Berkeley;California Institute of Technology",Berkeley;Pasadena,United States;United States,3.0,"Pan, Alexander;Bhatia, Kush;Steinhardt, Jacob",57426148700;57189097357;56461039200,60031581;60025038;60025038,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Reward hacking-where RL agents exploit gaps in misspecified reward functions-has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.",,74,0.0,,,NSF,2031985,National Science Foundation
2-s2.0-85138720458,,,,THE SPECTRAL BIAS OF POLYNOMIAL NEURAL NETWORKS,cp,Conference Paper,Choraria M.,60000745;60104653;126100075,University of Illinois Urbana-Champaign;Université Grenoble Alpes;EPFL,Urbana;Saint Martin d'Heres;Versoix,United States;France;Switzerland,5.0,"Choraria, Moulik;Dadi, Leello;Chrysos, Grigorios G.;Mairal, Julien;Cevher, Volkan",57216362693;57218709227;57188639985;35488349500;6506127004,60000745;126100075;126100075;60104653;126100075,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a spectral bias towards low-frequency functions, which yields faster learning of low-frequency components during training. Inspired by such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the Π-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies. We verify the theoretical bias through extensive experiments. We expect our analysis to provide novel insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials.",,15,0.0,,,ARO,ANR19-P3IA-0003,Army Research Office
2-s2.0-85200568592,,,,TIME-EFFICIENT REINFORCEMENT LEARNING WITH STOCHASTIC STATEFUL POLICIES,cp,Conference Paper,Al-Hafez F.,60011226,Technische Universität Darmstadt,Darmstadt,Germany,4.0,"Al-Hafez, Firas;Zhao, Guoping;Peters, Jan;Tateo, Davide",58141728700;57190954443;35248912800;57201780401,60011226;60011226;60011226;60011226,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Stateful policies play an important role in reinforcement learning, such as handling partially observable environments, enhancing robustness, or imposing an inductive bias directly into the policy structure. The conventional method for training stateful policies is Backpropagation Through Time (BPTT), which comes with significant drawbacks, such as slow training due to sequential gradient propagation and the occurrence of vanishing or exploding gradients. The gradient is often truncated to address these issues, resulting in a biased policy update. We present a novel approach for training stateful policies by decomposing the latter into a stochastic internal state kernel and a stateless policy, jointly optimized by following the stateful policy gradient. We introduce different versions of the stateful policy gradient theorem, enabling us to easily instantiate stateful variants of popular reinforcement learning and imitation learning algorithms. Furthermore, we provide a theoretical analysis of our new gradient estimator and compare it with BPTT. We evaluate our approach on complex continuous control tasks, e.g. humanoid locomotion, and demonstrate that our gradient estimator scales effectively with task complexity while offering a faster and simpler alternative to BPTT.",,0,0.0,,,BMBF,SE1042/41-1,Bundesministerium für Bildung und Forschung
2-s2.0-85148478424,10.1613/JAIR.1.13791,,,TOOLTANGO: Common sense Generalization in Predicting Sequential Tool Interactions for Robot Plan Synthesis,ar,Article,Tuli S.,60015150;60032730;60141508,Imperial College London;Indian Institute of Technology Delhi;Stanford Engineering,London;New Delhi;Stanford,United Kingdom;India;United States,4.0,"Tuli, Shreshth;Bansal, Rajas;Paul, Rohan;Mausam, ",57208274339;57219690883;24341969800;55963402300,60015150;60141508;60032730;60032730,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,1595-1631,"Robots assisting us in environments such as factories or homes must learn to make use of objects as tools to perform tasks, for instance, using a tray to carry objects. We consider the problem of learning common sense knowledge of when a tool may be useful and how its use may be composed with other tools to accomplish a high-level task instructed by a human. Specifically, we introduce a novel neural model, termed TOOLTANGO, that first predicts the next tool to be used, and then uses this information to predict the next action. We show that this joint model can inform learning of a fine-grained policy enabling the robot to use a particular tool in sequence and adds a significant value in making the model more accurate. TOOLTANGO encodes the world state, comprising objects and symbolic relationships between them, using a graph neural network and is trained using demonstrations from human teachers instructing a virtual robot in a physics simulator. The model learns to attend over the scene using knowledge of the goal and the action history, finally decoding the symbolic action to execute. Crucially, we address generalization to unseen environments where some known tools are missing, but unseen alternative tools are present. We show that by augmenting the representation of the environment with pre-trained embeddings derived from a knowledge-base, the model can generalize effectively to novel environments. Experimental results show at least 48.8-58.1% absolute improvement over the baselines in predicting successful symbolic plans for a simulated mobile manipulator in novel environments with unseen objects. This work takes a step in the direction of enabling robots to rapidly synthesize robust plans for complex tasks, particularly in novel settings.",,4,1.0,all publisherfullgold,All Open Access Gold,IBM,1MG,International Business Machines Corporation
2-s2.0-85148765302,,,,TOPOLOGICAL GRAPH NEURAL NETWORKS,cp,Conference Paper,Horn M.,60025063;60025858;60019722;60026285;128817995,KU Leuven;ETH Zürich;Technische Universität München;SIB Swiss Institute of Bioinformatics;Institute of AI for Health,Leuven;Zurich;Munich;Lausanne;Oberschleissheim,Belgium;Switzerland;Germany;Switzerland;Germany,6.0,"Horn, Max;De Brouwer, Edward;Moor, Michael;Moreau, Yves;Rieck, Bastian;Borgwardt, Karsten",57210642672;57216982652;57203169123;7006734235;55415757000;10044984100,60025858-60026285;60025063;60025858-60026285;60025063;60025858-60026285-128817995-60019722;60025858-60026285,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler-Lehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.",,57,0.0,,,VSC,06260,Nvidia
2-s2.0-85194212850,,,,TORCHRL: A DATA-DRIVEN DECISION-MAKING LIBRARY FOR PYTORCH,cp,Conference Paper,Bou A.,60031101;60355330;126335720;131533755;131534607,University of Cambridge;Meta Ai;Meta;Acellera;UPF,Cambridge;Menlo Park;Meta;;,United Kingdom;United States;United States;;,8.0,"Bou, Albert;Bettini, Matteo;Dittert, Sebastian;Kumar, Vikash;Sodhani, Shagun;Yang, Xiaomeng;De Fabritiis, Gianni;Moens, Vincent",57219741088;57448888500;57226120029;57202530476;57200219650;57779466200;22134417700;57204809097,131533755;60031101;131534607;60355330;60355330;60355330;131533755;126335720,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"PyTorch has ascended as a premier machine learning framework, yet it lacks a native and comprehensive library for decision and control tasks suitable for large development teams dealing with complex real-world data and environments. To address this issue, we propose TorchRL, a generalistic control library for PyTorch that provides well-integrated, yet standalone components. We introduce a new and flexible PyTorch primitive, the TensorDict, which facilitates streamlined algorithm development across the many branches of Reinforcement Learning (RL) and control. We provide a detailed description of the building blocks and an extensive overview of the library across domains and tasks. Finally, we experimentally demonstrate its reliability and flexibility and show comparative benchmarks to demonstrate its computational efficiency. TorchRL fosters long-term support and is publicly available on GitHub for greater reproducibility and collaboration within the research community. The code is open-sourced on GitHub.",,8,0.0,,,,,
2-s2.0-85200551357,,,,TOWARDS CODABLE WATERMARKING FOR INJECTING MULTI-BITS INFORMATION TO LLMS,cp,Conference Paper,Wang L.,60014966;60014402;60114181;130708614,Peking University;Renmin University of China;Tencent;DeepSeek-AI,Beijing;Beijing;Shenzhen;Beijing,China;China;China;China,8.0,"Wang, Lean;Yang, Wenkai;Chen, Deli;Zhou, Hao;Lin, Yankai;Meng, Fandong;Zhou, Jie;Sun, Xu",57980499600;57223326837;57211981413;56898234800;57155321900;55847567500;57211746430;55744667900,60014966-60114181;60014402-60114181;130708614;60114181;60014402;60114181;60114181;60014966,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns. However, we argue that existing LLM watermarking methods are encoding-inefficient and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.). In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit customizable information. First of all, we study the taxonomy of LLM watermarking technologies and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive evaluation system for CTWL: (1) watermarking success rate, (2) robustness against various corruptions, (3) coding rate of payload information, (4) encoding and decoding efficiency, (5) impacts on the quality of the generated text. To meet the requirements of these non-Pareto-improving metrics, we follow the most prominent vocabulary partition-based watermarking direction, and devise an advanced CTWL method named Balance-Marking. The core idea of our method is to use a proxy language model to split the vocabulary into probability-balanced parts, thereby effectively maintaining the quality of the watermarked text. Extensive experimental results show that our method outperforms the baseline under comprehensive evaluation. Our code is available at https://github.com/lancopku/codable-watermarking-for-llm.",,14,0.0,,,NSFC,62176002,National Natural Science Foundation of China
2-s2.0-85142387433,,,,TOWARDS MODEL-AGNOSTIC FEDERATED LEARNING USING KNOWLEDGE DISTILLATION,cp,Conference Paper,Afonin A.,60025038;126100075,"University of California, Berkeley;EPFL",Berkeley;Versoix,United States;Switzerland,2.0,"Afonin, Andrei;Karimireddy, Sai Praneeth",57223756854;57204808415,126100075;126100075-60025038,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Is it possible to design an universal API for federated learning using which an ad-hoc group of data-holders (agents) collaborate with each other and perform federated learning? Such an API would necessarily need to be model-agnostic i.e. make no assumption about the model architecture being used by the agents, and also cannot rely on having representative public data at hand. Knowledge distillation (KD) is the obvious tool of choice to design such protocols. However, surprisingly, we show that most natural KD-based federated learning protocols have poor performance. To investigate this, we propose a new theoretical framework, Federated Kernel ridge regression, which can capture both model heterogeneity as well as data heterogeneity. Our analysis shows that the degradation is largely due to a fundamental limitation of knowledge distillation under data heterogeneity. We further validate our framework by analyzing and designing new protocols based on KD. Their performance on real world experiments using neural networks, though still unsatisfactory, closely matches our theoretical predictions.",,23,0.0,,,EPFL,,École Polytechnique Fédérale de Lausanne
2-s2.0-85200571246,,,,TOWARDS OFFLINE OPPONENT MODELING WITH IN-CONTEXT LEARNING,cp,Conference Paper,Jing Y.,60027363;60025278;60018486;60114181;131534676,University of Chinese Academy of Sciences;Tsinghua University;Institute of Automation Chinese Academy of Sciences;Tencent;AiRiA,Beijing;Beijing;Beijing;Shenzhen;,China;China;China;China;,8.0,"Jing, Yuheng;Li, Kai;Liu, Bingyun;Zang, Yifan;Fu, Haobo;Fu, Qiang;Xing, Junliang;Cheng, Jian",59249396200;59361943300;59092743300;57220550404;57219645569;57219496314;35209161400;7405940032,60018486-60027363;60018486-60027363;60018486-60027363;60018486-60027363;60114181;60114181;60025278;60018486-60027363-131534676,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Opponent modeling aims at learning the opponent's behaviors, goals, or beliefs to reduce the uncertainty of the competitive environment and assist decision-making. Existing work has mostly focused on learning opponent models online, which is impractical and inefficient in practical scenarios. To this end, we formalize an Offline Opponent Modeling (OOM) problem with the objective of utilizing pre-collected offline datasets to learn opponent models that characterize the opponent from the viewpoint of the controlled agent, which aids in adapting to the unknown fixed policies of the opponent. Drawing on the promises of the Transformers for decision-making, we introduce a general approach, Transformer Against Opponent (TAO), for OOM. Essentially, TAO tackles the problem by harnessing the full potential of the supervised pre-trained Transformers' in-context learning capabilities. The foundation of TAO lies in three stages: an innovative offline policy embedding learning stage, an offline opponent-aware response policy training stage, and a deployment stage for opponent adaptation with in-context learning. Theoretical analysis establishes TAO's equivalence to Bayesian posterior sampling in opponent modeling and guarantees TAO's convergence in opponent policy recognition. Extensive experiments and ablation studies on competitive environments with sparse and dense rewards demonstrate the impressive performance of TAO. Our approach manifests remarkable prowess for fast adaptation, especially in the face of unseen opponent policies, confirming its in-context learning potency.",,1,0.0,,,CCF,BE2023016,China Computer Federation
2-s2.0-85200553157,,,,TOWARDS ROBUST MULTI-MODAL REASONING VIA MODEL SELECTION,cp,Conference Paper,Liu X.,60003970;60017161;60117660,Zhejiang University;National University of Singapore;Westlake University,Hangzhou;Singapore City;Hangzhou,China;Singapore;China,4.0,"Liu, Xiangyan;Li, Rongxue;Ji, Wei;Lin, Tao",58560998700;58664535400;57208491066;57193603023,60017161;60117660-60003970;60017161;60117660,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the “brain” of the agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will mainly invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges therein and propose the M<sup>3</sup> framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.",,2,0.0,,,,2022ZD0115101,Westlake University
2-s2.0-85150387669,,,,TRUST REGION POLICY OPTIMISATION IN MULTI-AGENT REINFORCEMENT LEARNING,cp,Conference Paper,Kuba J.G.,60026851;60022148;60025084;60014966;60105232;127751972,University of Oxford;University College London;Shanghai Jiao Tong University;Peking University;ShanghaiTech University;Huawei R&D UK,Oxford;London;Shanghai;Beijing;Shanghai;,United Kingdom;United Kingdom;China;China;China;United Kingdom,7.0,"Kuba, Jakub Grudzien;Chen, Ruiqing;Wen, Muning;Wen, Ying;Sun, Fanglei;Wang, Jun;Yang, Yaodong",57238555400;57730856400;57224978188;57193525684;57272074800;55902731900;56167927700,60026851-127751972;60105232;60025084;60025084;60105232;60022148;60014966,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Trust region methods rigorously enabled reinforcement learning (RL) agents to learn monotonically improving policies, leading to superior performance on a variety of tasks. Unfortunately, when it comes to multi-agent reinforcement learning (MARL), the property of monotonic improvement may not simply apply; this is because agents, even in cooperative games, could have conflicting directions of policy updates. As a result, achieving a guaranteed improvement on the joint policy where each agent acts individually remains an open challenge. In this paper, we extend the theory of trust region learning to cooperative MARL. Central to our findings are the multi-agent advantage decomposition lemma and the sequential policy update scheme. Based on these, we develop Heterogeneous-Agent Trust Region Policy Optimisation (HATPRO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO) algorithms. Unlike many existing MARL algorithms, HATRPO/HAPPO do not need agents to share parameters, nor do they need any restrictive assumptions on decomposibility of the joint value function. Most importantly, we justify in theory the monotonic improvement property of HATRPO/HAPPO. We evaluate the proposed methods on a series of Multi-Agent MuJoCo and StarCraftII tasks. Results show that HATRPO and HAPPO significantly outperform strong baselines such as IPPO, MAPPO and MADDPG on all tested tasks, thereby establishing a new state of the art.",,126,0.0,,,,,
2-s2.0-85174389052,,,,Task-Specific Skill Localization in Fine-tuned Language Models,cp,Conference Paper,Panigrahi A.,60141284,School of Engineering and Applied Science,Princeton,United States,4.0,"Panigrahi, Abhishek;Saunshi, Nikunj;Zhao, Haoyu;Arora, Sanjeev",57528517200;57205404603;59810711400;7202419160,60141284;60141284;60141284;60141284,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,27011-27033,"Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific “skills,” but there has been limited study of where these newlylearnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters (∼ 0.01% of model parameters) responsible for (> 95%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives a performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanilla fine-tuning with respect to calibration of predictions in-distribution (40-90% error reduction) as well as the quality of predictions out-of-distribution (OOD). In models trained on multiple tasks, a stronger notion of skill localization is observed, where the sparse regions corresponding to different tasks are almost disjoint, and their overlap (when it happens) is a proxy for task similarity. Experiments suggest that localization via grafting can assist certain forms of continual learning. Our code is available at Skill-Localization-by-grafting.",,29,0.0,,,NSF,,National Science Foundation
2-s2.0-85189549141,10.1609/aaai.v38i6.28330,,,Taxonomy Driven Fast Adversarial Training,cp,Conference Paper,Tong K.,60001604;60005244;60022422;60278946,Ministry of Education of the People's Republic of China;Southeast University;Ocean University of China;Purple Mountain Laboratory,Beijing;Nanjing;Qingdao;Nanjing,China;China;China;China,4.0,"Tong, Kun;Jiang, Chengze;Gui, Jie;Cao, Yuan",58972483700;57216225826;25122127900;56710768700,60005244;60005244;60005244-60001604-60278946;60022422,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,6.0,,5233-5242,"Adversarial training (AT) is an effective defense method against gradient-based attacks to enhance the robustness of neural networks. Among them, single-step AT has emerged as a hotspot topic due to its simplicity and efficiency, requiring only one gradient propagation in generating adversarial examples. Nonetheless, the problem of catastrophic overfitting (CO) that causes training collapse remains poorly understood, and there exists a gap between the robust accuracy achieved through single- and multi-step AT. In this paper, we present a surprising finding that the taxonomy of adversarial examples reveals the truth of CO. Based on this conclusion, we propose taxonomy driven fast adversarial training (TDAT) which jointly optimizes learning objective, loss function, and initialization method, thereby can be regarded as a new paradigm of single-step AT. Compared with other fast AT methods, TDAT can boost the robustness of neural networks, alleviate the influence of misclassified examples, and prevent CO during the training process while requiring almost no additional computational and memory resources. Our method achieves robust accuracy improvement of 1.59%, 1.62%, 0.71%, and 1.26% on CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet-100 datasets, when against projected gradient descent PGD10 attack with perturbation budget 8/255. Furthermore, our proposed method also achieves state-of-the-art robust accuracy against other attacks. Code is available at https://github.com/bookman233/TDAT.",,15,1.0,all publisherfullgold,All Open Access Gold,NSFC,62172090,National Natural Science Foundation of China
2-s2.0-85183902593,10.1613/jair.1.15315,,,The AI Race: Why Current Neural Network-based Architectures are a Poor Basis for Artificial General Intelligence,ar,Article,Sublime J.,128596244,DaSSIP Team,Issy-les-Moulineaux,France,1.0,"Sublime, Jérémie",56416433300,128596244,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,41-67,"Artificial General Intelligence is the idea that someday an hypothetical agent will arise from artificial intelligence (AI) progresses, and will surpass by far the brightest and most gifted human minds. This idea has been around since the early development of AI. Since then, scenarios on how such AI may behave towards humans have been the subject of many fictional and research works. This paper analyzes the current state of artificial intelligence progresses, and how the current AI race with the ever faster release of impressive new AI methods (that can deceive humans, outperform them at tasks we thought impossible to tackle by AI a mere decade ago, and that disrupt the job market) have raised concerns that Artificial General Intelligence (AGI) might be coming faster that we thought. In particular, we focus on 3 specific families of modern AIs to develop the idea that deep neural networks, which are the current backbone of nearly all artificial intelligence methods, are poor candidates for any AGI to arise due to their many limitations, and therefore that any threat coming from the recent AI race does not lie in AGI but in the limitations, uses, and lack of regulations of our current models and algorithms.",,10,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,
2-s2.0-85204693392,,,,The Brier Score under Administrative Censoring: Problems and a Solution,ar,Article,Kvamme H.,60010348,Universitetet i Oslo,Oslo,Norway,2.0,"Kvamme, Håvard;Borgan, Ørnulf",57201009656;57201057087,60010348;60010348,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"The Brier score is commonly used for evaluating probability predictions. In survival analysis, with right-censored observations of the event times, this score can be weighted by the inverse probability of censoring (IPCW) to retain its original interpretation. It is common practice to estimate the censoring distribution with the Kaplan-Meier estimator, even though it assumes that the censoring distribution is independent of the covariates. This paper investigates problems that may arise for the IPCW weighting scheme when the covariates used in the prediction model contain information about the censoring times. In particular, this may occur for administratively censored data if the distribution of the covariates varies with calendar time. For administratively censored data, we propose an alternative version of the Brier score. This administrative Brier score does not require estimation of the censoring distribution and is valid also when the censoring times can be predicted from the covariates.",customer churn | inverse probability weighting | progressive type I censoring | survival analysis | time-dependent case mix | time-to-event prediction,21,0.0,,,,237718,Norges Forskningsråd
2-s2.0-85201777571,10.1613/jair.1.15550,,,The Complexity of Subelection Isomorphism Problems,ar,Article,Faliszewski P.,60017351;60105740,AGH University of Krakow;Laboratoire d’Analyse et de Modélisation de Systèmes pour l’Aide à la Décision,Krakow;Paris,Poland;France,3.0,"Faliszewski, Piotr;Sornat, Krzysztof;Szufa, Stanisław",14044821700;56436627800;57204360148,60017351;60017351;60017351-60105740,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,1343-1371,"We study extensions of the Election Isomorphism problem, focused on the existence of isomorphic subelections. Specifically, we propose the Subelection Isomorphism and the Maximum Common Subelection problems and study their computational complexity and approximability. Using our problems in experiments, we provide some insights into the nature of several statistical models of elections.",,1,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,NCN,2018/29/N/ST6/01303,Narodowym Centrum Nauki
2-s2.0-85138825825,10.1613/JAIR.1.13547,,,The Computational Complexity of ReLU Network Training Parameterized by Data Dimensionality,ar,Article,Froese V.,60011604;60003059,Technische Universität Berlin;London School of Economics and Political Science,Berlin;London,Germany;United Kingdom,3.0,"Froese, Vincent;Hertrich, Christoph;Niedermeier, Rolf",55876905500;57219053609;7004137881,60011604;60003059;60011604,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,1775-1790,"Understanding the computational complexity of training simple neural networks with rectified linear units (ReLUs) has recently been a subject of intensive research. Closing gaps and complementing results from the literature, we present several results on the parameterized complexity of training two-layer ReLU networks with respect to various loss functions. After a brief discussion of other parameters, we focus on analyzing the influence of the dimension d of the training data on the computational complexity. We provide running time lower bounds in terms of W[1]-hardness for parameter d and prove that known brute-force strategies are essentially optimal (assuming the Exponential Time Hypothesis). In comparison with previous work, our results hold for a broad(er) range of loss functions, including `<sup>p</sup>-loss for all p ∈ [0, ∞]. In particular, we improve a known polynomial-time algorithm for constant d and convex loss functions to a more general class of loss functions, matching our running time lower bounds also in these cases.",,18,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,Technische Universität Berlin
2-s2.0-85168249339,10.1609/aaai.v37i7.26012,,,The Effect of Diversity in Meta-Learning,cp,Conference Paper,Kumar R.,60009507;60006191;127854087,University of Montreal;Google LLC;CIFAR,Montreal;Mountain View;Montreal,Canada;United States;Canada,3.0,"Kumar, Ramnath;Deleu, Tristan;Bengio, Yoshua",57202998350;57219508914;7003958245,60006191;60009507;60009507-127854087,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,8396-8404,"Recent studies show that task distribution plays a vital role in the meta-learner’s performance. Conventional wisdom is that task diversity should improve the performance of meta-learning. In this work, we find evidence to the contrary; (i) our experiments draw into question the efficacy of our learned models: similar manifolds can be learned with a subset of the data (lower task diversity). This finding questions the advantage of providing more data to the model, and (ii) adding diversity to the task distribution (higher task diversity) sometimes hinders the model and does not lead to a significant improvement in performance as previously believed. To strengthen our findings, we provide both empirical and theoretical evidence.",,4,1.0,all publisherfullgold,All Open Access Gold,,,Sony
2-s2.0-85143739333,,,,The First Optimal Acceleration of High-Order Methods in Smooth Convex Optimization,cp,Conference Paper,Kovalev D.,60020513;60092945;60000308;60025404,HSE University;King Abdullah University of Science and Technology;Moscow Institute of Physics and Technology;Institute for Information Transmission Problems of the Russian Academy of Sciences,Moscow;Thuwal;Dolgoprudny;Moscow,Russian Federation;Saudi Arabia;Russian Federation;Russian Federation,2.0,"Kovalev, Dmitry;Gasnikov, Alexander",57202914117;15762551000,60092945;60000308-60025404-60020513,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"In this paper, we study the fundamental open question of finding the optimal high-order algorithm for solving smooth convex minimization problems. Arjevani et al. (2019) established the lower bound Ω (ϵ<sup>−2/(3p+1))</sup> on the number of the p-th order oracle calls required by an algorithm to find an ϵ-accurate solution to the problem, where the p-th order oracle stands for the computation of the objective function value and the derivatives up to the order p. However, the existing state-of-the-art high-order methods of Gasnikov et al. (2019b); Bubeck et al. (2019); Jiang et al. (2019) achieve the oracle complexity O (ϵ<sup>−2/(3p+1)</sup> log(1/ϵ)), which does not match the lower bound. The reason for this is that these algorithms require performing a complex binary search procedure, which makes them neither optimal nor practical. We fix this fundamental issue by providing the first algorithm with O (ϵ<sup>−2/(3p+1))</sup> p-th order oracle complexity.",,22,0.0,,,,,
2-s2.0-85150003527,,,,The First Optimal Algorithm for Smooth and Strongly-Convex-Strongly-Concave Minimax Optimization,cp,Conference Paper,Kovalev D.,60020513;60092945;60000308;60028621,HSE University;King Abdullah University of Science and Technology;Moscow Institute of Physics and Technology;Ivannikov Institute for System Programming of the RAS,Moscow;Thuwal;Dolgoprudny;Moscow,Russian Federation;Saudi Arabia;Russian Federation;Russian Federation,2.0,"Kovalev, Dmitry;Gasnikov, Alexander",57202914117;15762551000,60092945;60000308-60028621-60020513,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"In this paper, we revisit the smooth and strongly-convex-strongly-concave minimax optimization problem. Zhang et al. (2021) and Ibrahim et al. (2020) established the lower bound (Equation presented) on the number of gradient evaluations required to find an ϵ-accurate solution, where κ<inf>x</inf> and κ<inf>y</inf> are condition numbers for the strong convexity and strong concavity assumptions. However, the existing state-of-the-art methods do not match this lower bound: algorithms of Lin et al. (2020) and Wang and Li (2020) have gradient evaluation complexity (Equation presented) and (Equation presented), respectively. We fix this fundamental issue by providing the first algorithm with (Equation presented) gradient evaluation complexity. We design our algorithm in three steps: (i) we reformulate the original problem as a minimization problem via the pointwise conjugate function; (ii) we apply a specific variant of the proximal point algorithm to the reformulated problem; (iii) we compute the proximal operator inexactly using the optimal algorithm for operator norm reduction in monotone inclusions.",,9,0.0,,,РАН,000000D730321P5Q0002,Analytical Center for the Government of the Russian Federation
2-s2.0-85163129942,,,,The Fundamental Price of Secure Aggregation in Differentially Private Federated Learning,cp,Conference Paper,Chen W.N.,60012708;60006191,Stanford University;Google LLC,Stanford;Mountain View,United States;United States,4.0,"Chen, Wei Ning;Choquette-Choo, Christopher A.;Kairouz, Peter;Suresh, Ananda Theertha",57219755708;57215435003;55652248700;59128494700,60012708-60006191;60006191;60006191;60006191,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,3056-3089,"We consider the problem of training a d dimensional model with distributed differential privacy (DP) where secure aggregation (SecAgg) is used to ensure that the server only sees the noisy sum of n model updates in every training round. Taking into account the constraints imposed by SecAgg, we characterize the fundamental communication cost required to obtain the best accuracy achievable under ε central DP (i.e. under a fully trusted server and no communication constraints). Our results show that Õ (min(n<sup>2</sup>ε<sup>2</sup>, d)) bits per client are both sufficient and necessary, and this fundamental limit can be achieved by a linear scheme based on sparse random projections. This provides a significant improvement relative to state-of-the-art SecAgg distributed DP schemes which use Õ(dlog(d/ε<sup>2</sup>)) bits per client. Empirically, we evaluate our proposed scheme on real-world federated learning tasks. We find that our theoretical analysis is well matched in practice. In particular, we show that we can reduce the communication cost to under 1.78 bits per parameter in realistic privacy settings without decreasing test-time performance. Our work hence theoretically and empirically specifies the fundamental price of using SecAgg.",,46,0.0,,,,,
2-s2.0-85203239221,10.1613/jair.1.15566,,,The Goal after Tomorrow: Offline Goal Reasoning with Norms,ar,Article,Pardo P.,60005322;60072562,Ruhr-Universitat Bochum;University of Luxembourg,Bochum;Esch-sur-Alzette,Germany;Luxembourg,2.0,"Pardo, Pere;Straßer, Christian",35240763000;32667990300,60072562;60005322,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,1703-1759,"Recent studies have focused on autonomous agents that select their own goals and then select actions to achieve these goals, using online Goal Reasoning (GR). GR agents can revise goals and plans at execution time if unexpected outcomes occur. However, for ethical or legal agent design, even the partial execution of an online plan may result in foreseeable norm violations. To prevent these violations, it is crucial to incorporate GR already at the planning phase. To this end, we design an offline GR system that can harbour normative systems or deontic logics for goal generation. Our main results include a characterization and comparison of the completeness classes for a variety of offline GR planners, and a discussion of the irreducibility of offline GR to pure planning methods.",,0,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85170390553,10.24963/ijcai.2023/636,,,The Hardness of Reasoning about Probabilities and Causality,cp,Conference Paper,van der Zander B.,60033241;60012408,Universität des Saarlandes;Universität zu Lübeck,Saarbrucken;Lubeck,Germany;Germany,3.0,"van der Zander, Benito;Bläser, Markus;Liśkiewicz, Maciej",36457384300;8987160600;6603913730,60012408;60033241;60012408,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,5730-5738,"We study formal languages which are capable of fully expressing quantitative probabilistic reasoning and do-calculus reasoning for causal effects, from a computational complexity perspective. We focus on satisfiability problems whose instance formulas allow expressing many tasks in probabilistic and causal inference. The main contribution of this work is establishing the exact computational complexity of these satisfiability problems. We introduce a new natural complexity class, named succ∃R, which can be viewed as a succinct variant of the well-studied class ∃R, and show that these problems are complete for succ∃R. Our results imply even stronger limitations on the use of algorithmic methods for reasoning about probabilities and causality than previous state-of-the-art results that rely only on the NP- or ∃R-completeness of the satisfiability problems for some restricted languages.",,4,1.0,all publisherfullgold,All Open Access Gold,DFG,471183316,Deutsche Forschungsgemeinschaft
2-s2.0-85208286887,10.1613/jair.1.15665,,,The Human in Interactive Machine Learning: Analysis and Perspectives for Ambient Intelligence,ar,Article,Delcourt K.,60027245;60020551,Université de Toulouse;Université Toulouse 1 Capitole,Toulouse;Toulouse,France;France,4.0,"Delcourt, Kévin;Trouilhet, Sylvie;Arcangeli, Jean Paul;Adreit, Françoise",57212034059;15926598100;6701444320;6504367429,60027245;60027245;60027245;60020551,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,263-305,"As the vision of Ambient Intelligence (AmI) becomes more feasible, the challenge of designing effective and usable human-machine interaction in this context becomes increasingly important. Interactive Machine Learning (IML) offers a set of techniques and tools to involve end-users in the machine learning process, making it possible to build more trustworthy and adaptable ambient systems. In this paper, our focus is on exploring approaches to effectively integrate and assist human users within ML-based AmI systems. Through a survey of key IML-related contributions, we identify principles for designing effective human-AI interaction in AmI applications. We apply them to the case of Opportunistic Composition, which is an approach to achieve AmI, to enhance collaboration between humans and Artificial Intelligence. Our study highlights the need for user-centered and context-aware design, and provides insights into the challenges and opportunities of integrating IML techniques into AmI systems.",,1,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,
2-s2.0-85177760077,,,,The Implicit Bias of Benign Overfitting,ar,Article,Shamir O.,60017563,Weizmann Institute of Science Israel,Rehovot,Israel,1.0,"Shamir, Ohad",25651750200,60017563,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,113,,"The phenomenon of benign overfitting, where a predictor perfectly fits noisy training data while attaining near-optimal expected loss, has received much attention in recent years, but still remains not fully understood beyond well-specified linear regression setups. In this paper, we provide several new results on when one can or cannot expect benign overfitting to occur, for both regression and classification tasks. We consider a prototypical and rather generic data model for benign overfitting of linear predictors, where an arbitrary input distribution of some fixed dimension k is concatenated with a high-dimensional distribution. For linear regression which is not necessarily well-specified, we show that the minimum-norm interpolating predictor (that standard training methods converge to) is biased towards an inconsistent solution in general, hence benign overfitting will generally not occur. Moreover, we show how this can be extended beyond standard linear regression, by an argument proving how the existence of benign overfitting on some regression problems precludes its existence on other regression problems. We then turn to classification problems, and show that the situation there is much more favorable. Specifically, we prove that the max-margin predictor (to which standard training methods are known to converge in direction) is asymptotically biased towards minimizing a weighted squared hinge loss. This allows us to reduce the question of benign overfitting in classification to the simpler question of whether this loss is a good surrogate for the misclassification error, and use it to show benign overfitting in some new settings.",benign overfitting | implicit bias | interpolating predictors | surrogate losses,6,0.0,,,ERC,754705,European Research Council
2-s2.0-85163176491,,,,The Implicit Delta Method,cp,Conference Paper,Kallus N.,60007776;129868626,Cornell University;Netflix Research,Ithaca;,United States;,2.0,"Kallus, Nathan;McInerney, James",56305894700;57188755418,60007776-129868626;129868626,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Epistemic uncertainty quantification is a crucial part of drawing credible conclusions from predictive models, whether concerned about the prediction at a given point or any downstream evaluation that uses the model as input. When the predictive model is simple and its evaluation differentiable, this task is solved by the delta method, where we propagate the asymptotically-normal uncertainty in the predictive model through the evaluation to compute standard errors and Wald confidence intervals. However, this becomes difficult when the model and/or evaluation becomes more complex. Remedies include the bootstrap, but it can be computationally infeasible when training the model even once is costly. In this paper, we propose an alternative, the implicit delta method, which works by infinitesimally regularizing the training loss of the predictive model to automatically assess downstream uncertainty. We show that the change in the evaluation due to regularization is consistent for the asymptotic variance of the evaluation estimator, even when the infinitesimal change is approximated by a finite difference. This provides both a reliable quantification of uncertainty in terms of standard errors as well as permits the construction of calibrated confidence intervals. We discuss connections to other approaches to uncertainty quantification, both Bayesian and frequentist, and demonstrate our approach empirically.",,4,0.0,,,,,
2-s2.0-85163096286,,,,The Importance of Non-Markovianity in Maximum State Entropy Exploration,cp,Conference Paper,Mutti M.,60025858;60028218;60023256,ETH Zürich;Alma Mater Studiorum Università di Bologna;Politecnico di Milano,Zurich;Bologna;Milan,Switzerland;Italy;Italy,3.0,"Mutti, Mirco;De Santi, Riccardo;Restelli, Marcello",57204810348;57453681400;6603404086,60023256-60028218;60025858;60023256,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,16223-16239,"In the maximum state entropy exploration framework, an agent interacts with a reward-free environment to learn a policy that maximizes the entropy of the expected state visitations it is inducing. Hazan et al. (2019) noted that the class of Markovian stochastic policies is sufficient for the maximum state entropy objective, and exploiting non-Markovianity is generally considered pointless in this setting. In this paper, we argue that non-Markovianity is instead paramount for maximum state entropy exploration in a finite-sample regime. Especially, we recast the objective to target the expected entropy of the induced state visitations in a single trial. Then, we show that the class of non-Markovian deterministic policies is sufficient for the introduced objective, while Markovian policies suffer non-zero regret in general. However, we prove that the problem of finding an optimal non-Markovian policy is NP-hard. Despite this negative result, we discuss avenues to address the problem in a tractable way and how non-Markovian exploration could benefit the sample efficiency of online reinforcement learning in future works.",,17,0.0,,,,,
2-s2.0-85166333229,10.1613/jair.1.14368,,,The Jiminy Advisor: Moral Agreements among Stakeholders Based on Norms and Argumentation,ar,Article,Liao B.,60003970;60029622;60072562;129854839,Zhejiang University;Universitetet i Bergen;University of Luxembourg;State Key Laboratory of Brain-machine Intelligence,Hangzhou;Bergen;Esch-sur-Alzette;Hangzhou,China;Norway;Luxembourg;China,4.0,"Liao, Beishui;Pardo, Pere;Slavkovik, Marija;van der Torre, Leendert",7102656594;35240763000;35209006200;37091872800,60003970-129854839;60072562;60029622;60003970-60072562,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,737-792,"An autonomous system is constructed by a manufacturer, operates in a society subject to norms and laws, and interacts with end users. All of these actors are stakeholders affected by the behavior of the autonomous system. We address the challenge of how the ethical views of such stakeholders can be integrated in the behavior of an autonomous system. We propose an ethical recommendation component called Jiminy which uses techniques from normative systems and formal argumentation to reach moral agreements among stakeholders. A Jiminy represents the ethical views of each stakeholder by using normative systems, and has three ways of resolving moral dilemmas that involve the opinions of the stakeholders. First, the Jiminy considers how the arguments of the stakeholders relate to one another, which may already resolve the dilemma. Secondly, the Jiminy combines the normative systems of the stakeholders such that the combined expertise of the stakeholders may resolve the dilemma. Thirdly, and only if these two other methods have failed, the Jiminy uses context-sensitive rules to decide which of the stakeholders take preference over the others. At the abstract level, these three methods are characterized by adding arguments, adding attacks between arguments, and revising attacks between arguments. We show how a Jiminy can be used not only for ethical reasoning and collaborative decision-making, but also to provide explanations about ethical behavior.",,14,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,FNR,INTER/CHIST/19/14589586,Fonds National de la Recherche Luxembourg
2-s2.0-85148449727,10.1613/JAIR.1.14034,,,The LM-Cut Heuristic Family for Optimal Numeric Planning with Simple Conditions,ar,Article,Kuroiwa R.,60016849;60029681;60002765;124848838,University of Toronto;Pontificia Universidad Católica de Chile;Bar-Ilan University;Augmenta Inc.,Toronto;Santiago;Ramat Gan;Toronto,Canada;Chile;Israel;Canada,5.0,"Kuroiwa, Ryo;Shleyfman, Alexander;Piacentini, Chiara;Castro, Margarita P.;Beck, J. Christopher",57194047545;55361470100;55860367500;57202111943;56601717700,60016849;60002765;124848838;60029681;60016849,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,1477-1548,"The LM-cut heuristic, both alone and as part of the operator counting framework, represents one of the most successful heuristics for classical planning. In this paper, we generalize LM-cut and its use in operator counting to optimal numeric planning with simple conditions and simple numeric effects, i.e., linear expressions over numeric state variables and actions that increase or decrease such variables by constant quantities. We introduce a variant of hmaxhbd (a previously proposed numeric hmax heuristic) based on the deleterelaxed version of such planning tasks and show that, although inadmissible by itself, our variant yields a numeric version of the classical LM-cut heuristic which is admissible. We classify the three existing families of heuristics for this class of numeric planning tasks and introduce the LM-cut family, proving dominance or incomparability between all pairs of existing max and LM-cut heuristics for numeric planning with simple conditions. Our extensive empirical evaluation shows that the new LM-cut heuristic, both on its own and as part of the operator counting framework, is the state-of-the-art for this class of numeric planning problem.",,15,1.0,all publisherfullgold,All Open Access Gold,NSERC,FB210017,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85137928959,10.24963/ijcai.2022/355,,,The Limits of Morality in Strategic Games,cp,Conference Paper,Cao R.,60010365;60025225,The University of British Columbia;University of Southampton,Vancouver;Southampton,Canada;United Kingdom,2.0,"Cao, Rui;Naumov, Pavel",57196123240;9534673600,60010365;60025225,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,2561-2567,"An agent, or a coalition of agents, is blameable for an outcome if she had a strategy to prevent it. In this paper we introduce a notion of limited blameworthiness, with a constraint on the amount of sacrifice required to prevent the outcome. The main technical contribution is a sound and complete logical system for reasoning about limited blameworthiness in the strategic game setting.",,1,1.0,all publisherfree2read,All Open Access Bronze,,,
2-s2.0-85213964083,,,,The Measure and Mismeasure of Fairness,ar,Article,Corbett-Davies S.,60021784;60141508;60006303;60006332,New York University;Stanford Engineering;Harvard Faculty of Arts and Sciences;John F. Kennedy School of Government,New York;Stanford;Cambridge;Cambridge,United States;United States;United States;United States,5.0,"Corbett-Davies, Sam;Gaebler, Johann D.;Nilforoshan, Hamed;Shroff, Ravi;Goel, Sharad",55579310100;57206209917;57203125753;57117515400;35785827100,60006303;60006303;60141508;60021784;60006332,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,312,,"The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.",consequentialism | discrimination | fair machine learning,95,0.0,,,HDSI,DGE-1656518,"Harvard Data Science Initiative, Harvard University"
2-s2.0-105000555924,,,,"The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models",cp,Conference Paper,Kirk H.R.,60026851;60006297;60021784;60021796;128344002;125087254;60353579;131281856;121693225;125941976;130216969;130407734,University of Oxford;University of Pennsylvania;New York University;Università Bocconi;Meta AI Research;AWS AI Labs;Contextual AI;Cohere;UCL;MLCommons;Factored AI;Meedan,Oxford;Philadelphia;New York;Milan;Meta;Santa Clara;Mountain View;;Bartlett;Mountain View;;Oxford,United Kingdom;United States;United States;Italy;United States;United States;United States;United States;United States;United States;United States;United Kingdom,12.0,"Kirk, Hannah Rose;Whitefield, Alexander;Röttger, Paul;Bean, Andrew;Margatina, Katerina;Ciro, Juan;Mosquera, Rafael;Bartolo, Max;Williams, Adina;He, He;Vidgen, Bertie;Hale, Scott A.",57224745452;59094919800;57222145325;58621567000;57216618124;57351878600;57819497600;57212149387;56387940900;55700423600;57213422744;54896048500,60026851;60006297;60021796;60026851;125087254;125941976-60353579;125941976-130216969;121693225-131281856;128344002;60021784;60026851-60353579;60026851-130407734,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a dataset that maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide what alignment data.",,22,0.0,,,NYU,IIS-2340345,New York University
2-s2.0-85163146027,,,,The Phenomenon of Policy Churn,cp,Conference Paper,Schaul T.,60111161,DeepMind Technologies Limited,London,United Kingdom,4.0,"Schaul, Tom;Barreto, André;Quan, John;Ostrovski, Georg",25640175100;36175458000;57193763049;36996117900,60111161;60111161;60111161;60111161,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"We identify and study the phenomenon of policy churn, that is, the rapid change of the greedy policy in value-based reinforcement learning. Policy churn operates at a surprisingly rapid pace, changing the greedy action in a large fraction of states within a handful of learning updates (in a typical deep RL set-up such as DQN on Atari). We characterise the phenomenon empirically, verifying that it is not limited to specific algorithm or environment properties. A number of ablations help whittle down the plausible explanations on why churn occurs, the most likely one being deep learning with high-variance updates. Finally, we hypothesise that policy churn is a potentially beneficial but overlooked form of implicit exploration, which casts ϵ-greedy exploration in a fresh light, namely that ϵ-noise plays a much smaller role than expected.",,11,0.0,,,,,
2-s2.0-105000535584,,,,The Secretary Problem with Predicted Additive Gap,cp,Conference Paper,Braun A.,60027950;60007493,Carnegie Mellon University;Universität Bonn,Pittsburgh;Bonn,United States;Germany,2.0,"Braun, Alexander;Sarkar, Sherry",57224980124;57214112805,60007493;60027950,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"The secretary problem is one of the fundamental problems in online decision making; a tight competitive ratio for this problem of <sup>1</sup>/e ≈ 0.368 has been known since the 1960s. Much more recently, the study of algorithms with predictions was introduced: The algorithm is equipped with a (possibly erroneous) additional piece of information upfront which can be used to improve the algorithm's performance. Complementing previous work on secretary problems with prior knowledge, we tackle the following question: What is the weakest piece of information that allows us to break the <sup>1</sup>/e barrier? To this end, we introduce the secretary problem with predicted additive gap. As in the classical problem, weights are fixed by an adversary and elements appear in random order. In contrast to previous variants of predictions, our algorithm only has access to a much weaker piece of information: an additive gap c. This gap is the difference between the highest and k-th highest weight in the sequence. Unlike previous pieces of advice, knowing an exact additive gap does not make the problem trivial. Our contribution is twofold. First, we show that for any index k and any gap c, we can obtain a competitive ratio of 0.4 when knowing the exact gap (even if we do not know k), hence beating the prevalent bound for the classical problem by a constant. Second, a slightly modified version of our algorithm allows to prove standard robustness-consistency properties as well as improved guarantees when knowing a range for the error of the prediction. The full version with proofs can be found at https://arxiv.org/abs/2409. 20460 [Braun and Sarkar, 2024].",,0,0.0,,,DFG,437739576,Deutsche Forschungsgemeinschaft
2-s2.0-85137744634,10.1609/aaai.v36i5.20446,,,The Semi-random Likelihood of Doctrinal Paradoxes,cp,Conference Paper,Liu A.,60148232,Department of Computer Science,Troy,United States,2.0,"Liu, Ao;Xia, Lirong",57210118926;23011050500,60148232;60148232,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,5124-5132,"When aggregating logically interconnected judgements from n agents, the result might be logically inconsistent. This phenomenon is known as the doctrinal paradox, which plays a central role in the field of judgement aggregation. Previous work has mostly focused on the worst-case analysis of the doctrinal paradox, leading to many impossibility results. Little is known about its likelihood of occurrence in practical settings, except for the study under certain distributions by List in 2005. In this paper, we characterize the likelihood of the doctrinal paradox under a general and realistic model called semi-random social choice framework (proposed by Xia in 2020). In the framework, agents' ground truth judgements can be arbitrarily correlated, while the noises are independent. Our main theorem states that under mild conditions, the semi-random likelihood of the doctrinal paradox is either 0, exp(-Θ(n)), Θ(n (-0.5)) or Θ(1). This not only answers open questions by List in 2005, but also draws clear lines between situations with frequent paradoxes and with vanishing paradoxes.",,3,1.0,all publisherfullgold,All Open Access Gold,NSF,1453542,National Science Foundation
2-s2.0-85196061343,10.1613/jair.1.14945,,,The Toad System for Totally Ordered HTN Planning,ar,Article,Höller D.,60033241,Universität des Saarlandes,Saarbrucken,Germany,1.0,"Höller, Daniel",56077435300,60033241,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,80,,,613-663,"We present an approach for translating Totally Ordered Hierarchical Task Network (HTN) planning problems to classical planning problems. While this enables the use of sophisticated classical planning systems to find solutions, we need to overcome the differences in expressiveness of these two planning formalisms. Prior work on this topic did this by translating bounded HTN problems. In contrast, we approximate them, i.e., we change the problem such that every action sequence that is a solution to the HTN problem is also a solution for the classical problem, but the latter might have more solutions. To obtain a sound overall approach, we verify solutions returned by the classical planning system to ensure that they are also solutions to the HTN problem. For translation and approximation, we use techniques introduced to approximate Context-Free Languages by using Finite Automata. We named our system Toad (Totally Ordered HTN Approximation using DFA). For a subset of HTN problems the translation is even possible without approximation. Whether or not it is necessary is decided based on the property of self-embedding, which comes also from the field of formal languages. We investigate the theoretical connection of self-embedding and tail-recursiveness, a property from the HTN literature used to identify a subclass of HTN planning problems that can be translated to classical planning, and show that it is more general. To guide the classical planner, we introduce a novel heuristic tailored towards our models. We evaluate Toad on the benchmark set of the 2020 International Planning Competition. Our evaluation shows that (1) most problems can be translated without approximation and that (2) Toad is competitive with the state of the art in HTN planning.",,0,1.0,all publisherfullgold,All Open Access Gold,DFG,232722074 – SFB 1102,Deutsche Forschungsgemeinschaft
2-s2.0-85124189047,,,,Theoretical Convergence of Multi-Step Model-Agnostic Meta-Learning,ar,Article,Ji K.,60149838,College of Engineering,Columbus,United States,3.0,"Ji, Kaiyi;Yang, Junjie;Liang, Yingbin",57203515939;57219589889;16068942200,60149838;60149838;60149838,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"As a popular meta-learning approach, the model-agnostic meta-learning (MAML) algorithm has been widely used due to its simplicity and effectiveness. However, the convergence of the general multi-step MAML still remains unexplored. In this paper, we develop a new theoretical framework to provide such convergence guarantee for two types of objective functions that are of interest in practice: (a) resampling case (e.g., reinforcement learning), where loss functions take the form in expectation and new data are sampled as the algorithm runs; and (b) finite-sum case (e.g., supervised learning), where loss functions take the finite-sum form with given samples. For both cases, we characterize the convergence rate and the computational complexity to attain an ε-accurate solution for multi-step MAML in the general nonconvex setting. In particular, our results suggest that an inner-stage stepsize needs to be chosen inversely proportional to the number N of inner-stage steps in order for N-step MAML to have guaranteed convergence. From the technical perspective, we develop novel techniques to deal with the nested structure of the meta gradient for multi-step MAML, which can be of independent interest.",Computational complexity | Convergence rate | Finite-sum | Meta-learning | Multi-step MAML | Nonconvex | Resampling,40,0.0,,,NSF,CCF-1761506,National Science Foundation
2-s2.0-85163168028,,,,Thompson Sampling Efficiently Learns to Control Diffusion Processes,cp,Conference Paper,Faradonbeh M.K.S.,60029747;60020928,University of Georgia;Stanford Graduate School of Business,Athens;Stanford,United States;United States,3.0,"Faradonbeh, Mohamad Kazem Shirani;Faradonbeh, Mohamad Sadegh Shirani;Bayati, Mohsen",57203861453;57222063177;57202824495,60029747;60020928;60020928,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Diffusion processes that evolve according to linear stochastic differential equations are an important family of continuous-time dynamic decision-making models. Optimal policies are well-studied for them, under full certainty about the drift matrices. However, little is known about data-driven control of diffusion processes with uncertain drift matrices as conventional discrete-time analysis techniques are not applicable. In addition, while the task can be viewed as a reinforcement learning problem involving exploration and exploitation trade-off, ensuring system stability is a fundamental component of designing optimal policies. We establish that the popular Thompson sampling algorithm learns optimal actions fast, incurring only a square-root of time regret, and also stabilizes the system in a short time period. To the best of our knowledge, this is the first such result for Thompson sampling in a diffusion process control problem. We validate our theoretical results through empirical simulations with real matrices. Moreover, we observe that Thompson sampling significantly improves (worst-case) regret, compared to the state-of-the-art algorithms, suggesting Thompson sampling explores in a more guarded fashion. Our theoretical analysis involves characterization of a certain optimality manifold that ties the local geometry of the drift parameters to the optimal control of the diffusion process. We expect this technique to be of broader interest.",,4,0.0,,,,,
2-s2.0-85147909825,,,,Three Rates of Convergence or Separation via U-Statistics in a Dependent Framework,ar,Article,Duchemin Q.,60030981;60122667,École Centrale de Lyon;Laboratoire d’Analyse et de Mathématiques Appliquées,Ecully;Marne-la-Vallee,France;France,3.0,"Duchemin, Quentin;De Castro, Yohann;Lacour, Claire",57219584598;55266238000;23091730400,60122667;60030981;60122667,2022-06-01,1 June 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,201,,"Despite the ubiquity of U-statistics in modern Probability and Statistics, their nonasymptotic analysis in a dependent framework may have been overlooked. In a recent work, a new concentration inequality for U-statistics of order two for uniformly ergodic discrete time Markov chains has been proved. In this paper, we put this theoretical breakthrough into action by pushing further the current state of knowledge in three different active fields of research. First, we establish a new exponential inequality for the estimation of spectra of integral operators with MCMC methods. The novelty is that this result holds for kernels with positive and negative eigenvalues, which is new as far as we know. In addition, we investigate generalization performance of online algorithms working with pairwise loss functions and Markov chain samples. We provide an online-to-batch conversion result by showing how we can extract a low risk hypothesis from the sequence of hypotheses generated by any online learner. We finally give a non-asymptotic analysis of a goodness-of-fit test on the density of the stationary measure of a Markov chain. We identify some classes of alternatives over which our test based on the L<sup>2</sup> distance has a prescribed power.",Concentration inequality | Integral operators | Markov chains | Non-parametric hypothesis testing | Online learning | U-statistics,1,0.0,,,,,
2-s2.0-85138065341,10.1613/JAIR.1.13661,,,Threshold Treewidth and Hypertree Width,ar,Article,Ganian R.,60018163,TU Wien,Vienna,Austria,4.0,"Ganian, Robert;Schidler, André;Sorge, Manuel;Szeider, Stefan",35118870500;57214890000;32667953300;6602786002,60018163;60018163;60018163;60018163,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,74,,,1687-1713,"Treewidth and hypertree width have proven to be highly successful structural parameters in the context of the Constraint Satisfaction Problem (CSP). When either of these parameters is bounded by a constant, then CSP becomes solvable in polynomial time. However, here the order of the polynomial in the running time depends on the width, and this is known to be unavoidable; therefore, the problem is not fixed-parameter tractable parameterized by either of these width measures. Here we introduce an enhancement of tree and hypertree width through a novel notion of thresholds, allowing the associated decompositions to take into account information about the computational costs associated with solving the given CSP instance. Aside from introducing these notions, we obtain efficient theoretical as well as empirical algorithms for computing threshold treewidth and hypertree width and show that these parameters give rise to fixed-parameter algorithms for CSP as well as other, more general problems. We complement our theoretical results with experimental evaluations in terms of heuristics as well as exact methods based on SAT/SMT encodings.",,1,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,ERC,W 1255,European Research Council
2-s2.0-105000508819,,,,Time-Constrained Robust MDPs,cp,Conference Paper,Zouitine A.,60027245;60024614;100375855;120452101;131281856;125066274;128080461,Université de Toulouse;INSA Toulouse;École Polytechnique;IRT Saint Exupéry;Cohere;ANITI;HeKA,Toulouse;Toulouse;;Toulouse;;Toulouse;Paris,France;France;France;France;United States;France;France,5.0,"Zouitine, Adil;Bertoin, David;Clavier, Pierre;Geist, Matthieu;Rachelson, Emmanuel",57221691257;57226024724;58119440600;25929145100;35748789700,120452101-60027245;120452101-60024614-125066274;100375855-128080461;131281856;60027245-125066274,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Robust reinforcement learning is essential for deploying reinforcement learning algorithms in real-world scenarios where environmental uncertainty predominates. Traditional robust reinforcement learning often depends on rectangularity assumptions, where adverse probability measures of outcome states are assumed to be independent across different states and actions. This assumption, rarely fulfilled in practice, leads to overly conservative policies. To address this problem, we introduce a new time-constrained robust MDP (TC-RMDP) formulation that considers multifactorial, correlated, and time-dependent disturbances, thus more accurately reflecting real-world dynamics. This formulation goes beyond the conventional rectangularity paradigm, offering new perspectives and expanding the analytical framework for robust RL. We propose three distinct algorithms, each using varying levels of environmental information, and evaluate them extensively on continuous control benchmarks. Our results demonstrate that these algorithms yield an efficient tradeoff between performance and robustness, outperforming traditional deep robust RL methods in time-constrained environments while preserving robustness in classical benchmarks. This study revisits the prevailing assumptions in robust RL and opens new avenues for developing more practical and realistic RL applications.",,1,0.0,,,FMJH,,Fondation Mathématique Jacques Hadamard
2-s2.0-85189520963,10.1609/aaai.v38i12.29249,,,Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding,cp,Conference Paper,Li D.,60025761;60017161,Huazhong University of Science and Technology;National University of Singapore,Wuhan;Singapore City,China;Singapore,6.0,"Li, Depeng;Wang, Tianqi;Chen, Junwei;Ren, Qining;Kawaguchi, Kenji;Zeng, Zhigang",57205602282;59871754000;58450014400;58848256300;57189097784;35316687300,60025761;60025761;60025761;60025761;60017161;60025761,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,12.0,,13464-13473,"Deep neural networks are susceptible to catastrophic forgetting when trained on sequential tasks. Various continual learning (CL) methods often rely on exemplar buffers or/and network expansion for balancing model stability and plasticity, which, however, compromises their practical value due to privacy and memory concerns. Instead, this paper considers a strict yet realistic setting, where the training data from previous tasks is unavailable and the model size remains relatively constant during sequential training. To achieve such desiderata, we propose a conceptually simple yet effective method that attributes forgetting to layer-wise parameter overwriting and the resulting decision boundary distortion. This is achieved by the synergy between two key components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten parameter updates mediated by Hilbert-Schmidt independence criterion in an orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary adaptation between old and new tasks with predefined basis vectors. Extensive experiments demonstrate that our method achieves competitive accuracy performance, even with absolute superiority of zero exemplar buffer and 1.02× the base model.",,8,1.0,all publisherfullgold,All Open Access Gold,NKRDPC,2021ZD0201300,National Key Research and Development Program of China
2-s2.0-85147331443,10.1613/jair.1.13639,,,Towards Evidence Retrieval Cost Reduction in Abstract Argumentation Frameworks with Fallible Evidence,ar,Article,Cohen A.,60013496,Universidad Nacional del Sur,Bahia Blanca,Argentina,4.0,"Cohen, Andrea;Gottifredi, Sebastian;García, Alejandro J.;Simari, Guillermo R.",35766382800;35755983000;57212029771;6507037875,60013496;60013496;60013496;60013496,2022-01-01,2022,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,75,,,1293-1322,"Arguments in argumentation systems cannot always be considered as standalone entities, requiring the consideration of the pieces of evidence they rely on. This evidence might have to be retrieved from external sources such as databases or the web, and each attempt to retrieve a piece of evidence comes with an associated cost. Moreover, a piece of evidence may be available in a given scenario but not in others, and this is not known beforehand. As a result, the collection of active arguments (whose entire set of evidence is available) that can be used by the argumentation machinery of the system may vary from one scenario to another. In this work, we consider an Abstract Argumentation Framework with Fallible Evidence that accounts for these issues, and propose a heuristic measure used as part of the acceptability calculus (specifically, for building pruned dialectical trees) with the aim of minimizing the evidence retrieval cost of the arguments involved in the reasoning process. We provide an algorithmic solution that is empirically tested against two baselines and formally show the correctness of our approach.",,2,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85196968177,,,,Towards Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with Transferrable Local Policy,cp,Conference Paper,Gao C.,60033100;60119391,Nanjing University;Huawei Noah's Ark Lab,Nanjing;Hong Kong,China;Hong Kong,5.0,"Gao, Chengrui;Shang, Haopu;Xue, Ke;Li, Dong;Qian, Chao",57219504523;57657368300;57220543052;56441814600;36615703600,60033100;60033100;60033100;60119391;60033100,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,6914-6922,"Machine learning has been adapted to help solve NP-hard combinatorial optimization problems. One prevalent way is learning to construct solutions by deep neural networks, which has been receiving more and more attention due to the high efficiency and less requirement for expert knowledge. However, many neural construction methods for Vehicle Routing Problems (VRPs) focus on synthetic problem instances with specified node distributions and limited scales, leading to poor performance on real-world problems which usually involve complex and unknown node distributions together with large scales. To make neural VRP solvers more practical, we design an auxiliary policy that learns from the local transferable topological features, named local policy, and integrate it with a typical construction policy (which learns from the global information of VRP instances) to form an ensemble policy. With joint training, the aggregated policies perform cooperatively and complementarily to boost generalization. The experimental results on two well-known benchmarks, TSPLIB and CVRPLIB, of travelling salesman problem and capacitated VRP show that the ensemble policy significantly improves both cross-distribution and cross-scale generalization performance, and even performs well on real-world problems with several thousand nodes.",,19,0.0,,,NSFC,2022ZD0116600,National Major Science and Technology Projects of China
2-s2.0-85200564138,,,,Towards Hybrid-grained Feature Interaction Selection for Deep Sparse Network,cp,Conference Paper,Lyu F.,60002494;60013983;60114181;130768012,Université McGill;City University of Hong Kong;Tencent;Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ),Montreal;Hong Kong;Shenzhen;Guangdong,Canada;Hong Kong;China;China,8.0,"Lyu, Fuyuan;Tang, Xing;Liu, Dugang;Ma, Chen;Luo, Weihong;Chen, Liang;He, Xiuqiang;Liu, Xue",57219793873;57205558093;57199027810;57199374297;58684862000;57192095855;55235819900;24178306600,60002494;60114181;130768012;60013983;60114181;60114181;60114181;60002494,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Deep sparse networks are widely investigated as a neural network architecture for prediction tasks with high-dimensional sparse features, with which feature interaction selection is a critical component. While previous methods primarily focus on how to search feature interaction in a coarse-grained space, less attention has been given to a finer granularity. In this work, we introduce a hybrid-grained feature interaction selection approach that targets both feature field and feature value for deep sparse networks. To explore such expansive space, we propose a decomposed space which is calculated on the fly. We then develop a selection algorithm called OptFeature, which efficiently selects the feature interaction from both the feature field and the feature value simultaneously. Results from experiments on three large real-world benchmark datasets demonstrate that OptFeature performs well in terms of accuracy and efficiency. Additional studies support the feasibility of our method. All source code are publicly available.",,3,0.0,,,,,
2-s2.0-85168248068,10.1609/aaai.v37i9.26230,,,Towards In-Distribution Compatible Out-of-Distribution Detection,cp,Conference Paper,Wu B.,60117933;60117750;126515404;125934312,"National Key Laboratory of Computer-Aided Design and Graphics Systems;School of Software Technology, Zhejiang University;Ltd.;Tencent Data Platform",Hangzhou;Ningbo;Ningbo;,China;China;China;China,10.0,"Wu, Boxi;Jiang, Jie;Ren, Haidong;Du, Zifan;Wang, Wenxiao;Li, Zhifeng;Cai, Deng;He, Xiaofei;Lin, Binbin;Liu, Wei",57220900497;57874428800;57873513600;57874243600;57209781549;55707159500;35228598300;36164098600;55466850100;58599862900,60117933;125934312;126515404;60117750;60117750;125934312;60117933;60117933;60117750;125934312,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,10333-10341,"Deep neural network, despite its remarkable capability of discriminating targeted in-distribution samples, shows poor performance on detecting anomalous out-of-distribution data. To address this defect, state-of-the-art solutions choose to train deep networks on an auxiliary dataset of outliers. Various training criteria for these auxiliary outliers are proposed based on heuristic intuitions. However, we find that these intuitively designed outlier training criteria can hurt in-distribution learning and eventually lead to inferior performance. To this end, we identify three causes of the in-distribution incompatibility: contradictory gradient, false likelihood, and distribution shift. Based on our new understandings, we propose a new out-of-distribution detection method by adapting both the top-design of deep models and the loss function. Our method achieves in-distribution compatibility by pursuing less interference with the probabilistic characteristic of in-distribution features. On several benchmarks, our method not only achieves the state-of-the-art out-of-distribution detection performance but also improves the in-distribution accuracy.",,2,1.0,all publisherfullgold,All Open Access Gold,NSFC,2021TD-05,National Natural Science Foundation of China
2-s2.0-85141795521,,,,Towards Practical Control of Singular Values of Convolutional Layers,cp,Conference Paper,Senderovich A.,60025858;60020513,ETH Zürich;HSE University,Zurich;Moscow,Switzerland;Russian Federation,4.0,"Senderovich, Alexandra;Bulatova, Ekaterina;Obukhov, Anton;Rakhuba, Maxim",57994712000;57994712100;57219628046;55631908800,60020513;60020513;60025858;60020513,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"In general, convolutional neural networks (CNNs) are easy to train, but their essential properties, such as generalization error and adversarial robustness, are hard to control. Recent research demonstrated that singular values of convolutional layers significantly affect such elusive properties and offered several methods for controlling them. Nevertheless, these methods present an intractable computational challenge or resort to coarse approximations. In this paper, we offer a principled approach to alleviating constraints of the prior art at the expense of an insignificant reduction in layer expressivity. Our method is based on the tensor-train decomposition; it retains control over the actual singular values of convolutional mappings while providing structurally sparse and hardware-friendly representation. We demonstrate the improved properties of modern CNNs with our method and analyze its impact on the model performance, calibration, and adversarial robustness.",,6,0.0,,,ACRF,000000D730321P5Q0002,Analytical Center for the Government of the Russian Federation
2-s2.0-85204282084,,,,Towards Robust Multi-Label Learning against Dirty Label Noise,cp,Conference Paper,Zhao Y.,60031863;60118697;60105478;60121755,"Northeastern University;College of Information Science and Engineering, Northeastern University;Singapore Institute of Technology;Singapore University of Social Sciences",Shenyang;Shenyang;Singapore City;Singapore City,China;China;Singapore;Singapore,8.0,"Zhao, Yuhai;Wang, Yejiang;Wang, Zhengkui;Shan, Wen;Huang, Miaomiao;Wang, Meixia;Huang, Min;Wang, Xingwei",55350109700;57221921148;37089837400;57541700800;58485768100;59447674400;8652589000;57729227100,60031863;60031863;60105478;60121755;60031863;60031863;60118697;60031863,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,5581-5589,"In multi-label learning, one of the major challenges is that the data are associated with label noise including the random noisy labels (e.g., data encoding errors) and noisy labels created by annotators (e.g., missing, extra, or error label), where noise is promoted by different structures (e.g., gaussian, sparse, or subjective). Existing methods are tailored to handle noise with one specific structure. However, they lack of consideration of the fact that the data are always with dirty noisy labels, simutaneously gaussian, sparse, and subjective, in real applications. In this paper, we formalize the multi-label learning with dirty noise as a new learning problem, namely Noisy Multi-label Learning (NML). To solve the NML problem, we decompose a corrupted label matrix as the noise matrix plus a true label matrix (maybe high-rank). For the noise matrix, a mixed norm penalty is developed as regularizer for dirty noise distribution. Under this norm, the conditions required for exact noise recovery are provided theoretically. For the true label matrix that is not necessarily low-rank, we apply a non-linear mapping to ensure its low-rankness such that the high-order label correlation can be utilized. Experimental results show that the proposed method outperforms the state-of-the-art methods significantly.",,2,0.0,,,NSFC,92267206,National Natural Science Foundation of China
2-s2.0-85203828366,,,,Towards Scalable and Versatile Weight Space Learning,cp,Conference Paper,Schürholt K.,60025038;60007174;60027786;60033286,"University of California, Berkeley;Lawrence Berkeley National Laboratory;University of St. Gallen;International Computer Science Institute",Berkeley;Berkeley;St Gallen;Berkeley,United States;United States;Switzerland;United States,3.0,"Schürholt, Konstantin;Mahoney, Michael W.;Borth, Damian",57219789947;7202006961;25652857800,60027786-60033286;60033286-60007174-60025038;60027786,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,43947-43966,"Learning representations of well-trained neural network models holds the promise to provide an understanding of the inner workings of those models. However, previous work has either faced limitations when processing larger networks or was task-specific to either discriminative or generative tasks. This paper introduces the SANE approach to weight-space learning. SANE overcomes previous limitations by learning task-agnostic representations of neural networks that are scalable to larger models of varying architectures and that show capabilities beyond a single task. Our method extends the idea of hyper-representations towards sequential processing of subsets of neural network weights, thus allowing one to embed larger neural networks as a set of tokens into the learned representation space. SANE reveals global model information from layer-wise embeddings, and it can sequentially generate unseen neural network models, which was unattainable with previous hyper-representation learning methods. Extensive empirical evaluation demonstrates that SANE matches or exceeds state-of-the-art performance on several weight representation learning benchmarks, particularly in initialization for new tasks and larger ResNet architectures.",,0,0.0,,,NSF,,National Science Foundation
2-s2.0-85163071321,,,,Towards Theoretical Analysis of Transformation Complexity of ReLU DNNs,cp,Conference Paper,Ren J.,60025084;60030612;60027950,"Shanghai Jiao Tong University;University of California, San Diego;Carnegie Mellon University",Shanghai;La Jolla;Pittsburgh,China;United States;United States,5.0,"Ren, Jie;Li, Mingjie;Zhou, Meng;Chan, Shih Han;Zhang, Quanshi",57220774718;57204472115;57216870465;57702055900;55243570400,60025084;60025084;60027950;60030612;60025084,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,18537-18558,"This paper aims to theoretically analyze the complexity of feature transformations encoded in piecewise linear DNNs with ReLU layers. We propose metrics to measure three types of complexities of transformations based on the information theory. We further discover and prove the strong correlation between the complexity and the disentanglement of transformations. Based on the proposed metrics, we analyze two typical phenomena of the change of the transformation complexity during the training process, and explore the ceiling of a DNN's complexity. The proposed metrics can also be used as a loss to learn a DNN with the minimum complexity, which also controls the over-fitting level of the DNN and influences adversarial robustness, adversarial transferability, and knowledge consistency. Comprehensive comparative studies have provided new perspectives to understand the DNN. The code is released at https://github.com/sjtu-XAI-lab/transformation-complexity.",,2,0.0,,,NSFC,21JC1403800,Natural Science Foundation of Shanghai
2-s2.0-85163098684,,,,Tractable Dendritic RNNs for Reconstructing Nonlinear Dynamical Systems,cp,Conference Paper,Brenner M.,60002483;60016908;60005429;60005285,Universiteit van Amsterdam;Universität Heidelberg;National Taiwan University;Zentralinstitut für Seelische Gesundheit,Amsterdam;Heidelberg;Taipei;Mannheim,Netherlands;Germany;Taiwan;Germany,7.0,"Brenner, Manuel;Hess, Florian;Mikhaeil, Jonas M.;Bereska, Leonard;Monfared, Zahra;Kuo, Po Chen;Durstewitz, Daniel",57814620900;57814621000;57313658700;57214439090;57191096827;57215530550;12771147900,60005285-60016908;60005285-60016908;60005285-60016908;60005285-60002483;60005285;60005429;60005285-60016908,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,2292-2320,"In many scientific disciplines, we are interested in inferring the nonlinear dynamical system underlying a set of observed time series, a challenging task in the face of chaotic behavior and noise. Previous deep learning approaches toward this goal often suffered from a lack of interpretability and tractability. In particular, the high-dimensional latent spaces often required for a faithful embedding, even when the underlying dynamics lives on a lower-dimensional manifold, can hamper theoretical analysis. Motivated by the emerging principles of dendritic computation, we augment a dynamically interpretable and mathematically tractable piecewise-linear (PL) recurrent neural network (RNN) by a linear spline basis expansion. We show that this approach retains all the theoretically appealing properties of the simple PLRNN, yet boosts its capacity for approximating arbitrary nonlinear dynamical systems in comparatively low dimensions. We employ two frameworks for training the system, one combining back-propagation-through-time (BPTT) with teacher forcing, and another based on fast and scalable variational inference. We show that the dendritically expanded PLRNN achieves better reconstructions with fewer parameters and dimensions on various dynamical systems benchmarks and compares favorably to other methods, while retaining a tractable and interpretable structure.",,30,0.0,,,DFG,Du354/10-1,Deutsche Forschungsgemeinschaft
2-s2.0-85163078396,,,,Tractable Uncertainty for Structure Learning,cp,Conference Paper,Wang B.,60026851,University of Oxford,Oxford,United Kingdom,3.0,"Wang, Benjie;Wicker, Matthew;Kwiatkowska, Marta",57219589615;57201701851;26643165900,60026851;60026851;60026851,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,23131-23150,"Bayesian structure learning allows one to capture uncertainty over the causal directed acyclic graph (DAG) responsible for generating given data. In this work, we present Tractable Uncertainty for STructure learning (TRUST), a framework for approximate posterior inference that relies on probabilistic circuits as the representation of our posterior belief. In contrast to sample-based posterior approximations, our representation can capture a much richer space of DAGs, while also being able to tractably reason about the uncertainty through a range of useful inference queries. We empirically show how probabilistic circuits can be used as an augmented representation for structure learning methods, leading to improvement in both the quality of inferred structures and posterior uncertainty. Experimental results on conditional query answering further demonstrate the practical utility of the representational capacity of TRUST.",,7,0.0,,,ERC,834115,European Research Council
2-s2.0-105018453306,,,,Trained Transformers Learn Linear Models In-Context,ar,Article,Zhang R.,60025038;60014439;60121438;60111161,"University of California, Berkeley;University of California, Davis;Department of Electrical Engineering and Computer Sciences;DeepMind Technologies Limited",Berkeley;Davis;Berkeley;London,United States;United States;United States;United Kingdom,3.0,"Zhang, Ruiqi;Frei, Spencer;Bartlett, Peter L.",58454950000;57192103008;7202466727,60025038;60014439;60121438-60111161,2024-01-01,2024,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,25,,,,"Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models’ predictions mimic those of ordinary least squares. Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this global minimum, when given a test prompt of labeled examples from a new prediction task, the transformer achieves prediction error competitive with the best linear predictor over the test prompt distribution. We additionally characterize the robustness of the trained transformer to a variety of distribution shifts and show that although a number of shifts are tolerated, shifts in the covariate distribution of the prompts are not. Motivated by this, we consider a generalized ICL setting where the covariate distributions can vary across prompts. We show that although gradient flow succeeds at finding a global minimum in this setting, the trained transformer is still brittle under mild covariate shifts. We complement this finding with experiments on large, nonlinear transformer architectures which we show are more robust under covariate shifts.",generalization | in-context learning | neural networks | self-attention | transformers,69,0.0,,,NSF,DMS-2031883,National Science Foundation
2-s2.0-85147734399,,,,Training and Evaluation of Deep Policies Using Reinforcement Learning and Generative Models,ar,Article,Ghadirzadeh A.,60012708;60002014;60103653,Stanford University;The Royal Institute of Technology (KTH);Aalto University,Stanford;Stockholm;Espoo,United States;Sweden;Finland,7.0,"Ghadirzadeh, Ali;Poklukar, Petra;Arndt, Karol;Finn, Chelsea;Kyrki, Ville;Kragic, Danica;Björkman, Mårten",57199671469;57219690293;57215561594;55938427500;6603653623;6701873248;7005103070,60012708;60002014;60103653;60012708;60103653;60002014;60002014,2022-07-01,1 July 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"We present a data-efficient framework for solving sequential decision-making problems which exploits the combination of reinforcement learning (RL) and latent variable generative models. The framework, called GenRL, trains deep policies by introducing an action latent variable such that the feed-forward policy search can be divided into two parts: (i) training a sub-policy that outputs a distribution over the action latent variable given a state of the system, and (ii) unsupervised training of a generative model that outputs a sequence of motor actions conditioned on the latent action variable. GenRL enables safe exploration and alleviates the data-inefficiency problem as it exploits prior knowledge about valid sequences of motor actions. Moreover, we provide a set of measures for evaluation of generative models such that we are able to predict the performance of the RL policy training prior to the actual training on a physical robot. We experimentally determine the characteristics of generative models that have most influence on the performance of the final policy training on two robotics tasks: shooting a hockey puck and throwing a basketball. Furthermore, we empirically demonstrate that GenRL is the only method which can safely and efficiently solve the robotics tasks compared to two state-of-the-art RL methods.",deep generative models | policy search | reinforcement learning | representation learning | robot learning,2,0.0,,,EU,,Emory University
2-s2.0-85205705910,,,,Transportability for Bandits with Data from Different Environments,cp,Conference Paper,Bellot A.,60111161,DeepMind Technologies Limited,London,United Kingdom,3.0,"Bellot, Alexis;Malek, Alan;Chiappa, Silvia",57201875515;59428819400;12242916900,60111161;60111161;60111161,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"A unifying theme in the design of intelligent agents is to efficiently optimize a policy based on what prior knowledge of the problem is available and what actions can be taken to learn more about it. Bandits are a canonical instance of this task that has been intensely studied in the literature. Most methods, however, typically rely solely on an agent's experimentation in a single environment (or multiple closely related environments). In this paper, we relax this assumption and consider the design of bandit algorithms from a combination of batch data and qualitative assumptions about the relatedness across different environments, represented in the form of causal models. In particular, we show that it is possible to exploit invariances across environments, wherever they may occur in the underlying causal model, to consistently improve learning. The resulting bandit algorithm has a sub-linear regret bound with an explicit dependency on a term that captures how informative related environments are for the task at hand; and may have substantially lower regret than experimentation-only bandit instances.",,4,0.0,,,,,
2-s2.0-85179780833,,,,Tree-AMP: Compositional Inference with Tree Approximate Message Passing,ar,Article,Baker A.,60106017;60000179,Université Paris-Saclay;École Normale Supérieure,Gif-sur-Yvette;Paris,France;France,4.0,"Baker, Antoine;Krzakala, Florent;Aubin, Benjamin;Zdeborová, Lenka",57216486995;24073553800;57208439344;22137140200,60000179;60000179;60106017;60106017,2023-01-01,2023,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,24,,,,"We introduce Tree-AMP, standing for Tree Approximate Message Passing, a python package for compositional inference in high-dimensional tree-structured models. The package provides a unifying framework to study several approximate message passing algorithms previously derived for a variety of machine learning tasks such as generalized linear models, inference in multi-layer networks, matrix factorization, and reconstruction using non-separable penalties. For some models, the asymptotic performance of the algorithm can be theoretically predicted by the state evolution, and the measurements entropy estimated by the free entropy formalism. The implementation is modular by design: each module, which implements a factor, can be composed at will with other modules to solve complex inference tasks. The user only needs to declare the factor graph of the model: the inference algorithm, state evolution and entropy estimation are fully automated. The source code is publicly available at https://github.com/sphinxteam/tramp and the documentation at https://sphinxteam.github.io/tramp.docs.",Bethe entropy | expectation propagation | graphical models | probabilistic programming | state evolution,1,0.0,,,ERC,ANR-17-CE23-0023-01 PAIL,European Research Council
2-s2.0-85213328346,10.1613/jair.1.15273,,,Truth-tracking with Non-expert Information Sources,ar,Article,Singleton J.,60023998,Cardiff University,Cardiff,United Kingdom,2.0,"Singleton, Joseph;Booth, Richard",57219760512;36191802300,60023998;60023998,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,619-641,"We study what can be learned when receiving propositional reports from multiple nonexpert information sources. We suppose that sources report all that they consider possible, given their expertise. This may result in false and inconsistent reports when sources lack expertise on a topic. A learning method is truth-tracking, roughly speaking, if it eventually converges to correct beliefs about the “actual” world. This involves finding both the actual state of affairs in the domain described by the sources, and finding the extent of the expertise of the sources themselves. We investigate the extent to which truth-tracking is possible, and describe what information can be learned even if the actual world cannot be pinned down uniquely. We find that a broad spread of expertise among the sources allows the actual state of affairs to be found, even if no individual source is an expert on all topics. On the other hand, narrower expertise at the individual level allows the actual expertise to be found more easily. Finally, we turn to learning methods themselves: we provide a postulate-based characterisation of truth-tracking for general methods under mild assumptions, before looking at a couple of specific classes of methods from the belief change literature.",,1,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,,,
2-s2.0-85138507671,10.1609/aaai.v36i5.20407,,,Truthful and Fair Mechanisms for Matroid-Rank Valuations,cp,Conference Paper,Barman S.,60009254;125186808,Purdue University;Indian Institute of Science,West Lafayette;Challakere,United States;India,2.0,"Barman, Siddharth;Verma, Paritosh",34879498400;57654821200,125186808;60009254,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,4801-4808,"We study the problem of allocating indivisible goods among strategic agents. We focus on settings wherein monetary transfers are not available and each agent's private valuation is a submodular function with binary marginals, i.e., the agents' valuations are matroid-rank functions. In this setup, we establish a notable dichotomy between two of the most well-studied fairness notions in discrete fair division; specifically, between envy-freeness up to one good (EF1) and maximin shares (MMS). First, we show that a known Pareto-efficient mechanism is group strategy-proof for finding EF1 allocations, under matroid-rank valuations. The group strategy-proofness guarantee strengthens an existing result that establishes truthfulness (individually for each agent) in the same context. Our result also generalizes prior work from binary additive valuations to the matroid-rank case. Next, we establish that an analogous positive result cannot be achieved for MMS, even when considering truthfulness on an individual level. Specifically, we prove that, for matroid-rank valuations, there does not exist a truthful mechanism that is index oblivious, Pareto efficient, and maximin fair. For establishing our results, we develop a characterization of truthful mechanisms for matroid-rank functions. This characterization in fact holds for a broader class of valuations (specifically, holds for binary XOS functions) and might be of independent interest.",,19,1.0,all publisherfullgold,All Open Access Gold,MSR,CRG/2021/006165,Microsoft Research
2-s2.0-85174419822,,,,Two-Scale Gradient Descent Ascent Dynamics Finds Mixed Nash Equilibria of Continuous Games: A Mean-Field Perspective,cp,Conference Paper,Lu Y.,130354129,Sachusetts,Amherst,United States,1.0,"Lu, Yulong",57208674616,130354129,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,22790-22811,"Finding the mixed Nash equilibria (MNE) of a two-player zero sum continuous game is an important and challenging problem in machine learning. A canonical algorithm to finding the MNE is the noisy gradient descent ascent method which in the infinite particle limit gives rise to the Mean-Field Gradient Descent Ascent (GDA) dynamics on the space of probability measures. In this paper, we first study the convergence of a two-scale Mean-Field GDA dynamics for finding the MNE of the entropy-regularized objective. More precisely we show that for each finite temperature (or regularization parameter), the two-scale Mean-Field GDA with a suitable finite scale ratio converges exponentially to the unique MNE without assuming the convexity or concavity of the interaction potential. The key ingredient of our proof lies in the construction of new Lyapunov functions that dissipate exponentially along the Mean-Field GDA. We further study the simulated annealing of the Mean-Field GDA dynamics. We show that with a temperature schedule that decays logarithmically in time the annealed Mean-Field GDA converges to the MNE of the original unregularized objective.",,7,0.0,,,NSF,DMS-2107934,National Science Foundation
2-s2.0-85148058631,,,,Two-mode Networks: Inference with as Many Parameters as Actors and Differential Privacy,ar,Article,Wang Q.,60008928;60010591;60163091;60026749,"The Hong Kong Polytechnic University;Central China Normal University;Faculty of Science, Engineering and Medicine;Zhaoqing University",Hong Kong;Wuhan;Coventry;Zhaoqing,Hong Kong;China;United Kingdom;China,4.0,"Wang, Qiuping;Yan, Ting;Jiang, Binyan;Leng, Chenlei",57215022464;35263349600;55489862600;15832364300,60026749;60010591;60008928;60163091,2022-10-01,1 October 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,292,,"Many network data encountered are two-mode networks. These networks are characterized by having two sets of nodes and links are only made between nodes belonging to different sets. While their two-mode feature triggers interesting interactions, it also increases the risk of privacy exposure, and it is essential to protect sensitive information from being disclosed when releasing these data. In this paper, we introduce a weak notion of edge differential privacy and propose to release the degree sequence of a two-mode network by adding non-negative Laplacian noises that satisfies this privacy definition. Under mild conditions for an exponential-family model for bipartite graphs in which each node is individually parameterized, we establish the consistency and asymptotic normality of two differential privacy estimators, the first based on moment equations and the second after denoising the noisy sequence. For the latter, we develop an efficient algorithm which produces a readily useful synthetic bipartite graph. Numerical simulations and a real data application are carried out to verify our theoretical results and demonstrate the usefulness of our proposal.",Asymptotic normality | Consistency | Differential privacy | Synthetic graph | Two-mode network,3,0.0,,,NSFC,GRF15302722,National Natural Science Foundation of China
2-s2.0-85204295782,10.24963/ijcai.2024/718,,,Two-stage Semi-supervised Speaker Recognition with Gated Label Learning,cp,Conference Paper,Wang X.,60008928;60003353;60018933,The Hong Kong Polytechnic University;Harbin Engineering University;Singapore Management University,Hong Kong;Harbin;Singapore City,Hong Kong;China;Singapore,5.0,"Wang, Xingmei;Meng, Jiaxiang;Lee, Kong Aik;Li, Boquan;Liu, Jinghan",34769235100;57371969800;7501503733;56129960300;58919291600,60003353;60003353;60008928;60003353-60018933;60003353,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,6495-6503,"Speaker recognition technologies have been successfully applied in diverse domains, benefiting from the advance of deep learning. Nevertheless, current efforts are still subject to the lack of labeled data. Such issues have been attempted in computer vision, through semi-supervised learning (SSL) that assigns pseudo labels for unlabeled data, undertaking the role of labeled ones. Through our empirical evaluations, the state-of-the-art SSL methods show unsatisfactory performance in speaker recognition tasks, due to the imbalance between the quantity and quality of pseudo labels. Therefore, in this work, we propose a two-stage SSL framework, with the aim to address the data scarcity challenge. We first construct an initial contrastive learning network, where the encoder outputs the embedding representation of utterances. Furthermore, we construct an iterative holistic semi-supervised learning network that involves a clustering strategy to assign pseudo labels, and a gated label learning (GLL) strategy to further select reliable pseudo-label data. Systematical evaluations show that our proposed framework achieves superior performance in speaker recognition than the state-of-the-art methods, matching the performance of supervised learning.",,3,1.0,all publisherfullgold,All Open Access Gold,,3072022JC0601,Fundamental Research Funds for Central Universities of the Central South University
2-s2.0-105000468002,,,,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,cp,Conference Paper,Gong B.,60003970;60013614;60118460;60121285,Zhejiang University;Hangzhou Dianzi University;Alibaba Group Holding Limited;Ant group,Hangzhou;Hangzhou;Hangzhou;Hangzhou,China;China;China;China,9.0,"Gong, Biao;Tan, Shuai;Feng, Yutong;Xie, Xiaoying;Li, Yuyuan;Chen, Chaochao;Zheng, Kecheng;Shen, Yujun;Zhao, Deli",57218919848;59368853600;57218925390;58115005600;57219622973;56023443400;57204973583;57207766466;55475656600,60118460;60121285;60118460;60118460;60013614;60003970;60121285;60121285;60118460,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"This work presents a unified knowledge protocol, called UKnow, which facilitates knowledge-based studies from the perspective of data. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image, in-text, cross-image, cross-text, and image-text, and set up an efficient pipeline to help construct the multimodal knowledge graph from any data collection. Thanks to the logical information naturally contained in knowledge graph, organizing datasets under UKnow format opens up more possibilities of data usage compared to the commonly used image-text pairs. Following UKnow protocol, we collect, from public international news, a large-scale multimodal knowledge graph dataset that consists of 1,388,568 nodes (with 571,791 vision-related ones) and 3,673,817 triplets. The dataset is also annotated with rich event tags, including 11 coarse labels and 9,185 fine labels. Experiments on 4 benchmarks demonstrate the potential of UKnow in supporting common-sense reasoning and boosting vision-language pre-training with a single dataset, benefiting from its unified form of knowledge organization. See Appendix A to download the dataset.",,1,0.0,,,,,
2-s2.0-85200582342,,,,UNCERTAINTY-AWARE CONSTRAINT INFERENCE IN INVERSE CONSTRAINED REINFORCEMENT LEARNING,cp,Conference Paper,Xu S.,60108865,"The Chinese University of Hong Kong, Shenzhen",Shenzhen,China,2.0,"Xu, Sheng;Liu, Guiliang",59249524700;57204467338,60108865;60108865,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Aiming for safe control, Inverse Constrained Reinforcement Learning (ICRL) considers inferring the constraints respected by expert agents from their demonstrations and learning imitation policies that adhere to these constraints. While previous ICRL works often neglected underlying uncertainties during training, we contend that modeling these uncertainties is crucial for facilitating robust constraint inference. This insight leads to the development of an Uncertainty-aware Inverse Constrained Reinforcement Learning (UAICRL) algorithm. Specifically, 1) aleatoric uncertainty arises from the inherent stochasticity of environment dynamics, leading to constraint-violating behaviors in imitation policies. To address this, UAICRL constructs risk-sensitive constraints by incorporating distributional Bellman updates into the cumulative costs model. 2) Epistemic uncertainty, resulting from the model's limited knowledge of Out-of-Distribution (OoD) samples, affects the accuracy of step-wise cost predictions. To tackle this issue, UAICRL develops an information-theoretic quantification of the epistemic uncertainty and mitigates its impact through flow-based generative data augmentation. Empirical results demonstrate that UAICRL consistently outperforms other baselines in continuous and discrete environments with stochastic dynamics. The code is available at https://github.com/Jasonxu1225/UAICRL.",,6,0.0,,,NKRDPC,JCYJ20230807114202005,Shenzhen Municipal Fundamental Research Program
2-s2.0-85150352749,,,,UNDERSTANDING AND LEVERAGING OVERPARAMETERIZATION IN RECURSIVE VALUE ESTIMATION,cp,Conference Paper,Xiao C.,60030835;129320565,University of Alberta;Google,Edmonton;,Canada;,7.0,"Xiao, Chenjun;Dai, Bo;Mei, Jincheng;Ramirez, Oscar;Gummadi, Ramki;Harris, Chris;Schuurmans, Dale",55365453800;58918912100;56393854100;56202810100;57209452592;56910562200;57204335408,129320565-60030835;129320565;129320565;129320565;129320565;129320565;129320565-60030835,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"The theory of function approximation in reinforcement learning (RL) typically considers low capacity representations that incur a tradeoff between approximation error, stability and generalization. Current deep architectures, however, operate in an overparameterized regime where approximation error is not necessarily a bottleneck. To better understand the utility of deep models in RL we present an analysis of recursive value estimation using overparameterized linear representations that provides useful, transferable findings. First, we show that classical updates such as temporal difference (TD) learning or fitted-value-iteration (FVI) converge to different fixed points than residual minimization (RM) in the overparameterized linear case. We then develop a unified interpretation of overparameterized linear value estimation as minimizing the Euclidean norm of the weights subject to alternative constraints. A practical consequence is that RM can be modified by a simple alteration of the backup targets to obtain the same fixed points as FVI and TD (when they converge), while universally ensuring stability. Further, we provide an analysis of the generalization error of these methods, demonstrating per iterate bounds on the value prediction error of FVI, and fixed point bounds for TD and RM. Given this understanding, we then develop new algorithmic tools for improving recursive value estimation with deep models. In particular, we extract two regularizers that penalize out-of-span top-layer weights and co-linearity in top-layer features respectively. Empirically we find that these regularizers dramatically improve the stability of TD and FVI, while allowing RM to match and even sometimes surpass their generalization performance with assured stability.",,2,0.0,,,NSERC,,Natural Sciences and Engineering Research Council of Canada
2-s2.0-85162815561,,,,UNDERSTANDING THE COVARIANCE STRUCTURE OF CONVOLUTIONAL FILTERS,cp,Conference Paper,Trockman A.,60027950;60345017,Carnegie Mellon University;Bosch Center for Artificial Intelligence,Pittsburgh;Renningen,United States;Germany,3.0,"Trockman, Asher;Willmott, Devin;Kolter, J. Zico",57202893894;57219622792;23110770000,60027950;60345017;60027950-60345017,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Neural network weights are typically initialized at random from univariate distributions, controlling just the variance of individual weights even in highly-structured operations like convolutions.Recent ViT-inspired convolutional networks such as ConvMixer and ConvNeXt use large-kernel depthwise convolutions whose learned filters have notable structure; this presents an opportunity to study their empirical covariances.In this work, we first observe that such learned filters have highly-structured covariance matrices, and moreover, we find that covariances calculated from a small network may be used to effectively initialize a variety of larger networks of different depths, widths, patch sizes, and kernel sizes, indicating a degree of model-independence to the covariance structure.Motivated by this finding, we then propose a learning-free multivariate initialization scheme for convolutional filters using a simple, closed-form construction of their covariance.Models using our initialization outperform those using traditional univariate initializations, and typically meet or exceed the performance of those initialized from the covariances of learned filters; in some cases, this improvement can be achieved without training the depthwise convolutional filters at all.Our code is available at https://github.com/locuslab/convcov.",,4,0.0,,,,,
2-s2.0-85183822909,,,,UNIVERSAL FEW-SHOT LEARNING OF DENSE PREDICTION TASKS WITH VISUAL TOKEN MATCHING,cp,Conference Paper,Kim D.,60032144;60098464,Korea Advanced Institute of Science and Technology;Microsoft Research Asia,Daejeon;Beijing,South Korea;China,5.0,"Kim, Donggyun;Kim, Jinwoo;Cho, Seongwoong;Luo, Chong;Hong, Seunghoon",57219634435;59866432600;57327014300;8343879100;56118655200,60032144;60032144;60032144;60098464;60032144,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Dense prediction tasks are a fundamental class of problems in computer vision. As supervised methods suffer from high pixel-wise labeling cost, a few-shot learning solution that can learn any dense task from a few labeled images is desired. Yet, current few-shot learning methods target a restricted set of tasks such as semantic segmentation, presumably due to challenges in designing a general and unified model that is able to flexibly and efficiently adapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks. It employs non-parametric matching on patch-level embedded tokens of images and labels that encapsulates all tasks. Also, VTM flexibly adapts to any task with a tiny amount of task-specific parameters that modulate the matching algorithm. We implement VTM as a powerful hierarchical encoder-decoder architecture involving ViT backbones where token matching is performed at multiple feature hierarchies. We experiment VTM on a challenging variant of Taskonomy dataset and observe that it robustly few-shot learns various unseen dense prediction tasks. Surprisingly, it is competitive with fully supervised baselines using only 10 labeled examples of novel tasks (0.004% of full supervision) and sometimes outperforms using 0.1% of full supervision. Codes are available at https://github.com/GitGyun/visual_token_matching.",,24,0.0,,,MSR,2021R1C1C1012540,Microsoft Research
2-s2.0-85194041577,,,,UNLEASHING THE POTENTIAL OF FRACTIONAL CALCULUS IN GRAPH NEURAL NETWORKS WITH FROND,cp,Conference Paper,Kang Q.,60005510;60001455;60121755;129304167,Nanyang Technological University;Anhui University;Singapore University of Social Sciences;C3 AI,Singapore City;Hefei;Singapore City;Singapore City,Singapore;China;Singapore;Singapore,8.0,"Kang, Qiyu;Zhao, Kai;Ding, Qinxu;Ji, Feng;Li, Xuhao;Liang, Wenfei;Song, Yang;Tay, Wee Peng",57196022385;57909978400;57204173700;57191196789;57191631029;59140170600;56149485400;18233764400,60005510;60005510;60121755;60005510;60001455;60005510;129304167;60005510,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"We introduce the FRactional-Order graph Neural Dynamical network (FROND), a new continuous graph neural network (GNN) framework. Unlike traditional continuous GNNs that rely on integer-order differential equations, FROND employs the Caputo fractional derivative to leverage the non-local properties of fractional calculus. This approach enables the capture of long-term dependencies in feature updates, moving beyond the Markovian update mechanisms in conventional integer-order models and offering enhanced capabilities in graph representation learning. We offer an interpretation of the node feature updating process in FROND from a non-Markovian random walk perspective when the feature updating is particularly governed by a diffusion process. We demonstrate analytically that oversmoothing can be mitigated in this setting. Experimentally, we validate the FROND framework by comparing the fractional adaptations of various established integer-order continuous GNNs, demonstrating their consistently improved performance and underscoring the framework's potential as an effective extension to enhance traditional continuous GNNs. The code is available at https://github.com/zknus/ICLR2024-FROND.",,11,0.0,,,MOE,MOE-T2EP20220-0002,Ministry of Education - Singapore
2-s2.0-85152531084,,,,UNSUPERVISED VISUALIZATION OF IMAGE DATASETS USING CONTRASTIVE LEARNING,cp,Conference Paper,Böhm J.N.,60017246,Eberhard Karls Universität Tübingen,Tubingen,Germany,3.0,"Böhm, Jan Niklas;Berens, Philipp;Kobak, Dmitry",57219764233;27267498000;55316161300,60017246;60017246;60017246,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Visualization methods based on the nearest neighbor graph, such as t-SNE or UMAP, are widely used for visualizing high-dimensional data. Yet, these approaches only produce meaningful results if the nearest neighbors themselves are meaningful. For images represented in pixel space this is not the case, as distances in pixel space are often not capturing our sense of similarity and therefore neighbors are not semantically close. This problem can be circumvented by self-supervised approaches based on contrastive learning, such as SimCLR, relying on data augmentation to generate implicit neighbors, but these methods do not produce two-dimensional embeddings suitable for visualization. Here, we present a new method, called t-SimCNE, for unsupervised visualization of image data. t-SimCNE combines ideas from contrastive learning and neighbor embeddings, and trains a parametric mapping from the high-dimensional pixel space into two dimensions. We show that the resulting 2D embeddings achieve classification accuracy comparable to the state-of-the-art high-dimensional SimCLR representations, thus faithfully capturing semantic relationships. Using t-SimCNE, we obtain informative visualizations of the CIFAR-10 and CIFAR-100 datasets, showing rich cluster structure and highlighting artifacts and outliers.",,12,0.0,,,DFG,390727645,Deutsche Forschungsgemeinschaft
2-s2.0-85203806923,,,,UP2ME: Univariate Pre-training to Multivariate Fine-tuning as a General-purpose Framework for Multivariate Time Series Analysis,cp,Conference Paper,Zhang Y.,60025084,Shanghai Jiao Tong University,Shanghai,China,4.0,"Zhang, Yunhao;Liu, Minghao;Zhou, Shengyang;Yan, Junchi",56449458300;59214046800;59325622200;36026971200,60025084;60025084;60025084;60025084,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,59358-59381,"Despite the success of self-supervised pre-training in texts and images, applying it to multivariate time series (MTS) falls behind tailored methods for tasks like forecasting, imputation and anomaly detection.We propose a general-purpose framework, named UP2ME (Univariate Pre-training to Multivariate Fine-tuning).It conducts task-agnostic pre-training when downstream tasks are unspecified.Once the task and setting (e.g.forecasting length) are determined, it gives sensible solutions with frozen pre-trained parameters, which has not been achieved before.UP2ME is further refined by fine-tuning.A univariate-to-multivariate paradigm is devised to address the heterogeneity of temporal and cross-channel dependencies.In univariate pre-training, univariate instances with diverse lengths are generated for Masked AutoEncoder (MAE) pre-training, discarding cross-channel dependency.The pretrained model handles downstream tasks by formulating them into specific mask-reconstruction problems.In multivariate fine-tuning, it constructs a dependency graph among channels using the pre-trained encoder to enhance cross-channel dependency capture.Experiments on eight real-world datasets show its SOTA performance in forecasting and imputation, approaching task-specific performance in anomaly detection.Our code is available at https://github.com/Thinklab-SJTU/UP2ME.",,1,0.0,,,NSFC,62222607,National Natural Science Foundation of China
2-s2.0-85204311580,,,,Unbiased Active Semi-supervised Binary Classification Models,cp,Conference Paper,Lee J.C.,60003711;60022659;60011754,University of Pennsylvania Perelman School of Medicine;University of Connecticut;Auburn University,Philadelphia;Storrs;Auburn,United States;United States;United States,3.0,"Lee, Joo Chul;Ma, Weidong;Wang, Ziyang",59332949500;57610836100;58120161400,60011754;60003711;60022659,2024-01-01,2024,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,4389-4397,"Active learning is known to be a well-motivated algorithm that aims to maximize model performance with relatively small data, but it introduces sampling bias due to active selection. To adjust the bias, current literature utilizes corrective weights in a supervised learning approach. However, those methods consider only a small amount of actively sampled data and thus estimation efficiency can be improved using unsampled data together. In this paper, we develop an actively improved augmented estimation equation (AI-AEE) based on corrective weights as well as imputation models that allow us to leverage unlabeled data. The asymptotic distribution of the proposed estimator as the solution to the AI-AEE is derived, and an optimal sampling scheme to minimize the asymptotic mean squared error of the estimator is proposed. We then propose a general practical algorithm for training prediction models in the active and semi-supervised learning framework. The superiority of our method is demonstrated on synthetic and real data examples.",,0,0.0,,,,,
2-s2.0-85170404089,10.24963/ijcai.2023/515,,,Unbiased Gradient Boosting Decision Tree with Unbiased Feature Importance,cp,Conference Paper,Zhang Z.,60025278,Tsinghua University,Beijing,China,3.0,"Zhang, Zheyu;Zhang, Tianping;Li, Jian",57994398300;57221139202;56199160700,60025278;60025278;60025278,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,4629-4637,"Gradient Boosting Decision Tree (GBDT) has achieved remarkable success in a wide variety of applications. The split finding algorithm, which determines the tree construction process, is one of the most crucial components of GBDT. However, the split finding algorithm has long been criticized for its bias towards features with a large number of potential splits. This bias introduces severe interpretability and overfitting issues in GBDT. To this end, we provide a fine-grained analysis of bias in GBDT and demonstrate that the bias originates from 1) the systematic bias in the gain estimation of each split and 2) the bias in the split finding algorithm resulting from the use of the same data to evaluate the split improvement and determine the best split. Based on the analysis, we propose unbiased gain, a new unbiased measurement of gain importance using out-of-bag samples. Moreover, we incorporate the unbiased property into the split finding algorithm and develop UnbiasedGBM to solve the overfitting issue of GBDT. We assess the performance of UnbiasedGBM and unbiased gain in a large-scale empirical study comprising 60 datasets and show that: 1) UnbiasedGBM exhibits better performance than popular GBDT implementations such as LightGBM, XGBoost, and Catboost on average on the 60 datasets and 2) unbiased gain achieves better average performance in feature selection than popular feature importance methods. The codes are available at https://github.com/ZheyuAqaZhang/UnbiasedGBM.",,4,1.0,all publisherfullgold,All Open Access Gold,NSFC,62161146004,National Natural Science Foundation of China
2-s2.0-85174398755,,,,Unconstrained Online Learning with Unbounded Losses,cp,Conference Paper,Jacobsen A.,60030835;60139515;60193824,University of Alberta;Boston University College of Engineering;Alberta Machine Intelligence Institute,Edmonton;Boston;Edmonton,Canada;United States;Canada,2.0,"Jacobsen, Andrew;Cutkosky, Ashok",57197840702;56979291000,60030835-60193824;60139515,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,14590-14630,"Algorithms for online learning typically require one or more boundedness assumptions: that the domain is bounded, that the losses are Lipschitz, or both. In this paper, we develop a new setting for online learning with unbounded domains and non-Lipschitz losses. For this setting we provide an algorithm which guarantees R<inf>T</inf> (u) ≤ O<sup>e</sup>(Gkuk<sup>√</sup>T + Lkuk<sup>2 √</sup>T ) regret on any problem where the subgradients satisfy kg<inf>t</inf>k ≤ G+Lkw<inf>t</inf>k, and show that this bound is unimprovable without further assumptions. We leverage this algorithm to develop new saddle-point optimization algorithms that converge in duality gap in unbounded domains, even in the absence of meaningful curvature. Finally, we provide the first algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses, as well as a matching lower bound. The regret of our dynamic regret algorithm automatically improves to a novel L<sup>∗</sup> bound when the losses are smooth.",,10,0.0,,,NSF,CCF-2211718,National Science Foundation
,,,,Underspecification Presents Challenges for Credibility in Modern Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,1040,,,,,,
2-s2.0-85170235863,,,,Understanding Oversquashing in GNNs through the Lens of Effective Resistance,cp,Conference Paper,Black M.,60030612;60137364,"University of California, San Diego;College of Engineering",La Jolla;Corvallis,United States;United States,4.0,"Black, Mitchell;Wan, Zhengchao;Nayyeri, Amir;Wang, Yusu",57226400104;57213689272;12808685600;57201620584,60137364;60030612;60137364;60030612,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,2528-2547,"Message passing graph neural networks (GNNs) are a popular learning architectures for graph-structured data. However, one problem GNNs experience is oversquashing, where a GNN has difficulty sending information between distant nodes. Understanding and mitigating oversquashing has recently received significant attention from the research community. In this paper, we continue this line of work by analyzing oversquashing through the lens of the effective resistance between nodes in the input graph. Effective resistance intuitively captures the “strength” of connection between two nodes by paths in the graph, and has a rich literature spanning many areas of graph theory. We propose to use total effective resistance as a bound of the total amount of oversquashing in a graph and provide theoretical justification for its use. We further develop an algorithm to identify edges to be added to an input graph to minimize the total effective resistance, thereby alleviating oversquashing. We provide empirical evidence of the effectiveness of our total effective resistance based rewiring strategies for improving the performance of GNNs.",,50,0.0,,,NSF,CCF-1816442,National Science Foundation
2-s2.0-85205764667,10.1613/jair.1.16422,,,Understanding What Affects the Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence,ar,Article,Lyu J.,60025278;60014966;60114181,Tsinghua University;Peking University;Tencent,Beijing;Beijing;Shenzhen,China;China;China,4.0,"Lyu, Jiafei;Wan, Le;Li, Xiu;Lu, Zongqing",57222381223;57933787600;7501701944;56275797700,60025278;60114181;60025278;60014966,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,81,,,1-42,"Recently, there are many efforts attempting to learn useful policies for continuous control in visual reinforcement learning (RL). In this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment. Many practical algorithms are proposed to handle this problem. However, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work. In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. Our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap. Our theoretical results are supported by the empirical evidence in the DMControl Generalization Benchmark (DMC-GB).",,2,1.0,all publisherfullgold,All Open Access Gold,STI,2021ZD0201404,Swiss Tumor Institute
2-s2.0-85163129136,,,,Understanding the Unstable Convergence of Gradient Descent,cp,Conference Paper,Ahn K.,60025278;60022195;116430107,Tsinghua University;Massachusetts Institute of Technology;Simons Institute,Beijing;Cambridge;Berkeley,China;United States;United States,3.0,"Ahn, Kwangjun;Zhang, Jingzhao;Sra, Suvrit",57219115138;57208444866;8865497700,60022195-116430107;60025278;60022195,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,247-257,"Most existing analyses of (stochastic) gradient descent rely on the condition that for L-smooth costs, the step size is less than 2/L. However, many works have observed that in machine learning applications step sizes often do not fulfill this condition, yet (stochastic) gradient descent still converges, albeit in an unstable manner. We investigate this unstable convergence phenomenon from first principles, and discuss key causes behind it. We also identify its main characteristics, and how they interrelate based on both theory and experiments, offering a principled view toward understanding the phenomenon.",,30,0.0,,,NSF,1846088,National Science Foundation
2-s2.0-105000534137,,,,Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE,cp,Conference Paper,Zhu X.,60025278;60016930;60104026,Tsinghua University;Beijing University of Posts and Telecommunications;Beijing National Research Center for Information Science and Technology,Beijing;Beijing;Beijing,China;China;China,5.0,"Zhu, Xun;Hu, Ying;Mo, Fanbin;Li, Miao;Wu, Ji",59610905500;58913650600;59096459500;57138821600;55714034400,60025278;60025278;60016930;60025278;60025278-60104026,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal multitask optimization in MLLMs, recent advances primarily focus on improving the LLM components, while neglecting the connector that bridges the gap between modalities. In this paper, we introduce Uni-Med, a novel medical generalist foundation model which consists of a universal visual feature extraction module, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting from the proposed CMoE that leverages a well-designed router with a mixture of projection experts at the connector, Uni-Med achieves efficient solution to the tug-of-war problem and can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. To the best of our knowledge, Uni-Med is the first effort to tackle multi-task interference at the connector in MLLMs. Extensive ablation experiments validate the effectiveness of introducing CMoE under any configuration, with up to an average 8% performance gains. We further provide interpretation analysis of the tug-of-war problem from the perspective of gradient optimization and parameter statistics. Compared to previous state-of-the-art medical MLLMs, Uni-Med achieves competitive or superior evaluation metrics on diverse tasks. Code and resources are available at https://github.com/MSIIP/Uni-Med.",,3,0.0,,,NKRDPC,2021ZD0113404,National Key Research and Development Program of China
2-s2.0-85174410057,,,,Unlocking Slot Attention by Changing Optimal Transport Costs,cp,Conference Paper,Zhang Y.,60002483;60019984;60113142;129147347;125577701,Universiteit van Amsterdam;Nederlandse Organisatie voor toegepast natuurwetenschappelijk onderzoek- TNO;Montreal Institute for Learning Algorithms;Canada CIFAR AI;Samsung,Amsterdam;The Hague;Montreal;;Montreal,Netherlands;Netherlands;Canada;Canada;Canada,5.0,"Zhang, Yan;Zhang, David W.;Lacoste-Julien, Simon;Burghouts, Gertjan J.;Snoek, Cees G.M.",57840814900;57221156992;8426902900;6507773172;9737430000,125577701;60002483;125577701-60113142-129147347;60019984;60002483,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,41931-41951,"Slot attention is a powerful method for object-centric modeling in images and videos. However, its set-equivariance limits its ability to handle videos with a dynamic number of objects because it cannot break ties. To overcome this limitation, we first establish a connection between slot attention and optimal transport. Based on this new perspective we propose MESH (Minimize Entropy of Sinkhorn): a cross-attention module that combines the tiebreaking properties of unregularized optimal transport with the speed of regularized optimal transport. We evaluate slot attention using MESH on multiple object-centric learning benchmarks and find significant improvements over slot attention in every setting.",,4,0.0,,,NWO,,Nederlandse Organisatie voor Wetenschappelijk Onderzoek
2-s2.0-105000548275,,,,Unravelling in Collaborative Learning,cp,Conference Paper,Capitaine A.,60106017;60000179;60105987,Université Paris-Saclay;École Normale Supérieure;Centre de Mathématiques Appliquées,Gif-sur-Yvette;Paris;Palaiseau,France;France;France,7.0,"Capitaine, Aymeric;Boursier, Etienne;Scheid, Antoine;Moulines, Eric;Jordan, Michael I.;El-Mhamdi, El Mahdi;Durmus, Alain",58956308600;57218717986;58956559300;7003817357;57209168184;57195391207;56653659200,60105987;60106017;60105987;60105987;60000179;60105987;60105987,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Collaborative learning offers a promising avenue for leveraging decentralized data. However, collaboration in groups of strategic learners is not a given. In this work, we consider strategic agents who wish to train a model together but have sampling distributions of different quality. The collaboration is organized by a benevolent aggregator who gathers samples so as to maximize total welfare, but is unaware of data quality. This setting allows us to shed light on the deleterious effect of adverse selection in collaborative learning. More precisely, we demonstrate that when data quality indices are private, the coalition may undergo a phenomenon known as unravelling, wherein it shrinks up to the point that it becomes empty or solely comprised of the worst agent. We show how this issue can be addressed without making use of external transfers, by proposing a novel method inspired by probabilistic verification. This approach makes the grand coalition a Nash equilibrium with high probability despite information asymmetry, thereby breaking unravelling.",,1,0.0,,,EC,101071601,European Commission
2-s2.0-85167989139,10.1609/aaai.v37i3.25404,,,Unsupervised Multi-Exposure Image Fusion Breaking Exposure Limits via Contrastive Learning,cp,Conference Paper,Xu H.,60029306,Wuhan University,Wuhan,China,3.0,"Xu, Han;Liang, Haochen;Ma, Jiayi",57201056465;7402853697;26638975600,60029306;60029306;60029306,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,3010-3017,"This paper proposes an unsupervised multi-exposure image fusion (MEF) method via contrastive learning, termed as MEF-CL. It breaks exposure limits and performance bottleneck faced by existing methods. MEF-CL firstly designs similarity constraints to preserve contents in source images. It eliminates the need for ground truth (actually not exist and created artificially) and thus avoids negative impacts of inappropriate ground truth on performance and generalization. Moreover, we explore a latent feature space and apply contrastive learning in this space to guide fused image to approximate normal-light samples and stay away from inappropriately exposed ones. In this way, characteristics of fused images (e.g., illumination, colors) can be further improved without being subject to source images. Therefore, MEFCL is applicable to image pairs of any multiple exposures rather than a pair of under-exposed and over-exposed images mandated by existing methods. By alleviating dependence on source images, MEF-CL shows better generalization for various scenes. Consequently, our results exhibit appropriate illumination, detailed textures, and saturated colors. Qualitative, quantitative, and ablation experiments validate the superiority and generalization of MEF-CL. Our code is publicly available at https://github.com/hanna-xu/MEF-CL.",,13,1.0,all publisherfullgold,All Open Access Gold,NSFC,62276192,National Natural Science Foundation of China
2-s2.0-85137943410,10.24963/ijcai.2022/526,,,Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast,cp,Conference Paper,Zhu B.,60024350,National University of Defense Technology China,Changsha,China,7.0,"Zhu, Boqing;Xu, Kele;Wang, Changjian;Qin, Zheng;Sun, Tao;Wang, Huaimin;Peng, Yuxing",57204110244;56413567000;55766741400;57202357497;56330250400;15838322600;55257095800,60024350;60024350;60024350;60024350;60024350;60024350;60024350,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,3787-3794,"We present an approach to learn voice-face representations from the talking face videos, without any identity labels. Previous works employ cross-modal instance discrimination tasks to establish the correlation of voice and face. These methods neglect the semantic content of different videos, introducing false-negative pairs as training noise. Furthermore, the positive pairs are constructed based on the natural correlation between audio clips and visual frames. However, this correlation might be weak or inaccurate in a large amount of real-world data, which leads to deviating positives into the contrastive paradigm. To address these issues, we propose the cross-modal prototype contrastive learning (CMPC), which takes advantage of contrastive methods and resists adverse effects of false negatives and deviate positives. On one hand, CMPC could learn the intra-class invariance by constructing semantic-wise positives via unsupervised clustering in different modalities. On the other hand, by comparing the similarities of cross-modal instances from that of cross-modal prototypes, we dynamically recalibrate the unlearnable instances' contribution to overall loss. Experiments show that the proposed approach outperforms state-of-the-art unsupervised methods on various voice-face association evaluation protocols. Additionally, in the low-shot supervision setting, our method also has a significant improvement compared to previous instance-wise contrastive learning.",,12,1.0,all publisherfree2read,All Open Access Bronze,,,
2-s2.0-85159484600,,,,Using natural language and program abstractions to instill human inductive biases in machines,cp,Conference Paper,Kumar S.,60003269;60141284;60111161,Princeton University;School of Engineering and Applied Science;DeepMind Technologies Limited,Princeton;Princeton;London,United States;United States;United Kingdom,10.0,"Kumar, Sreejan;Correa, Carlos G.;Dasgupta, Ishita;Marjieh, Raja;Hu, Michael Y.;Hawkins, Robert D.;Daw, Nathaniel D.;Cohen, Jonathan D.;Narasimhan, Karthik;Griffiths, Thomas L.",57210601181;57221322617;57194405744;59667007200;57221157962;59719611600;7005134854;54992680400;56577616300;57222226477,60003269;60003269;60111161;60003269;60141284;60003269;60003269;60003269;60141284;60003269-60141284,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Strong inductive biases give humans the ability to quickly learn to perform a variety of tasks. Although meta-learning is a method to endow neural networks with useful inductive biases, agents trained by meta-learning may sometimes acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and programs induced to generate such tasks guides them toward more human-like inductive biases. Human-generated language descriptions and program induction models that add new learned primitives both contain abstract concepts that can compress description length. Co-training on these representations result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without learned primitives), suggesting that the abstraction supported by these representations is key.",,14,0.0,,,NIH,T32MH065214,National Institutes of Health
2-s2.0-105000492033,,,,VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks,cp,Conference Paper,Li Y.,60012387;60151092;60018008,"Georgia State University;UConn College of Engineering;NEC Laboratories America, Inc.",Atlanta;Storrs;Princeton,United States;United States;United States,3.0,"Li, Yang;Han, Shaobo;Ji, Shihao",57221624393;57226591830;57207105926,60012387;60018008;60151092,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"As the adoption of large language models increases and the need for per-user or per-task model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs. To further reduce stored parameters, we introduce a ""divide- and-share"" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules, and layers by sharing parameters globally via a vector bank. As an instantiation of the paradigm to LoRA, our proposed VB-LoRA composites all the low-rank matrices of LoRA from a shared vector bank with a differentiable top-k admixture module. VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods. Extensive experiments demonstrate the effectiveness of VB-LoRA on natural language understanding, natural language generation, instruction tuning, and mathematical reasoning tasks. When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4% of LoRA's stored parameters, yet achieves superior results. Our source code is available at https://github.com/leo-yangli/VB-LoRA. This method has been merged into the Hugging Face PEFT package.",,2,0.0,,,,,
2-s2.0-85167982296,10.1609/aaai.v37i3.25393,,,VFI: Spiking-Based Video Frame Interpolation for High-Speed Motion,cp,Conference Paper,Xia L.,60014966;112611368;125682334,Peking University;National Computer Network Emergency Response Technical Team;Beijing Academy of Artificial Intelligence,Beijing;Beijing;Beijing,China;China;China,4.0,"Xia, Lujie;Zhao, Jing;Xiong, Ruiqin;Huang, Tiejun",58506716700;57206579986;8524403300;8914299500,60014966;60014966-112611368;60014966;60014966-125682334,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,2910-2918,"Occlusion and motion blur make it challenging to interpolate video frame, since estimating complex motions between two frames is hard and unreliable, especially in highly dynamic scenes. This paper aims to address these issues by exploiting spike stream as auxiliary visual information between frames to synthesize target frames. Instead of estimating motions by optical flow from RGB frames, we present a new dual-modal pipeline adopting both RGB frames and the corresponding spike stream as inputs (SVFI). It extracts the scene structure and objects' outline feature maps of the target frames from spike stream. Those feature maps are fused with the color and texture feature maps extracted from RGB frames to synthesize target frames. Benefited by the spike stream that contains consecutive information between two frames, SVFI can directly extract the information in occlusion and motion blur areas of target frames from spike stream, thus it is more robust than previous optical flow-based methods. Experiments show SVFI outperforms the SOTA methods on wide variety of datasets. For instance, in 7 and 15 frame skip evaluations, it shows up to 5.58 dB and 6.56 dB improvements in terms of PSNR over the corresponding second best methods BMBC and DAIN. SVFI also shows visually impressive performance in real-world scenes.",,12,1.0,all publisherfullgold,All Open Access Gold,NSFC,22127807,National Natural Science Foundation of China
2-s2.0-85200552275,,,,VISION TRANSFORMERS NEED REGISTERS,cp,Conference Paper,Darcet T.,60104653;126335720,Université Grenoble Alpes;Meta,Saint Martin d'Heres;Meta,France;United States,4.0,"Darcet, Timothée;Oquab, Maxime;Mairal, Julien;Bojanowski, Piotr",58251254300;56422254400;35488349500;56119369300,126335720-60104653;126335720;60104653;126335720,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.",,128,0.0,,,ERC,ANR-19-P3IA-0003,Engineering Research Centers
2-s2.0-85200567175,,,,VISUAL DATA-TYPE UNDERSTANDING DOES NOT EMERGE FROM SCALING VISION-LANGUAGE MODELS,cp,Conference Paper,Udandarao V.,60031101;60017246;60031514,University of Cambridge;Eberhard Karls Universität Tübingen;Georg-August-Universität Göttingen,Cambridge;Tubingen;Gottingen,United Kingdom;Germany;Germany,4.0,"Udandarao, Vishaal;Burg, Max F.;Albanie, Samuel;Bethge, Matthias",57209882385;57219829621;57202293265;57210225326,60031101-60017246;60017246-60031514;60031101;60017246,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic data-types, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual data-types through scaling. By analyzing the pre-training distributions of these models and incorporating data-type information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding.",,2,0.0,,,DFG,SFB 1233,Deutsche Forschungsgemeinschaft
2-s2.0-85163069942,,,,Variational Inference for Infinitely Deep Neural Networks,cp,Conference Paper,Nazaret A.,60030162;60031321,Columbia University;The Fu Foundation School of Engineering and Applied Science,New York;New York,United States;United States,2.0,"Nazaret, Achille;Blei, David",57219586944;55914504500,60031321;60031321-60030162,2022-01-01,2022,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,162,,,16447-16461,"We introduce the unbounded depth neural network (UDN), an infinitely deep probabilistic model that adapts its complexity to the training data. The UDN contains an infinite sequence of hidden layers and places an unbounded prior on a truncation ℓ, the layer from which it produces its data. Given a dataset of observations, the posterior UDN provides a conditional distribution of both the parameters of the infinite neural network and its truncation. We develop a novel variational inference algorithm to approximate this posterior, optimizing a distribution of the neural network weights and of the truncation depth ℓ, and without any upper limit on ℓ. To this end, the variational family has a special structure: it models neural network weights of arbitrary depth, and it dynamically creates or removes free variational parameters as its distribution of the truncation is optimized. (Unlike heuristic approaches to model search, it is solely through gradient-based optimization that this algorithm explores the space of truncations.) We study the UDN on real and synthetic data. We find that the UDN adapts its posterior depth to the dataset complexity; it outperforms standard neural networks of similar computational complexity; and it outperforms other approaches to infinite-depth neural networks.",,5,0.0,,,NSF,IIS 2127869,National Science Foundation
2-s2.0-85203786638,,,,Variational Partial Group Convolutions for Input-Aware Partial Equivariance of Rotations and Color-Shifts,cp,Conference Paper,Kim H.,60032144;60159728,"Korea Advanced Institute of Science and Technology;AItrics Co., Ltd.",Daejeon;Seoul,South Korea;South Korea,4.0,"Kim, Hyunsu;Kim, Yegon;Yang, Hongseok;Lee, Juho",58489315600;59238859500;55153940300;57132050100,60032144;60032144;60032144;60032144-60159728,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,24194-24209,"Group Equivariant CNNs (G-CNNs) have shown promising efficacy in various tasks, owing to their ability to capture hierarchical features in an equivariant manner. However, their equivariance is fixed to the symmetry of the whole group, limiting adaptability to diverse partial symmetries in real-world datasets, such as limited rotation symmetry of handwritten digit images and limited color-shift symmetry of flower images. Recent efforts address this limitation, one example being Partial G-CNN which restricts the output group space of convolution layers to break full equivariance. However, such an approach still fails to adjust equivariance levels across data. In this paper, we propose a novel approach, Variational Partial G-CNN (VP G-CNN), to capture varying levels of partial equivariance specific to each data instance. VP G-CNN redesigns the distribution of the output group elements to be conditioned on input data, leveraging variational inference to avoid overfitting. This enables the model to adjust its equivariance levels according to the needs of individual data points. Additionally, we address training instability inherent in discrete group equivariance models by redesigning the reparametrizable distribution. We demonstrate the effectiveness of VP G-CNN on both toy and real-world datasets, including MNIST67-180, CIFAR10, ColorMNIST, and Flowers102. Our results show robust performance, even in uncertainty metrics.",,0,0.0,,,KAIST,2022R1A5A7083908,Korea Advanced Institute of Science and Technology
2-s2.0-105000526649,,,,VeXKD: The Versatile Integration of Cross-Modal Fusion and Knowledge Distillation for 3D Perception,cp,Conference Paper,Ji Y.,60018308;60008592;60276981,Xi'an Jiaotong University;Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology (Guangzhou),Xi'an;Hong Kong;Guangzhou,China;Hong Kong;China,6.0,"Ji, Yuzhe;Chen, Yijie;Yang, Liuqing;Ding, Rui;Yang, Meng;Zheng, Xinhu",58864367400;58864056900;7406279582;59151417200;55703268600;57004047800,60276981;60276981;60276981-60008592;60018308;60018308;60276981-60008592,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Recent advancements in 3D perception have led to a proliferation of network architectures, particularly those involving multi-modal fusion algorithms. While these fusion algorithms improve accuracy, their complexity often impedes real-time performance. This paper introduces VeXKD, an effective and Versatile framework that integrates Cross-Modal Fusion with Knowledge Distillation. VeXKD applies knowledge distillation exclusively to the Bird's Eye View (BEV) feature maps, enabling the transfer of cross-modal insights to single-modal students without additional inference time overhead. It avoids volatile components that can vary across various 3D perception tasks and student modalities, thus improving versatility. The framework adopts a modality-general cross-modal fusion module to bridge the modality gap between the multi-modal teachers and single-modal students. Furthermore, leveraging byproducts generated during fusion, our BEV query guided mask generation network identifies crucial spatial locations across different BEV feature maps from different tasks and semantic levels in a data-driven manner, significantly enhancing the effectiveness of knowledge distillation. Extensive experiments on the nuScenes dataset demonstrate notable improvements, with up to 6.9%/4.2% increase in mAP and NDS for 3D detection tasks and up to 4.3% rise in mIoU for BEV map segmentation tasks, narrowing the performance gap with multi-modal models.",,2,0.0,,,NSFC,U23A20339,National Natural Science Foundation of China
2-s2.0-85170391492,10.24963/ijcai.2023/174,,,ViT-CX: Causal Explanation of Vision Transformers,cp,Conference Paper,Xie W.,60008592;60092530,"Hong Kong University of Science and Technology;Huawei Technologies Co., Ltd.",Hong Kong;Shenzhen,Hong Kong;China,4.0,"Xie, Weiyan;Li, Xiao Hui;Cao, Caleb Chen;Zhang, Nevin L.",57546752900;58612191700;55575117300;7401648059,60008592;60092530;60008592;60008592,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,1569-1577,"Despite the popularity of Vision Transformers (ViTs) and eXplainable AI (XAI), only a few explanation methods have been designed specially for ViTs thus far. They mostly use attention weights of the [CLS] token on patch embeddings and often produce unsatisfactory saliency maps. This paper proposes a novel method for explaining ViTs called ViT-CX. It is based on patch embeddings, rather than attentions paid to them, and their causal impacts on the model output. Other characteristics of ViTs such as causal overdetermination are also considered in the design of ViT-CX. The empirical results show that ViT-CX produces more meaningful saliency maps and does a better job revealing all important evidence for the predictions than previous methods. The explanation generated by ViT-CX also shows significantly better faithfulness to the model. The codes and appendix are available at https://github.com/vaynexie/CausalX-ViT.",,13,1.0,all publisherfullgold,All Open Access Gold,研究資助局,16204920,"Research Grants Council, University Grants Committee"
2-s2.0-85167986438,10.1609/aaai.v37i11.26613,,,VideoDubber: Machine Translation with Speech-Aware Length Control for Video Dubbing,cp,Conference Paper,Wu Y.,60014402;60098464;130081807;130082329,Renmin University of China;Microsoft Research Asia;Microsoft Azure Speech;Microsoft Azure Translation,Beijing;Beijing;;,China;China;;,10.0,"Wu, Yihan;Guo, Junliang;Tan, Xu;Zhang, Chen;Li, Bohan;Song, Ruihua;He, Lei;Zhao, Sheng;Menezes, Arul;Bian, Jiang",57639598900;57202367651;57204467223;59616590200;57222575618;59094457800;56413454100;57214168406;16022505400;57203105806,60014402;60098464;60098464;130081807;130081807;60014402;130081807;130081807;130082329;60098464,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,13772-13779,"Video dubbing aims to translate the original speech in a film or television program into the speech in a target language, which can be achieved with a cascaded system consisting of speech recognition, machine translation and speech synthesis. To ensure the translated speech to be well aligned with the corresponding video, the length/duration of the translated speech should be as close as possible to that of the original speech, which requires strict length control. Previous works usually control the number of words or characters generated by the machine translation model to be similar to the source sentence, without considering the isochronicity of speech as the speech duration of words/characters in different languages varies. In this paper, we propose VideoDubber, a machine translation system tailored for the task of video dubbing, which directly considers the speech duration of each token in translation, to match the length of source and target speech. Specifically, we control the speech length of generated sentence by guiding the prediction of each word with the duration information, including the speech duration of itself as well as how much duration is left for the remaining words. We design experiments on four language directions (German → English, Spanish → English, Chinese ↔ English), and the results show that VideoDubber achieves better length control ability on the generated speech than baseline methods. To make up the lack of real-world datasets, we also construct a real-world test set collected from films to provide comprehensive evaluations on the video dubbing task.",,14,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85203823459,,,,VideoPoet: A Large Language Model for Zero-Shot Video Generation,cp,Conference Paper,Kondratyuk D.,60027950;60006191,Carnegie Mellon University;Google LLC,Pittsburgh;Mountain View,United States;United States,31.0,"Kondratyuk, Dan;Yu, Lijun;Gu, Xiuye;Lezama, José;Huang, Jonathan;Schindler, Grant;Hornung, Rachel;Birodkar, Vighnesh;Yan, Jimmy;Chiu, Ming Chang;Somandepalli, Krishna;Akbari, Hassan;Alon, Yair;Cheng, Yong;Dillon, Josh;Gupta, Agrim;Hahn, Meera;Hauth, Anja;Hendon, David;Martinez, Alonso;Minnen, David;Sirotenko, Mikhail;Sohn, Kihyuk;Yang, Xuan;Adam, Hartwig;Yang, Ming Hsuan;Essa, Irfan;Wang, Huisheng;Ross, David A.;Seybold, Bryan;Jiang, Lu",57215714977;57216513773;57223753777;56422017700;50761096800;59001020000;58310055800;56045607500;59000927700;57215135814;56845601900;57204047284;57343024400;57155447900;57204805774;57200614112;57219478868;57219228502;58202423600;57210729395;57210588272;57219628579;55268296200;59001019900;27867442600;7404927015;6701806882;57961766700;56862626300;57204292444;55533967700,60006191;60006191-60027950;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191;60006191-60027950,2024-01-01,2024,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,235,,,25105-25124,"We present VideoPoet, a model for synthesizing high-quality videos from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs - including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that is adapted to a range of video generation tasks. We present results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting the generation of high-fidelity motions. Project page: https://sites.research.google/videopoet/.",,12,0.0,,,,,
2-s2.0-105000530803,,,,Visual Anchors Are Strong Information Aggregators For Multimodal Large Language Model,cp,Conference Paper,Liu H.,60027363;60018486;60159665,University of Chinese Academy of Sciences;Institute of Automation Chinese Academy of Sciences;ByteDance Ltd.,Beijing;Beijing;Beijing,China;China;China,7.0,"Liu, Haogeng;You, Quanzeng;Han, Xiaotian;Liu, Yongfei;Huang, Huaibo;He, Ran;Yang, Hongxia",58067078400;55919315100;58739217400;57215782324;57200614611;35764463900;57054215300,60018486-60027363;60159665;60159665;60159665;60018486-60027363;60018486-60027363;,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"In the realm of Multimodal Large Language Models (MLLMs), vision-language connector plays a crucial role to link the pre-trained vision encoders with Large Language Models (LLMs). Despite its importance, the vision-language connector has been relatively less explored. In this study, we aim to propose a strong vision-language connector that enables MLLMs to achieve high accuracy while maintain low computation cost. We first reveal the existence of the visual anchors in Vision Transformer and propose a cost-effective search algorithm to extract them. Building on these findings, we introduce the Anchor Former (AcFormer), a novel vision-language connector designed to leverage the rich prior knowledge obtained from these visual anchors during pretraining, guiding the aggregation of information. Through extensive experimentation, we demonstrate that the proposed method significantly reduces computational costs by nearly two-thirds compared with baseline, while simultaneously outperforming baseline methods. This highlights the effectiveness and efficiency of AcFormer. Codes are available at https://github.com/liuhaogeng/Anchor-Former.",,0,0.0,,,NSFC,U21B2045,National Natural Science Foundation of China
2-s2.0-105000560097,,,,Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction,cp,Conference Paper,Tian K.,60014966;60159665,Peking University;ByteDance Ltd.,Beijing;Beijing,China;China,5.0,"Tian, Keyu;Jiang, Yi;Yuan, Zehuan;Peng, Bingyue;Wang, Liwei",57219736078;57221050709;55322757000;59006988800;55721280000,60014966-60159665;60159665;60159665;60159665;60014966,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine “next-scale prediction” or “next-resolution prediction”, diverging from the standard raster-scan “next-token prediction”. This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and can generalize well: VAR, for the first time, makes GPT-style AR models surpass diffusion transformers in image generation. On ImageNet 256×256 benchmark, VAR significantly improve AR baseline by improving Fréchet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with 20× faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near −0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.",,76,0.0,,,NSFC,NSFC92470123,National Natural Science Foundation of China
2-s2.0-85137926831,10.24963/ijcai.2022/241,,,Visual Similarity Attention,cp,Conference Paper,Zheng M.,60025534;124500569,Rensselaer Polytechnic Institute;United Imaging Intelligence,Troy;Cambridge,United States;United States,5.0,"Zheng, Meng;Karanam, Srikrishna;Chen, Terrence;Radke, Richard J.;Wu, Ziyan",57200295352;38561629100;35207990900;7003281461;55500641100,124500569;124500569;124500569;60025534;124500569,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,1728-1735,"While there has been substantial progress in learning suitable distance metrics, these techniques in general lack transparency and decision reasoning, i.e., explaining why the input set of images is similar or dissimilar. In this work, we solve this key problem by proposing the first method to generate generic visual similarity explanations with gradient-based attention. We demonstrate that our technique is agnostic to the specific similarity model type, e.g., we show applicability to Siamese, triplet, and quadruplet models. Furthermore, we make our proposed similarity attention a principled part of the learning process, resulting in a new paradigm for learning similarity functions. We demonstrate that our learning mechanism results in more generalizable, as well as explainable, similarity models. Finally, we demonstrate the generality of our framework by means of experiments on a variety of tasks, including image retrieval, person re-identification, and low-shot semantic segmentation.",,0,1.0,all publisherfree2read,All Open Access Bronze,DHS,18STEXP00001-03-02,U.S. Department of Homeland Security
2-s2.0-85184080783,10.1613/jair.1.15185,,,"Visually Grounded Language Learning: a Review of Language Games, Datasets, Tasks, and Models",re,Review,Suglia A.,60019656,Heriot-Watt University,Edinburgh,United Kingdom,3.0,"Suglia, Alessandro;Konstas, Ioannis;Lemon, Oliver",57191042268;35229595400;6603015795,60019656;60019656;60019656,2024-01-01,2024,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,79,,,173-239,"In recent years, several machine learning models have been proposed. They are trained with a language modelling objective on large-scale text-only data. With such pretraining, they can achieve impressive results on many Natural Language Understanding and Generation tasks. However, many facets of meaning cannot be learned by “listening to the radio” only. In the literature, many Vision+Language (V+L) tasks have been defined with the aim of creating models that can ground symbols in the visual modality. In this work, we provide a systematic literature review of several tasks and models proposed in the V+L field. We rely on Wittgenstein’s idea of ‘language games’ to categorise such tasks into 3 different families: 1) discriminative games, 2) generative games, and 3) interactive games. Our analysis of the literature provides evidence that future work should be focusing on interactive games where communication in Natural Language is important to resolve ambiguities about object referents and action plans and that physical embodiment is essential to understand the semantics of situations and events. Overall, these represent key requirements for developing grounded meanings in neural models.",,7,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85150338333,,,,WHAT HAPPENS AFTER SGD REACHES ZERO LOSS? -A MATHEMATICAL FRAMEWORK,cp,Conference Paper,Li Z.,60005455;60141284,Yale University;School of Engineering and Applied Science,New Haven;Princeton,United States;United States,3.0,"Li, Zhiyuan;Wang, Tianhao;Arora, Sanjeev",57219471508;57221138247;7202419160,60141284;60005455;60141284,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function L can form a manifold. Intuitively, with a sufficiently small learning rate η, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the gradient noise prevents further convergence. In such regime, Blanc et al. (2020) proved that SGD with label noise locally decreases a regularizer-like term, the sharpness of loss, tr[∇<sup>2</sup>L]. The current paper gives a general framework for such analysis by adapting ideas from Katzenberger (1991). It allows in principle a complete characterization for the regularization effect of SGD around such manifold-i.e., the”implicit bias”-using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance. This yields some new results: (1) a global analysis of the implicit bias valid for η<sup>−2</sup> steps, in contrast to the local analysis of Blanc et al. (2020) that is only valid for η<sup>−1.6</sup> steps and (2) allowing arbitrary noise covariance. As an application, we show with arbitrary large initialization, label noise SGD can always escape the kernel regime and only requires O(κ ln d) samples for learning an κ-sparse overparametrized linear model in R<sup>d</sup> (Woodworth et al., 2020), while GD initialized in the kernel regime requires Ω(d) samples. This upper bound is minimax optimal and improves the previous O<sup>e</sup>(κ<sup>2</sup>) upper bound (HaoChen et al., 2020).",,36,0.0,,,NSF,,National Science Foundation
2-s2.0-85199923506,,,,WORDS ARE ALL YOU NEED? LANGUAGE AS AN APPROXIMATION FOR HUMAN SIMILARITY JUDGMENTS,cp,Conference Paper,Marjieh R.,60003269;60141284;60104956;129286616,Princeton University;School of Engineering and Applied Science;Max Planck Institute for Empirical Aesthetics;Max Planck Institute for Cognitive and Brain Sciences,Princeton;Princeton;Frankfurt am Main;,United States;United States;Germany;Germany,7.0,"Marjieh, Raja;van Rijn, Pol;Sucholutsky, Ilia;Sumers, Theodore R.;Lee, Harin;Griffiths, Thomas L.;Jacoby, Nori",59667007200;57221323490;57200529787;57221152458;57226892883;57222226477;54880377100,60003269;60104956;60141284;60141284;60104956-129286616;60003269-60141284;60104956,2023-01-01,2023,11th International Conference on Learning Representations Iclr 2023,,21101239717.0,,Conference Proceeding,,,,,"Human similarity judgments are a powerful supervision signal for machine learning applications based on techniques such as contrastive learning, information retrieval, and model alignment, but classical methods for collecting human similarity judgments are too expensive to be used at scale. Recent methods propose using pre-trained deep neural networks (DNNs) to approximate human similarity, but pre-trained DNNs may not be available for certain domains (e.g., medical images, low-resource languages) and their performance in approximating human similarity has not been extensively tested. We conducted an evaluation of 611 pre-trained models across three domains - images, audio, video - and found that there is a large gap in performance between human similarity judgments and pre-trained DNNs. To address this gap, we propose a new class of similarity approximation methods based on language. To collect the language data required by these new methods, we also developed and validated a novel adaptive tag collection pipeline. We find that our proposed language-based methods are significantly cheaper, in the number of human judgments, than classical methods, but still improve performance over the DNN-based methods. Finally, we also develop 'stacked' methods that combine language embeddings with DNN embeddings, and find that these consistently provide the best approximations for human similarity across all three of our modalities. Based on the results of this comprehensive study, we provide a concise guide for researchers interested in collecting or approximating human similarity data. To accompany this guide, we also release all of the similarity and language data, a total of 206,339 human judgments, that we collected in our experiments, along with a detailed breakdown of all modeling results.",,7,0.0,,,JTF,567554-2022,John Templeton Foundation
2-s2.0-85167995311,10.1609/aaai.v37i12.26667,,,"Walkability Optimization: Formulations, Algorithms, and a Case Study of Toronto",cp,Conference Paper,Huang W.,60016849,University of Toronto,Toronto,Canada,2.0,"Huang, Weimin;Khalil, Elias B.",57219491260;58904984100,60016849;60016849,2023-06-27,27 June 2023,Proceedings of the 37th Aaai Conference on Artificial Intelligence Aaai 2023,,21101170618.0,,Conference Proceeding,37,,,14249-14258,"The concept of walkable urban development has gained increased attention due to its public health, economic, and environmental sustainability benefits. Unfortunately, land zoning and historic under-investment have resulted in spatial inequality in walkability and social inequality among residents. We tackle the problem of Walkability Optimization through the lens of combinatorial optimization. The task is to select locations in which additional amenities (e.g., grocery stores, schools, restaurants) can be allocated to improve resident access via walking while taking into account existing amenities and providing multiple options (e.g., for restaurants). To this end, we derive Mixed-Integer Linear Programming (MILP) and Constraint Programming (CP) models. Moreover, we show that the problem’s objective function is submodular in special cases, which motivates an efficient greedy heuristic. We conduct a case study on 31 underserved neighborhoods in the City of Toronto, Canada. MILP finds the best solutions in most scenarios but does not scale well with network size. The greedy algorithm scales well and finds high-quality solutions. Our empirical evaluation shows that neighbourhoods with low walkability have a great potential for transformation into pedestrian-friendly neighbourhoods by strategically placing new amenities. Allocating 3 additional grocery stores, schools, and restaurants can improve the “WalkScore” by more than 50 points (on a scale of 100) for 4 neighbourhoods and reduce the walking distances to amenities for 75% of all residential locations to 10 minutes for all amenity types. Our code and paper appendix are available at https://github.com/khalil-research/walkability.",,4,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85141276077,10.1609/aaai.v36i3.20263,,,Weakly Supervised Video Moment Localization with Contrastive Negative Sample Mining,cp,Conference Paper,Zheng M.,60014966;125545708,Peking University;Beijing Institute for General Artificial Intelligence,Beijing;Beijing,China;China,4.0,"Zheng, Minghang;Huang, Yanjie;Chen, Qingchao;Liu, Yang",57221320427;57962087100;56939438700;57192561393,60014966;60014966;60014966;60014966-125545708,2022-06-30,30 June 2022,Proceedings of the 36th Aaai Conference on Artificial Intelligence Aaai 2022,,21101133594.0,,Conference Proceeding,36,,,3517-3525,"Video moment localization aims at localizing the video segments which are most related to the given free-form natural language query. The weakly supervised setting, where only video level description is available during training, is getting more and more attention due to its lower annotation cost. Prior weakly supervised methods mainly use sliding windows to generate temporal proposals, which are independent of video content and low quality, and train the model to distinguish matched video-query pairs and unmatched ones collected from different videos, while neglecting what the model needs is to distinguish the unaligned segments within the video. In this work, we propose a novel weakly supervised solution by introducing Contrastive Negative sample Mining (CNM). Specifically, we use a learnable Gaussian mask to generate positive samples, highlighting the video frames most related to the query, and consider other frames of the video and the whole video as easy and hard negative samples respectively. We then train our network with the Intra-Video Contrastive loss to make our positive and negative samples more discriminative. Our method has two advantages: (1) Our proposal generation process with a learnable Gaussian mask is more efficient and makes our positive sample higher quality. (2) The more difficult intra-video negative samples enable our model to distinguish highly confusing scenes. Experiments on two datasets show the effectiveness of our method. Code can be found at https://github.com/minghangz/cnm.",,91,1.0,all publisherfullgold,All Open Access Gold,,2022NB0AB05,
2-s2.0-85174420024,,,,Weighted Flow Diffusion for Local Graph Clustering with Node Attributes: an Algorithm and Statistical Guarantees,cp,Conference Paper,Yang S.,60000463,David R. Cheriton School of Computer Science,Waterloo,Canada,2.0,"Yang, Shenghao;Fountoulakis, Kimon",57219684790;57200679869,60000463;60000463,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,39252-39276,"Local graph clustering methods aim to detect small clusters in very large graphs without the need to process the whole graph. They are fundamental and scalable tools for a wide range of tasks such as local community detection, node ranking and node embedding. While prior work on local graph clustering mainly focuses on graphs without node attributes, modern real-world graph datasets typically come with node attributes that provide valuable additional information. We present a simple local graph clustering algorithm for graphs with node attributes, based on the idea of diffusing mass locally in the graph while accounting for both structural and attribute proximities. Using high-dimensional concentration results, we provide statistical guarantees on the performance of the algorithm for the recovery of a target cluster with a single seed node. We give conditions under which a target cluster generated from a fairly general contextual random graph model, which includes both the stochastic block model and the planted cluster model as special cases, can be fully recovered with bounded false positives. Empirically, we validate all theoretical claims using synthetic data, and we show that incorporating node attributes leads to superior local clustering performances using real-world graph datasets.",,2,0.0,,,NSERC,DGECR-2019-00147,Natural Sciences and Engineering Research Council of Canada
2-s2.0-105000486929,,,,What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration,cp,Conference Paper,Qin L.,60025278;60017060;60019616;60159665;126648402,Tsinghua University;Central South University;Harbin Institute of Technology;ByteDance Ltd.;Research Center for Social Computing and Information Retrieval,Beijing;Changsha;Harbin;Beijing;Harbin,China;China;China;China;China,6.0,"Qin, Libo;Chen, Qiguang;Fei, Hao;Chen, Zhi;Li, Min;Che, Wanxiang",57204392284;57289894300;57207467498;59283096900;56908226400;57204332760,60017060;126648402-60019616;60025278;60159665;60017060;126648402-60019616,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Recently, rapid advancements in Multi-Modal In-Context Learning (MM-ICL) have achieved notable success, which is capable of achieving superior performance across various tasks without requiring additional parameter tuning. However, the underlying rules for the effectiveness of MM-ICL remain under-explored. To fill this gap, this work aims to investigate the research question: “What factors affect the performance of MM-ICL?” To this end, we investigate extensive experiments on the three core steps of MM-ICL including demonstration retrieval, demonstration ordering, and prompt construction using 6 vision large language models and 20 strategies. Our findings highlight (1) the necessity of a multi-modal retriever for demonstration retrieval, (2) the importance of intra-demonstration ordering over inter-demonstration ordering, and (3) the enhancement of task comprehension through introductory instructions in prompts. We hope this study can serve as a foundational guide for optimizing MM-ICL strategies in future research.",,5,0.0,,,NSFC,2024RC3024,Science and Technology Program of Hunan Province
2-s2.0-85137864158,10.24963/ijcai.2022/21,,,When Votes Change and Committees Should (Not),cp,Conference Paper,Bredereck R.,60000762;60011604;60017351;60021841,Humboldt-Universität zu Berlin;Technische Universität Berlin;AGH University of Krakow;Technische Universität Clausthal,Berlin;Berlin;Krakow;Clausthal-Zellerfeld,Germany;Germany;Poland;Germany,3.0,"Bredereck, Robert;Fluschnik, Till;Kaczmarczyk, Andrzej",36720017100;57120199200;57193490864,60021841-60000762;60011604;60011604-60017351,2022-01-01,2022,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,,,,144-150,"Electing a single committee of a small size is a classical and well-understood voting situation. Being interested in a sequence of committees, we introduce two time-dependent multistage models based on simple scoring-based voting. Therein, we are given a sequence of voting profiles (stages) over the same set of agents and candidates, and our task is to find a small committee for each stage of high score. In the conservative model we additionally require that any two consecutive committees have a small symmetric difference. Analogously, in the revolutionary model we require large symmetric differences. We prove both models to be NP-hard even for a constant number of agents, and, based on this, initiate a parameterized complexity analysis for the most natural parameters and combinations thereof. Among other results, we prove both models to be in XP yet W[1]-hard regarding the number of stages, and that being revolutionary seems to be “easier” than being conservative.",,14,1.0,all publisherhybridgold,All Open Access Hybrid Gold,H2020,BR 5207/1,Horizon 2020 Framework Programme
2-s2.0-85174416049,,,,Why Target Networks Stabilise Temporal Difference Methods,cp,Conference Paper,Fellows M.,60026851,University of Oxford,Oxford,United Kingdom,3.0,"Fellows, Mattie;Smith, Matthew J.A.;Whiteson, Shimon",57203673934;58457574300;10240257400,60026851;60026851;60026851,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,9886-9909,"Integral to many recent successes in deep reinforcement learning has been a class of temporal difference methods that use infrequently updated target values for policy evaluation in a Markov Decision Process. At the same time, a complete theoretical explanation for the effectiveness of target networks remains elusive. In this work, we provide an analysis of this popular class of algorithms, to finally answer the question: “why do target networks stabilise TD learning”? To do so, we formalise the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad-the use of TD updates with (nonlinear) function approximation and off-policy data-which often leads to nonconvergent algorithms. This insight leads us to conclude that the use of target networks can mitigate the effects of poor conditioning in the Jacobian of the TD update. Furthermore, we show that under mild regularity conditions and a well tuned target network update frequency, convergence can be guaranteed even in the extremely challenging off-policy sampling and nonlinear function approximation setting.",,1,0.0,,,,,
2-s2.0-85185678650,,,,Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models,cp,Conference Paper,Gao T.,60004354,Iowa State University,Ames,United States,4.0,"Gao, Tianxiang;Huo, Xiaokai;Liu, Hailiang;Gao, Hongyang",57194206566;56019644000;26643434300;57201215455,60004354;60004354;60004354;60004354,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"Neural networks with wide layers have attracted significant attention due to their equivalence to Gaussian processes, enabling perfect fitting of training data while maintaining generalization performance, known as benign overfitting. However, existing results mainly focus on shallow or finite-depth networks, necessitating a comprehensive analysis of wide neural networks with infinite-depth layers, such as neural ordinary differential equations (ODEs) and deep equilibrium models (DEQs). In this paper, we specifically investigate the deep equilibrium model (DEQ), an infinite-depth neural network with shared weight matrices across layers. Our analysis reveals that as the width of DEQ layers approaches infinity, it converges to a Gaussian process, establishing what is known as the Neural Network and Gaussian Process (NNGP) correspondence. Remarkably, this convergence holds even when the limits of depth and width are interchanged, which is not observed in typical infinite-depth Multilayer Perceptron (MLP) networks. Furthermore, we demonstrate that the associated Gaussian vector remains non-degenerate for any pairwise distinct input data, ensuring a strictly positive smallest eigenvalue of the corresponding kernel matrix using the NNGP kernel. These findings serve as fundamental elements for studying the training and generalization of DEQs, laying the groundwork for future research in this area.",,8,0.0,,,NSF,DMS-1812666,National Science Foundation
2-s2.0-105000479372,,,,Wide Two-Layer Networks can Learn from Adversarial Perturbations,cp,Conference Paper,Kumano S.,60025272;60002787;60032824,The University of Tokyo;Chiba University;Zuse Institute Berlin,Tokyo;Chiba;Berlin,Japan;Japan;Germany,3.0,"Kumano, Soichiro;Kera, Hiroshi;Yamasaki, Toshihiko",57221688849;57185597700;7202317850,60025272;60002787-60032824;60025272,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Adversarial examples have raised several open questions, such as why they can deceive classifiers and transfer between different models. A prevailing hypothesis to explain these phenomena suggests that adversarial perturbations appear as random noise but contain class-specific features. This hypothesis is supported by the success of perturbation learning, where classifiers trained solely on adversarial examples and the corresponding incorrect labels generalize well to correctly labeled test data. Although this hypothesis and perturbation learning are effective in explaining intriguing properties of adversarial examples, their solid theoretical foundation is limited. In this study, we theoretically explain the counterintuitive success of perturbation learning. We assume wide two-layer networks and the results hold for any data distribution. We prove that adversarial perturbations contain sufficient class-specific features for networks to generalize from them. Moreover, the predictions of classifiers trained on mislabeled adversarial examples coincide with those of classifiers trained on correctly labeled clean samples. The code is available at https://github.com/s-kumano/perturbation-learning.",,0,0.0,,,JST,JP23KJ0789,Japan Science and Technology Agency
2-s2.0-85189545471,10.1609/aaai.v38i5.28254,,,X-RefSeg3D: Enhancing Referring 3D Instance Segmentation via Structured Cross-Modal Graph Neural Networks,cp,Conference Paper,Qian Z.,60018205,Xiamen University,Xiamen,China,4.0,"Qian, Zhipeng;Ma, Yiwei;Ji, Jiayi;Sun, Xiaoshuai",58972693900;57566612300;57218719159;57204332621,60018205;60018205;60018205;60018205,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,5.0,,4551-4559,"Referring 3D instance segmentation is a challenging task aimed at accurately segmenting a target instance within a 3D scene based on a given referring expression. However, previous methods have overlooked the distinct roles played by different words in referring expressions. Additionally, they have failed to incorporate the positional relationship within referring expressions with the spatial correlations in 3D scenes. To alleviate these issues, we present a novel model called XRefSeg3D, which constructs a cross-modal graph for the input 3D scene and unites textual and spatial relationships for reasoning via graph neural networks. Our approach begins by capturing object-specific text features, which are then fused with the instance features to construct a comprehensive crossmodal scene graph. Subsequently, we integrate the obtained cross-modal features into graph neural networks, leveraging the K-nearest algorithm to derive explicit instructions from expressions and factual relationships in scenes. This enables the effective capture of higher-order relationships among instances, thereby enhancing feature fusion and facilitating reasoning. Finally, the refined feature undergoes a matching module to compute the ultimate matching score. Experimental results on ScanRefer demonstrate the effectiveness of our method, surpassing previous approaches by a substantial margin of +3.67% in terms of mIOU.",,15,1.0,all publisherfullgold,All Open Access Gold,NSFC,2023M732948,China Postdoctoral Science Foundation
2-s2.0-85166232220,10.1613/jair.1.14863,,,Your College Dorm and Dormmates: Fair Resource Sharing with Externalities,ar,Article,Gan J.,60026851;60005455;60008928,University of Oxford;Yale University;The Hong Kong Polytechnic University,Oxford;New Haven;Hong Kong,United Kingdom;United States;Hong Kong,3.0,"Gan, Jiarui;Li, Bo;Li, Yingkai",56066627100;57189891507;57195338624,60026851;60008928;60005455,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,793-820,"We study a fair resource sharing problem, where a set of resources are to be shared among a group of agents. Each agent demands one resource and each resource can serve a limited number of agents. An agent cares about what resource they get as well as the externalities imposed by their mates, who share the same resource with them. Clearly, the strong notion of envy-freeness, where no agent envies another for their resource or mates, cannot always be achieved and we show that even deciding the existence of such a strongly envy-free assignment is an intractable problem. Hence, a more interesting question is whether (and in what situations) a relaxed notion of envy-freeness, the Pareto envyfreeness, can be achieved. Under this relaxed notion, an agent envies another only when they envy both the resource and the mates of the other agent. In particular, we are interested in a dorm assignment problem, where students are to be assigned to dorms with the same capacity and they have dichotomous preference over their dormmates. We show that when the capacity of each dorm is 2, a Pareto envy-free assignment always exists and we present a polynomial-time algorithm to compute such an assignment. Nevertheless, the result breaks immediately when the capacity increases to 3, in which case even Pareto envyfreeness cannot be guaranteed. In addition to the existential results, we also investigate the utility guarantees of (Pareto) envy-free assignments in our model.",,4,1.0,all publisherfullgold repository repositoryam,All Open Access Gold Green,NSFC,PolyU 25211321,National Natural Science Foundation of China
2-s2.0-85146373124,,,,Your Out-of-Distribution Detection Method is Not Robust!,cp,Conference Paper,Azizmalayeri M.,60027666,Sharif University of Technology,Tehran,Iran,6.0,"Azizmalayeri, Mohammad;Moakhar, Arshia Soltani;Zarei, Arman;Zohrabi, Reihaneh;Manzuri, Mohammad Taghi;Rohban, Mohammad Hossein",57222863785;57937519200;57938112500;57937667100;6506067964;27467671600,60027666;60027666;60027666;60027666;60027666;60027666,2022-01-01,2022,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,35,,,,"Out-of-distribution (OOD) detection has recently gained substantial attention due to the importance of identifying out-of-domain samples in reliability and safety. Although OOD detection methods have advanced by a great deal, they are still susceptible to adversarial examples, which is a violation of their purpose. To mitigate this issue, several defenses have recently been proposed. Nevertheless, these efforts remained ineffective, as their evaluations are based on either small perturbation sizes, or weak attacks. In this work, we re-examine these defenses against an end-to-end PGD attack on in/out data with larger perturbation sizes, e.g. up to commonly used ϵ = 8/255 for the CIFAR-10 dataset. Surprisingly, almost all of these defenses perform worse than a random detection under the adversarial setting. Next, we aim to provide a robust OOD detection method. In an ideal defense, the training should expose the model to almost all possible adversarial perturbations, which can be achieved through adversarial training. That is, such training perturbations should based on both in- and out-of-distribution samples. Therefore, unlike OOD detection in the standard setting, access to OOD, as well as in-distribution, samples sounds necessary in the adversarial training setup. These tips lead us to adopt generative OOD detection methods, such as OpenGAN, as a baseline. We subsequently propose the Adversarially Trained Discriminator (ATD), which utilizes a pre-trained robust model to extract robust features, and a generator model to create OOD samples. We noted that, for the sake of training stability, in the adversarial training of the discriminator, one should attack real in-distribution as well as real outliers, but not generated outliers. Using ATD with CIFAR-10 and CIFAR-100 as the in-distribution data, we could significantly outperform all previous methods in the robust AUROC while maintaining high standard AUROC and classification accuracy. The code repository is available at https://github.com/rohban-lab/ATD.",,17,0.0,,,,,
2-s2.0-85162100558,10.1613/jair.1.14157,,,Your Prompt is My Command: On Assessing the Human-Centred Generality of Multimodal Models,ar,Article,Schellaert W.,60031101;60027282;60022403;60011476;60112768;60085278,"University of Cambridge;Universidad Complutense de Madrid;Technion - Israel Institute of Technology;Universitat Politècnica de València;Cambridge Judge Business School;University of Toronto, Institute for the History and Philosophy of Science and Technology",Cambridge;Madrid;Haifa;Valencia;Cambridge;Toronto,United Kingdom;Spain;Israel;Spain;United Kingdom;Canada,10.0,"Schellaert, Wout;Martínez-Plumed, Fernando;Vold, Karina;Burden, John;Casares, Pablo A.M.;Loe, Bao Sheng;Reichart, Roi;Héigeartaigh, Sean;Korhonen, Anna;Hernández-Orallo, José",57819225300;36716636100;57203843248;57215671397;57218687993;57196032428;51665948700;44061738900;23987049200;6602454434,60011476;60011476;60085278;60031101;60027282;60112768;60022403;60031101;60031101;60011476,2023-01-01,2023,Journal of Artificial Intelligence Research,10769757.0,24330.0,,Journal,77,,,377-394,"Even with obvious deficiencies, large prompt-commanded multimodal models are proving to be flexible cognitive tools representing an unprecedented generality. But the directness, diversity, and degree of user interaction create a distinctive ""human-centred generality""(HCG), rather than a fully autonomous one. HCG implies that - for a specific user - a system is only as general as it is effective for the user's relevant tasks and their prevalent ways of prompting. A human-centred evaluation of general-purpose AI systems therefore needs to reflect the personal nature of interaction, tasks and cognition. We argue that the best way to understand these systems is as highly-coupled cognitive extenders, and to analyse the bidirectional cognitive adaptations between them and humans. In this paper, we give a formulation of HCG, as well as a high-level overview of the elements and trade-offs involved in the prompting process. We end the paper by outlining some essential research questions and suggestions for improving evaluation practices, which we envision as characteristic for the evaluation of general artificial intelligence in the future.",,17,1.0,all publisherfullgold,All Open Access Gold,EC,PROMETEO/2019/098,European Commission
2-s2.0-85190481253,,,,Your representations are in the network: composable and parallel adaptation for large scale models,cp,Conference Paper,Dukler Y.,131185496,AWS AI Labs,,,10.0,"Dukler, Yonatan;Achille, Alessandro;Yang, Hao;Bowman, Benjamin;Vivek, Varsha;Zancato, Luca;Ravichandran, Avinash;Fowlkes, Charless;Swaminathan, Ashwin;Soatto, Stefano",57214896876;57200414453;57881346100;57421044300;58150682900;57219797373;8732051900;6701549596;58146711400;7004080670,131185496;131185496;131185496;131185496;131185496;131185496;131185496;131185496;131185496;131185496,2023-01-01,2023,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,36,,,,"We present a framework for transfer learning that efficiently adapts a large base-model by learning lightweight cross-attention modules attached to its intermediate activations.We name our approach InCA (Introspective-Cross-Attention) and show that it can efficiently survey a network's representations and identify strong performing adapter models for a downstream task.During training, InCA enables training numerous adapters efficiently and in parallel, isolated from the frozen base model.On the ViT-L/16 architecture, our experiments show that a single adapter, 1.3% of the full model, is able to reach full fine-tuning accuracy on average across 11 challenging downstream classification tasks.Compared with other forms of parameter-efficient adaptation, the isolated nature of the InCA adaptation is computationally desirable for large-scale models.For instance, we adapt ViT-G/14 (1.8B+ parameters) quickly with 20+ adapters in parallel on a single V100 GPU (76% GPU memory reduction) and exhaustively identify its most useful representations.We further demonstrate how the adapters learned by InCA can be incrementally modified or combined for flexible learning scenarios and our approach achieves state of the art performance on the ImageNet-to-Sketch multi-task benchmark.",,2,0.0,,,,,
2-s2.0-85144173615,,,,ZEROFL: EFFICIENT ON-DEVICE TRAINING FOR FEDERATED LEARNING WITH LOCAL SPARSITY,cp,Conference Paper,Qiu X.,60026851;60031214;60119999,University of Oxford;Université d'Avignon et des Pays du Vaucluse;Department of Computer Science and Technology,Oxford;Avignon;Cambridge,United Kingdom;France;United Kingdom,6.0,"Qiu, Xinchi;Fernandez-Marques, Javier;Gusmao, Pedro P.B.;Gao, Yan;Parcollet, Titouan;Lane, Nicholas D.",57219740531;57039003200;53984166500;55516979100;57193704479;23135333200,60119999;60026851;60119999;60119999;60031214;60119999,2022-01-01,2022,Iclr 2022 10th International Conference on Learning Representations,,21101141642.0,,Conference Proceeding,,,,,"When the available hardware cannot meet the memory and compute requirements to efficiently train high performing machine learning models, a compromise in either the training quality or the model complexity is needed. In Federated Learning (FL), nodes are orders of magnitude more constrained than traditional server-grade hardware and are often battery powered, severely limiting the sophistication of models that can be trained under this paradigm. While most research has focused on designing better aggregation strategies to improve convergence rates and in alleviating the communication costs of FL, fewer efforts have been devoted to accelerating on-device training. Such stage, which repeats hundreds of times (i.e. every round) and can involve thousands of devices, accounts for the majority of the time required to train federated models and, the totality of the energy consumption at the client side. In this work, we present the first study on the unique aspects that arise when introducing sparsity at training time in FL workloads. We then propose ZeroFL, a framework that relies on highly sparse operations to accelerate on-device training. Models trained with ZeroFL and 95% sparsity achieve up to 2.3% higher accuracy compared to competitive baselines obtained from adapting a state-of-the-art sparse training framework to the FL setting.",,55,0.0,,,EPSRC,EP/M50659X/1,Engineering and Physical Sciences Research Council
2-s2.0-85188017475,10.1613/jair.1.14849,,,conDENSE: Conditional Density Estimation for Time Series Anomaly Detection.,ar,Article,Moore A.,60026851;60278934,University of Oxford;Huma Therapeutics Limited,Oxford;London,United Kingdom;United Kingdom,2.0,"Moore, Alex;Morelli, Davide",58826704600;16231472800,60278934;60026851,2024-01-01,2024,Daimon,11300507.0,21100204105.0,19894651.0,Journal,79,,,801-824,"In recent years deep learning methods, based on reconstruction errors, have facilitated huge improvements in unsupervised anomaly detection. These methods make the limiting assumption that the greater the distance between an observation and a prediction the lower the likelihood of that observation. In this paper we propose conDENSE, a novel anomaly detection algorithm, which does not use reconstruction errors but rather uses conditional density estimation in masked autoregressive flows. By directly estimating the likelihood of data, our model moves beyond approximating expected behaviour with a single point estimate, as is the case in reconstruction error models. We show how conditioning on a dense representation of the current trajectory, extracted from a variational autoencoder with a gated recurrent unit (GRU VAE), produces a model that is suitable for periodic datasets, while also improving performance on non-periodic datasets. Experiments on 31 time-series, including real-world anomaly detection benchmark datasets and synthetically generated data, show that the model can outperform state-of-the-art deep learning methods.",,2,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-105000499563,,,,dattri: A Library for Efficient Data Attribution,cp,Conference Paper,Deng J.,60025778;60000745,"University of Michigan, Ann Arbor;University of Illinois Urbana-Champaign",Ann Arbor;Urbana,United States;United States,10.0,"Deng, Junwei;Li, Ting Wei;Zhang, Shiyuan;Liu, Shixuan;Pan, Yijun;Huang, Hao;Wang, Xinhe;Hu, Pingbang;Zhang, Xingjian;Ma, Jiaqi W.",58784367200;58389783900;59293339700;59409493200;59409328800;59409328900;59357817600;59005810900;57222528105;57203396057,60000745;60000745;60000745;60025778;60025778;60000745;60025778;60000745;60025778;60000745,2024-01-01,2024,Advances in Neural Information Processing Systems,10495258.0,23669.0,,Conference Proceeding,37,,,,"Data attribution methods aim to quantify the influence of individual training samples on the prediction of artificial intelligence (AI) models. As training data plays an increasingly crucial role in the modern development of large-scale AI models, data attribution has found broad applications in improving AI performance and safety. However, despite a surge of new data attribution methods being developed recently, there lacks a comprehensive library that facilitates the development, benchmarking, and deployment of different data attribution methods. In this work, we introduce dattri, an open-source data attribution library that addresses the above needs. Specifically, dattri highlights three novel design features. Firstly, dattri proposes a unified and easy-to-use API, allowing users to integrate different data attribution methods into their PyTorch-based machine learning pipeline with a few lines of code changed. Secondly, dattri modularizes low-level utility functions that are commonly used in data attribution methods, such as Hessian-vector product, inverse-Hessian-vector product or random projection, making it easier for researchers to develop new data attribution methods. Thirdly, dattri provides a comprehensive benchmark framework with pre-trained models and ground truth annotations for a variety of benchmark settings, including generative AI settings. We have implemented a variety of state-of-the-art efficient data attribution methods that can be applied to large-scale neural network models, and will continuously update the library in the future. Using the developed dattri library, we are able to perform a comprehensive and fair benchmark analysis across a wide range of data attribution methods. The source code of dattri is available at https://github.com/TRAIS-Lab/dattri.",,1,0.0,,,NSF,2138307,National Science Foundation
2-s2.0-85174425135,,,,dugMatting: Decomposed-Uncertainty-Guided Matting,cp,Conference Paper,Wu J.,60016521;60019533;60025710;60004630;60004678;60088078,"Sichuan University;Tianjin University;Agency for Science, Technology and Research, Singapore;Fujian Agriculture and Forestry University;A-Star, Institute of High Performance Computing;Minjiang University",Chengdu;Tianjin;Singapore City;Fuzhou;Singapore City;Fuzhou,China;China;Singapore;China;Singapore;China,6.0,"Wu, Jiawei;Zhang, Changqing;Li, Zuoyong;Fu, Huazhu;Peng, Xi;Zhou, Joey Tianyi",57210108740;56313466500;57022472600;35317209500;55555975700;56335714100,60004630;60019533;60088078;60004678;60016521;60004678-60025710,2023-01-01,2023,Proceedings of Machine Learning Research,,21101153015.0,26403498.0,Conference Proceeding,202,,,37846-37859,"Cutting out an object and estimating its opacity mask, known as image matting, is a key task in image and video editing. Due to the highly ill-posed issue, additional inputs, typically user-defined trimaps or scribbles, are usually needed to reduce the uncertainty. Although effective, it is either time consuming or only suitable for experienced users who know where to place the strokes. In this work, we propose a decomposed-uncertainty-guided matting (dugMatting) algorithm, which explores the explicitly decomposed uncertainties to efficiently and effectively improve the results. Basing on the characteristic of these uncertainties, the epistemic uncertainty is reduced in the process of guiding interaction (which introduces prior knowledge), while the aleatoric uncertainty is reduced in modeling data distribution (which introduces statistics for both data and possible noise). The proposed matting framework relieves the requirement for users to determine the interaction areas by using simple and efficient labeling. Extensively quantitative and qualitative results validate that the proposed method significantly improves the original matting algorithms in terms of both efficiency and efficacy.",,2,0.0,,,A*STAR,61972187,"Agency for Science, Technology and Research"
2-s2.0-85131438217,,,,ktrain: A Low-Code Library for Augmented Machine Learning,ar,Article,Maiya A.S.,60022889,Institute for Defense Analyses,Alexandria,United States,1.0,"Maiya, Arun S.",26326445200,60022889,2022-01-01,2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,,,"We present ktrain, a low-code Python library that makes machine learning more ac- cessible and easier to apply. As a wrapper to TensorFlow and many other libraries (e.g., transformers, scikit-learn, stellargraph), it is designed to make sophis- ticated, state-of-the-art machine learning models simple to build, train, inspect, and apply by both beginners and experienced practitioners. Featuring modules that support text data (e.g., text classification, sequence tagging, open-domain question-answering), vision data (e.g., image classification), graph data (e.g., node classification, link prediction), and tabular data, ktrain presents a simple unified interface enabling one to quickly solve a wide range of tasks in as little as three or four ""commands"" or lines of code.",computer vision | graphs | low-code machine learning | nlp | tabular data,52,0.0,,,,,
2-s2.0-85170375754,10.24963/ijcai.2023/521,,,pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting,cp,Conference Paper,Zhou Y.,60021918;60121285,University of Virginia;Ant group,Charlottesville;Hangzhou,United States;China,6.0,"Zhou, Yunyi;Chu, Zhixuan;Ruan, Yijia;Jin, Ge;Huang, Yuchen;Li, Sheng",58315484100;57218845228;58314988700;57323633700;57861464800;55812663300,60121285;60121285;60121285;60121285;60121285;60021918,2023-01-01,2023,Ijcai International Joint Conference on Artificial Intelligence,10450823.0,19400157504.0,,Conference Proceeding,2023-August,,,4684-4692,"Various probabilistic time series forecasting models have sprung up and shown remarkably good performance. However, the choice of model highly relies on the characteristics of the input time series and the fixed distribution that the model is based on. Due to the fact that the probability distributions cannot be averaged over different models straightforwardly, the current time series model ensemble methods cannot be directly applied to improve the robustness and accuracy of forecasting. To address this issue, we propose pTSE, a multi-model distribution ensemble method for probabilistic forecasting based on Hidden Markov Model (HMM). pTSE only takes off-the-shelf outputs from member models without requiring further information about each model. Besides, we provide a complete theoretical analysis of pTSE to prove that the empirical distribution of time series subject to an HMM will converge to the stationary distribution almost surely. Experiments on benchmarks show the superiority of pTSE over all member models and competitive ensemble methods.",,8,1.0,all publisherfullgold,All Open Access Gold,,,
2-s2.0-85135990239,,,,tntorch: Tensor Network Learning with PyTorch,ar,Article,Usvyatsov M.,60025858;60109034,ETH Zürich;IE Universidad,Zurich;Segovia,Switzerland;Spain,3.0,"Usvyatsov, Mikhail;Ballester-Ripoll, Rafael;Schindler, Konrad",57199864440;55813449900;8557497200,60025858;60109034;60025858,2022-06-01,1 June 2022,Journal of Machine Learning Research,15324435.0,20969.0,15337928.0,Journal,23,,208,,"We present tntorch, a tensor learning framework that supports multiple decompositions (including Candecomp/Parafac, Tucker, and Tensor Train) under a unified interface. With our library, the user can learn and handle low-rank tensors with automatic differentiation, seamless GPU support, and the convenience of PyTorch’s API. Besides decomposition algorithms, tntorch implements differentiable tensor algebra, rank truncation, cross-approximation, batch processing, comprehensive tensor arithmetics, and more.",low-rank methods | multilinear algebra | pytorch | tensor decompositions,11,0.0,,,,,
2-s2.0-85200584096,,,,π2VEC: POLICY REPRESENTATION WITH SUCCESSOR FEATURES,cp,Conference Paper,Scarpellini G.,60102151;60111161,Istituto Italiano di Tecnologia;DeepMind Technologies Limited,Genoa;London,Italy;United Kingdom,6.0,"Scarpellini, Gianluca;Konyushkova, Ksenia;Fantacci, Claudio;Le Paine, Tom;Chen, Yutian;Denil, Misha",57223734491;55641057300;56009956300;57188643576;57220895088;36132695800,60102151;60111161;60111161;60111161;60111161;60111161,2024-01-01,2024,12th International Conference on Learning Representations Iclr 2024,,21101240864.0,,Conference Proceeding,,,,,"This paper introduces π2vec, a method for representing black box policies as comparable feature vectors. Our method combines the strengths of foundation models that serve as generic and powerful state representations and successor features that can model the future occurrence of the states for a policy. π2vec represents the behaviors of policies by capturing statistics of how the behavior evolves the features from a pretrained model, using a successor feature framework. We focus on the offline setting where both policies and their representations are trained on a fixed dataset of trajectories. Finally, we employ linear regression on π2vec vector representations to predict the performance of held out policies. The synergy of these techniques results in a method for efficient policy evaluation in resource constrained environments.",,0,0.0,,,,,
2-s2.0-85189612523,10.1609/aaai.v38i21.30381,,,“Allot?” Is “A Lot!” Towards Developing More Generalized Speech Recognition System for Accessible Communication,cp,Conference Paper,Bandodkar G.,60153736;60019740,College of Engineering;Kennesaw State University,Davis;Kennesaw,United States;United States,5.0,"Bandodkar, Grisha;Agarwal, Shyam;Sughosh, Athul Krishna;Singh, Sahilbir;Choi, Taeyeong",58151719900;58973443400;58973815400;58973443500;57201460075,60153736;60153736;60153736;60153736;60019740,2024-03-25,25 March 2024,Proceedings of the Aaai Conference on Artificial Intelligence,21595399.0,21101185504.0,23743468.0,Conference Proceeding,38,21.0,,23327-23334,"The proliferation of Automatic Speech Recognition (ASR) systems has revolutionized translation and transcription. However, challenges persist in ensuring inclusive communication for non-native English speakers. This study quantifies the gap between accented and native English speech using Wav2Vec 2.0, a state-of-the-art transformer model. Notably, we found that accented speech exhibits significantly higher word error rates of 30-50%, in contrast to native speakers' 2-8% (Baevski et al. 2020). Our exploration extends to leveraging accessible online datasets to highlight the potential of enhancing speech recognition by fine-tuning the Wav2Vec 2.0 model. Through experimentation and analysis, we highlight the challenges with training models on accented speech. By refining models and addressing data quality issues, our work presents a pipeline for future investigations aimed at developing an integrated system capable of effectively engaging with a broader range of individuals with diverse backgrounds. Accurate recognition of accented speech is a pivotal step toward democratizing AI-driven communication products.",,1,1.0,all publisherfullgold,All Open Access Gold,,,
